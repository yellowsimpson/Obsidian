{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1RkhlZLfPMS",
        "outputId": "6e846c08-3cf4-4a13-8dda-5596eaa6aaad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "oFZW2xvwDAjg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }"
      ],
      "metadata": {
        "id": "DpWS7TbcDCWh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the XLM-ProphetNet model and tokenizer\n",
        "model_name = 'microsoft/xprophetnet-large-wiki100-cased'\n",
        "model = XLMProphetNetForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = XLMProphetNetTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73HS0vLcDDo3",
        "outputId": "b2cca56d-6974-4c89-b960-b7e487c2493a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/xprophetnet-large-wiki100-cased were not used when initializing XLMProphetNetForCausalLM: ['prophetnet.encoder.layers.11.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.8.self_attn.key_proj.weight', 'prophetnet.encoder.layers.9.self_attn.query_proj.weight', 'prophetnet.encoder.layers.2.self_attn.query_proj.weight', 'prophetnet.encoder.layers.7.self_attn.out_proj.weight', 'prophetnet.encoder.layers.0.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.6.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.9.self_attn.key_proj.weight', 'prophetnet.encoder.layers.2.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.10.feed_forward.output.weight', 'prophetnet.encoder.layers.7.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.2.self_attn.value_proj.weight', 'prophetnet.encoder.layers.1.feed_forward.output.bias', 'prophetnet.encoder.layers.10.self_attn.value_proj.weight', 'prophetnet.encoder.layers.1.self_attn.value_proj.weight', 'prophetnet.encoder.layers.6.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.8.self_attn.query_proj.bias', 'prophetnet.encoder.layers.10.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.6.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.7.self_attn.query_proj.weight', 'prophetnet.encoder.layers.3.self_attn.key_proj.bias', 'prophetnet.encoder.layers.2.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.4.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.6.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.10.self_attn.out_proj.bias', 'prophetnet.encoder.position_embeddings.weight', 'prophetnet.encoder.layers.11.feed_forward.output.bias', 'prophetnet.encoder.layers.3.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.9.feed_forward.output.weight', 'prophetnet.encoder.layers.4.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.11.self_attn.out_proj.weight', 'prophetnet.encoder.layers.3.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.0.feed_forward.output.bias', 'prophetnet.encoder.layers.10.feed_forward.output.bias', 'prophetnet.encoder.layers.6.self_attn.out_proj.bias', 'prophetnet.encoder.layers.0.feed_forward.output.weight', 'prophetnet.encoder.layers.1.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.7.feed_forward.output.bias', 'prophetnet.encoder.layers.2.self_attn.key_proj.weight', 'prophetnet.encoder.layers.5.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.0.self_attn.key_proj.bias', 'prophetnet.encoder.layers.3.feed_forward.output.bias', 'prophetnet.encoder.layers.3.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.1.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.0.self_attn.key_proj.weight', 'prophetnet.encoder.layers.3.self_attn.out_proj.bias', 'prophetnet.encoder.layers.10.self_attn.query_proj.weight', 'prophetnet.encoder.layers.10.self_attn.key_proj.weight', 'prophetnet.encoder.layers.5.self_attn.key_proj.bias', 'prophetnet.encoder.layers.6.self_attn.out_proj.weight', 'prophetnet.encoder.layers.8.self_attn.key_proj.bias', 'prophetnet.encoder.layers.0.self_attn.value_proj.weight', 'prophetnet.encoder.layers.6.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.4.self_attn.query_proj.bias', 'prophetnet.encoder.layers.8.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.9.feed_forward.output.bias', 'prophetnet.encoder.layers.1.self_attn.key_proj.weight', 'prophetnet.encoder.layers.9.self_attn.out_proj.bias', 'prophetnet.encoder.layers.9.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.1.self_attn.query_proj.weight', 'prophetnet.encoder.layers.5.self_attn.out_proj.bias', 'prophetnet.encoder.layers.11.feed_forward.output.weight', 'prophetnet.encoder.layers.2.self_attn.value_proj.bias', 'prophetnet.encoder.layers.1.feed_forward.output.weight', 'prophetnet.encoder.layers.11.self_attn.value_proj.bias', 'prophetnet.encoder.layers.4.self_attn.query_proj.weight', 'prophetnet.encoder.layers.5.self_attn.query_proj.bias', 'prophetnet.encoder.layers.6.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.5.self_attn.value_proj.weight', 'prophetnet.encoder.layers.3.self_attn.query_proj.weight', 'prophetnet.encoder.layers.0.self_attn.out_proj.weight', 'prophetnet.encoder.layers.11.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.5.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.1.self_attn.query_proj.bias', 'prophetnet.encoder.layers.6.feed_forward.output.bias', 'prophetnet.encoder.layers.4.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.1.self_attn.value_proj.bias', 'prophetnet.encoder.layers.2.self_attn.out_proj.weight', 'prophetnet.encoder.layers.8.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.10.self_attn.key_proj.bias', 'prophetnet.encoder.layers.5.feed_forward.intermediate.bias', 'prophetnet.word_embeddings.weight', 'prophetnet.encoder.layers.6.self_attn.query_proj.weight', 'prophetnet.encoder.layers.9.self_attn.value_proj.bias', 'prophetnet.encoder.layers.4.self_attn.out_proj.bias', 'prophetnet.encoder.layers.10.self_attn.out_proj.weight', 'prophetnet.encoder.layers.7.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.4.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.0.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.2.feed_forward.output.bias', 'prophetnet.encoder.layers.3.self_attn.value_proj.bias', 'prophetnet.encoder.layers.4.self_attn.out_proj.weight', 'prophetnet.encoder.layers.3.feed_forward.output.weight', 'prophetnet.encoder.layers.9.self_attn.value_proj.weight', 'prophetnet.encoder.layers.10.self_attn.query_proj.bias', 'prophetnet.encoder.layers.8.self_attn.value_proj.weight', 'prophetnet.encoder.layers.8.self_attn.out_proj.weight', 'prophetnet.encoder.layers.7.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.9.self_attn.key_proj.bias', 'prophetnet.encoder.layers.4.self_attn.value_proj.bias', 'prophetnet.encoder.layers.9.self_attn.out_proj.weight', 'prophetnet.encoder.layers.3.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.4.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.7.feed_forward.output.weight', 'prophetnet.encoder.layers.8.self_attn.out_proj.bias', 'prophetnet.encoder.layers.8.self_attn.value_proj.bias', 'prophetnet.encoder.layers.2.self_attn.key_proj.bias', 'prophetnet.encoder.layers.7.self_attn.key_proj.bias', 'prophetnet.encoder.layers.11.self_attn.key_proj.bias', 'prophetnet.encoder.layers.4.self_attn.key_proj.bias', 'prophetnet.encoder.layers.10.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.3.self_attn.query_proj.bias', 'prophetnet.encoder.layers.7.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.3.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.9.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.3.self_attn.key_proj.weight', 'prophetnet.encoder.layers.10.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.3.self_attn.value_proj.weight', 'prophetnet.encoder.layers.5.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.11.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.9.self_attn.query_proj.bias', 'prophetnet.encoder.layers.11.self_attn.key_proj.weight', 'prophetnet.encoder.layers.7.self_attn.value_proj.weight', 'prophetnet.encoder.layers.5.self_attn.key_proj.weight', 'prophetnet.encoder.layers.0.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.2.self_attn.query_proj.bias', 'prophetnet.encoder.word_embeddings.weight', 'prophetnet.encoder.layers.6.self_attn.query_proj.bias', 'prophetnet.encoder.layers.0.self_attn.query_proj.weight', 'prophetnet.encoder.layers.9.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.11.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.5.self_attn.out_proj.weight', 'prophetnet.encoder.layers.2.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.11.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.8.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.2.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.11.self_attn.query_proj.weight', 'prophetnet.encoder.layers.9.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.8.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.11.self_attn.query_proj.bias', 'prophetnet.encoder.layers.1.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.4.self_attn.value_proj.weight', 'prophetnet.encoder.layers.0.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.10.self_attn.value_proj.bias', 'prophetnet.encoder.layers.1.self_attn.key_proj.bias', 'prophetnet.encoder.layers.6.feed_forward.output.weight', 'prophetnet.encoder.layers.5.self_attn.query_proj.weight', 'prophetnet.encoder.layers.0.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.2.feed_forward.output.weight', 'prophetnet.encoder.layers.10.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.3.self_attn.out_proj.weight', 'prophetnet.encoder.layers.8.feed_forward.output.bias', 'prophetnet.encoder.layers.7.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.9.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.6.self_attn.value_proj.bias', 'prophetnet.encoder.layers.1.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.6.self_attn.key_proj.weight', 'prophetnet.encoder.layers.10.feed_forward_layer_norm.bias', 'prophetnet.encoder.layers.4.feed_forward.output.weight', 'prophetnet.encoder.layers.6.self_attn.key_proj.bias', 'prophetnet.encoder.layers.1.self_attn.out_proj.weight', 'prophetnet.encoder.layers.5.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.4.feed_forward.output.bias', 'prophetnet.encoder.layers.8.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.1.feed_forward.intermediate.weight', 'prophetnet.encoder.layers.7.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.4.self_attn.key_proj.weight', 'prophetnet.encoder.layers.6.self_attn.value_proj.weight', 'prophetnet.encoder.layers.3.feed_forward.intermediate.weight', 'prophetnet.encoder.embeddings_layer_norm.weight', 'prophetnet.encoder.layers.5.self_attn.value_proj.bias', 'prophetnet.encoder.layers.1.feed_forward_layer_norm.weight', 'prophetnet.encoder.layers.0.self_attn.value_proj.bias', 'prophetnet.encoder.layers.5.feed_forward.output.bias', 'prophetnet.encoder.embeddings_layer_norm.bias', 'prophetnet.encoder.layers.2.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.2.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.8.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.5.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.7.self_attn.out_proj.bias', 'prophetnet.encoder.layers.5.feed_forward.output.weight', 'prophetnet.encoder.layers.8.self_attn.query_proj.weight', 'prophetnet.encoder.layers.11.self_attn.value_proj.weight', 'prophetnet.encoder.layers.2.self_attn.out_proj.bias', 'prophetnet.encoder.layers.7.self_attn.query_proj.bias', 'prophetnet.encoder.layers.8.feed_forward.output.weight', 'prophetnet.encoder.layers.4.self_attn_layer_norm.bias', 'prophetnet.encoder.layers.7.self_attn.key_proj.weight', 'prophetnet.encoder.layers.11.self_attn.out_proj.bias', 'prophetnet.encoder.layers.10.self_attn_layer_norm.weight', 'prophetnet.encoder.layers.0.self_attn.query_proj.bias', 'prophetnet.encoder.layers.7.self_attn.value_proj.bias', 'prophetnet.encoder.layers.0.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.1.self_attn.out_proj.bias', 'prophetnet.encoder.layers.0.self_attn.out_proj.bias', 'prophetnet.encoder.layers.11.feed_forward.intermediate.bias', 'prophetnet.encoder.layers.9.feed_forward.intermediate.bias']\n",
            "- This IS expected if you are initializing XLMProphetNetForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMProphetNetForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-gJ5F3iDE2s",
        "outputId": "de435960-00a5-4088-b388-4227def7270d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMProphetNetForCausalLM(\n",
              "  (prophetnet): XLMProphetNetDecoderWrapper(\n",
              "    (decoder): XLMProphetNetDecoder(\n",
              "      (word_embeddings): Embedding(250012, 1024, padding_idx=0)\n",
              "      (position_embeddings): XLMProphetNetPositionalEmbeddings(512, 1024, padding_idx=0)\n",
              "      (ngram_embeddings): Embedding(2, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x XLMProphetNetDecoderLayer(\n",
              "          (self_attn): XLMProphetNetNgramSelfAttention(\n",
              "            (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (relative_pos_embeddings): Linear(in_features=1024, out_features=512, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn): XLMProphetNetAttention(\n",
              "            (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (cross_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): XLMProphetNetFeedForward(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (intermediate): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (output): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "          (feed_forward_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (embeddings_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=250012, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "batch_size = 1\n",
        "max_length = 128\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5"
      ],
      "metadata": {
        "id": "Uz51hqUKDGHP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:10]')  # Load the first 100 examples from the WikiText-2 dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSCHblOmDHvd",
        "outputId": "21b9d380-e908-40c6-9fcc-0bfd266d87d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset wikitext (/root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SABHO_czEyyG",
        "outputId": "e851284e-a9d4-45b1-b077-c6ad490e4416"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract texts from the dataset\n",
        "texts = dataset['text']"
      ],
      "metadata": {
        "id": "ZbKpHf5NDJp0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the custom dataset\n",
        "custom_dataset = CustomDataset(texts, tokenizer, max_length)\n",
        "\n",
        "# Create the data loader\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ZwfEJOQrDLjK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
      ],
      "metadata": {
        "id": "fafRX6VYDNOL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dks5gO3395FU",
        "outputId": "32b30cbe-3579-4761-f424-bb87dcead25a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 11.2526\n",
            "Epoch 2/3, Loss: 8.2574\n",
            "Epoch 3/3, Loss: 6.6093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "model.eval()\n",
        "example_prompt = 'Once upon a time'\n",
        "input_ids = tokenizer.encode(example_prompt, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(input_ids, max_length=50)\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    print(f'Example Prompt: {example_prompt}')\n",
        "    print(f'Generated Text: {generated_text}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3If5zGT952i",
        "outputId": "e47d73bf-8cc0-4cdf-a139-7b6b05897c69"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Prompt: Once upon a time\n",
            "Generated Text: Once upon a time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pNY05fLBDU2i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}