{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kh-vHz4bXhbx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, LayerNormalization\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dropout, LayerNormalization\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, max_sequence_length, num_layers=12, num_heads=8, d_model=512, dff=1024, dropout_rate=0.1):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = self.positional_encoding(max_sequence_length, d_model)\n",
        "        \n",
        "        self.encoder_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        \n",
        "        self.final_layer = Dense(vocab_size)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        sequence_length = tf.shape(inputs)[1]\n",
        "        mask = self.create_padding_mask(inputs)\n",
        "        \n",
        "        # Embedding and positional encoding\n",
        "        x = self.embedding(inputs)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.positional_encoding[:, :sequence_length, :]\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Encoder layers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoder_layers[i](x, mask)\n",
        "        \n",
        "        output = self.final_layer(x)\n",
        "        return output\n",
        "    \n",
        "    def positional_encoding(self, sequence_length, d_model):\n",
        "        position = tf.expand_dims(tf.range(0, sequence_length, dtype=tf.float32), axis=1)\n",
        "        div_term = tf.pow(10000, 2 * tf.range(0, d_model, 2, dtype=tf.float32) / d_model)\n",
        "        pe = tf.concat([tf.sin(position / div_term), tf.cos(position / div_term)], axis=1)\n",
        "        return tf.expand_dims(pe, axis=0)\n",
        "    \n",
        "    def create_padding_mask(self, sequence):\n",
        "        mask = tf.cast(tf.math.equal(sequence, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "8azouPxHXorL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.dropout1 = Dropout(dropout_rate)\n",
        "        self.ln1 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.ffn = self.point_wise_feed_forward_network(d_model, dff)\n",
        "        self.dropout2 = Dropout(dropout_rate)\n",
        "        self.ln2 = LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, mask):\n",
        "        attention_output = self.mha(inputs, inputs, attention_mask=mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        output1 = self.ln1(inputs + attention_output)\n",
        "        \n",
        "        ffn_output = self.ffn(output1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        output2 = self.ln2(output1 + ffn_output)\n",
        "        \n",
        "        return output2\n",
        "    \n",
        "    def point_wise_feed_forward_network(self, d_model, dff):\n",
        "        return tf.keras.Sequential([\n",
        "            Dense(dff, activation='relu'),\n",
        "            Dense(d_model)\n",
        "        ])"
      ],
      "metadata": {
        "id": "Tn9aSzEjXldt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (continued):\n",
        "\n",
        "# Define hyperparameters\n",
        "num_layers = 12\n",
        "num_heads = 8\n",
        "d_model = 512\n",
        "dff = 1024\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create an instance of the GPT-2 model\n",
        "gpt2 = GPT2(vocab_size, max_sequence_length, num_layers, num_heads, d_model, dff, dropout_rate)\n",
        "\n",
        "# Compile the model\n",
        "gpt2.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Train the model\n",
        "gpt2.fit(train_dataset, epochs=10)\n",
        "\n",
        "# Generate text using the trained model\n",
        "input_sequence = \"Once upon a time\"\n",
        "generated_text = generate_text(gpt2, input_sequence, max_length=100)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Q_ULtvEPXq0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjxGa_JEYRrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT2 with HuggingFace transformers**"
      ],
      "metadata": {
        "id": "qK1mc8ceYR94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "BFwVOxW0b8Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "model = TFGPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Generate text using the GPT-2 model\n",
        "def generate_text(input_text, max_length=100):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage:\n",
        "input_sequence = \"Once upon a time\"\n",
        "generated_text = generate_text(input_sequence, max_length=100)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "Agg2kAUbYRoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}