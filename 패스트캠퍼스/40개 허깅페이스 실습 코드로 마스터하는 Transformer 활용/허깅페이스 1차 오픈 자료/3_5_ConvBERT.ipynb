{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rh2UCR-k-N-S"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ConvBertTokenizer, ConvBertForMaskedLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "kd-varh6-Ygn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(sentence):\n",
        "    # Load ConvBERT tokenizer and model\n",
        "    tokenizer = ConvBertTokenizer.from_pretrained('YituTech/conv-bert-base')\n",
        "    model = ConvBertForMaskedLM.from_pretrained('YituTech/conv-bert-base')\n",
        "\n",
        "    # Tokenize input sentence\n",
        "    tokens = tokenizer.encode(sentence, add_special_tokens=True, return_tensors='pt')\n",
        "\n",
        "    # Find the masked token\n",
        "    masked_index = torch.where(tokens == tokenizer.mask_token_id)[1].tolist()[0]\n",
        "\n",
        "    # Generate predictions for the masked token\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens)\n",
        "        predictions = outputs.logits[0, masked_index]\n",
        "\n",
        "    # Get the top-k predicted tokens and their probabilities\n",
        "    top_predictions = torch.topk(predictions, k=5)\n",
        "    predicted_tokens = tokenizer.convert_ids_to_tokens(top_predictions.indices.tolist())\n",
        "    probabilities = top_predictions.values.exp().tolist()\n",
        "\n",
        "    return predicted_tokens, probabilities"
      ],
      "metadata": {
        "id": "LOTQMMH8-b0b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I want to [MASK] a car\"\n",
        "\n",
        "predicted_tokens, probabilities = predict_next_word(sentence)\n",
        "for token, prob in zip(predicted_tokens, probabilities):\n",
        "    print(f\"Token: {token}, Probability: {prob}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGviZ9nL-OV7",
        "outputId": "826a5223-1087-4ad9-9195-c6e8a58b1ea4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ConvBertForMaskedLM were not initialized from the model checkpoint at YituTech/conv-bert-base and are newly initialized: ['generator_predictions.LayerNorm.bias', 'generator_predictions.dense.weight', 'generator_predictions.dense.bias', 'generator_lm_head.bias', 'generator_predictions.LayerNorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: pinyin, Probability: 237.00958251953125\n",
            "Token: vida, Probability: 156.7217559814453\n",
            "Token: ##sund, Probability: 150.42002868652344\n",
            "Token: intersection, Probability: 123.30018615722656\n",
            "Token: transactions, Probability: 117.59205627441406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w--0XgAT-fBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}