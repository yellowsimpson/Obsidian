{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63b70cb-3ab6-4358-9cec-2e4043d934de"
      },
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q tensorflow"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue2kOQhXTAMU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f62c68c-3061-472b-e4d1-aa3bd30f8aff"
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.eos_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2OHHnR9Q_7h",
        "outputId": "572d1153-d2e4-4803-90de-07b1a97cf44d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50256"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWLd_J6lXz_t"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode('My name is Hasnain and I am a software engineer.', return_tensors='tf')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = model.generate(input_ids, max_length=50)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output:\\n\")\n",
        "print(tokenizer.decode(greedy_output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJOBVKULO8Op",
        "outputId": "17b0652f-6aa1-4794-d8ec-a3f1238bdc01"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "\n",
            "My name is Hasnain and I am a software engineer. I am a member of the Open Source Software Foundation. I am a member of the Open Source Software Foundation. I am a member of the Open Source Software Foundation. I am a member\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3iVJgfnkMi"
      },
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50, \n",
        "    num_beams=5, \n",
        "    no_repeat_ngram_size=2, \n",
        "    early_stopping=True\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output:\\n\")\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ2N28_ZPZsd",
        "outputId": "2ec781bc-97c3-4763-ca0a-b6e9c0f8d393"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "\n",
            "My name is Hasnain and I am a software engineer. I have been working in the IT industry for over 20 years.\n",
            "\n",
            "I have a passion for software development and have worked on many projects over the past few years, including:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4r7X9s7RgL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}