[코드 5-10 레이어 16짜리]
```python
from tensorflow.keras.datasets import imdb
(train_data, train_labels), _ = imdb.load_data(num_words=10000)

def vectorize_sequences(sequences, dimension=10000):
	results = np.zeros((len(sequences), dimension))
	for i, sequence in enumerate(sequences):
		results[i, sequence] = 1.
	return results
train_data = vectorize_sequences(train_data)

model = keras.Sequential([
	layers.Dense(16, activation="relu"),     #레이어 16 짜리
	layers.Dense(16, activation="relu"),
	layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="rmsprop",
	loss="binary_crossentropy",
	metrics=["accuracy"])
history_original = model.fit(
	train_data, train_labels,
	epochs=20, batch_size=512, validation_split=0.4)
```

[코드 5-11 레이어 4짜리]
```python
model = keras.Sequential([
	layers.Dense(4, activation="relu"),   #레이어 4짜리
	layers.Dense(4, activation="relu"),
	layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="rmsprop",
	loss="binary_crossentropy",
	metrics=["accuracy"])
history_smaller_model = model.fit(
	train_data, train_labels,
	epochs=20, batch_size=512, validation_split=0.4)
```

[코드 5-12 레이어 512짜리]
```python
model = keras.Sequential([
	layers.Dense(512, activation="relu"),   #레이어 512짜리
	layers.Dense(512, activation="relu"),
	layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="rmsprop",
	loss="binary_crossentropy",
	metrics=["accuracy"])
history_smaller_model = model.fit(
	train_data, train_labels,
	epochs=20, batch_size=512, validation_split=0.4)

```

![[Pasted image 20250416183009.png]]

![[Pasted image 20250416183228.png]]


-> 용량이 많은 모델일 수록 더 빠르게 훈련 데이터를 모델링 할 수 있다. 하지만 과대적합에 민감해진다.
