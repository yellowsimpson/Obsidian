
오캄의 면도날 이론
-> 어떤 것에 대한 두가지의 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳다는 것이라는 이론.

__가중치 규제(weight regularization)__

- L1 규제: 가중치의 __절대값__ 에 비례하는 비용이 추가된다.
- L2 규제: 가중치의 __제곱__ 에 비례하는 비용이 추가된다. 

[코드 5-13]
```python
from tensorflow.keras import regularizers      #여기서 regularizers이게 파라미터 규제임

model = keras.Sequential([
	layers.Dense(16,
	kernel_regularizer=regularizers.l2(0.002),     #L2 규제를 사용
	activation="relu"),

	layers.Dense(16,
		kernel_regularizer=regularizers.l2(0.002), #L2 규제를 사용
		activation="relu"),
		layers.Dense(1, activation="sigmoid")
	])

model.compile(optimizer="rmsprop",
		loss="binary_crossentropy",
		metrics=["accuracy"])

history_l2_reg = model.fit(
		train_data, train_labels,
		epochs=20, batch_size=512, validation_split=0.4)
```

![[Pasted image 20250416184821.png]]


[코드 5-14]
```python
from tensorflow.keras import regularizers      #여기서 regularizers이게 파라미터 규제임
	regularizers.l1(0.001)                     #L1 규제  
	regularizers.l1_l2(l1=0.001, l2=0.001)     #L1, L2 모두 규제
```


-> 가중치 규제는 일반적으로 작은 딥러닝 모델에서 사용한다.
	대규모 딥러닝 모델은 파라미터가 너무 많기 때문에 모델 일반화에 큰 영향을 못미침