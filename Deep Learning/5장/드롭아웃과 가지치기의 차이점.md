
### ✅ 1. **드롭아웃 (Dropout)**

- **언제 쓰는가?**  
    → **학습 중(training time)**에 사용됨.
- **무슨 원리인가?**  
    → 학습할 때 **임의로 뉴런(노드)을 일정 확률로 꺼버림** (예: 50% 확률로). 즉, 네트워크가 무작위로 얇아지는 것처럼 작동함.
- **목적**  
    → **과적합 방지 (regularization)**  
    → 다양한 서브 네트워크를 ensemble한 효과를 줌.
- **특징**
    - 학습 때만 적용하고, 추론(inference)할 때는 사용하지 않음.
    - 구현이 간단하고 매우 자주 사용됨.
    - 파라미터를 줄이지는 않음 (메모리 최적화에는 도움 안 됨).
---
### ✅ 2. **가지치기 (Pruning)**

- **언제 쓰는가?**  
    → **학습 후 (혹은 일부 학습 중간에)** 적용됨.
- **무슨 원리인가?**  
    → **중요하지 않은 가중치나 노드, 연결을 제거해서 네트워크를 실제로 줄이는 것.**
- **목적**  
    → **모델 압축 (compression)**, **속도 향상**, **메모리 절약**
- **특징**
    - 파라미터 수 자체를 줄임 → 모델 사이즈 작아짐.
    - 성능 유지를 위해 잘 설계된 기준(예: 작은 weight, 영향력 적은 뉴런 등)이 필요.
    - 모바일/임베디드 환경에서 유용함.
        

---
### 🎯 비교 요약표

| 항목         | 드롭아웃 (Dropout)     | 가지치기 (Pruning)       |
| ---------- | ------------------ | -------------------- |
| 시점         | 학습 중               | 학습 후 (또는 중간)         |
| 작동 방식      | 뉴런을 무작위로 꺼서 학습     | 중요하지 않은 weight/노드 제거 |
| 목적         | 과적합 방지 (일종의 정규화)   | 모델 경량화, 속도 개선        |
| 파라미터 줄임?   | ❌ (학습 때만 무작위로 줄어듦) | ✅ (실제로 파라미터 수 감소)    |
| 추론 시 적용 여부 | ❌ (사용 안 함)         | ✅ (작아진 모델로 추론)       |
| 주요 사용 목적   | 일반화 성능 향상          | 실용적 최적화 (모바일 등 경량화)  |
