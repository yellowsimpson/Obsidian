모델을 학습 할때 __최적적합이 될려면 먼저 과대적합(overfitting)__ 이 되어야 한다. 
Why? -> 언제가 최솟값(경계)인지 알지 못하기 때문에

1. 시간이 지나도 훈련 손실이 줄어들지 않는다 -> 훈련 안됨
2. 상식 수준의 기준점을 넘을 수 없다 -> 훈련은 잘 시작되었지만 모델이 의미 있는 일반화를 달성하지 못했다.
3. 시간이 지남에 따라 훈련과 검증 손실 모두 줄어들고 기준점을 넘어설 수 있지만 과대적합되지 않을 것 같다. -> 여전히 과소적합이다.

- 5.3.1 경사 하강법의 핵심 파라미터 튜닝하기
__핵심 파라미터란? 학습률(learning rate), 배치 크기(batch size)__ *시험!!*
-> 이거 2개 튜닝하는걸로 거의 해결됨

[코드 5-7]
```python
(train_images, train_labels), _ = mnist.load_data()
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype("float32") / 255

model = keras.Sequential([
	layers.Dense(512, activation="relu"),
	layers.Dense(10, activation="softmax")
])

model.compile(optimizer=keras.optimizers.RMSprop(1.), #학습률이 1이라는 말!! (시험!) 
	loss="sparse_categorical_crossentropy",
	metrics=["accuracy"])

model.fit(train_images, train_labels,
	epochs=10,
	batch_size=128,
	validation_split=0.2)
```
[코드 5-8]
```python
model = keras.Sequential([
	layers.Dense(512, activation="relu"),
	layers.Dense(10, activation="softmax")
])

model.compile(optimizer=keras.optimizers.RMSprop(1e-2), #학습률이 0.01 [코드 5-7]이랑 비교 시험!!
	loss="sparse_categorical_crossentropy",
	metrics=["accuracy"])

model.fit(train_images, train_labels,
	epochs=10,
	batch_size=128,
	validation_split=0.2)
```



- 5.3.2 구조에 대해 더 나은 가정하기
-> 모델 자체를 바꾸겠다는 말

1. __입력 데이터에 타깃 예측을 위한 정보가 충분하지 않을 수 있다. (feature가 충분하지 않음)
2. __모델의 종류가 적합하지 않을 수 있다.

- 5.3.3 모델 용량 늘리기
-> 파라미터가 부족하면 늘려야지

[코드 5-9] 
```python
model = keras.Sequential([layers.Dense(10, activation="softmax")])
														#logistic regression
model.compile(optimizer = "rmsprop",
				loss = "sparse_categorical_crossentropy",
				metrics=["accuracy"])
history_small_model = model.fit(
	train_images, train_labels,
	epochs = 20,
	batch_size=128,
	validation_split=0.2)
```

![[스크린샷 2025-02-20 오전 9.46.43.png]]

-> 검증 손실이 0.26에 도달한 후 그 지점에 정체되어 있다.
훈련에서 과대 적합은 항상 가능!!
과대 적합이 되지 않는 것처럼 보인다면 모델의 *표현 능력*이 부족한 것이다.
-> 용량이 더 큰 모델을 사용하면 된다. 
1. 더 많은 정보를 저장할 수 있는 모델
2. 층을 추가 (더 많은 가중치를 가지도록)
3. 현재 문제에 더 적합한 종류의 층을 사용


