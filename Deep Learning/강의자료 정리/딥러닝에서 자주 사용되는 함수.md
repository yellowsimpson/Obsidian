
### 1. **활성화 함수 (Activation Functions)**

- `sigmoid(x) = 1 / (1 + exp(-x))`
- `tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`
- `ReLU(x) = max(0, x)`
- `LeakyReLU(x) = max(0.01x, x)`
- `ELU(x) = x if x > 0 else α * (exp(x) - 1)`
- `Softmax(x) = exp(x) / sum(exp(x))`

### 2. **손실 함수 (Loss Functions)**

- 회귀 문제
    - `MSE (Mean Squared Error)`: 평균 제곱 오차
    - `MAE (Mean Absolute Error)`: 평균 절대 오차
    - `Huber Loss`: MSE와 MAE의 절충
- 분류 문제
    - `Cross-Entropy Loss`: 다중 클래스 분류에서 사용
    - `Binary Cross-Entropy`: 이진 분류 문제에서 사용

### 3. **최적화 함수 (Optimization Functions)**

- `SGD (Stochastic Gradient Descent)`
- `Momentum SGD`
- `Adam (Adaptive Moment Estimation)`
- `RMSprop`
- `Adagrad`
- `Adadelta`

### 4. **정규화 함수 (Normalization Functions)**

- `Batch Normalization`
- `Layer Normalization`
- `Instance Normalization`
- `Group Normalization`

### 5. **컨볼루션 관련 함수 (Convolutional Functions)**

- `Conv2D` (2D Convolution)
- `MaxPooling2D`
- `AveragePooling2D`
- `Dropout`

### 6. **순환 신경망 관련 함수 (RNN Functions)**

- `LSTM (Long Short-Term Memory)`
- `GRU (Gated Recurrent Unit)`

