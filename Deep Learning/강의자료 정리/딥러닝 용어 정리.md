- GPT(Generative pre-trained transformer)
- **Transfer learning**(전이 학습)
- Pre-training (사전 학습)
- Fine-tuning (파인 튜닝)
[[Pre-training & Fine-tuning의 차이점]]
- VAE(Variational Auto Encoder)

- convolutional neural network(합성곱 신경망)
* __Loss == Error == Cost
* Derivative (function)  
  == 도함수
  == Gradient
  == **변화율**
  == 기울기
  == 경사

* Stochastic == Random
* Learning rate (학습률)
    == Step == Step size == 보폭

 * Data == Train data + Test data   (훈련, 시험)
    Training data == True training data + Validation data (검증)
 * DNN parameters: weights and bias of neurons  (targets for training)
 * DNN hyper parameters: learning rate, # of neurons, # of layers, ratio of validation/test data

__DFS(Depth-First Search)(깊이 우선 탐색)(큰 그림: 숲)__ : 루트 노드(혹은 다른 임의의 노드)에서 시작해서 다음 분기(branch)로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법
__BFS(Bread-First Search)(너비 우선 탐색)(작은 그림: 나무)__ : 인접한 노드를 먼저 탐색

- 강의자료
Testing(시험) == Inference(추론) == prediction(예측)
앙상블 == 집단지성 == 여러개가 모여있는 것
transfer function == activation 함수 == 활성 함수

- 1장
역전파(backpropagation)
옵티마이저(optimizer)
앙상블
특성 공학(feature engineering)
활성 함수(activation function)
가중치 초기화(weight initalization)
배치 정규화(batch normalization)
residual connection(잔차 연결)
depthwise separable convolution(깊이별 분리 합성곱)

- 4장
oov (out of vocabulary) : 사전에 없는 단어는 뺌
mse(mean squared error) : 평균 제곱 오차
mae(mean absolute error) : 평균 절대 오차
K-겹 교차 검증(K-fold cross-validation)

0 : padding
1 : start of the sentence
2 : oov(out of vocabulary)

5장
- 일반화(generalization) == preventing overfitting
- 최적화(optimization)
- 보간 (interpolation)
- 특성공학(feature enginering)
- 규제(regularization)

6장
- 개념 이동(concept draft)
- oov (out of vocabulary) : 어려운 단어는 뺌
- 가중치 가지치기 (weight puning)
- 가중치 양자화 (weight quantization)
- 