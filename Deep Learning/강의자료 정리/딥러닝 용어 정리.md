- GPT(Generative pre-trained transformer)
- **Transfer learning**(전이 학습)
- Pre-training (사전 학습)
- Fine-tuning (파인 튜닝)
[[Pre-training & Fine-tuning의 차이점]]

- convolutional neural network(합성곱 신경망)
* Loss == Error == Cost
* Derivative (function)  
  == 도함수
  == Gradient
  == **변화율**
  == 기울기
  == 경사

* Stochastic == Random
* Learning rate (학습률)
    == Step == Step size == 보폭

 * Data == Train data + Test data   (훈련, 시험)
    Training data == True training data + Validation data (검증)
 * DNN parameters: weights and bias of neurons  (targets for training)
 * DNN hyper parameters: learning rate, # of neurons, # of layers, ratio of validation/test data

__DFS(Depth-First Search)(깊이 우선 탐색)(큰 그림: 숲)__ : 루트 노드(혹은 다른 임의의 노드)에서 시작해서 다음 분기(branch)로 넘어가기 전에 해당 분기를 완벽하게 탐색하는 방법
__BFS(Bread-First Search)(너비 우선 탐색)(작은 그림: 나무)__ : 인접한 노드를 먼저 탐색

4장
oov (out of vocabulary) : 사전에 없는 단어는 뺌

0 : padding
1 : start of the sentence
2 : oov(out of vocabulary)

5장
- 일반화(generalization) == preventing overfitting
- 최적화(optimization)
- 보간 (interpolation)
- 특성공학(feature enginering)
- 규제(regularization)

6장
- 개념 이동(concept draft)
- oov (out of vocabulary) : 어려운 단어는 뺌
- 가중치 가지치기 (weight puning)
- 가중치 양자화 (weight quantization)
- 