1. __가중치 가지치기(weight prunning)__: 가중치 텐서의 모든 값이 예측에 동일하게 기여하지 않는다. 가장 큰 값만 남기면 모델 층에 있는 파라미터 개수를 크게 낮출 수 있다. 이렇게 하면 성능에 약간의 손해를 보기는 하지만 모델이 사용하는 메모리와 계산 자원을 줄인다. 

2. __가중치 양자화(weight quantization)__: 딥러닝 모델은 단정도 부동 소수점(float32) 가중치로 훈련된다. 하지만 가중치를 8bit의 부호 있는 정수(int8)로 압축하는 것이 가능하다. 