딥러닝 day2
2025.03.10 월 오전 10:19 ・ 30분 59초
심승환


참석자 1 00:00
뭔가 뭔가 여러분들 지금 훈련을 하고 있는 거잖아요.
그쵸 나 때문에 계속 뭔가 값이 변하고 있는 거죠. 그쵸 그래서 나 갑시다 말하고 난 다음에는 사실 여러분 자전거 타는 건 비슷한데 그러니까 공부할 때는 천천히 대고 막 막 이거 아닌가 보다 막 이러면서 이게 맞나 저나 이러면서 하는데 나중에 이제 실제 쓸 때는 거의 그냥 자동으로 쫙 나가게 되잖아요.
그쵸 문제는 이제 이게 또 문제인데 업 러닝이라는 게 되게 중요해요.
요즘에 잘못 학습 시켜놓으면은 잘 안 바뀌잖아요.
그쵸 뭐 예를 들어서 자전거 운전도 습관 잘못 들이면 되게 이상해지고 자전거도 잘못 습관 들여놓으면 되게 위험하게 타고 습관이 되게 중요하잖아요.
3번 한 번 외우고 나면 하드 와이어 돼버리잖아요.
사실 그 사람도 자전거를 타기 시작하면 다 못 하기 힘들어요.
그쵸 여러분 자전거 타지마 그런 게 있어요. 그래서 걷기 시작하면 못 걷기가 힘들어요.
그렇죠 근데 처음에 걷는 거 되게 힘들잖아요.

참석자 1 00:51
그쵸 근데 뭐냐면은 이거 훈련은 되게 힘든데 훈련된 거를 그냥 쓰는 거는 굉장히 자원을 별로 안 쓴다고요.
이 빠르고 근데 이것조차도 근데 사실은 좀 더 빠르게 하기 위한 방법이 여러 가지가 필요하고 그래서 온 디바이스 AI 이런 걸 열심히 하고 있는 거고 그렇게 하는 걸로 일단 감이 잡혀 계세요.
여러분 그래요. 그리고 여기서 언급하고 싶은 게 이 인퍼런스라는 용어가 원래 전문 용어로 이제 사실 테스팅 하는 거랑 같은 용어를 쓰고 있거든요.
그래서 이제 그리고 우리나라 말로 번역을 추론이라고 하는데 문제는 이 추론이 우리나라 말로는 하나인데 영어로는 추론을 번역을 이제 리즈닝이라고 하기도 하고 리즈닝이라는 거 여러분 들어봤어요.
요즘에 직급 나 딥시큐가 자기가 잘하는 게 뭐 프로그래 근데 그건 영어가 리드닝이에요.

참석자 1 01:42
이 임플런스가 그 아이야 그 제가 약간 좀 조심스러운 게 실제로 지금 머신러닝이나 딥러닝 쪽이 막 계속 발전하고 있는데 더 이상 우리나라 사람들이 새로 발전시키는 건 별로 없고 당연히 영어 쓰는 사람들이 발전시키는데 용어가 새로 사주 나오잖아요.
번역은 다 똑같이 하는 거야. 이해돼요. 여러분 구조랑 아기처럼 다른데 계속 구조를 번역하면 사실은 오해가 있잖아요.
그러니까 영어로 알아야 되는 면이 있다고요 여기 지금 이번에 챗gpt 45 나온 거랑 딥시크 새로운 버전은 추론 기능이 강화됐다고 이제 뉴스가 나와요.
근데 그게 인플러스 기능이 아니잖아요. 리즈닝 기능이지 리즈닝은 뭔가 이렇게 체인 오브 띵킹이라고 그래가지고 다음에 뭐가 필요한지 생각하고 또 생각해보고 생각하고 하는 거 있잖아요.

참석자 1 02:31
그거를 이제 리즈니이라고 하는데 체인 오브 띵킹이라고 그래서 cot라고 부르기도 하고 이러는데 그런데 그거는 번역을 인퍼런스라 인퍼런스가 아니거든요.
근데 그걸 추론이라고 번역하니까 똑같은 걸로 추론은 이미 하고 있는 건데 갑자기 뭔 소리인가 그런 생각이 들잖아요.
그리고 조심할 건 이제 기자들이나 뉴스에 나오는 거 보면은 트랜스포머는 딥러닝이 아니고 딥러닝과 달리 이런 다 있어요.
되게 이상한 거예요. 그렇죠 남자 사람은 사람과 닮은 거 이해돼요.
여러분 그런 식으로 말하게 하는 거지 남자 사람은 사람이 아닌가 그렇죠 웃기잖아요.
그쵸 너무 웃기죠. 그런데 그런 식으로 만일 여러분은 그렇게 얘기하면 어떻게 되겠어요?
같이 안 놀겠지 사람들이 그렇죠 아니 뭔 얘기인지 알겠죠.
같이 안 는다는 게 이사가 안 된다는 뜻이에요. 여러분 그래서 그래도 말을 잘해야 돼요.

참석자 1 03:25
그렇죠 특히나 요즘 같은 시대에는 AI가 거의 다 해주는 시대는 왜냐하면 AI가 환각에 시달리기 때문에 여러분이 정확한 개념을 가지고 있는 게 되게 중요하다고 알겠죠.
특히 단어도 우리나라 말로만 이해하면 큰일 나요.
여러분 그쵸? 영어로 다 뭔지 알아야 돼요. 추론이라고 이해하지 말고 인퍼런스라고 이해하지.
추론이 리즈닝이라고 생각해야 되고 지금 나온 새로운 거라고 알겠죠 여러분 이 그래요 그래요.
그다음에 이제 여기 머신러닝의 3종류 이런 거 유명해서 이거 이 용어는 이따 알아야 돼서 교과서에서도 그냥 아무렇지도 않게 막 나오거든요.
근데 그걸 그때마다 설명하는 게 별로라서 지금 이제 다 해버리는 거예요.
알겠죠? 지도 비지도 강화가 유명해요.

참석자 1 04:07
꼭 보통 얘기 개 세 가지 이렇게 얘기할 때 사실 이게 엄밀하게 맞는 건 아니지만 그냥 보통 학습에는 크게 이렇게 세 가지가 있다 얘기하는데 이것도 사실은 좀 그렇게 바람직하지는 않아요.
왜냐하면 지도의 반대말은 비지도잖아요. 여러분 강화가 여기 또 있잖아 이거 이상하잖아요.
그쵸 이상하다고 이게 별로 안 예뻐요. 사실 이렇게 논리적으로 아름답지 않은 상황이에요.
알겠죠 그냥 그렇다고 근데 그렇게 사람들이 쓰고 있다는 거 알고 계시고 여러분도 이상하다고 생각하시고 이상하다고 생각하는 게 되게 중요하겠죠.
이상하다고 생각 안 하고 넘어가기 바쁘지 비지도 비지도 하고 끝났잖아.
사실은 여러분 지도 아니면 비지도잖아요. 그렇잖아요.
여러분 강화는 그러면 도대체 좀 다르다고 또 해놓은 거지.
사실은 사실은 지도적인 것도 있고 비지도적인 것도 있는데 사실은 그래 갑시다.

참석자 1 04:56
여기서 일단 사람들이 뭘 이렇게 분리하고 있는지 지금 명확히 분리하고 있는 체계가 있으니까 어쨌든 지도 학습은 정답 레이블을 주는 게 핵심이에요.
레이블 레이블이라고 해서 정답 출력과 이거는 거의 이 교과서 전체에서 거의 이것만 할 곡이라서 은근히 제가 이 슬라이드 별로 없어요.
이 슬라이드 지도 학습 내용이 안 나오는 이유는 거의 이 책에서도 거의 레이블이 있는 데이터만 그러니까 아까 예를 들어서 사람이면 사람 사진 갖다 놓고 사람 적혀 있고 고양이 사진 갖고 고양이 적혀 있고 이런 식으로 레이스 잡혀 있다는 거죠.
이미지 분류한다 그러면 그런 것들이 그런 그렇게 이제 입력 데이터가 이미지 같은 경우는 거기 정답으로 레이블이 붙어 있는 거 그러면 저 결괏값으로 만약에 사람 보고 고양이 이러면은 아니네 이러고 고치고 그치 그렇죠 그렇게 지도해 주는 거죠.
그렇죠 이게 되게 쉽죠. 여러분 보통 딥러닝 하는 건 거의 딥러닝이 아니라 머신러닝이고 딥러닝이고 다 지도 학습이에요.

참석자 1 05:53
그리고 분류도 있지만 온갖 여러 가지 보통 이제 키 갖고 몸무게 예측하고 이러는 것도 키가 얼마인데 몸무게가 얼마 데이터가 있을 거 아니에요?
그렇죠 데이터 기반으로 원래 이제 입력 데이터의 정답 출력 값이 같이 있는 거예요.
그리고 그거를 계속 보정하면서 이제 그 웨이트 값 아까 전에 얘기했던 파라미터 값들을 제대로 될 값을 찾아 나가는 거지 감은 오죠 여러분 이렇게 크게 감을 잡으세요 알겠죠?
그거고 이건 딥러닝 상관없이 모든 머신러닝이 다 이렇다고 딥러닝이 뭔지도 모르는데 그쵸 이 설명하고 있으니까 또 별로기도 하죠.
그냥 일단은 이렇게 일단 한 다음에 다시 넣어볼게요.
그다음에 비지도 학습은 반대니까 지도 안 한다는 거잖아요.
레이블이 없는 거예요. 그쵸? 레이블이 없어 그쵸?
레이블이 없는 정답 레이블이 없는데 정답 정답 출력 값을 레이블이라고 보통 불러요.
여러분 레이블이 꼭 이름표 같은 거잖아요. 이름표.

참석자 1 06:50
그런데 지금 이 출력 결과를 가지고 계속 얘가 비슷한 것끼리 모아보고 이러는 걸 하는 거예요.
특성을 가지고 분석해서 좀 뭔가 스스로 파악을 해서 폭력 목표 같은 걸 파악을 해서 뭔가 분류를 해보는 거죠.
스스로 모아보는 작업을 하는 거예요. 이것도 감이 잘 안 오죠 여러분 그쵸 나 먼저 보여주고 하는 게 어떨까요?
여러분 표정이 안 좋아서 먼저 보여주세요. 표정이 안 좋아서 이거 보여줄게요.
여기 제가 뒤에 새로 갖다놨는데 비지도 학습에 대표적인 게 베리에이션 오토 인코더랑 그다음에 프린서플 컴포넌트 어네렉시스랑 근데 케이미지 클러스터링 이런 게 있거든요.
많이 쓰는 거예요. 여러분 케이미지 클러스터링은 이거는 딥러닝이 뭔지 설명 안 타고나는 게 아니라 어쨌든 이거 이거 먼저 보여드리니 내가 VI가 있는데 되게 재미있어가지고 교과서에 있어야 사실 교과서 몇 페이지에 있는지 보여줄 수 있지만 그냥 여러분 보여줄까요?

참석자 1 07:46
그래도 관심 있어 하는 교과서 갖고 온 사람들은 저 그림이 교과서에 있는 거예요.
이건 완전 논문에 있는 거라요. 사실은 515쪽에 있어요.
515쪽 교과서 515쪽에 있어요. 잘 보이잖아.
그래가지고 이거 비싼 건데 우리 딴 말로 이렇게 인사하려면 그렇죠.
여러분 515쪽에 있어요. 그렇죠 515쪽 12.4절에 12.4.1절에 있어요.
이게 비지도 학습의 대표적인 건데 이게 뭐냐면 데이터를 잔뜩 때려 박아 넣어주면 이제 형량 포에 의해서 사람 얼굴은 기본적으로 여기 머리카락 있고 눈 있고 코 있고 입이 있는 거 알잖아요.
그쵸 얼굴 선 있고 이거의 분포들을 학습을 해서 얘네 비슷한 확률적으로 비슷한 것끼리 모아보는 거예요.
이렇게 그리고 생성도 하는 거지. 그래서 사실 사람 얼굴에 잔뜩 넣어주면 여러분들 얼굴 잔뜩 넣어가지고 학습시키잖아요.
v 탭 그러면은 지금 예를 들어서 여기 앉아 있는 두 사람 있죠.
두 사람 사이의 얼굴도 만들어져요.

참석자 1 08:46
특성이 비슷한 게 있고 다른 게 있잖아요. 그리고 머리카락이 점점 변하고 눈도 변하고 안경 쓴 게 있다가 없어지고 이런 식으로 이해되시죠?
여러분 그런 식으로 다 뭔가 비슷한 게 공통점과 차이점이 있잖아요.
그쵸 그게 다 확률적인 거잖아요. 그쵸. 절대로 여기 눈이 달리지는 않는데 이제 눈이 달리면 이제 기묘의 칼날인가 그런 거지.
그쵸 이해되죠 여러분 네 그래요. 그래서 창의적이라고 그랬지.
우리가 하여튼 VA를 쓰면 현재 있는 데이터에서 가장 뭔가 비슷한 것 안 비슷한 것끼리 잘 이렇게 정리를 해가지고 모아줘서 보면은 지금 이 사람이랑 이 사람이랑 머리 이렇게 보면 왼쪽 오른쪽 다 비슷비슷하잖아요.
그쵸 인접한 것끼리는 점점점 변하는 거 보이죠. 여러분 그렇죠 여러분 교과서 보는 김에 다음 페이지 한번 볼래요.
여러분 교과서 보는 김에 어디 있냐 이거 교과서 보는 김에 어디 있지?
여기 525쪽 있죠 525쪽 제가 이거 그냥 올릴게 이니까 이거 이거예요.

참석자 1 09:50
여기 숫자 숫자 0부터 9까지 숫자 있잖아요. 걔도 비슷한 것끼리 모아보는데 어떤 거 보면은 이게 여기 보면 비슷한 것끼리 모아놓은 거 보면은 9랑 9에서 8로 변하는 게 보이고 7에서 9로 변하는 게 보이거든요.
7도 이상하게 쓰면은 9 같이 보이기도 하고 여러분 사실 시력 검사할 때 7인가 9가 맞히고 싶은데 그렇죠 그런 거 있잖아요.
여러분 그런 거 뭉개지는 거 그거도 실제로 보여주고 있는 거죠.
숫자로 학습해 놨더니 비슷한 분끼리 이렇게 확률적으로 이렇게 생길 수밖에 없다는 거죠.
그렇죠 이게 재미있죠. 여러분 이거는 우리가 라이브를 주거나 정답 값 준 게 아니라 실제 존재하는 데이터들의 특성을 비교해서 공통점 차이점을 통해서 이제 보관이라고 그래서 중간중간 단계에 뭐가 있는지를 보여주면서 이렇게 하니까 이게 이것도 대표적인 생성형 AI 중에 하나예요.

참석자 1 10:38
왜냐하면 뭐 만들어낼 수 있겠죠. 이걸로 자동차를 디자인해보라고 저런 차가 이런 변형이 있을 수 있으니까 하면서 만들어낼 수 있겠죠.
그쵸 약간 더 샤프하게 만들어서 샤프 하는 쪽으로 가고 이게 더 재미있는 게 여기 다음 페이지 보시면 이거 슬라이드 이것도 교과서에 있는 건데 이거 몇 쪽에 있나요?
여기 아까 516쪽 한번 보시면 돼요. 5 16일 다른 국가들보다 이 국가 훨씬 좋긴 좋아요.
전 세계에서 제일 미국에서 제일 많이 버는 국가소라서 이것도 여전히 보면은 아까 그 사람 같은 사람이었던 어요.
같은 저자가 쓴 건데 여기 이 사람 얼굴 보이죠. 여기 하게 환하게 그냥 무표정 한 건데 이쪽으로 환하게 웃죠.
그쵸 그러잖아요. 여러분 여기는 똑같은 얼굴인데 여기 되게 막 너무 우울해 보여 죽을 것 같아요.
그쵸? 이해되죠? 여러분 이게 스마일 벡터라는 걸로 스마일 벡터를 강화시키는 방향 이거는 약화시키는 방향으로 하면은 이렇게 산성이 가능한 거예요.

참석자 1 11:34
여러분 어차피 웃으면 일이 벌어지는 거 알고 있고 눈이 이렇게 옆으로 터지는 거 아니고 그렇죠 우울하면은 입꼬리가 내려가고 눈이 좀 어두워진다는 거예요.
의해서 이렇게 만들어낸다는 거죠. 이게 이것도 지도를 한 게 아니라 저절로 이제 있는 특성을 모으다 보니까 이런 벡터를 찾아낸 거예요.
벡터가 이제 변한 풍경 포상 이쪽으로 가면 웃는구나 이쪽으로 가면 우는구나 이러면서 굉장히 재밌죠.
여러분 되게 이거 이거 지금 굉장히 유명해서 지금 뭔가 새로운 좀 생성해야 괜찮은 게 나오면 더 이거 기반으로 하는 게 많아서 되게 재미있는 것 같아요.
이게 진짜 지금 원래 개니 할 때 되게 떴잖아요. 개니라고 들어본 사람 있을 거예요.
근데 걔보다 사실 얘가 더 재밌는 거 걔는 잘 학습이 잘 안 돼요.
학습이 잘 돼요. 그리고 뭔가 의미 있는 데이터를 찾을 수 있기 때문에 이거는 교과서에 있는 건데 그리고 데이터를 이렇게 막 다 때려박으면은 이거에 뭔가 잠재적인 뭔가 확률 분포를 찾는 거예요.

참석자 1 12:32
레이턴트 스페이스라고 돼 있는데 이게 뭔가 내재적인 뭔가 우리도 이제 사람 얼굴에 이렇게 생겨야지 이런 감이 있잖아요.
눈이 제일 중요하고 일단 그렇죠. 눈은 이렇게 양쪽으로 있고 이런 거 있잖아요.
그런 거에 대한 우리가 후포가 생기는 거잖아요. 그쵸 규묘리 칼라처럼 안 되는 거지.
그쵸 규묘리 칼라도 너무 이상하지 않게 문이 달리면 이렇게 달리기.
어쨌든 절대로 여기 다른 데 가지는 않잖아요. 그렇죠 손으로 가기도 하고 동그랗게 어쨌든 그래서 그거 보고 이제 신기하니까 또 보는 거지.
이 사람들이 어쨌든 뭔가 브로드한 이미지에 대한 뭔가 이렇게 학습을 하는 거지 그래서 여기서 아무 이제 뭔가 특정 랜덤 변수를 만들어내는 이 공간에서 거기에 해당하는 이미지가 나오는 거죠.
그런 식으로 해요. 그리고 그걸 여기 이거 얘네들이 만들어낸 방법은 그리고 잠재 공간 학습한 다음에 인접한 값들로 주면서 생성하니까 이렇게 된 거예요.

참석자 1 13:26
이것도 마찬가지고 근데 이게 차원이 2차원 1차원 이게 아니라 굉장히 크기 때문에 사실 그리고 또 가오시 분포 이런 거 쓰기 때문에 원래 여러분들 평균 가가하고 분산이라는 거 있잖아요.
그걸 조정하면서 나오는 거라서 다양하게 나올 수 있는 거지 그래요.
이것도 교과서에 다 어떻게 실제 코드 다 있어요? 이거를 하는 코드까지 굉장히 훌륭해야 할 책이 진짜 우리 거의 중요한 건 다 있는 거예요.
그리고 요것도 요거는 PCA도 교과서에 언급이 되는데 자세한 내용은 사실 머신러닝 책에 나오고 이거 머신러닝에서 굉장히 중요한 부분인데 이거 여기 다 딥뉴럴 네트워크 써야 되는 거고 이건 딥뉴럴 네트워크라는 거 안 쓰고 아직 안 배웠지만 이따가 또 해줄게요.
PCA는 필수 프로퍼포먼트 어널리시스라고 이거 보면은 여러분 패션 앱리스트라는 데이트가 있어요.
그게 뭔지 뭐냐면은 패션 이스트 제가 인터넷에 보여줄게요.

참석자 1 14:20
그냥 패션 리스트 앱 리스트는 사실 원래 아까 제가 숫자에 있는 거 있죠.
숫자 보여줬죠. 숫자 0 1 2 3 4 이런 거 숫자에 있는 데이터를 모아놓은 건데 그게 워낙에 머신러닝 하다 보니까 그거를 이제 패션 쪽에다가 적용해가지고 이미지들을 이렇게 막 샘플로 뭔가 숫자가 10가지 있잖아요.
0부터 9까지 그것처럼 옷도 패션 쪽에서도 10가지 아이템을 한번 그러니까 분류하는 걸 만든 거 이런 거

참석자 1 14:53
보이죠. 여러분 이게 보면 가방도 있고 옷도 있고 그렇죠 신발도 있고 이런 게 10가지가 있는 거예요.
구분이 되는 거 이게 딱 보면 이거 신발인데 이 신발 이 신발 좀 다르잖아요.
그쵸 그리고 얘는 원피스고 얘는 바지고 이거 우리 알잖아 그쵸.
근데 사실 확률 포장으로도 얘도 비슷한 놈들이잖아요.
사실은 그렇죠 어쨌든 그래서 여기 이것도 VR도 아까 베리셜 오토 커도 보여줬던 걸로도 할 수 있지만 기본적으로 프레스퍼 컴포넌트 어너리스라는 거는 가장 프리스터폴이라는 게 뭐예요?
뜻이 중요한 컴포넌트 구성 분석이라는 뜻이잖아요.
중요한 요소 분석 구성 요소에서 그래서 여기에서 가장 다른 그런 요소들이 있을 거 아니에요 공통점 말고 차이점 차이점을 기반으로 뭔가 분류를 하기 시작하는 거예요.
그러니까 이게 이 바지 같은 경우에는 여기에 가운데가 분명히 퍼져 있고 그렇죠 여기 이런 거 있잖아요.

참석자 1 15:48
이게 이게 다른 애들이랑 다르잖아요. 그쵸 여기가 있고 여기가 구멍이 쫙 있다는 게 여기랑 다르잖아요.
그런 식으로 이제 모아보는 거예요. 수학적으로 그래서 차원을 사실은 축소를 시켜요.
얘는 지금 차원이 이게 차원이라는 게 뭐냐면은 데이터의 개수예요.
여러분 그러니까 이게 우리가 지금 3차원에 살고 있잖아요.
1차원 2차원 3차원 알죠? 여러분 1차원은 데이터가 얼마예요?
항상 그냥 값 하나잖아요. 그쵸? 2차원은 가로 곱하기 세로잖아요.
그렇죠 근데 가로의 개수와 세로의 개수에 따라서 정보가 달라지는 거죠.
그렇죠 그래서 이게 지금 얘는 지금 2차원이잖아요.
근데 2차원이지만 사실 데이터가 가로 곱하기 세로가 지금 28 곱하기 28이거든요.
28개의 픽셀 28개밖에 안 돼요. 여러분 28개면은 여기 28개 점이에요.
28개 점 여기 28개 점으로 다 표현된 거예요. 전부 다 28개 성정 때 이해되죠 여러분 그래서 별로 그것도 충분히 구분 가능하죠.

참석자 1 16:46
28 부하 28인데 그거를 좀 더 작은 데이터 더 작은 정보를 바꿔서 제일 중요한 것만 남기고 다른 것만 수학적으로 만들어서 매핑을 하고 나면 이렇게 그리고 아까 변이를 한 거긴 한데 그 제가 아까 보여준 것처럼 이런 식으로 잘 안 보이지 되나 이게 된다 안 되네.
이거 보면은 여기 믹도오리가 몰려 있고 가방이 몰려 있고 이거 보이죠.
여러분 비슷한 것끼리 뭐 하니까 얘는 확실히 얘랑은 먼 정도에 따라서 더 멀리 갖다 놓은 거예요.
느낌이 와요. 여러분 가방은 좀 멀잖아 그리고 신발끼리 모아놓고 그쵸?
이해되죠? 여러분 그리고 우도리까지 모여 또 바지끼리 모여 있고 이런 식이에요.
여러분 알겠죠 이렇게 저절로 된다는 거죠. 우리가 사람들이 뭐 분류하라고 시키지 라벨을 안 알려줘도 얘가 이건 비슷한 거다 안 비슷한 거다 하고 분류할 수 있을 거 아니야 수학적으로 그렇게 하는 게 비지도 학습들이라고 이해돼요.
여러분 확률 분포를 주로 하는 거예요. 알겠죠? 그래요.

참석자 1 17:49
얘는 확률 포라기보다는 이제 해당 차이점들 별로 한 거고 다 있구나.
그리고 이제 케인 밍지 플러스틱이라는 거는 워낙 유명한데 이거는 이제 머신러닝에서 배 지금 이것도 항상 유용해요.
뭐냐면은 이게 뭔가 3개의 평균을 가지고 뭔가 분지타 해봐라 모아봐라 그래서 여기 지금 이거 종류는 잘 모르겠지만 세 가지로 분류해요.
하면 세 가지로 제일 비슷한 끼리 모아주는 거예요.
5개라고 하면 5가지로 분류해 주고 정해줘야 되긴 하는데 굉장히 가벼운 금방 학습되는 비지도 학습이라서 여기 이런 거죠.
여러분을 예를 들어서 케미지 클러스팅으로 제가 이 두 가지로 보면 가장 반은 두 가지로 분류해 줄 수 있다고요.
근데 그거를 이제 외형을 가지고 갈지 만약 목소리를 가지고 갈지 다를 수 있잖아요.

참석자 1 18:33
여러분 그렇죠 목소리를 가지고 하면 남녀로 구분될 수도 있을 것이고 그렇죠 그냥 모형 갖고 하면 안경 쓴 걸로 구분할 수도 있을 것이고 무슨 얘기인지 알겠죠 여러분 분류를 해준다고 근데 이렇게 두 가지로 하든지 5가지로 하든지 그러면 또 어떻게 하겠지 옷을 벗어서 입었는지 안 입었는지 이런 식으로 얘네들 얘가 분류한다고요 이해되죠?
여러분 그래서 이런 게 다 비지도 학습이에요. 이건 뭐예요?
비지도 학습이 지도 학습은 은근히 뻔하잖아요. 사실 여러분 원래 정답 값이 있는데 그래서 그가 그걸 하는 거 그렇죠 데이터를 가지고 그러는 거고 비지도 학습 이런 것들이고 비지도 학습이 굉장히 재미있는데 교과서에서 거의 안 다뤄요.
지금 보면 아까 나오는 것들 이거 어디 있어요? 거의 끝에 있잖아요.
그쵸 밑에 있잖아요. 그래서 좀 그만큼 이제 덜 다루기 때문에 제가 더 열심히 강조하는 면이 있었어요.
알겠죠 아 저 수학 시간에 첫째 주에 그래요.

참석자 1 19:25
그리고 강화 학습이라는 게 있는데 이것도 재미있는데 강화 학습은 이거는 왜 맨날 자율주행도 사실 이거 갖고 했지롱 이러고 맨날 이게 뭐냐면은 이거는 여기는 이제 실제 공부시킬 때도 여러분 지도 학습을 계속 선생님이 옆에서 이거 답은 이건데 너 이거라고 대답하지 않으면 안 돼 이런 거고 비지도 학습은 니가 알아서 좀 뭔가 분류해 봐 이런 거고 그렇죠 강화 학습은 그런 거 정답 답 알려주지 않고 니가 이제 공부해 오면서 점수를 많이 따게끔 어떻게 하나 뭔가 점수가 있는 거예요.
점수 리워드 펑션이라는 게 있죠. 보상 보상 그러니까 정답을 알려주지 않고 뭔가 잘하면은 점수를 높게 주고 못하면 점수를 낮게 줘서 점수만 높게끔 훈련시키는 거예요.
실제로 여러분 뭔가 좀 자기주도 학습이 저희 보상 함수 자기주도 학습이 다 그렇거든요.

참석자 1 20:15
그냥 뭐 이렇게 자꾸 막 하라 하라 이러면서 이거 아니다 틀렸다 이렇게 하는 것보다 스스로 이제 장기적으로 뭔가 장기적으로 뭔가 잘 되는 쪽으로 학습하는 게 바람직하잖아요.
그래서 이게 이쪽 지도 학습은 정답 값이 사람이 만들어주는 것 같아 데이터에 의해서 여기도 여기 강화 학습은 데이터를 사람이 주지 않겠죠.
정답 값을 특히나 이쪽은 의사결정이나 예측 쪽에서 많이 하는데 그러니까 뭐냐면 예를 들어서 여러분이 알파고 같은 거 있잖아요.
알파고 바둑 뜨는 거 바둑 뜨는 거 정답 가스는 어떻게 되겠어요?
여러분 지도 학습을 한다면 흙이랑 백이랑 이렇게 판이 있을 때 다음에 흙은 어디로 가야 될지가 옛날 기호에 의해서 나오겠죠.
옛날 사람들이 했던 거 여기다 둬야 되는데 여기다 두면 이상하게 이럴 수 있잖아요.
근데 이 가마수면 어떻게 하겠어요? 일단 둬봐 그래서 나중에 점수 이기면 그만이죠.

참석자 1 21:04
그쵸 이기면 되는 거잖아요. 그러니까 희한하게 두는 거죠.
얘가 근데 이기지 이해돼요. 여러분 사람보다 더 나을 수도 있잖아요.
여러분 그러니까 예를 들어 주식 투자를 하는데 이상하게 투자하는데 결국 잘 돈 벌면 그말 아니에요.
그쵸 근데 이 정답 지도 학습을 하면 어떻게 돼? 지도 학습을 철저하게 인간이 뭔가 만든 데이터에 의해서 하는 거잖아요.
그러니까 좀 더 창의적인 게 나오기 힘들잖아요. 그렇죠 더 대단하게 나오기 힘든데 학습 강화 학습은 고상 뭔가 점수 결국은 진짜 원하는 그 고를 위해서 뭔가 학습이 되기 때문에 굉장히 더 재미있는 게 나올 수 있는 건데 문제는 강화 학습은 여러분 선불로 조절하기 힘든 게 결국은 영원히 학습이 안 되는 경우도 있어요.
헤매기만 하다 끝날 수도 있어요.

참석자 1 21:48
그러니까 옛날에 알파고 같은 거는 사실은 성공을 못했지 바둑이 인간보다 바둑은 인간이 맨날 이긴다고 그랬어요.
그러니까 도저히 안 될 거라고 컴퓨터는 근데 이제 워낙에 지금 NVIDIA가 잘해가지고 사실 된 거죠.
사실은 하드웨어가 너무 빨라져가지고 그런 면이 있어요.
이해되죠. 여러분 그래요. 지금은 근데 강학습 또 이제 이것도 방법이 여러 가지가 나왔는데 굉장히 또 알고리즘적으로 좋은 게 많이 나와서 지금은 그리고 또 강화 학습도 사실은 처음부터 시작하는 게 아니라 요즘에는 인간 데이터 같은 걸로 뭔가 잘 된 것들만 모아서 뭔가 트라이어 레러 하다가 좋은 것만 모아서 그걸로 그걸 기반으로 시작하는 게 많이 있어요.
그래서 어쨌든 학습 강화 학습은 영원히 제일 항상 사할 수 없는 분야겠죠.
그렇죠 왜냐하면 컴퓨터 니가 잘해 봐 이건데 실제로 그런 게 많잖아요.
게임 같은 것도 그렇고 그래요.

참석자 1 22:38
그래서 이렇게 일단 분리가 되고 이거 지금 다 하면 너무 정신없을 것 같아서 제가 일단 이거 먼저 해줘야 지난 시간에도 이거 먼저 해줬던 것 같은데 머신러닝 딥러닝 그쵸 근데 이게 딥러닝이 뭔지 먼저 좀 알려주긴 해야 될 거 진짜 딥러닝은 8분 남았네.
그래서 어쨌든 머신러닝의 일종이에요. 머신러닝 중개 딥러닝이 있는 거예요.
그러니까 모든 딥러닝은 머신러닝이에요. 여러분 그러니까 얘도 뭔가 프라이터 값을 주고 그걸 학습시키는 게 맞다고요?
그런데 머신러닝하고 다른 뭔가가 있는 거죠. 그쵸 여기 이거 그냥 보통 머신러닝 중에 딥러닝이 아닌 것도 있고 딥러닝은 다 학습을 한다는 거죠.
그렇죠 그래요. 그래서 딥러닝의 핵심이 뭐냐면은 여기 보면은 심층 신경망이 적혀 있죠.
그쵸 심층 신경망이라는 그런 자료 구조를 쓰는 거예요.

참석자 1 23:29
근데 심층 신경망이라는 게 도대체 뭐냐 그러니까 이거 여기는 심층 신경망이 아니라는 거지 머신러닝은 알겠어요 일단 딥러닝은 심층 신경망 심층 신경망은 영어로도 알아야 되잖아.
그사람 그쵸 딥 뉴럴 네트워크이겠다. 그쵸 번역하면 딥 심층 딥 신경 뉴럴 네트워크 그쵸 이거 적어봐요.
여러분 딥 뉴럴 네트워크 딥 뉴럴 네트워크 영어로 약자로 보통 하는 거 아니에요?
그 기면 뭐라 그럴 것 같아요. 여러분 DNN 외워야 돼.
DNN DNN이라고 그래 DNN DNN 하면 이제 여러분 그게 뭐예요?
이러면 안 되지 그쵸 딥 뉴럴 네트워크에 알겠죠 DNN 심층 신경관 그리고 그래서 여기 보면은 여기 지금 적어놨어요.
뒤에 적어놨네. 디밀러 적어놨네. 그렇죠 예 적으라고 뒤에 나오네.
여기서 신경망 중 딥러닝이 아닌 것도 있다고 제가 적어놨죠.
신경망 신경망이 뭔지도 모르는데 신경망이 뭔지 설명한 다음에 다시 돌아올게요.

참석자 1 24:31
여러분 신경망 이거부터 뉴럴 네트워크 신경망이에요.
여러분 신경망 슬라이 다 있잖아요. 그쵸 뒤에 가면은 신경망 이거는 신경망은 사람이 이렇게 신경망이 있어요.
근데 여기 뇌 핸드라고 적어놨는데 뇌 말고도 여기도 다 신경망이에요.
여러분 여러분 다 여기 지금 신경만 있잖아요. 그쵸 신경이 없으면 우리가 정답이 없잖아요.
그쵸 뇌에 특히 뭔가 신경망이 좀 빼곡빼곡하게 있어가지고 이렇게 부르는데 뉴러는 신경세포라는 뜻이고 사람 뇌에 천억 개가 있다.
이거 지금 항상 볼 때마다 재미있어요. 사람한테 천억 개가 있는데 개수가 사람에 따라 차이가 없고 새로 생성되지 않아요.
태어날 때부터 거의 그대로 있는데 중요한 건 이제 이게 학습을 하면 여기 이게 얘가 이런이 있는데 여기 연결이 되는 거 있죠 연결이 공고를 하면 연결이 되고 여기다가 뭔가 값이 저장이 되는 거예요.
중간 연결 부분에 연결이 강화되기도 하고 이제 약해지기도 하고 그러면서 학습이 된대요.

참석자 1 25:30
근데 여기 이렇게 이게 이렇게 붙은 거를 시냅스라고 부르는데 이게 필리스 한 유런에다가 이렇게 많이 붙는데요.
그러면 유런 하나가 이렇게 연결이 많이 높아 그래서 뇌가 이제 이렇게 실내 수가 많이 생기는데 공부를 하면 이게 많이 생기고 안 하면 점점 사라진대 또 어때요?
어쨌든 뇌를 많이 쓰면 이게 이제 생기고 안 쓰면 없어지고 그래서 학습하고 기억하는 거 지금 여러분 공부하는 것도 이걸 만들고 있는 중이에요.
그렇죠 그리고 이게 이제 자야지 계속 남아 있는데요.
그래도 잠을 자야지 잠을 계속 새면 안 되는지 그래요.
어쨌든 이게 이건 뉴럴 네트워크이고 이거 사실 운동 신경 운동 세포 그다음에 감각 세포 이런 거 있잖아요.
그것도 다 누런들이에요. 사실은 그래서 어쨌든 자전거 타는 것도 이런 식으로 다 배우는 거고 일단 걔네 말고 손해도 있고 그래요.

참석자 1 26:24
재미있지만 그냥 넘어가면 다 이제 여기 보면은 이제 여기 이거는 여러분 사람의 신경 휴먼 유발 유라 에서 휴먼이나 아니면 동물도 다 있겠죠.
근데 아티피셜 리브레트 네트워크라는 거는 이제 이것도 줄여서 ann이라고 부르거든요.
여러분 ann aln이라고 불러요. 그래서 이제 DNN 말고 ann이라고 부르는 사람들 많아요.
ann 이것도 외워요. 알겠죠 알아야 돼요. ann ann도 왜 눈물이 안 올지다 이렇게 나오겠네요.
aln이 인공신경망인데 뭐냐면은 이게 뉴런에 따라서 혈압 뉴런에 따라서 이제 사실 만든 거죠.
프로그래밍 사실 사람 이런 따르지 않고 원래 이렇게 만들었다가 사람이랑 비슷해서 이렇게 치면 변하고 있는데 그래서 이 책에서는 전혀 안 나와요.
이 책은 그런 거 이 책은 이 사람은 또 비유하는 거 되게 별로라고 그래가지고 책에는 이런 거 사람 얘기 하나도 안 나오거든요.
이 책은 근데 나는 그게 별로라서 또 이거 넣었어요.

참석자 1 27:16
그래서 중요한 게 뭐냐면은 여기 이제 여기 이건 뉴런 이게 뉴런인데 뉴런에 여러 가지 여러 유런이 연결돼 있다고 그랬잖아요.
아까 몇 개 했어요. 여기 천에서 만 개 붙어 있댔잖아요.
그래서 여러분이 이제 예를 들어서 내가 이렇게 여기 감각을 했어요.
손이 감각을 했어 눌렀잖아요. 누른 걸 여기서 알잖아요.
그쵸 아는데 이게 여기 쫙 타고 오는 거예요. 근데 이게 만약에 여기서 중간에 끊어지잖아요.
그럼 안 오겠지 그쵸 그리고 이제 여기서 지금 뾰족한 거 누르면 제가 뭔가 반응을 해서 이제 뭔가 좀 위기 신호가 발동해야 되잖아요.
그쵸 뜨거운 거 누르거나 그런 것들이 다 감각이 증폭되는 거잖아요.
사실은 그래서 이게 여기서 막 오는 신호 중에서 어떤 거는 무시해야 되고 어떤 거는 가야 돼요.
그쵸 근데 사실 여러분이 지금 시간이 별로 안 남았다 이게 재미있으니까 할게요.

참석자 1 28:05
계속 그러면은 여러분 여기서 아무도 돌아다니고 있지 않죠 노래 부르고 있지도 않고 근데 사실 여러 노래 부르고 싶은 욕구도 있고 어떤 쪽에서는 막 어떤 이런 담배하고 막 노래 불러야지 어떤 데 돌아다니고 싶어 자고 싶어 이러는데 지금 참으면서 여기 앉아 있잖아요.
사실 차단하고 있는 거지 그게 여러분 어떻게 지금 이 나이 여기 우리 대학교 입학하려면 그거 안 하면 입학을 못해요.
돌아다니는 사람이 입학을 못해 걸르잖아 우리가 이해돼요.
여러분 그런 게 다 이제 여기 이렇게 오는 신호 중에서 일로 보내는 거 있죠 다른 데로 보내는 거 그러니까 아까 막 돌아다니고 싶은 욕구는 다 차단시켜버리는 거지 그리고 지금 나의 말을 듣는 거는 열심히 전파시키고 있는 거잖아요.
이해돼요. 여러분 자전거를 타면서 이제 일로 가는 거는 차단시키고 저기로 가는 거 강화시키고 이런 걸 학습을 시키는 거죠.

참석자 1 28:50
그래서 이게 중요한 게 뭐냐면은 신호들이 오는 거에 신호들이 오는데 여기 지금 원래 값들이 올 거 아니에요?
이게 신호들이 신호들이 올 거 아니에요? 그렇죠 신호들을 어떤 거는 0으로 만들어야 되고 어떤 거는 그대로 보내야 되고 이럴 거 아니에요?
그쵸? 여기 지금 이게 곱하는 걸 비슷하잖아 그 웨이트라고 그러는 거죠.
그쵸 제가 여러분 주소 적을 때는 5표 0.05만 곱한다니까 진짜 만들 거예요.
굳이 0.5만 곱하는 거지 그거를 100으로 만들어 버리면 이상했잖아요.
수동을 열심히 해야겠지 아무것도 안 하고 그렇죠 아니 그런 식으로 이제 뭔가 중요한 거는 열심히 여기 세게 만들고 다 여기 값을 가벼운 파라미터를 하고 있다고요.
지금 우리가 CS를 연결하고 안 하고가 다 그런 거라고요.
신입스로 연결했는데 여기 얘기 듣고 공부했다라 홍보할 때마다 이렇게 듣고 하는데 이거를 강화시켜야지 이거를 약하게 만들어야지 이러고 있는 거라고요.
여러분이 알겠죠.

참석자 1 29:40
그리고 그게 웨이트 값을 학습시키는 거랑 비슷하다고 그리고 또 중요한 거는 그냥 그런 다음에 이제 다 오르는 신호들을 다 합해요.
다 해 다 합한 다음에 그거를 또 여기다 이걸 적당한 값으로 이제 뭔가 트랜스포드를 뭔가 변환을 시켜야 되는데 그거를 이제 액티베이션 활성화 함수라고 불러요.
활성화 함소가 너무 신호가 이제 좀 미미한 거는 그냥 또 없애버리는 게 나을 수도 있고 그렇죠 어떤 신호는 더 증폭시킬 수도 있고 그렇죠 어떤 바운더리 안에 넣어야지 모든 신호가 다 크게 하면은 이제 어떻게 되냐면은 자폐 여러분 알아요.
아니면은 조울증 신호가 막 이상하게 갑자기 막 갑자기 귀에 뭔가 환청이 들리고 정신이 하나도 없는 거예요.
왜냐하면 그게 차단이 안 되니까 그게 액티베이션 포션이 제대로 되는 게 아니고 그래서 약을 먹으면 좋아지는 것도 있는 거지.

참석자 1 30:26
진짜로 그래서 이게 어쨌든 인공신경망 그러니까 ADHD도 그래 ADHD도 지금 뭐냐면은 이게 이 활성 감소가 제대로 안 돼가지고 그냥 막 하고 싶은 대로 하는 거죠.
그렇죠 나는 마음은 이제 하지 말라고 그러는데 이쪽에서 판단이 안 되고 막 움직이고 난리가 아닌 거죠.
그렇죠 점점점 이제 나이가 들면 학습이 돼가지고 멀쩡해지는 거지.
그렇죠 그래요. 50분이 다 돼 버렸다. 어쨌든 다음 시간에 이어서 어쨌든 여기 약간 긴 테일에서 잡는 거니까 더 할게요.
여러분.


clovanote.naver.com