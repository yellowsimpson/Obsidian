
```python
output = relu(dot(W, input) + b)

```
__W(weight): 가중치, 훈련되는 파라미터

훈련 반복 루프
1. 훈련 샘플 x와 이에 상응하는 타깃 y_true의 배치를 추출한다.
2. x를 사용하여 모델을 실행하고(정방향 패스), 예측 y_pred를 구한다.
3. y_pred와 y_true의 차이를 측정하여 이 배치에 대한 모델의 손실을 계산한다.
4. 배치에 대한 손실이 조금 감소되도록 모델의 모든 가중치를 업데이트한다.
-> 즉 예측 y_pred와 y_true의 오차가 매우 작아질것

- 2.4.1 도함수란?
미분 가능한 함수에서 기울기
```
도함수(Derivative)
	== 변화율
	==gradient
	==기울기
	==경사
```
- 2.4.2 텐서 연산의 도함수: 그레이디언트

```
W1 = W0 - step * grad(f(W0), W0)
```
*W1 = 미래의 W*
*W0 = 과거의 W*
*step == running rate

- 2.4.3 __확률적 경사 하강법 (SGD == stocahstic gradient desent)__
함수의 최솟값은 도함수가 0인 지점이다.

```
grad(f(W), W) = 0
#이 식은 N개의 변수로 이루어진 다항식이다. N: 모델의 가중치 갯수
```
**그레이디언트의 반대 방향으로 가중치를 업데이트하면 손실이 매번 조금씩 감소한다**
1. 훈련 샘플 배치 x와 이에 상응하는 타깃 y_true를 추출한다.
2. x로 모델을 실행하고 예측 y_pred를 구한다.
3. 이 배치에서 y_pred와 y_true 사이의 오차를 측정해서 모델의 손실을 계산한다.
4. 모델의 파라미터에 대한 손실 함수의 그레이디언트를 계산한다. __(역방향 패스 = backward pass)
5. 그레이디언트의 반대 방향으로 파라미터를 조금 이동시킨다. 예를 들어 **W -= learning_rate * gradient** 처럼 하면 배치에 대한 손실이 조금 감소할 것이다. __학습률은 경사 하강법 과정의 속도를 조절하는 스칼라값__ 이다.

![[스크린샷 2025-03-15 오후 7.24.20.png]]

![[스크린샷 2025-03-15 오후 7.24.28.png]]
-> 점점 모델의 최적화를 위해 W값을 변경해 찾아가는 과정
__최적화 방법 == 옵티마이저(optimization method)__
![[스크린샷 2025-03-15 오후 7.35.16.png]]

```python
past_velocity = 0
momentum = 0.1.  #모멘텀 상수
while loss > 0.01:  #최적화 반복 루프
	w, loss, gradient = get_current_parameters()
	w = w + momentum * velocity - learning_rate * gradient
	past_velocity = velocity
	update_parameter(w)
```

- 2.2.4 도함수 연결: 역전파 알고리즘
역전파는 간단한 연산의 도함수를 사용해서 복잡한 연산의 gradient 를 쉽게 계산하는 방법이다.

```python
loss_value = loss(y_true, softmax(dot(relu(dot(inputs, W1) + b1), W2) + b2))
```

연쇄 법칙 
```python
def fg(x):
	x1 = g(x)
	y = f(x1)
	return y
```

```python
def fghj(x):
	x1 = j(x)
	x2 = h(x1)
	x3 = g(x2)
```

- 계산 그래프를 활용한 자동 미분

![[스크린샷 2025-03-16 오전 12.19.57.png]]

![[스크린샷 2025-03-16 오전 12.20.10.png]]

![[스크린샷 2025-03-16 오전 12.20.25.png]]

![[스크린샷 2025-03-16 오전 12.46.10.png]]





