__텍스트 벡터화__ : 텍스트를 수치 텐서로 바꾸는 과정
1. 텍스트 표준화
		-> 소문자로 바꾸거나 구두점을 제거하는 행위
2. 텍스트를 토큰화 단위로 분할
		->  문자, 단어, 단어의 그룹으로 분할
3. 인덱싱(indexing)
		-> 각 토큰을 수치 벡터로 변경

![[IMG_0077 2.jpg]]

- 11.2.1 텍스트 표준화
머신러닝은 'i'와 'I'를 구분하지 못함 그래서 학습을 잘 시키기 위해 여러 기법을 사용함
1. 텍스트 표준화: 모델이 인코딩 차이를 고려하지 않도록 이를 제거하기 위한 기초적인 특성 공학의 한 형태
	-> 대문자를 소문자로 바꿈, 구두점 문자를 삭제
2. 어간 추출(stemming): (동사의 여러 형태 was staring, stared 같은)어형이 변형된 단어를 공통된 단어로 표현
```
"sunset came. i was staring at the Mexico sky. isnt nature splendid??"
"sunset came; I stared at the Mexico sky. isn't nature splendid?"
```

- 11.2.2 텍스트 분할(토큰화)
토큰화: 텍스트를 표준화한 후 벡터화할 단위(토큰)으로 나누는 과정

1. __단어 수준 토큰화__: 토큰을 공백으로(또는 구두점으로) 구분된 부분 문자열로 나눔
	-> 'staring' => 'star + ing'
	-> 'called'   => 'call + ed'
2. __N-그램(N-gram) 토큰화__: 토큰이 N개의 연속된 단어 그룹
	-> 'the cat' => 2-그램 토큰(바이그램(bigram))
3. 문자 수준 토큰화: 각 문자가 하나의 토큰
	-> 텍스트 생성이나 음성 인식 같은 특별한 작업에서 사용

텍스트 처리 방법 2가지
- 시퀀스 모델(sequence model): __단어의 순서__ 로 고려
		-> 단어의 수준 토큰화
- BoW(Bag of Words model): __단어를 집합__ 으로 고려 (순서는 무시)
		-> 가방에 단어를 넣는 느낌
		-> N-그램 토큰화

- 11.2.3 어휘 사전 인덱싱
텍스트를 토큰으로 나눈 후 각 토큰을 __수치 표현으로 인코딩__ 해야됨
-> 훈련 데이터에 있는 모든 토큰을 인덱스(__어휘 사전(vocabulary)__)를 만들어고 어휘 사전 항목에 고유한 정수를 할당해서 사용

어휘 사전에 인덱스 되지 않은 새로운 토큰이 없을 수 있다.
-> 이런 경우 __예외 어휘 OOV(Out Of Vocabulary)__ 를 사용한다. => *OOV 인덱스는 일반적으로 1 이다.*
			__== [UNK] (Unkwon)
*참고사항:  인덱스 0은 마스킹 토큰(masking) 이다.

OOV : 인식할 수 없는 토큰
UNK : 단어가 아니라 무시할 수 있는 토큰

- 11.2.4 TextVectorization층 사용하기

```python
import string

calss Vectorizer:
	def standardize(self, text):
		text = text.lower()
		return "".join(char for char in text
						if char not in string.puncutation)

	def tokenize(self, text):
		return text.split()

	def make_vocabulary(self, dataset):
		self.vocabulary = {"": 0, "[UNK]": 1}


		for text in dataset:
			text = self.standarize(text)
			tokens = self.tokenize(text)
			for token in tokens:
				if token not in self.vocabulary:
					self.vocabulary[token] = len(self.vocabulary)

		self.inverse_vocabulary = dict(
		(v, k) for k, v in self.vocabulary.items())

	def encode(self, text):
		text = self.standardize(text)
		tokens = self.tokenize(text)
		return [self.vocabulary.get(token, 1) for token in tokens]

	def decode(self, int_sequence):
		return " ".join(
		self.inverse_vocabulary.get(i, "[UNK]") for i int_sequence)

vectorizer = Vectorizer()
dataset = [
	"I write, erase, rewrite",
	"Erase again, and then",
	"A poppy blooms",
]
vectorizer.make_vocabulary(dataset)
```




