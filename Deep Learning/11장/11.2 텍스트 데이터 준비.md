__텍스트 벡터화__ : 텍스트를 수치 텐서로 바꾸는 과정
1. 텍스트 표준화
		-> 소문자로 바꾸거나 구두점을 제거하는 행위
2. 텍스트를 토큰화 단위로 분할 
		->  문자, 단어, 단어의 그룹으로 분할
3. 인덱싱
		-> 각 토큰을 수치 벡터로 변경

![[IMG_0077 2.jpg]]

- 11.2.1 텍스트 표준화

머신러닝은 'i'와 'I'를 구분하지 못함

- 11.2.2 텍스트 분할(토큰화)
토근화:  텍스트를 표준화하고 나면 벡터화할 단위(토큰)으로 나눠야한다.

1. __단어 수준 토큰화__: 토큰을 공백으로 구분된 부분 문자열로 나눔
	-> 'staring' => star + ing
2. __N-그램(N-gram) 토큰화__: 토큰이 N개의 연속된 단어 그룹
	-> 'the cat' => 2-그램 토큰
3. 문자 수준 토큰화: 각 문자가 하나의 토큰
	-> 텍스트 생성이나 음성 인식 같은 특별한 작업에서 사용

텍스트 처리 방법 2가지
- 시퀀스 모델(sequence model): 단어의 순서를 고려
		-> 단어의 수준 토큰화
- BoW(Bag of Words model): 단어를 집합으로 고려 (순서는 무시)
		-> 가방에 단어를 넣는 느낌
		-> N-그램 토큰화


- 11.2.3 어휘 사전 인덱싱

- 11.2.4 TextVectorization층 사용하기

```python
import string

calss Vectorizer:
	def standardize(self, text):
		text = text.lower()
		return "".join(char for char in text
						if char not in string.puncutation)

	def tokenize(self, text):
		return text.split()

	def make_vocabulary(self, dataset):
		self.vocabulary = {"": 0, "[UNK]": 1}


		for text in dataset:
			text = self.standarize(text)
			tokens = self.tokenize(text)
			for token in tokens:
				if token not in self.vocabulary:
					self.vocabulary[token] = len(self.vocabulary)

		self.inverse_vocabulary = dict(
		(v, k) for k, v in self.vocabulary.items())

	def encode(self, text):
		text = self.standardize(text)
		tokens = self.tokenize(text)
		return [self.vocabulary.get(token, 1) for token in tokens]

	def decode(self, int_sequence):
		return " ".join(
		self.inverse_vocabulary.get(i, "[UNK]") for i int_sequence)

vectorizer = Vectorizer()
dataset = [
	"I write, erase, rewrite",
	"Erase again, and then",
	"A poppy blooms",
]
vectorizer.make_vocabulary(dataset)
```




