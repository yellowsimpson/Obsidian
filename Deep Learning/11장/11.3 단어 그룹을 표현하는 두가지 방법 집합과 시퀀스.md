
머신러닝 모델이 개별 단어를 어떻게 표현해야 하는지는 비교적 논쟁이 없음.
하지만 단어를 문장으로 구성하는 방식인 단어 순서를 인코팅하는 방법이 중요!!

단어의 순서를 결정하는 방법
1. Bow(Back of Word): 순서를 무시하고 텍스트를 단어의 __집합__ 으로 처리하는 것
2. RNN, 트랜스포머: __단어의 순서를 고려 (시퀀스 모델)

- 11.3.1 IMDB 영화 리뷰 데이터 준비하기
- 11.3.2 단어를 집합으로 처리하기: BoW 방식

인코딩 하는 방법 2가지
1.개별단어(유니그램)
2.연속된 토큰 그룹(N-그램)


TextVectorization층 사용하기




```python
text_only_train_ds = train_ds.map(lambda x, y: x)
text_vectorization.adapt(text_only_train_ds)
binary_1gram_train_ds = train_ds.map(
	lambda x, y: (text_vectoriztion(x), y)
	num_parallel_calls = 4)
text_vectorization.adapt(text_only_train_ds)
binary_1gram_val_ds = train_ds.map(
	lambda x, y: (text_vectoriztion(x), y)
	num_parallel_calls = 4)
text_vectorization.adapt(text_only_train_ds)
binary_1gram_test_ds = train_ds.map(
	lambda x, y: (text_vectoriztion(x), y)
	num_parallel_calls = 4)
```


[코드11-7 바이그램을 반환하는 TextVectorization 층 만들기]
```python
text_vectorization = TextVectorization(
	ngrams=2,
	max_tokens=20000,
	output_mode='여기 오는거' # 1.multi_hot, 2.count, 3.tf_idf
)
```


TF-IDF(Term Frequency-Inverse Document Frequency) 정규화: TF-IDF는 '단어 빈도-역문서 빈도'


- 11.3.3 단어를 시퀀스으로 처리하기: 시퀀스(RNN) 모델 방식

[코드11-12 정수 시퀀스 데이터셋 준비하기]
```python
from tenworflow.keras import layers

max_length = 600   #단어 600개
max_tokens = 20000 #사전에서 잘 쓰이는 단어 20,000개만 사용
text_vectorization = layers.TextVectorization(
	max_tokens = max_tokens,
	output_mode = 'int',
	output_sequence_lenght = max_length, #600개 단어 이후는 잘라버림
)

text_vectorization.adapt(text_only_train_ds)
int_train_ds = train_ds.map(
	lambda x, y: (text_vectorization(x), y),
	num_parallel_calls = 4)
int_val_ds = train_ds.map(
	lambda x, y: (text_vectorization(x), y),
	num_parallel_calls = 4)
int_test_ds = train_ds.map(
	lambda x, y: (text_vectorization(x), y),
	num_parallel_calls = 4)
```

[코드11-13 원-핫 인코딩된 벡터 시퀀스로 시퀀스 모델 만들기]
```python
import tensorflow as tf

inputs = kears.Input(shape=(None,), dtype='int64') #입력은 정수 시퀀스
embeded = tf.one_hot(inputs, depth=max_tokens) #정수를 20,000차원의 이진 벡터로 인코딩
x = layers.Bidirectional(layers.LSTM(32))(embedded) #양방향 LSTM 층을 추가
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation = "sigmoid")(x)
model = keras.Model(inputs,outpus)
model.compile(optimizer = "rmsprop",
			  loss = "binary_crossentropy",
			  metrics =["accuracy"]
)
model.summary()
```

[코드11-13 원-핫 인코딩된 벡터 시퀀스로 시퀀스 모델 만들기]
```python
import tensorflow as tf

inputs = kears.Input(shape=(None,), dtype='int64') #입력은 정수 시퀀스
embeded = tf.one_hot(inputs, depth=max_tokens) #정수를 20,000차원의 이진 벡터로 인코딩
x = layers.Bidirectional(layers.LSTM(32))(embedded) #양방향 LSTM 층을 추가
x = layers.Dropout(0.5)(x)

outputs = layers.Dense(1, activation = "sigmoid")(x)
model = keras.Model(inputs,outpus)
model.compile(optimizer = "rmsprop",
			  loss = "binary_crossentropy",
			  metrics =["accuracy"]
)
model.summary()
```

