2종 분류 = 이진 분류

- 4.1.1 IMDB 데이터셋
-> 영화 데이터베이스 (리뷰 5만개로 이루어져 있음)
-> train_data : 25,000 test_data: 25,000
-> MNIST 데이터셋 처럼 IMDB 데이터셋도 케라스에 포함

[코드 4-1  IMDB 데이터셋 로드하기]
```python
from tensorflow.keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
	num_words=10000)  #데이터 10000개
```
__OOV(Out Of Vocblury): 사전에 없는 단어는 뺌


- 4.1.2 데이터 준비
신경망에 숫자 리스트를 바로 주입 할 수 없다.

<리스트를 텐서로 바꾸는 방법 2가지> *시험!!*
1. 같은 길이가 되도록 __리스트에 패딩(padding)을 추가하고 (samples, max_length) 크기의 정수를 텐서로 변환__ 한다.  *(Embedding층) 보통 이 방법을 많이씀
2. 리스트를 __Multi-hot Encoding__ 해서 0과 1로 변환한다. (노다가 방법)
		*-> hot이 의미하는게 1, cold가 의미하는게 0
		->여기선 단어가 등장하면 1, 안하면 0으로 표시

[코드 4-3 정수 시퀀스를 멀티-핫 인코딩으로 인코딩하기]
```python
import numpy as np
def vectorize_sequences(sequences, dimension=10000):  #10,000개 데이터
	results = np.zeros((len(sequences), dimension))   
	for i, sequence in enumerate(sequences):
		for j in sequence:
			results[i, j] = 1.   #소수로 할려고 1.를 붙여놈
	return results

x_train = vectorize_sequences(train_data) #25,000개
x_test = vectorize_sequences(test_data)   #25,000개
```

- 4.1.3 신경망 모델 만들기
Dense 층을 쌓을 때 필요조건
1. 얼마나 많은 층을 사용할 것인가?
2. 각 층에 얼마나 많은 유닛을 둘 것인가?

![[스크린샷 2025-02-18 오전 10.44.13.png]]

![[스크린샷 2025-02-18 오전 10.56.59.png]]![[스크린샷 2025-02-18 오전 10.57.22 1.png]]

[코드 4-4 모델 정의하기]
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
	layers.Dense(16, activation="relu"),
	layers.Dense(16, activation="relu"),
	layers.Dense(1, activation="sigmoid") #확률만 뽑아내니까 sigmoid
	                                      #값이 1개만 나와야 되니까 1
])
```
-> 보통 __이진 분류 문제__ 에서는 
*layers.Dense(1, activation="sigmoid")* 로 많이함

[코드 4-5 모델 컴파일하기]
```python
model.compile(optimizer="rmsprop",
			  loss="binary_crossentropy",
			  metrics=["accuracy"])
```

- 4.1.4 훈련 검증
[코드 4-6 검증 세트 준비하기]
```python
x_val = x_train[:10000]    #25,000개 데이터 중 앞에 10,000개를 val에 사용
partial_x_train = x_train[10000:]  #뒤에 10,000개 데이터를 train에 사용
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
```

[코드 4-7 모델 훈련하기]
```python
history = model.fit(partial_x_train,
					partial_y_train,
					epochs=20,
					batch_size=512,
					validation_data=(x_val, y_val))
```


#history(변수 이름) .history(이게 중요!!!)

[코드 4-8 훈련과 검증 손실 그리기]
```python
import matplotlib.pyplot as plt

history_dict = history.history #history(변수 이름) .history(이게 중요!!!)
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()
```



[코드 4-9 훈련과 검증 손실 그리기]
```python
plt.clf()
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]
plt.plot(epochs, acc, "ro", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()
```



[코드 4-10 모델을 처음부터 다시 훈련하기]
```python
model = keras.Sequential([
	layers.Dense(16, activation="relu"),
	layers.Dense(16, activation="relu"),
	layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="rmsprop",
			loss="binary_crossentropy",
			metrics=["accuracy"])

model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)
```


```python
model.predict(x_test)
-----------------------------------------
782/782 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step

array([[0.1602 ],
       [0.999  ],
       [0.5825 ],
       ...,
       [0.06866],
       [0.04468],
       [0.4067 ]], dtype=float16)

```

