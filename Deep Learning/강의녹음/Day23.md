딥러닝 day23_1
2025.06.04 수 오전 10:04 ・ 46분 15초
심승환


참석자 1 00:00
다 없는 거나 마찬가지고요. 내용이 14장은 그냥 관상 같은 것들이고 13장에서는 재밌는 거는 별로 그러니까 프리시전이잖아요.
프리시전 정밀도별로 플로팅 포인트가 30이냐 10년에 따라 그런 것들은 한번 비슷할 약간 보라고 그러고 여기 12장이 11장 12장이 거의 핵심 내용이에요.
거의 비슷해서 지금 11장 하고 있으니까 다음 주도 있잖아요.
그렇죠 우리가 대충 처음에 다 끝낸다 그래서 그래서 지금 또 문제는 제가 이거 교과서 저도 이제 처음에는 잘 모르다가 점점 절 하면서 이게 지금 빨리 지금 진도를 나가는 방법 개발을 해서 지금 지금 내가 뭐냐면 저번 시간에 여기 12장 2절까지 한 셈이에요.
그쵸? 요거 13절 들어가면서 지금 약간 전체적으로 훑었거든요.

참석자 1 00:57
여러분은 지금 갑자기 당황 근데 이게 이게 여기 14 4종 내 사장 내용이 사실 지금 잠깐 간단하게 나와 있지만 교과서에서 지금 예를 들어서 전의 학습이 CNN d에 조금 나와 있지만 되게 중요하다 그랬잖아요.
마찬가지로 지금 여기 여기쯤 나올 수밖에 없지만 이게 되게 중요해요.
지금은 지금은 제일 이게 핫하고 제일 잘 되고 인사이트가 많이 녹아 들어가 있어서 여러분 허빙 페이스 이런 거 좋은데 이런 거 쓰면 다 이거 갖고 쓰는 거니까 그 세트 PT도 다 이거잖아요.
그쵸 딥cq고 뭐고 다 이거거든요. 그리고 NVIDIA 주가가 높은 것도 그래요.
그래서 이 내용이 좀 중요해서 제가 슬라이드 따로 만들었거든요.
이걸로는 너무 잘 이해하기 힘든 것 같아서 작년까지 그냥 책으로만 하다가 올해 새로 해봤고 그거 내용을 하면서 이렇게 다 나가겠다고 지금 야심찬 계획을 가지고 있죠.
그래서 질문할 거 많이 하시고요.

참석자 1 01:57
여러분 어쨌든 지금 저는 이제 이게 우리가 책을 다 꼼꼼히 이렇게 보려고 하지는 않잖아요.
저도 이제 중요한 내용만 보려고 하니까 지금 뭐 하고 있는지 다시 얘기하면 우리는 지금 텍스트 처리하는 내용을 하고 있는데 어쨌든 지금 우리 AI에서 지금 제일 핫한 게 생성형 AI 쪽이고 그중에서도 LM 쪽이에요.
랭기즈 모델 쪽이에요. 저번에 제가 구글 노트북 LM 소개해 줬죠.
근데 LM이 뭐냐 하면 이제 딱 알아먹어요. 그쵸 LM이 뭐야?
랭기즈 모델 알겠죠? 구글 LM이잖아. 그게 LM이 랭기즈 모델인지 아는지 모르는지에 따라서 이제 식자가 전공을 하는 사람은 LM이 이제 랭귀지 모델인지 알 것이고 전용 안 하는 사람은 LM인지 뭔지 모르게 하는 거지.
그쵸 알겠죠. 지금 얘기하고 싶은 거는 이제 랭귀지 모델이 중요하고 랭귀지 모델은 트랜스포머다.
다른 것도 이렇게 나왔지만 다 결국은 이것만은 못해요.
알겠죠 그래서 거기를 좀 약간 인사이트를 기르면 좋겠어요.
알겠죠 그래요.

참석자 1 03:01
이용자라고 할지라도 반도체도 진출할 수 있고 이렇게 하려고 그러면은 그런 측면에서 여러분 다시 한 번 이 트랜스포머가 중요하고 지금 저도 지금 아까 간 보면서 어떻게 진행할지 약간 좀 왔다 갔다 한번 해보려고 그래요.
이거를 완전히 탑다운으로 이걸 그대로 쫙 가버리면은 문제는 이제 나무밖에 못 보거든 쉽게 몰라서 랭귀지 모델이 하는지 지금 내가 트랜스포머가 랭귀지 모델인지도 모르고 그러고 살기 때문에 지금 자꾸 이러고 있어요.
알겠죠 일단 텍스트를 위한 텍스트가 중요한 게 아니고 트랜스포머가 중요한 거예요.
일단 알겠죠 트랜스포머는 비전 쪽에서도 쓰이고 저번에 올해 CES라고 여러분 아나 CES 컨시블리 트랜스 쇼예요.
가전 쇼야 가전 쇼 이게 이제 변질돼가지고 점점점 뭔가 전 세계의 체크를 주도하는 그런 뭔가 병영장이 돼버려서 CES의 가정 쇼의 NVIDIA가 맨날 나와가지고 주가조작 키 입고 나와가지고 올해 설치고 막 작년에는 현대자동차가 설치고 어쨌든 그런 게 있어요.

참석자 1 04:02
여러분 그러니까 라이카 CES가 거의 지표예요.
그전에 한 10년 전에는 CES에서 뭔가 키워드 발표할 때마다 막 난리 났거든요.
이번에 운영 체제 다 이러면 운영 체제 되고 이런 식이에요.
지금은 이제 GPU 약간 AI죠 그래서 지금은 우리는 그런데 거기서 MB 제니스랑이 나와가지고 뭐라 그랬냐면은 여기 모든 것은 토큰화 된다 이런 얘기를 했어요.
그러니까 이게 지금 우리가 여기서 우리가 일단 제가 여러분한테 11점 2절 테스트 이제 준비할 때 기본적으로 이제 교과서 사회 실체 쪽 할 때 이 내용을 했잖아요.
이렇게 텍스트가 있으면 텍스트를 표준화 스탠다데이션시켜서 이상한 거 배우자 소문자 이런 것들을 구분해 없애버리고 그죠 그다음에 토크 토크를 만들잖아요.
그쵸 토크나이제이션을 하지 그쵸 그리고 토크는 무조건 인덱스를 맺을 수밖에 없고 그다음에 이거를 이제 인코딩을 하죠.
벡터 빅터라이제이션을 하잖아요. 그렇죠 이런 거를 일종의 봤잖아요.

참석자 1 05:04
그쵸 근데 이제 그 NVIDIA CSA 제가 이런 얘기를 자꾸 하는 이유는 교과서대로 공부하지 말고 약간 좀 여러분 지금은 AI 그랑 싸워봐야지 여러분이 정말 여러분 저는 진짜 제가 생각하고 있던 중요한 사항이 뭐냐면은 여러분은 경쟁자가 AI야 저는 AI가 저의 도우미고요.
여러분은 사실 도우미가 아니라 경쟁자이기도 해요.
왜냐하면 저는 지금 어느 정도 어쨌든 미안해요. 기득권자가 돼버렸고 그렇잖아요.
여러분 맨날 심사위원 하라고 난리인데 피해 다니고 막 농업 신산업 만이 하라고 그래서 이해됐죠.
여러분 다 심사 그런데 AI 책임질 수는 없으니까 심사위원을 시키잖아.

참석자 1 05:41
예를 들어서 그런데 여러분한테는 여러분이 심사를 받는 사람이 돼 있잖아 지금은 그런데 이제 뭐 일 시킬 때 여러분 지금 예를 들어서 넥센이나 같은 것도 하면은 AI한테 시킨 거랑 여러분이랑 비교하지 이해돼요.
여러분 마찬가지로 코딩을 시켜도 이 AI를 한 거랑 여러분이 지금 새로 들어온 애랑 AI랑 누가 나랑 이렇게 된다고 그런 면이 있어요.
여러분 그래서 여러분은 AI보다는 AI처럼만 하면 되는 게 아니라 AI 플러스 알파로 하는 게 필요한데 그러기 위해서는 좀 달달달 외우는 거가 별로 아니라는 거지 알겠죠.
자꾸 좀 생각을 해야 돼. 그렇죠. 잘 활용해 먹어야 되고 약간 이렇게 키는 게 되게 필요해요.
여러분 이런 이런 걸 타면서 어떻게 하는 게 더 효율적일까 생각해야지 좀 근덕.
그냥 막 무턱대고 하는 게 내공을 무턱대고 쌓는 게 아니라 효율적으로 쌓아야 돼.
알겠죠.

참석자 1 06:36
그래서 지금 여기서 이게 지금 우리가 자연어 처리를 얘기하고 있지만 제가 자꾸 딴 얘기를 하는 것도 그냥 생각하지 말고 이게 뭐 하는 짓인지 생각해 보라는 거죠.
지금 텍스트만 갖고 얘기하고 있잖아. 이 챕터는 그쵸 텍스트만 토큰으로 뿌리고 이런 거 있잖아요.
근데 이번에 이제 NVIDIA 벤스랑이 나와서 얘기한 기존 CES에서 2025에서 기존 연설한 내용은 뭐냐면은 모든 것이 토큰이 된다 이거였어요.
에브리티 컨스 토큰 모든 것이 토크 토큰인 원래 텍스트만 하는 거였잖아요.
여기는 근데 모든 것이 토큰이라는 거는 세상에 있는 이의의 대상들을 다 토큰화시킨다는 거예요.
추상화 그리고 옛날부터 얘기했던 그 디지털 트윈이라고 하는 거 있잖아요.
그러니까 지금 오프라인 세계에 있는 모든 것을 다 인프딩시켜서 이렇게 이 토큰으로 만들어서 토큰이라는 게 핵심이 뭐예요?
여러분 식별 가능한 무엇인가잖아요. 토큰에서 구분되는 무엇인가잖아 토큰화시킨다는 거야.

참석자 1 07:33
그래서 그렇게 되면 모든 것을 AI로 다 시킬 수 있는 거잖아요.
사실은 모든 인식 가능한 것들은 다 토큰으로 시키는 거잖아요.
센서 액추에이션 이렇게 자동차 쪽도 보고 로봇도 그렇고 마찬가지로 우리가 인식하는 모든 것은 다 상징으로 뭔가 속에서 의미를 부여할 수 있는 뭔가 구분되는 것들이잖아요.
꼭 말만 그런 게 아니지 여러분도 얘기 지금 공부를 더 하고 나면 제가 더 깨달을 텐데 시간 되면 더 떠들어줄게요.
여러분 어쨌든 이게 인사이트를 길러야 돼요. 쿠폰이라는 거는 여기 지금 어쨌든 여기 나오는 것도 시험 보지 잘 보시고 시험 제가 가르친 거 최소한 강조한 건 다 외우시고 여러분 챗gpt한테 물어보면 다 틀려요.
진짜 제가 챗gpt 믿고 AI 공학 계열은 강의 자료 만들었다가 다 틀려가지고 생각해 보니까 다시 지금 그냥 걔를 저도 너무 신뢰해가지고 발표자를 만들었다가 다시 지금 자다가 생각났어 이거 아니구나라고 채팅 때 너무 신뢰했던 거예요.

참석자 1 08:26
그때 그래서 얘기하고 싶은 거는 여기 보면은 일단 여기 토크 나로 넘어갈 때 여러분 달달히 오지 마시라고 이렇게 떨어도 항상 약간 다 문제가 있어요.
항상 지금은 왜냐하면 너무 발전하고 있는 중이라서 이게 지금 더 케이스 온더 메스 되는데 토크화를 시켰잖아요.
토크를 이렇게 토크화를 하는데 뭐 뭐 뭐가 있다고 그랬던가 토크나이제이션의 세 가지 방법이 있었어요.
교과서에 배웠어. 지난 시간에 이거는 진도 제대로 나간 거예요.
뭐였더라 토크 라이제이션이 세 가지가 있었거든요.
여기 418쪽에 있는데 바로 그다음 페이지예요.
이 다음 페이지에 단어 수준이 있고 그다음에 앵그램이 있고 문자 수준이 있고 그랬어요.
그쵸 이렇게 만들 수 있었잖아요. 그쵸 근데 지금 이 토크나를 얘기할 때 지금 단어 수준으로 하는 거는 이제 이렇게 지금 뭔지 어떤 뜻인지 알죠?

참석자 1 09:21
여러분 단어 하나하나가 그쵸 그리고 그게 이제 지금 얘기하는 아까 제스랑 얘기한 거는 저 세상에 다른 것들도 다 패턴화 시키겠다는 거지 이렇게 말뿐만 아니라 여러분 온도 이런 것들 있잖아요.
여러 가지 퓨처들 있잖아요. 예를 들어서 여러분이 클라우드 네이티브 컴퓨팅 소프트웨어 공학 거기서 이제 로키스트레이트를 만드는데 토큰을 뭘로 볼 것이냐 이거지 자연어처리만 보는 게 아니야.
거기에서 모든 상태 시스템 상태 같은 것들이 포트화가 되는 거지 어떤 노드의 상태 어떤 리소스의 사용량 가용량 테스크의 속성 원하는 속성 맞춰야 되는 리콰이먼트 이런 것들이 토크라가 되는 거지 그게 토큰이 그리고 보면 여기 앵글의 토크라도 봤듯이 예를 들어서 우리가 자연어 처리만 해도 이 그림에서는 어떻게 돼요?
여러분 무조건 더 케이스 셋 온더 맵 이렇게만 나오잖아.

참석자 1 10:12
토큰이 그게 아니라 만약에 앵그래을 하면 어떻게 되냐면 버켓이 또 하나의 토큰이 되고 캣셋도 하나의 토큰이고 그쵸 셀롬도 하나의 토큰이고 이런 식으로 그쵸 이게 앵그리즈 바이어그램이라면 만약에 트리 그램이라 그러면은 3개씩 묶어서 그쵸 그렇게 할 수 있는 거지 그러니까 의미 토큰도 여러 개 만들 여러 가지로 만들 수 있는 거잖아.
지금 이해되죠 여러분 꼭 세상에 존재하는 걸 하나씩 북한을 만드는 게 아니라고요.
알겠죠 의미가 다를 수 있으니까 더 예를 들어서 요크랑 뉴욕이랑 다를 수 있잖아.
그리고 또 예를 들어서 뭐가 다를까 다른 거 두 개씩 끊으면 완전히 달라지는 거 맞죠?
사우스 코리아랑 코리아랑 너무 다르지 소스 코리아 소스 코리아랑 다르잖아요.
그쵸. 그러니까 그거를 그냥 코리아로 해놓으면 뭐가 안 되잖아요.
그쵸. 그래서 토큰을 만들 때 바이그램이 필요하기도 하고 그렇죠 그런 거죠.
그쵸.

참석자 1 11:02
그런데 그렇게 토큰을 만들어야 되는 거는 사실은 이 마지막에 벡터라이제이션에서 미리 여러분 탑다운으로 공부하시듯이 나중에 다 외운 다음에 얘기하면 정신 하나도 없을 것 같아.
맨 마지막에 이 원아 인코딩 또는 인베딩하고 적혀 있잖아요.
그렇죠 마지막에 이렇게 되어 있고 실제로 교과서에서 지금 여러분한테 보여준 거는 텍스트 벡터라이션이라는 걸 가르쳐줬잖아요.
제가 여러분한테 지난 시간에 마지막에 텍스트 벡터라이제이션에서는 뭐 뭐 뭐가 된다고 그랬어요.
인테죠 그쵸. 그리고 또 뭐 있었어요 카운트도 있고 tfidf도 있고 막 그랬잖아요.
그쵸 걔는 이거랑 다르잖아요. 또 어떻게 된 거야 그쵸 그쵸 여러분 이상하잖아요.
그쵸 그래서 사실은 이거 공부할 때 이게 사실 저도 그래서 이렇게 이렇게 가르치면 안 된다는 생각이 사실 들었어요.
교과서도 보면은 챕터 제목이 챕터가 아니라 이게 하브 섹션 제목이 이렇게 되어 있는데 여기 이게 핵심이었어요.

참석자 1 11:59
집합하고 시퀀스 단어를 텍스트들을 결국은 테크나이션 시킨 다음에 이거를 이제 실제로 이제 우리가 입력 뭔가 판단을 내리기 위해서 집합으로 볼 거냐 어떤 문장.


clovanote.naver.com

딥러닝 day23_2
2025.06.04 수 오전 11:00 ・ 51분 30초
심승환


참석자 1 00:00
아 요거 인벤딩 드디어 맨날 인벤딩 오베딩 하는데 심지어 여기 사실은 원아 핑크딩 하는 하이 교과서에서 앞에 앞에 우리 여기 뭐야 12 13에서 워낙 또 임베디드라고 불렀죠.
그쵸? 이것도 넓게 보면 임베딩이 이쪽으로 오는데 사실 아무도 그렇게 얘기 안 하는데 이거 오해하지 마시고 여러분 뒤에 이거 임베딩이라고 해.
별로요. 아무도 모르지 모르는 걸로 오해받을 수 있으니까 이건 무시하세요.
여러분 알겠죠? 이거

참석자 1 47:20
텍스트 말고 혹시 이렇게 하단에 써져 있어요 찾았다 4 23쪽에 텍스트 만물 코포스라는 게 나와 있는 잘했어요.
콜리스 트는 데이터를 말뭉치라 부르고 포커스라고 부르는데 이거 모르면 또 이제 제가 딥러닝 수업 들었나 하면 오해를 받을 수 있어서 알겠죠 여러분 알겠죠?
됐죠 훈련에 사용되는 데이터 테스트 데이터를 프로포스라고 불러요.
알겠어요 여러분 훈련에 사용하는 텍스트 데이터를 우리나라 말로는 말뭉치 영어로는 코포스라고 불러요.
중요한 용어인데 제가 이거를 어쩌다가 안 하고 넘어가서 다른 데로 집중하라고 알겠죠 어텐션이 안 돼가지고 알겠죠?
됐어요. 예 그래요. 그다음에 뭐 곤란하니까 제가 굳이 왔어요.
그냥 시험에 낼게요. 보낼 수도 있지만 알겠죠 모르면 이상한 거예요.
알겠죠? 코퍼스가 뭔지 모른다. 말뭉치가 코퍼스인지 모른다.
말뭉치라는 게 뭔지 모른다 안 된다고 알겠죠? 됐어요.

참석자 1 48:20
그래서 다시 얘기하면은 근데 코사인 시밀러리티는 유사일지랑 완벽일지를 그런 구분을 못 하나요 미사일처럼 매개치자는 게 영 건마 4랑 0건마 5의 데이터가 하나씩 있다고 치면 그 두 개는 동일 데이터와 비교한 것과 동일하게 코사인 0도니까 1로 나오는 거 아닌가요?
다시 다시 다시 얘기해 봐. 무슨 얘기인지 0콤만 사람 다시 요거 어디 어디 어디 볼까?
거기서 오렌지 위에 네 영콤마 5 부분에 영 콤마 5 부분이 어디지 프로피스 탱크에는 여기 푸니스가 5인 부분이 있으면 여기 만약에 여기 여기 이런 게 있어 네 그러면 코사인으로 비교하면 똑같아 똑같아요.
이거 약간 프레스가 더 강한 거 있죠 그래도 상관없어.
서로 하나도 안 닮았으면 그냥 다 안 닮은 걸로 끝나면 되지 더 정도가 높다고 해서 그럴 건 없지 사실은 그렇죠 그래요.
끝이야 진짜로 그게 끝이에요.

참석자 1 49:21
점검을 해도 0이랑 곱해보면 다 사라져 점검을 해버려도 0이랑 곱해버리면 사라져요.
이쪽이 하나도 없는 놈이랑 곱해버리면 사라져요.
근데 약간은 있는 놈이라고 하면 이게 더 살아나지 표상이 쪽에서는 그럼 어떠냐 각도가 똑같죠.
근데 그때는 똑같을 수밖에 없잖아요. 그쵸 그거는 사라지는 거 맞고요.
그래서 그게 싫어가지고 여기는 사실은 스케일 라프로젝트를 쓰는 거네 사실은 그렇죠 좋네요.
좋은 얘기다. 이해준 님 제가 잠 한 번 했어요. 플라스 있죠 그래서 맞네.
코산 시뮬러리티보다 스케 다프라트가 그게 좋네 같은 특수성이 약간 더 큰 거에 대해서 더 살아나는 정보가 코 사이는 그게 사라지는데 다 좋은 점은 전부 다 완벽하게 그냥 마이너스 1 하고 이 사이로 만드는 거 좋은데 희생이 있네.
스케일 자 프로트는 그냥 무조건 벡터 크기로만 나누니까 숫자는 작아지는데 어느 정도 그렇죠 그렇죠 알겠죠 좋은 질문이었어요.

참석자 1 50:17
60분데 그렸네. 그래요. 좋은 질문이었어요.
그래서 이거를 이게 어디션 스코어라는 거 일단 알고 있고 이게 어쨌든 원래는 이게 두 개 따로따로 그려야 되거든요.
어디 스코어를 이 이거랑 이거를 쓸 때 이거 사실 코퍼스는 아니고 이거 학습된 상태에서 보통 학습 상태도 이렇게 하고 어쨌든 코퍼스가 뭔지는 알았죠 여러분 어쨌든 이게 어텐션을 항상 구해야 되거든요.
이게 셀프 어텐션을 하나 하기 위해서 어텐션 스법을 만드는 거였어야겠죠.
여기 여기 있는 거 이 표는 어떻게 나오느냐 지금 있는 그 지금 원래 있는 문맥에서의 그 단어들만 나온 거예요.
이 내가 지금 생각하고 있는 문맥 킹 받는다고 그러면 힘 받는 다도 여기 다 있는데 킹 이시연은 지금 없다고 이시연은 없는 거잖아요.
그렇지 알겠죠. 그리고 사실은 이제 나중에 하지만 순서도 정보도 넣어요.

참석자 1 51:01
포지션을 인코딩이라고 그래서 약간 달라 RNN이랑 거기까지 다 해줄게요.
여러분 그리고 여러분 유튜브 이거 번거면 좋지 이 사람 거 한번 보고 와보셔.
아니 안 봐도 돼요. 근데 봐도 된다고 알겠죠. 왜냐하면 이 사람한테 미안해서 그래 내가 다 뺏겨 와가지고 내가 안 만들었어요.
만들려고 그러다가 너무 좋아지면서 다 갚았어 이해되죠?
여러분 그래요. 질문도 많이 하면 플러스 1점 줄게요.
여러분 좋은 질문하면 알겠죠 그래요. 여기까지 할게요.


clovanote.naver.com