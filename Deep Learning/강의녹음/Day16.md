딥러닝 day16
2025.05.12 월 오전 10:00 ・ 49분 50초
심승환


참석자 1 00:00
시아 배재현 송지섭, 신용섭, 김승환 양진영 영준아 이하원 이종원 이병진 이승호

참석자 1 00:24
2에서

참석자 1 00:35
정현우 조서경, 진주혁, 최윤서 박성현. 네. 양진영 네 영준환 김준영 지금 CNN 하고 있죠?
그쵸? CNN 제가 슬라이드 만들어놓은 거

참석자 1 01:05
그래서 용어들이 나왔고 여기서 다 했던 것 같아요.
그렇죠 근데 여기 밑에 있는 설명은 제가 제대로 안 했죠.
그쵸 되게 열심히 적어놨는데 그렇죠 이거가 여러분 여러 부분에서 나와서 여기도 다 다 하긴 했네.
그렇죠 다 나갔다. 여기도 이거는 설명 안 했는데 여기서부터 안 했구나.
여기서부터 그렇죠 24페이지부터 안 했는데 그렇죠 그런 거 있죠 다음과 같아요.
그렇죠 근데 이게 사실 여러 제가 인터넷에서 찾아보다가 제가 설명 그림 안 그리고 이제 잘 그린 그림 다 그냥 모아놓은 거예요.
다 중복된 내용이 되게 많아요. 그 내용이 많지가 않아요.
사실 이거 다 그건 잘 이해되라고 이제 다 갖다 놓은 거예요.
이해되시죠? 여러분 그러니까 너무 막 많은 단임을 나가는 게 아니고 오해하는 걸 방지하기 위해서 이렇게 말한 것뿐이라고요.
여러분 알겠죠? 진도를 막 무시무시하게 나가는 게 아니라는 거죠.

참석자 1 02:12
그래서 뭔지 모르면 곤란하다는 거고 계속 계속 뭐 하고 있냐면은 필터랑 용어 용어들 사실은 필터 커널 이런 용어가 막 섞어 나오잖아요.
그쵸. 그리고 이제 컨볼루션이라는 용어도 나왔고 그쵸 피처 맵이라는 용어도 나왔고 그렇죠 여기다 뭔지 이제 아는 거죠.
여러분들 이제 알아야 되는 거예요. 그런 거 헷갈리지 말라고 했죠.
그래서 어쨌든 원래 피처라는 말을 계속 쓰고 있었는데 이렇게 2차원으로 뭔가 컨버레이션 해가지고 나온 결과를 피츠 맵이라고 불러요.
그쵸 그쵸 그리고 사실 이게 여러분들 이제 원래 원래부터 우리 DNN 쓸 때부터 뭔가 한 번 나오면 끝나는 게 아니라 다시 계층은 딥러닝이 그런 거잖아요.
한 번 하고 또 하고 또 하고 그러는 거잖아요. 그쵸 추상화를 여러 번 쌓는 거잖아.
레이어를 여러 개칭 하는 게 DB잖아요. 한 번 하는 게 아니라 두 번 이상이라고 DB라고 그런다고 그랬잖아요.

참석자 1 03:10
그쵸 피처 맵도 사실은 한 번 더 하겠구먼. 그렇죠 또 하고 또 하고 또 하겠네.
또 콤블로션하고 또 콤블로션하고 이러는 거지. 그래서 이 피처 맵이 다시 입력이 돼요.
이게 인풋 필처가 사실은 이는 피처형으로 봐야 된다고 그러니까 이게 다시 또 또 계층을 또 쌓아주니까 이게 피처 맵이 입력이 된다고 다시 또 인풋 피처가 된다고 아니 다음 계층에서 또 컨버레이션을 하면 또 하고 또 하고 또 하니까 여러분 벌써 교과서에서 교과서에서 일단 API를 대충 봤잖아요.
몇 페이지에 있냐면 교과서 8장에 시작할 때 바로 탁 나오잖아요.
그렇죠 8장에 시작할 때 283쪽에 간단한 컴브넷 만들기 코드 8 1 이거 먼저 구경하면 당연하게 약간 이거 그냥 아직 제대로 나간 건 아닌데 이거 그냥 맨 처음에 딱 주잖아요.
교과서에서 보면은 컴브 2D 컴브 2D 컴브 2D 계속 하잖아요.

참석자 1 04:08
그쵸 이게 인풋이 이제 뭐가 들어올지는 여기 안 적혀 있지만 208 곱하기 20평 곱하기 1로 한 번 이렇게 리스트 같은 이런 이미지일 텐데 그게 인풋 피천인데 컴퓨트기 통과하고 나면은 그걸 피처 맵이라고 부르잖아요.
컨볼루션 레이어라고 부르는데 걔가 다시 입력으로 또 들어가는 거지.
맥스 플레이 2D 통과하고 나서 그쵸 근데 맥스 플레이 2D 없어도 또 그냥 컴퓨트 있으면 상관없거든요.
여러분 이해돼요. 내가 뭔 말하는지 여러분 다시 피처 맵이 다시 인풋 피처가 된다고 그다음에 원래 원래 딥러닝 계속 그랬잖아요.
그쵸 레이어 통과하고 나면 원래 인풋 피처가 바뀌어가지고 다시 또 그렇죠 인프 처가 되고 또 이러니까 그거 똑같은 말이라고요.
알겠죠 혹시나 모를까 봐 강조해 준 거예요. 알겠죠

참석자 1 04:57
그래요. 됐고 그래서 터널이 그리고 패딩이라는 개념이 있다는 거 알죠?
여러분 원래 이제 컨볼루션 하고 나면은 줄어들어요.
그쵸? 줄어들어요. 그렇죠 왜냐면은 커널이 다 차지하고 나면 차지하고 나면 한 번 여기 여기서 여기를 얘를 쓰면 세 글자를 쓰면 여기 여기서 이 표시해 놨지 이렇게 이렇게 해놓고 면 여기 끝나고 나면 여기 다 한 번 더 이상 못 나가니까 정보가 딱 한 개밖에 안 나오잖아 그쵸 하나밖에 안 나오고 그래서 이미지가 이걸로 끝나는 거죠.
여기서 하고 나면 이걸로 하나로 대표되고 끝나는 거예요.
이거 요 위치 있죠 요 위치 요 위치에 가면은 요기 요 숫자 요 숫자 하나로 끝나 느낌이 좀 가죠.
여러분 요거 맨 처음에 하고 나면 요 숫자 하나로 끝나 요 있지 요 앞에 딱 하나 나와 느낌이 와요.
여러분 그래요.

참석자 1 05:47
가운데에다가 딱 맞추면 가운데 숫자 여기 딱 이렇게 가운데가 없구나 가운데 없지 이렇게 4개가 가운데 네 번 한 거가 이렇게 가운데 차지하고 있고 그렇죠 여기서는 마찬가지로 이렇게 크면 코너가 이렇게 크면 여기 한번 하고 여기 맨 처음에 이렇게 한 번 하고 나서 이 숫자 하나로 딱 끝나 그쵸 맨 마지막에는 이렇게 하고 나서 이렇게 숫자 하나 끝나고 여러분 이거 지금 당연히 처음 보는 처음 생각해 보는 거니까 헷갈리거든요.
헷갈리는데 이렇게 약간 음미하셔야 되겠죠. 여러분 자꾸 이제 느낌이 오지 그쵸 그래요.
그래가지고 이렇게 큰 코너를 하고 나면 이거밖에 안 나와 그쵸 원래 컨볼루션 하고 나면 근데 이러면 컨볼루션을 이렇게 일단 제가 지난 시간에는 정보가 사라지고 어쩌고저쩌고 했지만 사실은 아까 보였지만 컨버레이션을 여러 번 하고 싶잖아요.
우리가 그래야지 뭔가 딥러닝이지 그쵸 추상화가 잘 되고 뭔가 복잡한 걸 만들 수가 있잖아요.

참석자 1 06:43
그래서 이렇게 조그마지면은 뭐 할 게 없잖아요. 그쵸 그렇기 때문에 더욱더 헤딩을 해야 되는 거예요.
이해돼요. 패딩을 해서 원래 이미지 사이즈를 유지해야지 뭔가 계속 컨볼레션을 여러 번 할 수 있지 할 게 없잖아.
이렇게 줄어들고 나면은 할 게 없잖아요. 그렇죠 다음에 못하잖아.
아니 그러니까 이거 이렇게 이렇게 만들어지면 필터 적용을 못해 당연한 얘기지만 요가 여기서는 필터가 이렇게 두 개 다 못 쓰죠.
그러면 덮을 수가 없잖아. 전 곱해서 덮으려고 그러는데 이거밖에 안 되면 덮을 수가 없잖아요.
얘는 아예 못 덮잖아. 아무도 이 모자르잖아 세 칸 필요한데 가로로 얘 5칸 필요한데 이거 두 칸밖에 없으니까 이해돼요.
내 말이 패딩이 필수적이라는 거 느낌이 오죠. 여러분 패딩을 하는 이유가 가해해서 컴볼트 한 번밖에 못하는 것도 있지만 그거 말고도 추상을 여러 번 하고 싶으니까 그런 관점 그 약간 당연하다는 거지 그치 알겠죠 그래요.

참석자 1 07:41
그리고 이제 패딩을 안 하는 거를 또 이제 전부 다 우리 여기 보면은 용어들을 그냥 같이 쓰고 싶어 하거든요.
패딩을 안 하는 것도 뭔가 무슨 패딩이라고 부르고 싶어서 밸리드 패딩이라고 부르는 거예요.
알겠죠 밸리드 패딩이 여러분 이것도 되게 용어가 헷갈리는데 밸리드 하면은 뭔가 패딩 하는 것 같잖아요.
그쵸 밸리드 패딩 패딩 안 하는 거 알겠죠 펠리딩 안 하는 걸 밸리딩 패딩이라고 그런다는 것도 헷갈리잖아요.
그쵸. 이렇게 음미하고 넘어가야지 기억이 나지.
그쵸. 용어가 밸리 패딩 아닌 거가 이제 나머지 해프 패딩 세인 패딩 제로 패딩이 붙어 있는 것들이 진짜 패딩 하는 거라고 알겠죠.
그래서 이거 하는 거 뭐 하는 거냐면은 그냥 가에다가 다 채워주는 거잖아요.
그쵸. 이 이거 말고 패딩을 조그마한 것도 있고 여러 가지 있을 수 있는데 그런 거 거의 안 쓰고 이거밖에 안 남았어.
사실은 거의 원래 모양 유지하거나 아니면 줄어들게 냅두거나 이것만 한다고요.

참석자 1 08:40
사람들이 그래서 원래 모양 유지하는 게 헤프 패딩이라고 부르기도 하고 세이 패딩 제로 패딩 다 섞어서 부른다고 이게 다 같은 말이라는 거죠.
알겠죠 용어의 의도는 다 여러분이 이해가 되지 왜 해표 패딩이라고 그러냐 왜냐하면 양쪽에 이게 2차원이니까 양쪽에 채우고 아래 위로 양쪽에 채우니까 패딩하는 거 여기 사실은 두 칸 채워도 한 칸 채운다고 얘기할 수 있잖아요.
그쵸 그걸 상피 패딩이라고 부르고 가로를 하나 늘린다고 이쪽으로 양쪽으로 하나씩 반씩 늘린다고 그렇죠 피 패딩 세인 패딩 똑같이 만든다고 출력이랑 입력이랑 똑같은 크기라고 세인 패딩이라고 부르고 제로 패딩 0으로 채우니까 제로 패딩이라고 부르고 알겠죠 led 패딩은 패딩 안 하는 건데 우리 패딩이라고 부르는 걸 헷갈리네.
이렇게 하고 공부하고 넘어가야지. 근데 이게 여러분 지금 계속 제가 강의하는 게 이제 사회에 나갈 거 아니에요?

참석자 1 09:28
여러분이 4학년 끝나고 계속 끊임없이 배울 수밖에 없어요.
여러분은 끊임없이 배우는 직종에 있으면 여러분 돈을 많이 벌어 끊임없이 배우지 않는 직종에 있으면 여러분이 돈을 못 벌어요.
이게 낮은 페이로 아주 그냥 거의 녹아들어 뛰게 된다고 알겠어요.
계속 배워야 되는 직종이 있어야지 잘 먹고 잘 사는데 그러려고 그러면 여러분 잘 공부하는 그 스킬을 익혀야 되잖아요.
잘 공부하는 빨리빨리 잘 익히는 스킬 그거는 자꾸 뭔가 보면서 그냥 달달 외우지 말고 생각해 보면서 외우면 잘 안 까먹어요.
그쵸 알겠죠? 그래요. 그다음에 용어 다시 인프 티처 피처맵 그쵸 이 거 똑같이 나오더라.
다른 그림인데도 그렇죠 이거 내가 그랬구나. 어쨌든 그래요.
그다음에 여기 패딩에서 이제 짝스도 얘기했었고 보폭이라는 얘기도 했었죠 그렇죠 보폭 스트라이드 그쵸?
스트라이드 기본적으로 한 칸씩 가는 거 그쵸?

참석자 1 10:23
두 칸씩 갈 수도 있고 그렇죠 그러면 두 칸씩 가면은 정보가 이제 뭔가 너무 촘촘하게 안 하고 싶고 빨리빨리 하고 싶을 때 이렇게 스트라이드를 하는 거고 그래요.
그리고 이제 커널에 따라서 나오는 거 여기 여러 가지 관점에서 얘기하고 있는데 사실 이걸 이게 극단적으로 보여주는 게 이 그림인데 이 그림이 참 되게 마음에 들었는데 이거 극단적으로 보여주는 게 완전히 자기랑 똑같이 생긴 거가 제일 숫자가 크게 나오는 거예요.
알겠죠? 자기는 너무 다르게 생기면 그냥 거의 정보가 사라지는 거고 그러니까 찾고 싶은 특성 모양으로 필터가 학습이 되는 거예요.
처음에는 시작할 때는 익명한 필터들 이런 거 막 여러 개 갖다 터널들.
그쵸 터널 필터 개념 알죠? 여러분 덱스가 있는 게 그냥 필터고 덱스 없는 걸 터널이라고 부른다 그랬죠?
그쵸? 그렇죠 여러분 2차원이 커널이고 3차원이 PT야.
그렇죠 됐지 이래요.

참석자 1 11:18
그래가지고 원래 이렇게 처음에는 커널 필터 이런 게 디폴트로 시작하지만 나중에 학습을 잘하고 나면은 내가 이제 문제를 푸는 데 중요한 필터만 피터로 변한다는 거지.
사실 여러분 우리도 공부할 때 우리 여러분들 내가 이제 뭔가 사과랑 배 사과가 처음에는 다 똑같이 보이지만 나중에는 사과가 이제 이게 무사인지 홍옥인지 여러분 그런 거 알아요.
얼른.


clovanote.naver.com