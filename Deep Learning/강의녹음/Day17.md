딥러닝 day17_1
2025.05.14 수 오전 10:01 ・ 49분 18초
심승환


참석자 1 00:05
그래서 여기까지 했었어요. 그쵸 여기까지 했었지 맥스 플링까지 했어요.
그렇죠 맥스 클릭 클링에 사실 맥스 클링 이영기까지 했고 이어서 할게요.
여러분 그래서 이것도 얘기했던 것 같기도 하고 필터 개수는 거의 1

참석자 1 00:25
그리고 이거는 제가 그냥 이미지 쇼 하는 거 워낙 많이 그리니까 이미지 쪽 나오니까 얘가 리전 쪽 하니까 이미지가 많이 나오는데 그림 그리는 거에 대해서 그냥 갖다 붙여놨어요.
잘 볼 것 같아가지고 그래서 보면 이제 저는 저기 이번 중간고사 흑백 이미지가 사실은 제가 문제 중에 이런 게 있었거든요.
랭크가 디멘전이 2냐 3이냐 이런 거 물어본 적 있었잖아요.
흑백 이미지도 사실 이제 실제 이제 CNN에 입력을 줄 때는 이 데스트를 주긴 줘야 되거든요.
CNN은 기본적으로 이제 배치까지 해서 4차원으로 기대하고 있잖아요.
입력이 뭔 말인지 알아요. 여러분 저기 옛날에 댄스 네트워크는 입력이 2차원인 걸 랭크 2를 기대하고 있죠.
배치 때문에 그쵸 플랫한 거 원래 벡터 하나랑 배치 때문에 두 개 그쵸 그래서 앤 님이 2고 얘는 지금 NDB 4를 바래요.
그쵸 왜냐하면 배치랑 그다음에 이제 가로 세로 가로 여기 여기 있다.

참석자 1 01:31
여기 세로 가로 그다음 이거 아니지 세로 미니 배치 사이즈 세로 가로 그다음에 이제 뎁스 그쵸 이렇게 되잖아요.
여러분 그렇거든요. 흑백 이미지를 줘도 채널을 이렇게 일을 만들어 줘야 되긴 하는데 근데 이미지 그릴 때 여러분 그냥 원래 이미지는 스펙 이미지는 여기가 채널이 쓸데없이 나 필요가 없잖아요.
그쵸 그러니까 차원이 당연히 2개로 끝나요. 그래서 이미지 그릴 때 이렇게 이미지 이제 이미지 쇼라는 거 여러분 옛날 사실 여러분 써봤거든요.
사실은 제가 숫자 같은 거 표현할 때는 이미지 쇼 이런 걸 보여줬었어요.
여기 여기 다 그렇게 돼 있어요. 여기 전부 다 여기 지금 그래프 그린 것도 다 이미지 쇼야 그랬어요.
여러분 그래가지고 그리는 거 보면은 그냥 흑백 이미지는 mm이고 이렇게 세로 가로만 있고 컬러 이미지는 알지이면 이렇게 써 이렇게 보이네 이런 식이에요.
알겠죠 여러분 다 받아들여요.

참석자 1 02:34
이거는 여러분 rgba까지 해가지고 뭔가 트랜스페어런시까지 들어간 거고 알겠죠 여러분 이미지가 이거 이상으로 된 건 안 되고 그래요.
이거 써 이미지 시는 다 받아드려요. 이렇게 2차원으로 와도 이거는 칼라가 없는 흑백 이미지구만 하면서 알아먹는데 어쨌든 흑백 이미지 자체는 2차원이라는 거지 알겠죠 그래요.
그다음에 칼라 메이 저도 좀 궁금해 가지고 옛날에 그림 그리는데 어떤 데는 어두운 색이 보라색이고 밝은색 노란색이고 중간색은 이제 중간에 이제 초록색으로 나오는 게 있어요.
이걸 그린 리스트고 이게 디폴트예요. 이게 헷갈리는 점인데 그림 디폴트가 요 요 위에요.
흑백 이미지 c 맵이 카라 맵이 알지b는 필요 없겠죠 여러분 특백 해지만 필요하겠다.
그쵸 어떻게 원래는 여러분 보통 볼 때 바이너리나 그레이 서트스 이거 다 가르쳤잖아요.

참석자 1 03:28
제가 그러니까 적은 숫자 0 숫자를 흰색으로 할 거냐 0 숫자를 검은색으로 할 거냐 바이너리 그레이 이렇게 보여주고 있잖아요.
그래서 이거 저도 저도 저음에 맨 처음에 이거 딥러닝 막 자료 나올 때 되게 이거 왜 어떤 데는 숫자가 흰색이고 어떤 숫자가 검은색이고 되게 모르겠더라고 진짜 궁금했거든요.
진짜 셋째 피티 있었으면 바로 알려줬을 텐데 그때 그때는 막 찾아봐가지고 잘 모르겠어.
지금은 이제 어쨌든 알겠죠. 여러분 무슨 얘기인지 그리고 그냥 그리면 만약에 컬라맥 시냅이라는 거 컬라미터 없이 그리면은 이렇게 나온다고 숫자가 노란색으로 나오고 배경 보라색으로 나와요.
여러분 안 해봤지만 여러분이 한 번도 알겠죠 그래요.
그리고 본이라는 것도 있는데 이건 엑스레이 찍은 것처럼 이렇게 막 나오고 뼈 같이 중간에 이렇게 해서 여러분 제트라는 것도 제가 재미있어서 많이 쓰는데 제트가 이게 파란색이 온도 같이 보여요.

참석자 1 04:24
온도 온도 같이 보여서 온도가 높으면은 빨간색이 되고 온도가 낮으면 파란색이 되거든요.
그래가지고 재미있는 게 여러분 적외선 카메라 있잖아요.
적외선 카메라 적외선 카메라 여기 온도 감지해서 찍는 거 알아요 그거 적외선 카메라는 당연히 거의 온도로 그냥 숫자로 스케일 돼서 0부터 255까지 나오는데 그거를 그냥 온도라는 느낌을 주게 하려면은 이 제트로 찔러주는 게 좋잖아요.
빨간 데가 온도가 높다는 느낌이 나고 파란 데가 온도가 낮다는 느낌 나오니까 이해되죠.
여러분 근데 그게 마치 무슨 여러 차원이 있는 걸로 오해하고 있는 애들이 많이 있어서 사람들이 그냥 걔도 그냥 숫자 0부터 255까지 쫙 있는 것뿐이라고 알겠죠.
표현을 그냥 이렇게 할 수도 있고 이렇게 해볼 수도 있고 그러니까 자기들이 그림 그려보니까 이렇게 나오니까 왜 이렇게 안 나오니까 이건 비싼 저는 옛날 조업 설계 하는데 그 친구들이 이제 비싼 어플 445만 원짜리 사야지 이런 걸 그려준다고 주장하는 거예요.

참석자 1 05:23
라즈베리 파이로는 이렇게밖에 안 그려지고 이렇게 그래 내가 이 칼라를 바꿔보라니까 이렇게 그려지지 무슨 얘기인지 알겠어요 여러분 그러니까 여러분 이게 이런 게 중요하다고 알겠죠.
그래서 45만 원 주고 규지 그냥 하나만 바꾸면 되는 거를 가지고 아니 그럴 수 있잖아.
근데 사실 여러분 근데 이거 진짜로 무식하면 여러분 돈을 많이 쓰게 돼 그 알겠죠 그래요.
그냥 적외선 카메라 라즈베리카에서 사면 3만 원인데 클라이 바꾸는 거 갖고 프라미터 하나 주는 거 갖고 40만 원 더 줄 필요가 있냐고 알겠죠 그래요.
그다음에 그때 그때도 어쨌든 여러분 저도 그냥 상식적으로 그렇지 않겠냐 해가지고 해봤더니 진짜 그런 거였어요.
여러분 저도 알고 있었던 게 아니라 무슨 얘기인지 알겠어요 여러분 추론을 하시라고 합니다.
알겠죠? 설마 굳이 그렇겠냐 수 있어요. 진짜 아니에요 그래요 그다음에 요거는 이제 앞에서도 한 번 보여준 적 있었고 약간 약간 브록처럼 있는 거야.
알겠죠?

참석자 1 06:24
다 오픈북 볼 때 혹시 뭐 나 여러분 실제로는 이제 일할 때는 찾아봐야겠지 그쵸?
그래요. 그다음에 이거 제가 앞에 플링 나올 때 맥스만 나오고 마는데 사실 에버리지 플링도 쓰긴 써요.
가끔 필요할 때 에버리지 플링은 평균 내는 거겠죠 뭐 감이 오죠 여러분 아까는 맥스 요기 요기 요거는 여기는 이제 맥스는 6만 딱 살아 남는 건데 평균 내면 이걸 더 평균 내서 값을 쓸 수도 있잖아요.
그쵸 이해되죠? 여러분 1 다 더해가지고 4로 나누는 거죠.
그쵸? 그걸 쓸 수 있잖아요 그쵸 근데 그게 여러분 맨 처음에 초반부에는 플링 할 때 그렇게 썼었어요.
에버리지 플링으로 그래서 예를 들어서 지금 이 그림이 있는데 이거를 이게 위에가 이제 이게 위에가 맥스 툴이고 밑에가 이게 에버리트이거든요.
위에 그림이 더 뭔가 선명하잖아 그렇죠 이해되죠 어떻게 되는 건지 그래서 이게 사실 우리도 알아보기 쉬운 게 결국은 러닝도 잘 되는 경향이 있어요.

참석자 1 07:23
그쵸 그래서 과거에는 에버리지 플링 때문에 사용했었는데 정보 안 잃어버린다고 그런데 현재는 맥스 플링이 학습이 잘 된다는 거를 알아서 그리고 속도도 에버리지 내는 것보다 맥스가 더 빨라요.
여러분 최대값 찾는 게 알고 있는 상태 그래서 실제로 다 매스클릭 쓰고 있어요.
알겠죠 그래서 제가 시험에 맨날 내거든요. 이거 클로즈 등급으로 맥스트 플링이 에버리지 플링이라 어느 게 더 많이 사용하는 거고 어느 학습이 잘 되냐 근데 희한하게 여기 이렇게 다 끄덕끄덕하면서도 시험 보면은 이제 선배들도 보면 에버리지 플링이래 맥스트 플링이 알겠죠 우리 교과서도 다 맥스 플링 나오잖아요 그쵸 알겠죠?
그래요.

참석자 1 08:05
그리고 글로벌 레버리지 플링이라는 게 있는데 이거는 이제 글로벌하게 본다는 게 원래 맥스플링은 여기 이렇게 커널 크기가 있어가지고 커널 사이즈가 있었죠 이렇게 보면은 여기 여기에 이제 필터 크기가 있었잖아요.
그치 2 곱하기 그쵸 그런 거 없이 전체에 대해서 그냥 에버리지 평균 내고 그러는 게 있는 거예요.
여기 이 플랜인 전체에 대해서 요거 전체에 대해서 평균 대서 5 평균 내서 5 3 이렇게 나오게 할 수 있어요.
여러분 그런 거를 글로벌 에브리지 플링이라고 불러요.
이거를 실제로 나중에 가면 많이 써서 제가 갖다 놨어요.
알겠죠? 글로벌 에버지 플링이라는 건 플레이 전체에 대해서 평균 낸 거예요.
이거 해볼까 귀찮죠? 여러분 귀찮아 1 더하기 5 더하기 4 더하기 5 다 해가지고요.
평균 내면 뭐가 나와야겠죠? 여러분 알겠죠?

참석자 1 08:53
말로 설명했으면 됐지 뭐 그 그리고 여기 구체적인 거는 그다음에 리듀스민이라는 것도 중간중간 많이 나오거든요.
여러분 평균 내는 거 평균 내서 평균 내면 여러분 줄어들 거 아니에요?
디벤전이 그쵸? 평균 내면 여러분 가 기본이 줄어들잖아.
그래서 리듀스란 말을 붙여놓은 거야. 알겠죠? 여러 개의 값을 하나로 바꾸니까 그래서 리듀스 맥스도 있겠지 리듀스 민도 있고 리듀스 알겠죠?
여러분 그런 거예요. 이런 것도 제가 저는 좀 헷갈리길래 혹시 정리해 봤어요?
알겠죠? 그래요. 정리하기 아까워서 여기다 넣어놨어요.
그다음에 교과서에 이제 패션 인리스트 2급 교사 말고 제가 이 머신러닝 교과서 이거 말고 딴 거 우리는 계속 지금 근육인 리스트 숫자 갖고 했는데 패션 리스트 갖고 한 필스 리스트라는 것도 여러분 옷이랑 이런 거 쪽으로 한 건데 거기서 10까지 구분한 건데 거기에 대한 익셉들이 딴 데 있는데 갖다 놓은 거예요.

참석자 1 09:51
잘 보여서 그래가지고 여기 이거 이것도 근데 그래서 뭐 이런 거에 대해서 비슷하게 뭐 해본 거 어쨌든 여기서 보여주고 싶은 거는 엑스트레인이 원래 데이터가 지금 이건 패스 레니스트도 그렇고 엠리스트도 그렇고 기본적으로 흑백 이미지니까 넣으려고 그러면은 샤워도 하나 늘려야 돼.
뎁스를 늘려야 되잖아요. 그쵸? 뎁스를 하나 늘려야 될 거 아니야 그쵸?
창고를 늘리기 위해서 창고를 이렇게 할 수 있어요.
여러분 이렇게 원래 거에다가 이렇게 땡땡땡 한 다음에 이렇게 쫙 추가할 수도 있고 류쉐입 할 수도 있고 류쉐입도 알잖아요.
여러분 그쵸? 그렇게 할 수도 있고 이해되죠? 여러분 상관없어요.
근데 사실 오픈 소스 본다고 생각하시고 여러 가지가 있을 수 있지 그치 알아보면 되는 거지 알겠죠?
그래요. 그래서 원래 이제 뭐 이런 게 있으면 차원이 이게 이게 이런 어레이가 있었으면은 차원 늘리면 이렇게 하나씩 하나씩 들어가겠지.

참석자 1 10:52
그렇죠 처음 내린다는 게 무슨 의미인지 제가 알려준 거예요.
알겠죠? 그래요. 그리고 이제 칼라바를 표시하면 더 명확할 때가 있어서 이게 어느 게 숫자가 큰지 여기 보면은 여기 칼라바 클라 칼라바 이렇게 이렇게 요거 한번 불러주면 옆에 칼라바가 생겨요.
여러분 그러면 이제 숫자가 이게 여기는 250 이게 여기 둘 다 이제 제일 큰 숫자가 250이고 여기는 제일 큰 숫자가 2.5잖아요.
이거는 오른쪽은 어떻게 된 거냐? 이건 지금 정규 분포로 노멀라이제이션 한 거예요.
노말레이제이션 이해돼요 여러분 노멀라이제이션 레뮬레이션 할 수도 있고 안 할 수도 있고 그렇잖아요.

참석자 1 11:34
그쵸 이미지 여러분 넣을 때 옛날에 리스트 넣을 때 그냥 리맥스 스케일링 할 수도 있고 그렇죠 노멀라이즈 할 수도 있고 그쵸 지금 이렇게 이렇게 나오는 건 노멀라이즈 한 거지 만약에 그냥 255로 나눠서 인맥스 스케일링 하면 이거랑 똑같이 나와야 왼쪽처럼 나오겠지 숫자가 다만 여기가 250이 아니라 1이겠지 내 말 알아들어요.
여러분 그쵸? 그리고 이제 이 교과서 말고 다른 교과서에 나오는 건데 이제 이거는 이거 그냥 한번 봅시다.
여러분 그래서 이게 지금 원래 커브 2D를 만들 때 코너 사이즈를 3으로 하고 컴퓨터디 쓰고 렐루 쓰고 패디빙을 세임으로 하는 거 있잖아요.
이거를 매번 쓰면 길어지잖아요. 귀찮아서 그래서 이걸 이렇게 이거를 파셜 해가지고 이렇게 정의할 수가 있어요.
여러분 이해되죠? 파셜이라는 명령을 파셜이라는 펑션 툴이 있어요.

참석자 1 12:33
여러분 그래가지고 파셜이라는 거 해놓고 이렇게 이렇게 적어 놓으면 이 함수에다가 이런 파라미터가 들어가는 거를 이 글자 하나 바꿀 수가 있는 거예요.
이해되죠? 여러분 그래서 디폴트 컴브투디라는 걸 부르 여기 써놓은 게 바로 사실은 캐라스트 레이의 컴브투디하고 이제 파라미터를 준 거랑 똑같다는 거지 알겠죠?
그래요. 많이 썼어요. 사람들이 알아먹죠. 여러분 그러니까 그쵸 그래서 필터는 이제 필터 사이즈는 다 다르게 준 건데 64 128 256 이렇게 전전을 하고 있잖아요.
그쵸 그렇게 다르게 주고 그리고 이름도 다 매겨주고 인풋 시 적어줬고 여기는 이런 식으로 해서 했는데 그리고 보통 이제 항상 하는 방식이 교과서도 나오지만 여기 컨볼루션 이렇게 쭉 하다가 근데 컨볼루션 할 때마다 넥스플링을 중간에 넥스플링을 중간에 넣어주고요.
스프링 중간에 넣어주고 마지막에 이제 플래트 하고 그리고 댄스를 추가해 줘요.

참석자 1 13:37
마지막에 또 댄스가 있어야지 이제 결국 분류를 하니까 댄스를 한 개만 하는 경우도 있지만 또 중간에 이렇게 많이 넣어주는 경우도 있고 이해되죠 여러분 최소한 이제 어쨌든 이거는 이거는 있어야 되는데 그전에 한 번 더 댄스를 추가해 줄 수도 있다는 거죠.
알겠죠? 댄스 드라아웃은 여러분 드라바웃이 굉장히 훈련할 때 벤스 할 때는 저어주는 게 굉장히 훈련이 잘 돼요.
스무드하게 드라바웃이 여기 보면 지금 들어갔죠?
그쵸 댄스 다음에 드라아웃 항상 넣었죠? 그쵸 컨볼루션 쪽에 드라아웃은 잘 안 넣어주고 댄스 쪽에 넣어줘요.
보통 항상 알겠죠? 교과서도 또 나와요. 됐죠? 여러분 그래요 그래가지고 여기 이거를 이제 서머리 하면은 이렇게 쭉 나오는데 여기 이제 이름을 컴브1 컴버트 이렇게 붙여줬기 때문에 여기 이름이 이렇게 이렇게 나온 거야.
그쵸 여기 네임 붙여줬기 때문에 그렇죠 안 그러면 이제 이거 맨날 부를 때마다 달라져서 귀찮거든요.

참석자 1 14:34
알아보기 힘들어서 레임을 붙여놓기 때문에 이렇게 쭉 나왔고 이해되죠?
여러분 그다음에 아웃풋 쉐입이 이렇게 적혀 있는데 그리고 파라미터가 적혀 있는데 이거 아웃풋이 구경시켜줄게요.
여러분 여기 보면은 맨 처음에 컴브 1의 아웃풋이 28 28 64 28 28 24 돼 있는데 이 맨 처음에 여기 앞에 나 적혀 있는 거 이거 전부 다 뭐겠어요?
여러분 이거 전체가 배치 사이즈죠? 그쵸? 안 지워져요 이지 그다음에 여기 뒤에 적혀 있는 거 요거 요거 세로 가로 뎁스죠 그쵸 어떻게 해 어떻게 나온 거냐?
이게 컴프투디 이거 요 컴버이에요. 얘 얘 얘 얘 얘 얘 여기 그림 그리고 요거에 이제 요거 요건데 컴브원 적혀 있으니까.
그쵸 아웃풋이 28 28 가로세로 하고 그다음에 여기 원래는 인풋은 1위 채널이 1이었는데 그쵸 근데 필터가 64개니까 64가 튀어나오는 거예요.

참석자 1 15:33
그쵸 플랜이 64개의 특성을 뽑아라니까 그쵸 잎사귀 플레임 만들 거 아니에요 그쵸 잎사귀 되는 거 알겠죠?
그래요. 그리고 파라미터 개수는 어떻게 구했는지 제가 적어놨잖아요.
친절하게 그렇죠 계산하는 거 이거 할 수 있어야 되겠죠.
이거 어떻게 되는지 여러분 알잖아. 그렇죠 이게 코너 사이즈가 무려 7이라서 코너를 좀 크게 잡았네.
그렇죠 7 7로 잡은 게 이게 인풋 쉐입이 28이니까 네 번 약간 이렇게 4배 되게 해놨네.
그쵸 4분의 1로 해놨네. 그쵸 가로 세로가 28 곱하기 28이거든요.
거기 이렇게 번 네 번 들어가는 코너를 해놨네. 그렇죠 스트라이드 크게 했을 때 그런 식으로 해놨어요.
그래서 어쨌든 이게 왜 이렇게 됐는지 여러분 알게 바이어스 하나 해가지고 이렇게 64 곱하기 해서 나온 거지.
그쵸 필터별로 필터 개수별로 있으니까 그쵸 파라미터가 이해되죠?
여러분 필터가 파라미터지 그쵸 됐어요. 여러분 계산할 수 있어야 돼요.
여러분 시험에 나와요.

참석자 1 16:37
기말고사에 알겠죠 그다음에 풀링을 하고 나면 풀링을 하는데 풀 사이즈를 2로 하면 반으로 줄어들어서 반으로 줄어드는데 플레인이 줄어들지 않기 때문에 14 14 64가 되는 거야 이해되죠?
여러분 가로 세로가 줄어들어요. 반으로 그쵸 풀 사이즈가 2기 때문에 파라미터는 없어요.
여기는 학습하는 건 없기 때문에. 그렇죠 이해되죠 여러분 그래요.
그다음에 똑같은 식이지 똑같이 이제 다시 컴프투를 하는데 컴프투가 어떻게 어떻게 생겼냐면은 얜데 필터가 128개이고 그리고 커너 사이즈가 여기 디폴트 커너 사이즈를 여기 따로 여기 오버라이딩 했는데 여기 오버라이딩 안 했으니까 코너 사이즈가 얼마냐면 3이지 그쵸 그럼 3이면은 원래 이제 3의 제곱이 되는 거죠.
그쵸 근데 이제 앞에 플레이는 64역이었기 때문에 64 곱하기가 되지 컬러 레스가 확 늘어나잖아요.
그쵸 컬러 데스가 아니 피트 레스가 64 곱하기 3의 제곱이 필터 하나의 그 모양이죠.
그쵸?

참석자 1 17:46
더하기 1 하는 게 바이러스 필터별로 바이러스 하나 그쵸 그럼 필터가 몇 개냐 128개 있으니까 128을 곱하는 거지 이해돼요.
여러분 다시 할까? 다시 해 다시 하는 거야. 괜찮은 거야 괜찮지 괜찮은 거죠.
여러분 다시 해줘요. 다시 해줄게 여기 다시 할게요.
이거 이거 이거 이상하다 이거 이거 7 3 8 5 6 이거 시험에 나오면 여러분 계산 다 안 하고 그냥 이렇게 숫자로 적어도 괜찮아요.
알겠죠? 과정이 중요하니까 이제 여러분 사실 이 계산돼 있는데 뭐 그렇죠 알겠죠 이렇게 이제 적으라고 이렇게 진짜 시험 문제 답을 적을 때 알겠죠 다시 할게요.
여러분 이거 128이 여기 여기 128개가 있잖아요.
이게 요 컴브투딘 컴브투 이게 여기 적혀 있는 게 아웃풋 쉐입이에요.
그쵸 아웃풋 셰이 1282개의 필터가 있다는 얘기지.
128개가 128은 어떻게 나왔냐 여기 여기 필터가 128이잖아요.
128이니까 아웃풋이 128개가 돼.

참석자 1 18:53
그렇죠 그렇죠 여러분 그리고 여기 원래 패딩이 세임이었으니까 이거는 그대로 나온 거야.
이거 알겠죠? 알겠어요 내 말 지금 여러분 이거 아웃풋 쉐도 이 코드에서 바로 나올 수 있어야 돼요.
여러분 그렇치 추 추출할 수 있어야 돼. 알겠죠? 됐죠 그다음에 파라미터 개수 구하는 거 할게요.
이제 파라미터 개수 어떻게 구하느냐 필터의 개수죠 결국 여러분 그쵸 필터에 들어간 파라미터의 개수잖아요.
그쵸 한 필터당 파라미터가 한 필터당 한 필터의 쉐입이 어떻게 생겼냐면은 이게 커널이 사이즈가 3이니까 3 곱하기 3이지 그쵸 그리고 입력이 지금 64개의 채널로 들어오니까 그쵸 뎁스가 64인 거야.
그렇죠 그래서 제곱하는 게 2천 원이라서 필터 코널이 항상 됐구나.
다시 준비 잘했어요. 코널이 하나 생긴 모양이

참석자 1 20:03
이렇게 생겨 먹었죠 그렇죠 이게 커널 하나 생긴 모양이죠 그쵸 이게 커널이 이게 커널 사이즈가 3이면 3 곱하기 3 지금 여기 지금 피트 사이즈가 얼만가 여기 지금 여기 이게 한 필터를 보여주고 있고요.
터널이 3개 있어요. 그죠? 네 전부 몇 개인가 그래서 3 곱하기 3이죠 그쵸 그리고 얘에 대해서 통틀러즈 바이러스가 하나 있어요.
그리고 3 곱하기 3이고 여기에 3 곱하기 3 곱하기 3짜리가 3개 있지 그쵸 이게 인풋이 개 인풋 채널이 3개라서 그런 건데 인풋 채널에 이게 만약에 더 있었어 그만큼 여기 만약에 5개씩 쓰면 여기가 5로 바뀌고 그쵸 여기는 근데 항상 정사각형 쓰니까 보통 필터가 여기가 3이 제곱이라고 쓴 거거든.
그쵸 됐죠?

참석자 1 20:52
이런 필터가 몇 개 있느냐가 이제 필터 z잖아 여기 전체 해가지고 이렇게 필터 그쵸 개수 곱하는 거지 그쵸 그러니까 약간 여러분 그림을 머릿속에 넣으세요.
알겠죠? 여기 제가 그림을 다양하게 보여주는데 혹시 오해할까 봐 제가 다양하게 보여준 거예요.
다양하게 보여준 거는 하나의 그림을 하면 또 오해할 수 있거든.
또 뭐 이상하게 사람이 그래가지고 알겠죠 정확히 이해합시다.
여기도 봐봐. 여기 여기 필터 하나 먼 샵인데 지금 인풋이 뎁스가 3이면은 얘도 필터가 여기는 앞에 적혀 있구먼.
여기 알겠죠? 저는 거꾸로 져가고 지금 그쵸 5의 제곱이지 얘가 5 곱하기 5라서 그쵸 알겠죠 그리고 이런 필터 이거 봐 나중에 여기 필터가 여러 개 있으면은 여기 피터 그림 다시 안 그려놨구나.
피터가 여기 하나 더 들어가면은 그만큼 필터 파라미터 계속 늘어나는 거지 그쵸 질문 잘했어요.

참석자 1 21:50
그래서 여러분 뭐 이거 틀린 사람은 어깨 이를 다시 어떻게 나오는 거 알겠죠?
이거는 어디서 왔냐 여기서 왔고 그쵸 얘는 어디서 왔냐 여기서 왔는데 이것도 어떻게 구하는지 알죠?
여러분 다 그렇죠 됐죠 나머지 똑같은 거 이제 승진 그만할게요.
여러분 됐죠 됐지 한 번 더 해줘. 한 번 더 해줘요. 됐지 갑시다.
그런 다음에 이제 이거 돌리는 거 아까 저걸 컴파일 해가지고 돌리는 게 나오는 거고 뭐 되겠지 당연히 그래요.
그다음에 이거 다른 교과서에서 나오는 건데 어쨌든 이거 지금 파라미터 개수가 다른 비교적 표가 좋아가지고 그냥 냅뒀는데 어 이게 이제 패셔니 리스트 똑같은 거에 대해서 하는 거죠.

참석자 1 22:44
패션 리스트가 히든 데이터의 이제 개수가 거기 옛날에 다른 교과서에서 있었던 건데 어쨌든 보통 여기는 트렌드 파라미터가 훨씬 더 이게 많게 돼 있지만 보통 이제 여기는 이게 더 이게 많은 경우도 있고 아 그래요 이거 이것도 내가 지금 다시 보니까 좀 지금 다시 보니까 좀 별로긴 하네.
그래요 그냥 합시다. 어쨌든 보통 결과적으로 항상 엔크루시 값 CLNC 잘 나와요.
CNN이 훨씬 aqc가 잘 나오고 파라미터 개수가 더 적은 경우가 많은데 여기는 좀 많이 해가지고 좀 그렇긴 하다.
보통 트레이너 파라미터를 더 적게 하거든요. 보통은 그래 그만 합시다.
이거는 이 예 만들기 나름인데 다시 보니까 좀 그러네.
그다음에 이거는 제가 지난 시간에 보여줬던 거고 보여줬었지 보여주지 않았어 옛날에 중간에 위로 가면 갈수록 필터들의 모양이 지금 이렇게 좀 원시적이다가 점점점 약간 좀 멀리서 보는 느낌이 나죠.

참석자 1 23:39
그쵸 뭔가 원래 우리가 눈으로 보이는 거와 사이즈고 이거 굉장히 막 이거는 코를 박고 보는데 근시로 보는 느낌 이것도 재 떨어져서 보는 느낌 나무를 보고 여기서 숲을 보죠 그쵸 이해되죠?
여러분 맥스 플릭 하니까 그렇게 되는 거예요. 그쵸 이해되죠?
여러분 그래요. 그리고 여기는 중간중간에 이제 CLN의 아웃풋을 보여주고 있고 이거랑 그림이 달라요.
여러분 얘는 필터의 모습을 보여주고 있고 얘는 나온 아웃핏을 보여주고 있어요.
여러분 알겠어요 여러분 다른 점 다른 점 알겠죠? 필터가 여기 지금 몇 개 있는 거야?
이거는 하나 둘 셋 넷 다 일곱 여덟 아홉 10개씩 지금 필터를 쓰고 있구먼 그쵸 그래서 필터 10개 플레이 탁 나온 거죠.
그쵸 또 10개 플레이나 이런 식으로 한 거죠. 그쵸 이해되죠 여러분 그래요.
그리고 이거 또 여기는 지금 여기서 보여주는 거는 필터가 지금 되게 많아요.
필터가 여기 여기 뭐 하나 둘 셋 넷 다 6 곱하기 6 36개씩 있는 거를 보여준 거예요.

참석자 1 24:45
그렇죠 더 많을 수도 있는데 필터의 모양 보여주는 거 다른 거 알겠어요?
여러분 필터의 모양 얘는 얘는 중간에 아웃풋의 모양 피처 맵의 모양이지 그쵸 필터 하나당 이런 게 나오는 거지 그쵸 자기가 강조하는 게 세게 나오는 거지 그쵸 그쵸 그런 거예요.
그리고 이게 혹시나 또 이것도 그냥 한번 보여주면은 이거 이게 여기 이게 저 사이트 가면은 이거

참석자 1 25:17
스탠포드 대학교에서 계속하고 있는 건데 이게 실시간으로 지금 계속 이미지 바꿔주면서 예측하게 나오고 있어요.
브라우저에서 돌아요. 이게 여기 인터넷 끊어도 돌아가 이거 그러니까 여기서 지금 했잖아요.
그러면 제가 이거 인터넷 끊어도 돌아가 이거 로컬에 돈다는 거죠.
브라우저에서 지금 되게 자바스크립트로 라이브러리가 잘 돼 있어서 잘 돌아요.
지금 그래서 아예 자바스크립트로만 하는 그런 온디바이스 AI 이것도 일종에 보니까 로딩만 되면 이해돼요.
그래서 여기서 지금 이게 예측하는 게 천가지인데 상위 5개만 보여주고 있는 거예요.
이해되죠? 여러분 이거 너무 그림 이미지가 조그매가지고 이게 근데 에어플레인이 두 번째로 나오고 아까 비행기였는데 에어플레인이 두 번째로 나왔거든요.
이게 뭐냐면 확률이에요. 확률 다 더해서 1 되는 거 이해돼요.
여러분 그런데 이제 사실 두 번째 거가 답인 경우 사타 5라고 그래가지고 5개 중에서 맞췄으면 그냥 맞춘 걸로 보고 그러는 것도 있어요.

참석자 1 26:17
여러분 하도 맞추기 힘들어서 지금은 너무 잘하니까 사실은 옛날에는 그런 것도 있었어요.
지금은 맨 처음에 탑 원으로 그냥 마찰 맞추는데 뭔 말인지 알아요.
여러분 청바지의 클래스를 구분해야 되는 거면 각각 5개, 상위 5개 중에 이제 맞춰도 그냥 맞춰둔 걸로 볼 수도 있잖아요.
여러분 너무 힘드니까. 근데 지금은 그냥 이 탑 5이라 그래서 또 탑텐이라고 그래가지고 그렇게 스코어를 탑텐 스코어 탑원 탑 5 스코어 이러거든요.
지금 톱 5를 보여주고 있는 건 탑 5를 항상 거의 맞추고 있어요.
근데 요즘 나오는 딥러닝은 그냥 타고으로 다 맞춘다고 하나로만 더 맨 위에 것만 봐도 맞추는 경향이 있는데 옛날에는 탑 5 톤으로 많이 했었지 전 경진대회를 지금은 그냥 알아서 알겠죠.

참석자 1 27:03
무슨 얘기인지 이거는 지금 사파비 스코어를 보여주고 있는 거지 알겠죠 이 지금 다 맞추고 있지롱 이러면서 여기 보면은 그래야 이 사슴을 보고 호수라고 그러고 막 그러고 있어요.
그 두 번째 가면 디어가 나와요. 알겠죠 그래요. 근데 이거 굉장히 딱 빠르게 잘 돌아가잖아.
어쨌든 근데 지금 온디바스 AI 쪽에서는 따 5로 맞추는 게 의미가 있다고 배우고 있어요.
왜냐면은 이래야 경량화 할 수 있거든 보면은 트럭이랑 칸은 그래도 비슷하잖아요.
홀스랑 독도 약간 하여튼 비슷하잖아. 호스나 독이랑 뭔 차이야 우리한테 그러니까 뭔가 우리가 일할 때 근데 메모리를 100메가 써야 되는 거를 1kg만 쓸 수도 있는 거야 이렇게 이렇게 하면 그러니까 무조건 원아 벡터 예스 노로 문제를 푸는 게 아니라 비슷한 것끼리 구분하는 능력이 오히려 더 좋다는 거지.
스무드하게 그러니까 오히려 로스트 값이 큰 게 좋다는 거예요.

참석자 1 27:53
로스가 너무 너무 딱 0으로 나오는 거 있잖아요. 그거는 완전히 뭐라 그럴까 편협한 거야.
되게 이해돼요. 여러분 근데 세상도 그런 것 같아요.
요즘 보기 그래요. 그거는 안 좋아 그런 면이 있어요.
여러분 뭐 사실 세상에 일이 안 그렇잖아요. 그래서 제가 얘기한 온디바이스 AI 쪽에서는 스트 디스트레이션 이런 게 중요한데 다시 또 이제 이런 게 더 중요해지고 있다고 이게 트럭이라고 카 트럭이라 약간 애매하게 맞춰도 상관없다는 거지 훨씬 메모리 조금 쓸 수 있으니까 웨이트도 줄일 수 있고 알겠죠 여러분 그래요.
그래서 여러분 그것도 되게 유명한 게 있는데 상관이 있어요.
제가 보기에는 박사 과정 들어올 때 평균 4.평균 평점이 4.0 넘는 사람이 되게 잘 못하는 경향이 있었어요.
3.5 언저리가 잘하는 거야.

참석자 1 28:44
일을 너무 디테일하게 외워버리면 안 되고 사실은 뭔가 비슷한 거 이렇게 공부를 하다 보면은 뭔가 이렇게 비슷한 것끼리 하고 막 생각을 많이 하면 여러분 성적이 좀 나쁜 거 알죠 근데 아주 나쁘진 않아.
그래서 3.5 대 3.5를 연구직으로 서는 3.5대가 낫다.
그래서 제가 여러분 실제로 이 사람은 4.0만 넘으면 되지 사적에 넣는 게 그렇게 선호되지는 않아요.
이쪽 연구계 쪽에서는 연구하는 쪽 RND 쪽에서는 왜냐하면 이상한 짓을 자꾸 해봐야 되거든.
무조건 효율적으로 하는 게 좋은 게 아니라서 연구는 항상 RND는 과잉 투자, 과잉 투자는 다른 생각도 자꾸 해봐야 되고 그렇죠 그래야 창의적인 게 나오거든요.
그래서 이거 지금 여기 맞춘 거 나왔는데 사실은 틀린 거 되게 많거든.
보통 타 파이브도 근데 맞아 그래도 여기 보면 기어가고 있는 건 좀 그렇지만 여기까지는 별로 큰 문제없거든요.
사실은 카트럭 크죠 그렇죠 그래요 갑시다.

참석자 1 29:41
그리고 이제 c 아키텍처 교과서에도 나오는데 여기도 지금 제가 갖다 놨는데 이거 지금 할까 하다가 진도를 빨리 나가야지 숙제를 해서 이제 그 발하고 이제 나중에 필요하면 다시 올게요.
여러분 알겠죠 그래서 숙제 교과서로 갑시다. 교과서 교과서에 뭐라고 그랬지 교과서에 지금 8.1 교과서 완결성이 있으니까

참석자 1 30:16
그래서 신경 합성물 신경망 소개하고 쭉 나오죠. 여기 지금 여기서 보여주고 싶은 거는 283쪽에서 여러 번 보이는데 283쪽이에요.
8 1 여러분 좀 코렛 있잖아요. 코레 좀 가서 놀아봐야 알겠죠.
저 진도 빨리 나가려고 지금 코레가 안 들어가는 것뿐이라고 알겠죠.
한꺼번에 코드를 보는 게 좀 필요하거든요. 지금 교과서를 보면 이렇게 흩어져 있잖아 설명 때문에 그렇죠 크랩 보면은 쫙 있잖아요.
코드가 돌려보고 바꿔도 보고 이래야 된다고 알겠죠.
그래서 3천명 넘는 친구들이 이제 다 맞췄는데 실제 시험 문제는 사실 딴 짓을 안 해 본 경우가 많아 딴 짓을 해 보면은 평점이 낮은 면이 있어요.
저도 옛날에 학교 다닐 때도 평점은 높은데 무시당하는 친구가 있지 1등이야 항상 근데 무시당하지 잘 몰라 사실은 생각을 안 해봤어.
아마 그런 게 있다라고 숙제를 못해 맨날 그냥 이것저것 해보다가 근데 그 친구 말고 다 잘 돼요.

참석자 1 31:17
사실 나중에 잘 된다는 게 뭐냐 나중에 이제 상무가 돼 있고 전무가 돼 있고 부사장이 돼 있고 이런 거지 알겠죠 여러분 인성이 좋아야 돼.
팀워크 잘하는 사람 여즘 알겠죠 무슨 얘기냐면 내가 여러분이 이것 좀 코드 돌려보라고 자꾸 그 효율적인 게 무조건 중요한 게 아니라 궁금해하면서 자꾸 해보는 게 필요한데 여러분 너무 코드를 안 돌려보는 것 같아서 돌려보내시라고 알겠죠 그래요.
그다음에 계속 갈게요. 그래서 여기서 보면은 인풋을 해놓고 여기 지금 이거는 펑셔널 AI로 해놨는데 그쵸 펑셔널 AI로 API로 해놨는데 그쵸 사실 시퀀셜 API로 해도 돼 이거는 그렇죠 보니까 그냥 그랬을 때 그렇죠 무슨 얘기인지 알지 여러분 이제 인풋 콤부투디 이렇게 쭉 쓰면 되잖아요.
똑같이 그렇죠 여기 컴투디 레스 플링 컴프트디 레스플링 컴프투디 하다가 마지막에 댄스 넣어야 돼.

참석자 1 32:08
그쵸 데스트 이거 10개 하는 이유는 이게 10가지 구분하는 거라서 이거 지금 엠리스트 하고 있거든요.
계속 그쵸 마지막 소프트 웨이스트 해줘야 돼. 그쵸 그려 그리고 사실 뒤에 가면 나오지만 여기 사실은 보통 이렇게 안 하고 이 댄스 여기 플랜트 하고 나서 데스트를 보통 여러 개 또 만들어요.
층을 데스 층 한 개로 끝나지 않고 좀 몇 개 넣기도 해요.
많이 알겠죠 그리고 데스트 넣을 때는 드라마도 많이 쓰고 이해되죠.
여러분 그리고 보통 이제 패턴을 보여주고 있는데 컴퓨투디하고는 맥스쿨링이 나오고 그리고 필터의 개수가 증가하는 게 경향이고요.
여러분 점점 추상화 많이 시키는 거지 그래요. 여러분 일단은 그래요.
얘는 그래서 여기 그 교과서에 지금 여기 보면은 243쪽에 바로 알아와 있는데 컴플릿이 배치 차원을 제외하고 배치 차원을 제외하고서는 이렇게 나와 있죠 교과서에 그쵸 제가 방금 얘기한 거 그쵸 벌써부터 교과서 잘 적혀 있죠 그쵸 이거 시험에 내겠어요?
안 내겠어요?

참석자 1 33:18
여러분 안 돌아가거든 이렇게 안 넣으면 여러분 그러니까 여러분 엠리스트 이미지 이거 그냥 넣으면 안 들어가고 이렇게 처음 늘려줘야 돼.
그쵸 엠리스트 이미지는 요 차원밖에 없어 이 두 개밖에 없잖아 그쵸 원래 데이터가 하이트 위드밖에 없잖아.
숫자가 그쵸 선 하나 늘려줘야 돼. 그렇죠 채널 없는데도 늘려줘야 돼.
그렇죠 어쨌든 지금은 조작은 안 했지만 일단 인풋이 이렇게 적어줘야 되고 알겠죠 28 28로 적어주면 여러분 이거 에러 나 알겠죠 에러 알겠죠 네 그래요.
그다음에 서머리 하면은 이렇게 나오고 320 계산되는 거 이거 어떻게 된 거냐 320 320 계산 어떻게 된 거야?
320은 한번 해줄게요. 그래도 시험에 낸다고 그랬으니까 320에서 이게 32 어떻게 나왔느냐 32 그렇죠 이거부터 32가 어떻게 나왔어요?
32는 여기 8 1에서 필터가 32개니까 그쵸 32개가 나온 거예요.

참석자 1 34:23
그쵸 그다음에 여기 지금 컴 투디를 인풋에서 컴브 튜디를 했는데 이게 줄었죠 그쵸 왜 줄었냐?
커널 사이즈가 3이라서 3 하고 나면은 이제 두 개가 날아가잖아요.
마지막에 그렇죠 그래서 26일 그렇지 이해되죠 여러분 여기는 지금 세임 안 해줬거든 여기는 지금 세임 안 했잖아요.
여기 여기 여기 봐 봐. 여기에 패딩이라는 파라미터 없잖아.
그러니까 줄어들었어요. 그쵸 이해되죠 그다음에 이 파라미터 계산은 어떻게 되는지 보면은 파라미터 계선은 얘가 지금 이거 이거 컴퓨터디를 위해서 지금 이거 지금 여러분 이거 밑에 거 보는 게 아니라 여기서 계산하는 거죠.
그쵸 요거를 위한 거니까 일단 32개가 나오고 필터가 32개가 있어요.
총 그쵸 곱하기 가로가 나오죠. 한 필터에 그쵸 한 필터에서 파라미터 그쵸 이해되죠 여러분 피터가 총 32개 한 필터에 몇 개가 있냐 이거잖아요.
그쵸 한 피터는 지금 걔가 필터가 커널 사이즈가 얼마야 3이니까 그쵸 3의 제곱 그쵸.

참석자 1 35:29
그게 이제 한 터널의 가로세로고 그리고 뎁스가 얼만가 여기 여기 여기 앞에 나오는 게 1 2 1이잖아.
그쵸 이게 1 곱하기 그렇죠 이해되죠. 여러분 그리고 바이어스가 있으니까 일 더 해야겠지.
그쵸 바이어스 안 쓰라고 안 하면 나는 바이어스가 있어.
바이어스 없으면 안 하고 싶으면 바이어스는 난 이렇게 쓸 수도 있어요.
여러분 파라미터로 디폴트는 있다는 거지 알겠죠?
있는 게 정상이에요. 알겠죠 그러면 이게 얼마가 9 더하기 하면 10이잖아요.
그쵸 그래서 320이 나왔죠 그쵸 할 수 있겠죠 여러분 그래요.
그래서 답을 320일 써도 좋지만 난 이렇게 쓰는 거 좋아한다는 거죠.
여러분 시험에 3곡 이제 3 곱하기 3 쓰던지 3 10 쓰던지 어떻게 알아 알겠죠?
여러분 그다음에 그래서 총 파라미터 개수가 나왔고 그래서 설명을 코드 3 2로 그냥 갈까요?

참석자 1 36:25
3 2로 가면은 여기서 중요한 게 뭐냐 아까 지금 이제 얘기했듯이 이미 여기 이제 합성고 여기다가 컨버레션 뉴럴 네트워크에다가 입력을 넣기 위해서는 항상 모양이 이 4차원이어야 돼요.
그쵸 원래 이제 6천 개고 28 곱하기 28이었는데 그쵸 배치가 6천 개고 6만 개고 6만 개래 6만 개지 6천 개가 아니라 그렇죠 28 28이었는데 이를 추가해 주는 거죠.
그쵸 이해되죠 여러분 됐어요. 이거 이거 이거 6만 208 인편 써주는 귀찮잖아 그쵸 그러면은 제가 아까 보여준 코드 있었잖아요.
땡땡땡 하고 mp7 티 엑시스 하는 거 있잖아요. 그렇게 할 수도 있었지 그쵸 그리고 제가 많이 하는 데 있으니 이렇게 안 하면 귀찮아서 원래 트레인 이미지 점 쉐입을 적어주고 콤마 일하기도 하고 트레인 이미지 점 쉐입 요 쉐입 하면 여기 나오잖아.
그쵸. 근데 걔를 또 풀어헤쳐야 돼.

참석자 1 37:23
풀어 헤치는 방법은 또 이제 거기다가 0 써가지고 하고 이렇게 할 수 있는데 어쨌든 알겠죠 여러분 방법은 많아요.
알겠어요 어쨌든 중요한 건 뭐냐 여러분 이렇게 알면 안 돌아간다는 이런 거 알고 있어야 돼요.
알겠죠? 그래요. 그다음에 250으로 나눠버렸네.
그냥 영어 이사회를 만들어버렸네. 그쵸 이해되죠?
여러분 똑같이 텍스트 이미지도 똑같이 이제 1 추가해 주는 거 했죠 그렇죠 여러분 그래요.
컴파일 하는 거 했고 그다음에 이제 트레이 피 다 털 p3 f 5로 해가지고 뭔가 해봤더니 6991이었대요.
그래서 여기 여기 나와 있는데 완전 연결 네트워크는 97.8%였는데 여기 99.1로 되게 잘 돼요.
이게 여러분 조심할 게 90점 에큐러시가 97.8에서 99.1라면은 이거 뭐야 지금 2%도 안 늘었네 이러면서 2%도 안 늘었다고 생각할 수 있잖아요.
여러분 봐봐요.

참석자 1 38:23
이게 얼마 늘었어 이게 빼기 하면 얼마냐 1.3이잖아 1.3%잖아요.
늘어나는 이야 이게 정확도가 겨우 1.3% 늘었어 이렇게 할 수 있잖아요.
여러분 생각할 수 있잖아요. 근데 그게 아니라고요.
왜냐면은 에러가 사실은 이게 얼마였어요? 말 틀린 게 몇 프로 틀린 거야 2.2% 틀리고 있었죠 그쵸 여기는 얼마 틀리는 거예요?
9% 틀리죠 그쵸 0.2% 0.9% 그쵸 요거를 가지고 비교를 해서 60%나 줄었다고 얘기를 다는 거지 에러가 60% 주는 건 되게 어마어마한 일이잖아요.
여러분 틀리는 60%가 줄었잖아요. 그래서 이 숫자를 여러분 실제 우리가 어떤 게 중요하냐 에러 효율이 중요하죠.

참석자 1 39:06
사실은 정확도가 0.1% 늘고 이러는 게 이쪽 큰 쪽에서는 그게 만약에 원래 정확도가 만약에 60% 이랬어 그러면서 62% 넣는 건 별거 아니지만 997.8에서 99.2 되는 건 어마어마하게 잘 한 일이라는 거를 여러분이 알아야 된다는 거지 이것도 실제로 국어적인 문제지만 굉장히 중요하겠죠.
여러분 이렇게 별로 가치를 못 알아볼 수 있잖아요.
괜히 겨우 1.3% 이거 갖고 잘했다고 할 수 있어요.
이렇게 표현하면 안 되고 뻥 치는 게 아니라 실제로 에러율이 60%나 줄었다고 이렇게 얘기해야 된다고 알겠죠.
알겠죠. 여러분 일하기가 그렇다고요 알겠지 그럼 CNN이 엄청 좋은 거지 그러니까 댄스보다 알겠죠.
이거 또 해놓고도 이렇게 진가를 못 알아보면 또 곤란하잖아.
알겠죠. 틀리는 정도가 60%나 줄었는데 그렇죠 엄청나잖아요.
그렇죠 그래요. 1등으로 가느냐 마느냐 그러면 그렇죠 여러분 알겠죠 그래요.

참석자 1 40:08
그다음에 다음 페이지 286쪽에 여기 보면은 여기 보면 이게 멀리서 보면은 이제 사로 보이지만 사실 자세히 뜯어보면 미시적으로 보면 또 이제 막 거의 글씨처럼 보면 이런 특성이 보이는 사에는 그렇죠 선 같은 게 있어야 돼 사회는 그렇죠 그런 거고 지역 피터 그리고 이제 엣지 누가 모서리 그리고 질감이 이제 부드럽냐 이런 거 있잖아요.
그런 거 되게 가까이서 봐야지 보이는 거잖아요. 여러분 그렇죠 멀리서 보면 안 보이죠.
근데 그런 게 중요한 특성일 수도 있잖아요. 그쵸 사실 실제로는 안 그래 그쵸 이게 여러분 d에 이거 뭐야 저기 엠리스트를 이상한 쪽으로 학습할 수도 있죠.

참석자 1 40:48
라벨의 이상을 붙여가지고 예를 들어서 어느 지역 사람들은 또 나이에 따라서 나이를 분류할 수도 있고 그런 거 할 수도 있을 거 아니야 라벨에 신기한 거 붙여놓으면은 그러면 이제 쓰는 힘의 이런 것도 그쵸.
텍스처 이런 게 중요할 수도 있고 그렇다는 거지. 아니 그러니까 목적에 따라서는 텍스처가 되게 중요할 수도 있잖아요.
그쵸 그리고 여기 예를 들어서 제가 옛날에 소고기 문제 잘 내는데 소고기가 한우가 꽂혀가지고 한우에서의 텍스처.
그쵸 바르잖아 그런 거 흑백으로 봐도 그렇죠 여러분 그러니까 흰 게에 있는 게 다 골고루 섞여 있어야지 텍스처가 어쨌든 텍스처가 다르죠.
그래서 그래서 여기 보면은 여기 고양이 여기도 이제 아주 가까이 근시로 본 것들이죠.
여러분 그렇죠 근시 약간 좀 멀리서 본 거. 그쵸 그렇죠 여러분 전체적으로 보는 게 아니라 이렇게 이렇게 보면서 점점 추상화시키는 거지.

참석자 1 41:38
그쵸 근데 이게 이렇게 보려면 좀 멀리서 봐야 되니까 이미지를 작게 만들어서 볼 필요가 있는 거지 그쵸 이해되죠 여러분 그래요.
그리고 교과서 이제 288쪽에 나오는 게 이게 여기 교과서에 여러분 진한 글씨로 되어 있는 거 있죠?
아까 286쪽이나 287쪽에 진한 글씨로 되어 있는 거 다 중요하겠죠 제가 했던 거예요 라는데 알겠죠 아세요?
알겠죠? 하나 더 할까 그래도 그 그리고 287쪽에 맨 아래에 287쪽에 맨 아래에 응답 및 리스펀스 맵이라고 돼 있잖아요.
리스펀스 맵 저는 이 용어 한 번도 안 썼잖아요. 그쵸 이게 액티베이션 맵이라고 저는 썼었어요.
그쵸 활성화되는 것처럼 해가지고 그렇죠 여기 리스퍼스 맵이라는 용어를 썼어요.
그쵸 근데 뭐 같은 건지 알겠죠? 여러분 됐죠 그다음에 288쪽은 288쪽은 특성 피처 맵이죠.
피처 맵 이게 영어로 피처 맵이 얘기했죠. 특성 맵이 그쵸 특수성 맵이라는 게 딱 나오잖아요.

참석자 1 42:49
그쵸 그래서 결국은 깊이 축에 있는 각 차원이라는 게 그 플레인을 의미하는 각 플레인.
그쵸. 깊이 축에 있는 각 차원이라는 게 플레인이라는 거 알겠어요?
여러분 가로 세로만 있는 거잖아요. 그쵸 알아먹죠.
여러분 우리가 보는 이미지 같은 거죠. 그쵸 그게 여기에 응답 맵 적혀 있는데 저는 여기 리스펀스 맵 돼 있지만 저는 이제 액티베이션 맵이라는 용어를 썼어요.
그쵸 피처 맵 그쵸 그게 이런 게 여러 개 쌓여 있는 거잖아요.
그쵸 컴블 할 때마다 필터 개수만큼 그쵸. 필터 하나가 이렇게 쌓여 있는 거잖아요.
그쵸 필터 하나가 이거 하나 만들었네. 그쵸 이해돼요 여러분 이 필터는 여러분 뭐예요?
정체가 이 필터 교과서 지금 이거는 지금 이 사선으로 위로 왼쪽 위로 올라가는 그렇죠 왼쪽 위로 올라가는 사선 내 특성을 잡으려고 잡은 거잖아요.
그쵸 왼쪽 위로 올라가는 것만 나오는 이 진하게 나오는 게 아니라 숫자가 큰 거죠.

참석자 1 43:47
여기 흰색 검은색이 0이고 그렇죠 이렇게 통과하고 나면 이렇게 된다는 거지.
왼쪽 위로 올라가는 부분만 지금 활성화됐잖아. 됐죠 이 느낌이 오죠.
무슨 얘기인지 이게 자기와 비슷한 놈을 뽑아내는 거야.
그렇죠 그러니까 그렇게 하는 거예요. 이게 그래서 그런 걸 학습시키는 거지.
이게 내가 뭔가 특성을 파악하는 데 도움이 되는 걸로 필터가 변형이 되는 거지.
이 필터가 필요하면 숫자 인식하는데 그럼 그 필터가 살아남는 거고 그쵸 없으면 변형돼가지고 다른 걸로 바뀌는 거고 그렇죠 학습 안 시키면 이해되죠 여러분 그래요.
그리고 289쪽에 이제 설명이 이 그림이 있는데 이거 설명하는 데 시간 쓰는 것도 너무 아까운 것 같아요.
별로 괜히 어렵게 설명해 나와서 저는 별로인 것 같아요.
그리고 약간 화살표도 틀리고 해가지고 고쳐줄까요?
그냥 넘어갈까요? 초소 넘어갑시다. 그냥 제가 열심히 보고 주라 있잖아요.
여러분 실시간 보세요. 알겠죠?

참석자 1 44:42
이거 270쪽이랑 251쪽에도 열심히 설명 잘해놓으셨는데 나는 굳이 이렇게 설명하지 말고 아까 제 슬라이드 있는 게 편해서 그냥 넘어갈게요.
여러분 시간 낭비하지 말고 알겠죠 이분이 좀 독특하게 설명하시려고 노력해가지고 난 결론 같아 이거 알겠죠 그래서 292쪽으로 넘어와서 최대 플링도 지금 나오잖아.
29 이쪽이 292쪽 최대 플링 맥스 플링이죠. 그쵸 여러분 그렇죠 그리고 이것도 여러분 맥스라는 말은 여러분 이제 너무 한글 같지 않아요 다 알잖아.
그렇죠 그럼 최대 플링이 아니라 맥스 프링이야 사실은 그냥 그쵸 맥스를 0으로 맥스 필링이 온 거겠죠 맥스 프린이 그래요.
이것도 그림이 없잖아요 여기는 그쵸 제 그림으로 다 이게 잘 되잖아.
그렇죠 안 그래요 중요한 건데 이거 이거 볼 필요 없지 이거 지금이지 내가 왜 이러고 있는지 알겠죠?
여러분 그게 아까 그림이 낫지 않나 이거보다 이렇게 공부하는 것보다 됐죠 넘어갈게요.

참석자 1 45:43
고급 내용이 많은데 쉬운 내용은 또 너무 즉흥적으로 설명하셔가지고 불편해 아예 넘어갈게요.
그래서 8.2절 할게요. 여러분 그래서 새로운 내용이 이제 8.2절이 그러니까 제가 슬라이드를 그렇게 길게 했던 게 한체 물절로 끝났고 다른 표현으로 돼 있는 거 알겠죠?
여러분 잘 안 보이잖아. 근데 제가 막 강조했던 내용이 그쵸 잘 모르겠잖아요.
여러분 피터 커널 잘 모르겠잖아. 근데 지금 저한테 아까 그 슬라이드 보니까 잘 안 왔잖아.
그쵸 그 슬라이드를 8.2절에 대한 보조로 쓰는 거야.
알겠죠? 그리고 이제 8절 2절부터는 교과 지금 제 슬라이드에 사실은 없어요.
알겠죠? 있는 것도 있지만 이거 여기가 더 잘 돼 있으니까 이걸로 볼게요.
여러분 여기는 8.2절이 사실은 그냥 8.2절이 중요한 게 아니고 다시 차례로 봅시다.
차례 88분이네 차례 차례 차례예요. 여러분 맨 앞에 숙소 없는 차례 여기 여기 요기 있죠?
요기 요기 요기 요기 보세요.

참석자 1 46:41
보이죠 여러분 이거 이거 알겠죠? 여기 여기 봐봐요.
큰 그림 큰 그림을 보면은 8장에 여러분 8.1절에 합성 신경만 소개하면 되고 그렇죠 8.2절이랑 8.3절이 뭐예요?
제목이 8.2절의 제목이 뭐야? 소규모 데이터센으로부터 이렇게 있죠 그렇죠 8.3절이 사전 훈련된 모든 사람이 이렇게 있죠 그렇죠 여러분 그래서 8쪽 2절이 중요한 게 아니고 8.3절이 중요한 거예요.
사실은 8쪽 2절은 밑바닥부터라고 적혀 있죠. 8.3절은 사전 모델 활용하기 이게 있죠 밑바닥부터 하느냐 뭔가 기존에 있는 걸 쓰느냐 이 두 가지가 갈림길이 있는 거예요.
이해돼요. 8.2절은 이렇게 하지 말라는 얘기예요.
사실은 보통 이제 앞으로는 여러분이 8.3절로 가야 된다는 거에 이게 떡밥이에요.
알겠죠? 8.2절은 상대적으로 덜 중요해요. 그니까 제목 자체로는 알겠죠.

참석자 1 47:42
지금 사실 8.3절이 이제 영어로 여러분이 필기해 놓을 필요가 있는데 교과서에 그렇게 이 용어가 안 나오는데 트랜스퍼 런이 우리나라 말로 전이 학습이라는 거예요.
그리고 이게 너무 중요해요. 지금은 우리 교과서에는 지금 8장에 나오는데 딴 교과서에는 이게 막 2장이 튀어나오고 3장이 튀어나오고 막 이래요.
근데 그 교과서 별로 안 좋아서 못 쓰는 거지 또 다른 것도 엉망이라 가지고 알겠죠 최신 버전 최신 책들은 거의 저녁 학습 매하게 나오기 시작해 오픈소스 활용하는 거 비슷한 거예요.
여러분 요즘 생으로 짜는 게 어디 있어? 라이브러리에다 갖다 써야 될 거 아니야 여러분 테스트 플로우로 짤 거예요.
여러분이 아니지 갖다 써야지 그쵸? 텐스 플로우 짜는 걸로 이걸 왜 배워?
지금 알겠죠? 마찬가지로 밑바닥부터 하는 건 사실 별로 안 좋아해요.
알겠죠?

참석자 1 48:25
뭔가 오픈 소스를 잘 부합해서 구해가지고 웨이트 구해 와서 전이 학습시키는 게 맞다는 거지 알겠죠 그래서 8.3절이 여러분 저 필기 트랜스퍼 러닝 전이 sc가 주관식으로 낼 거예요.
알겠죠? 클로즈 등으로 이게 뭐 하는 거냐면 용어 가지 두 단어를 쓰셔 하면 트랜스퍼 러닝 쓰든지 전이 학습 쓰든지 알겠죠 사전 훈련된 웨이트를 가지고 와서 훈련하는 게 전이 학습이에요.
알겠죠? 반대말은 밑바닥부터 하는 거예요. 알겠죠?
아무것도 없이 생으로 하는 지금까지 되게 생으로 했죠.
여러분 그렇죠 생으로 하는 거는 요즘 세상에 이상한 일이다.
쓸 만한 거 하려면은 그냥 생으로 하는 거는 생으로 하는 건 여러분이 되게 돈이 많아야 되는 돈이 많아야 돼.
뭔가 알겠죠? 무슨 얘기인지 알겠죠? 이게 컴퓨터가 되게 좋아야지 되지 알겠죠 10분 쉬었다 할게요.
여러분 지약한 사람 얘기하세요.


clovanote.naver.com

딥러닝 day17_2
2025.05.14 수 오전 11:01 ・ 50분 21초
심승환


참석자 1 00:01
원 파일을 잘 모르는 것 같긴 한데

참석자 1 00:07
원 파일 여러분 다음 시간에 보충 설명을 하긴 하는 게 낫겠다.
원 파일로 보충 설명을 하고 원 파일 슬라이드를 만들어 볼게요.
여러분 사실 만들어 놓은 게 있어요. 여러분 왜냐하면 중간고사 때도 사실은 여러분이 제가 이게 넘판 설명을 그때 녹화한 적 있었잖아요.
그때 거기다가 대충 했거든요. 그 시험이 다 끝났더라고요.
보니까 그래서 뭔가 전달이 잘 안 되는 것 같아서 이 파일 처트를 올릴게요.
잠깐 대충 강의해 줄게요. 대충이라는 게 이제 다 알아서 하시면 좋겠고 여러분이 어쨌든 지금 8장 이제 진도 나가는데 8장에 목차를 다시 보면은 우리 목차 여기 이게 밑바닥부터가 영어로도 중요한데 프롬 스크래치라고 불러요.
여러분 스크래치 성 스크래치 상체를 리키 막 쓰고 막 이런 아무 막 그냥 있는 걸 말하잖아요.
그쵸 바닥 뭔가 끄적끄적거리는 이런 뜻이거든 스크래치가 이게 프롬 스크래치 스크래치라는 것도 있어요.

참석자 1 01:09
여러분 그림으로 막 그리는 거 그래서 스크래치가 프로 스크래치가 8.2절이고 8.3절이 트랜스퍼 러닝이라고 트랜스퍼 트랜스퍼 러닝이에요.
그래요. 그리고 보면 지금 8.3절에 70% 런닝 들어가서 설명하지 일단 8장 2절 내용을 봅시다.
8장 2절 내용은 데이터가 소규모예요. 소규모 많은 경우에 여러분 데이터가 부족해 맨날 항상 데이터는 항상 부족해요.
교과서 들어갈까요? 바로 보면 몇 쪽에 있냐 해서 8점 들어가면

참석자 1 01:58
294쪽인데 294쪽에 두 번째 줄에 보통 적은 샘플이랑 수백 개에서 수만 개 수만 개도 여러분 잡대.
어쨌든 지금 우리 앱 리스트를 6만 개잖아요. 그쵸 근데 그 정도 하면 사실 많은 건데 보통 수백 개면 작다고 봐요.
여러분 알겠죠 한 100개 해도 이 정도면 됐지 하는데 사실은 작아요.
사실은 그래서 지금 작은 걸 예를 들게 하기 위해서 교과서에서 지금 5천 개의 강화자 고양이 사진 이 정도면 작다 이거지 그걸 가지고 뭔가 이미지 분류하는 걸 해보려고 하는 거예요.
그래서 훈련을 위해서 2천 개 쓰고 검증에 만 천 개 쓰고 테스트 2천 개 써서 해보겠다는 거예요.
알겠죠 그리고 제가 이거 한번 언뜻 넘어가 버려가지고 원래 여러분 데이터 맨날 이렇게 할 때 이거 계속 까먹고 안 할 것 같아서 지금 잠깐 얘기해 주면은 미리미리 얘기해 줄게요.
언제 해야 될지 모르겠어요. 보면은 우리가 핏 할 때 나중에 보면 할 것 같아서 여러분 언젠가 하겠지 내가 이거 어쨌든 까먹지 않고 할게요.

참석자 1 03:10
뭐가 있는데 지금 그래서 제가 까먹어서 안 해서 이거 이거 다시 할게요.
여러분 요거 이렇게 하는 거 맨날 여러분 이번에 시험도 냈는데 실제로 이걸 우리가 슬라이싱으로 하고 있었죠.
그쵸 슬라이싱으로 원래 데이터 전체 갖고 로딩 해가지고 그렇게 하는 거 있었죠 그렇죠 그거 말고도 방법이 여러 가지가 있는데 제가 그거 까먹지 않고 할게요.
그리고 이렇게 하기로 했고 이건 중요한 게 아니라 이렇게 하기로 했다는 거지.
그다음에 이게 35쪽에 작은 데이터셋 문제에서 딥러닝의 타당성 이렇게 돼 있는데 8.21조 제목이죠.
제목이 중요하다니까요. 여러분 제목이 자금 대타세 문제가 문제가 이제 작아도 실제로 대부분 경우에는 항상 데이터 부족하지 그래도 충분하다.
상대적으로 충분하면은 괜찮다. 그래서 이 문제 규제가 잘 돼 모델이 자꾸 규제가 잘 돼 있다는 건 여러분 적절히 규제라는 거 배웠잖아요.

참석자 1 04:07
우리가 그렇죠 너무 언더오버 피팅하지 않게 잘 만들어주면은 은근히 수백 개의 샘플로도 충분한 경우가 많은 게 되게 유용하게 알려져 있어요.
그래서 할 만하다 그 얘기를 하고 있거든요. 진짜 많은 경우에 돼요.
그래서 그걸 해보려고 하는 거고 그래요. 그리고 사실 데이터셋이 작은 걸 해결하기 위해서 지금 294쪽에 괄호 적혀 있죠.
데이터 증식이라고 적혀 있죠. 데이터 증식이라는 게 씨라는 글씨로 적혀 있죠 그쵸 영어로 데이라 오그멘테이션을 해야 돼요.
다른 데이터를 잡으면 반드시 해야 되고 안 하면 이상한 거.
그렇죠 한때는 데이터 오브 베티션 했더니 너무 잘 되더라 이런 게 논문이 맨날 나왔었거든요.
한 3년 전 4년 전에 지금 안 하면 좀 이상한 거지 그러니까 그래요.
그리고 어쨌든 다그데이테스 데이터 모멘테이션 해가지고 잘해보려고 하는 거고 그다음에

참석자 1 05:13
그래가지고 지금 이거 뭐 하려고 하냐면은 데이터 내려받는 거를 한 데이터 샘플 데이터를 위해서 태그 걸 쓰겠다고 돼 있는데 그리고 296쪽에 데이터 내려받기 이렇게 쫙 적혀 있거든요.
태그해 갖고 하는 거 케이글이 데이터셋이 많아서 여러분이 쓸 만한 거 하고 싶은 걸 여기서 많이 구할 수가 있어요.
케글이 새로운 게 자꾸 올라오니까 거기 보면 강아지 고양이도 많이 있어요.
재밌는 건 이제 실제로 더 여러분 더 현실적으로 강아지 고양이가 이게 이미지가 굉장히 막 다양하죠.
그쵸 크기도 다양해 막 이렇게 작은 것도 있고 이렇게 이렇게 생긴 것도 있고 그렇죠 똑똑 같지가 않잖아요.
그쵸 데이터 전처리를 실제로 많이 해야 되거든요.
그런 뭔가 아까 리스트 같은 건 되게 예쁘게 딱 28 곱하기 28로 돼 있잖아요.

참석자 1 05:55
근데 얘는 지금 진짜로 케이블 같은 데 오히려 약간 더 현실적이지.
그쵸. 데이터가 제각각이니까 그 데이터 전처리를 오히려 더 잘해야 되기 때문에 더 의미가 있는 거지 교과서에 지금 이렇게 쓰는 거야.
알겠죠? 이것도 도움이 되겠지 그쵸 그래가지고 실제로 이거 받아가지고 저거 다운로드 받아가지고 그 296쪽에 처럼 해가지고 298쪽에서 다시 이렇게 이런 이미지 이렇게 디렉토리 만들어가지고 하는 이게 보통 하는 방식이에요.
여러분 실제 여러분 일할 때 이렇게 해야 돼 그렇게 알아서 하세요.
알겠죠? 알겠죠? 보고 하세요. 이거 보고 이거 강의까지 할 건 아닌 것 같아요.
알겠죠 따라서 잘 하세요. 알겠죠? 실제로 일할 때 이렇게 하는 거예요.
그리고 그거를 이제 데이터 받은 거를 아까 만들어진 디렉터 이렇게 만들어서 트레인 이렇게 만들어 가지고 하는 거 있죠 그 코드도 여기 있어요.
여러분 알겠죠? 알겠죠? 여러분 이거 파일선이야 잘하세요.
알겠죠 우리 있으니까 잘하세요.

참석자 1 06:58
그리고 299쪽에서 모델 만들기 이렇게 있죠 그래서 모델 만드는 게 있는데 이렇게 코드 8 7 보면은 이제 그냥 디폴트로 모델을 만들었는데 보면 재밌는 게 리스케일링이라는 레이어가 있어요.
여기에 보면 텐스 플로우 버전이 높아지면서 계속 모든 걸 이제 레이어로 만들기 시작했어요.
사실 이게 댄스 플로우가 파이토치보다 약간 좀 망하기 시작해가지고 케라스 갖고 오면서 이제 타이 토치로 따라하기 시작했는데 파이토치에서는 액티베이션도 여러분 계층으로 해놨어요.
멜로드 같은 것도 왜 왜냐하면 이제 리뉴얼 한 다음에 사실 이게 여기까지 컴퓨터 디구나 이거 댄스 댄스가 사실은 여러분 리뉴얼 한 다음에 액티베이션 통과하는 거잖아요.
그쵸 그러니까 리뉴얼하고 그냥 댄스를 아예 그냥 리니어 하고 그다음에 이제 소프트맥스나 시그모이드나 멜루나 이런 걸 계층을 통과시키는 걸로 해놨거든요.
그거 더 와닿잖아요.

참석자 1 08:05
그렇죠 안 그래요 여러분 그래서 그런 추세에 따라서 여기도 얘도 이제 텐스 플로우도 사실은 아까 리스 캘링 우리가 일일이 다 했잖아요.
맨날 데이터에 대해서 이해돼요. 여러분 그거를 우리가 레뉴얼로 넣을 수가 있어요.
레이어로 넣으면 좋은 게 데이터를 직접 변형 안 해도 여기서 진짜 무조건 해 주니까 편하잖아요.
그쵸? 이해 되죠 여러분 입력을 무조건 250으로 나누는 거 알겠죠?
여기 점 붙여주는 거 왜 그런지 알아요? 소수로 만드는 거예요 소수로 1점 하면 여러분 1점이 소수가 되잖아.
그렇죠 혹시나 정수로 만들어버릴까 봐 사실은 파이썬은 디폴트가 지금은 소수잖아요.

참석자 1 08:43
거기 c가 안 그렇지 c는 숫자 1로 해버리면 그 내림 돼버리니까 1 나누기 255 하면 0 돼버리잖아.
1 1 나누기 255 해야지 이거 그러니까 연습이라니까 뭐랄까 이거 지금

참석자 1 09:01
관습이죠. 관습 그래요. 그다음에 어쨌든 아까 있던 코드에 비해서 펌프 2D 맥스 플린 이거 한번 하나 더 추가했고요.
레이어도 하나 좀 더 늘어났고 256짜리가 아까에 비해서 했어요.
보니까 제가 비교해 봤더니 그렇더라고요. 그래서

참석자 1 09:27
이렇게 했고 그다음에 이거는 지금 이 문제는 맨 마지막에 시그노이드고 1로 했는데 이거는 지금 무슨 얘기예요?
여러분 이렇게 돼 있으면 2개 300쪽에 있어요.
300쪽 300쪽에 이거 있죠 300쪽에 위에 아웃피스하고 댄스 이거 이거 하고 cg5에 두면 여러분 무슨 문제 푸는 거예요?
바이너리 프리시케이션이죠. 그렇죠 한류로 나오는 거죠.
그렇죠 그 개랑 고양이 물이랑 지금 분리하려고 하는 거예요.
개하고 고양이 그래서 개인지 고양이인지 맞추는 거는 여러분 여기를 2로 할 필요가 없지.
그쵸 일로 하고 시그모드로 하는 게 정상이지 그쵸 그래서 정해야 돼.
개일 확률인지 고양이일 확률인지 정해야 돼. 그쵸 이해되죠 여러분 성중이 문제 이런 거 정하고 나서 거기에 맞춰서 해야지 그래요.
무조건 개인 확률을 구한다. 무조건 고양이 확률을 구한다.
그런 거 없어도 다 그거 정확히 확인하고 해야 돼. 그렇죠 마음대로 생각하면 안 돼요.

참석자 1 10:23
그다음에 데이터 전처리가 벌써 82.4절에 나오지 그래요.
이거를 더 가르쳐주고 싶었던 거지 요구서에서 그래서 이게 한 301쪽에 지금 JP 콘텐츠를 RGB로 바꾸고 이런 거 있잖아요.
그쵸 그리고 사이즈를 다 똑같이 만드는 게 좋죠. 그래서 189면 180 만들고 이런 거 배치로 묶는 거 원래 옛날에 우리가 텐서 플로우 앱 리스트 이런 거 데이터 로딩하면 저절로 그렇게 됐었거든요.
원래 배치로 되게 배치로 이렇게 딱딱 피딩하게 근데 그런 게 안 되는 진짜 임시 작부 할 때는 교과서에 지금 나온 것처럼 이렇게 지금 302쪽에 보면은 이렇게 이미지 이 데이터 셋 프로 디렉토리라는 이런 펑션을 이제 인포트 시켜가지고 유틸에 있어요.
유틸에 이걸 인포트 시켜서 요 함수를 써서 그 디렉토리 디렉토리에다가 지금 디렉토리 이거 이거 스팅이에요.
스트링 스틴 디스 이름 적어주고 그리고 이미지 사이즈 어떻게 하면 배치 사이즈가 얼마인지 이렇게 정해줘서 데이터셋을 만들 수가 있어요.

참석자 1 11:28
옛날에 엠리스트 하던 것처럼 그래요 알겠죠 그래요.
그다음에 데이터셋 객체에 대해서 이제 여기 이게 전부 다 데이터셋에 이렇게 이게 리턴되는 게 데이터셋인데 데이터셋은 어떤 거냐면은 302쪽이랑 303쪽에 주황색 박스로 나와 있는데 데이터셋 객체 이야기 보이죠.
여러분 그쵸 요거 있죠 요거 요거 봅시다. 여러분 보면은 이게 예를 들어서 그냥 여기서는 이제 랜덤하게 뽑아내는 건데 그래서 여기 TF TF 데이터 데이터 셋 이런 요 클래스로 우리가 뭔가 만들고 싶은 거예요.
근데 프롬 텐스 슬라이스 지금 이렇게 아무거나 아 변수 갖고 할 수도 있고 아무 어레이 없고 할 수도 있고 어쨌든 이 타입으로 질문이 있었나 이 타입으로 어쨌든 만들면은 뭐가 좋냐면은 요렇게 아까 그 데이터 셋 있잖아요.
만든 거 걔 이니머레이트 해가지고 뽑아내면은 지금 하나씩 뽑아내는데 폼은 돌아서 지금 인터레이션 2까지 돌았잖아요.
여러분 브레이크 하니까 그러니까 0 22만 찍었잖아요.

참석자 1 12:42
그쵸 계속 나올 건데 이게 16개씩 튀어나오고 있죠 16개씩 튀어나왔냐면은 아까

참석자 1 12:53
16개씩 튀어나가는 게 샘플 하나씩 하나씩 튀어나오게 하는 거지 샘플 하나씩 샘플을 원래 이게 사이즈가 116이니까 이게 하나가 16개씩 들어 있으니까 이게 천 번씩 돌겠죠 그러니까 이게 천 번씩 돌면서 나올 거예요.
알겠죠? 하나씩 하나씩 튀어나오게 돼 있어요. 기본적으로 그런데 우리가 보통 그렇게 이렇게 하나씩 하나씩 해서 우리가 스텝을 도는 거가 트루스gd죠 그쵸 나중에 트레인 시킬 때 데이터가 이렇게 하나씩 하나씩 튀어나와서 트레이닝 시키는 거는 트루 SGD라고 그랬죠.
우리가 보통 많이 쓰는 거 미니 배치 쓴다고 그랬잖아요.
미니 배치 미니 배치에서는 어떻게 한다고 그랬어요?
여러분 미니 배치에서는 배치 사이즈를 우리가 1 이상을 줘야지 그쵸 그래서 이렇게 데이터셋 점 배치 이렇게 하면 설정을 하는 거예요.
이렇게 하면 배치가 s 배치 사이즈로 생각하면 돼요.
여러분 알겠어요 이게 어디 있냐 교과서에 303쪽에 데이터 셋 점 배치 적혀 있죠 보이죠.

참석자 1 13:53
여러분 이렇게 데이터셋에다가 점 배치 불러주면은 여기 이렇게 여기 리턴 받아서 여기다 여기 리턴 받은 게 요 배치만큼 이제 뽑아지게 변형시키는 거예요.
배치 사이즈만큼. 그래서 아까 그 배치 데이터셋에다가 다시 이렇게 폼을 돌리고 이제 찍으면 32개씩 나와요.
32개씩 이해돼요. 여러분 아까 1개씩 튀어나오는 게 이제 32개씩 이거 왜 하는 거냐면은 원래 우리가 트랜시킬 때 전부 다 미니 배치로 하는 게 좋잖아요.
그쵸. 하나씩 하는 게 아니라 알겠죠 여러분 그래서 이렇게 하는 거예요.
그리고 이제 또 이게 원래 이런 게 데이터셋에서 이런 거 많이 하는 게 셔플 하는 거 많이 필요하고 해.
셔플 셔플이 버퍼 사이즈 이렇게 해놓으면은 버퍼 사이즈 크기만큼씩 끌어가지고 쏘는 거야.
예를 들어서 그러니까 버퍼 예를 들어 전체 데이터가 여기 몇 개였어 천 개인데 버퍼 사이즈를 만약에 1010으로 했었어 그러면 10개씩 끊어가지고 섞는 거예요.
이해되죠?

참석자 1 14:56
여러분 이게 막 숫자가 몰려 있을 수 있잖아요. 데이터가 몰려 있을 수 있잖아요.
쭉 엠리스트 데이터 같은 경우는 만약에 m 리스트가 01이 막 섞여 있었잖아요.
처음에 1이었다가 그다음에 3이었다가 막 이렇게 섞여 있는데 이게 너무 막 000 1 1 2 막 이렇게 섞여 있잖아요.
근데 이런 게 적당히 안 섞여 있을 구간 예를 들어 100개면 100개씩 해가지고 협을 시키는 게 필요한 거죠.
그쵸 너무 데이터가 몰려 있으면 안 좋으니까 이거 전체 다로 할 수도 있고 전체 배치 전체 사이즈로 할 수도 있고 이해되죠 여러분 그래요.
프리 패치는 미리 메모리에 로딩을 해놓으면은 좀 더 빨리 돌아갈 수 있어서 프리 패치로 몇 개씩 할 건지 미리 정할 수도 있고 그리고 점 메타 함수 이거는 여러분 이거 원래 파이썬에 있는 거지.
그쵸 점 메타면은 뭔가 이거 이 함수 불러주는 거잖아요.
변형시켜주는 거 그쵸 그 코드 길이 줄이려고 하는 거 알죠?
여러분 그쵸 맵하고 it 많이 쓰면 돼요.

참석자 1 15:52
여러분 사실 그렇죠 인테저 밖으로 스트링 받아가지고 인풋할 때 알지 않나 여러분 그래요.
그런 거 이거 이거 보면은 그래서 33쪽 밑에 나오는 거 보면은 데이터 셋을 매해가지고 지금 함수를 그냥 남자를 만들어버렸네요.
그쵸 그래서 이렇게 리쉐입을 시켜버리는 걸 넣었네.
그렇죠 원래 x가 들어와서 리쉐입을 4 곱하기 4로 바꾸는 거죠.
그쵸 그렇게 해서 매입을 시켜가지고 하면은 변형됐으니까 원래 데이터 스텝이 하나씩 튀어나오는 게 이제 44씩 튀어나온다는 거지.
그쵸 이거는 지금 데이터셋이에요. 배치 안 한 거 아직 그쵸 그런 거예요.
여러분 맵은 맵은 원래 여러분 그냥 파이썬에서 많이 쓰던 거 이 함수를 그냥 통과시켜서 튀어나오는 거 알겠죠?
여러 줄 안 만들고 싶어가지고 이렇게 하는 거예요.
편하게 그래가지고 지금 교과서에 지금 하시 하교시

참석자 1 16:59
한 달을 안 갖다 놨네. 이 사람이 그래 하지 말지 뭐 그래요 한 다시 10분 넘어갑시다.
그러면 그다음에 8 11 8 11은 이제 밸리데이션 데이터를 메리 데이터에서 따로 해놓고 에 그래요.
바로 갑시다. 여러분

참석자 1 17:26
그냥 그림 그래 그리는 걸 한 거고 그래서 실제로 이제 훈련시켜가지고 해봤더니 지금 이게 그림 8 30 오쪽 그림인데 미안해요.
305쪽 그림인데 이게 지금 밸리데이션 에큐레시가 정말 중간을 못하고 있죠 그쵸 여기 로스도 너무 너무 끔찍하다.
그렇죠 잘 안 돼. 그렇죠 잘 안 된다는 걸 보여주고 있어요.
그렇죠 거의 너무 과대 적합이 금방 일어나 그렇죠 OPT 여기가 오피팅 된 거죠.
여기서부터 그렇죠 오피팅 된 거잖아요. 그쵸 그래서 그리고 되게 막 들쭉날쭉하고 있죠 그쵸 별로죠.
그래서 잘 안 되는 거지. 그거 그래서 이제 뭐 하냐 데이터 오그멘테이션 하겠다는 거예요.
알겠죠? 데이터 오그멘테이션 데이터가 너무 부족하다.
그래서 이거는 지금 얼마나 나올지 결국 그래서 테스트 정확도 해봤더니 테스트는 어떻게 가는 거냐면은 이밸류에이션 하는 걸로 돼 있어요.
메메이션하는 거 아닌 걸 알았나

참석자 1 18:35
여기 보면은 교과서 306쪽에 306쪽 306쪽에 이벨류에이트 이게 이게 여러분 검증하는 거 중에 마지막에 이제 테스트 정합 테스트하는 이벨류에이션 하는 거는 이렇게 이벨레이트 부르는 거예요.
여러분 여기 테스트 데이터 써 넣어야지. 여기다 밸리데이션 테스터를 쓰는 거나 그러면 안 돼요.
그쵸 중간고사에 냈는데 많이 틀렸더라고요. 여러분 알겠죠?
다시 정리하고 테스트 데이터 서 써야 돼. 그쵸 여러분 이거 이거 있죠?
테스트 이터셋 이거 사실은 추풀이에요. 그쵸 원래 이미지랑 정답 값이랑 같이 있는 거예요.
그래요.

참석자 1 19:14
그래 해가지고 로스랑 에큐이 나왔는데 69.5% 이거 못한다.
그렇죠 69.5% 예스만 맞추는 건데 69.5% 70%밖에 못 맞죠?
그렇죠 30%나 틀리고 있어요. 그렇죠 못 믿는 놈이에요.
그렇죠 자꾸 고양이 보고 개라 그러고 그러고 있는 거죠.
그렇죠 그래요. 그래서 데이터 증식을 사용하는데 데이터 증식을 하는 게 되게 편해진 게 사실 뭐 그냥 데이터를 막 그냥 뿌리고 늘리는 거 할 수도 있는데 케라스에 지금 이 좋은 게 생겨서 이런 이런 레이아웃도 생겼어요.
이런 랜덤 플린 랜덤 오테이션 랜덤 춤 이런 게 있어요.
그래가지고 여기 교과서에 지금 307쪽에 나오죠.
이게 지금 이런 레이어를 중간에 꺼주면은 이 이미지가 들어온 지 이미지가 이렇게 풀립시키기도 하고 뒤집는 거지 파리젠탈은 이제 이렇게 거울 보듯이 뒤집는 거예요.
버티칼로 하면 여러분 아래 위가 바뀌는 거겠죠 그쵸 이해되죠?

참석자 1 20:14
여러분 그다음에 로테이션을 돌리는 건데 이게 0.1이 3명 시도의 퍼센트예요.
여러분 이게 그러니까 36도 마이너스 36도 돌리는 거예요.
알겠죠? 그다음에 랜덤 줌은 0.2 하면은 확 축소하고 확대하고 하는 걸 확인하겠다는 거야.
알겠죠? 마이너스 20% 플러스 20% 그래서 이 그림을 실제로 그려보면은 이거 원래 이미지가 트레인 데이터셋에서 테이크 1 이렇게 테이크 처음 보는 건데 테이크 하면 하나씩 뽑겠다는 거 있거든 원래 트레인 데이터셋에서 그냥 뽑으라면은 배치만큼 나오잖아요.
근데 그렇게 하기 싫어서 그냥 테이크 1 하면 한 개만 튀어나오게 해요.
했죠 여러분 아니 그렇죠 한 개만 알겠죠 여러분 일하니까 알겠어요.
여러분 그래서 한 개의 이미지를 가지고 한 개의 이미지가 튀어나왔는데 한 개의 이미지를 지금 9번 돌리고 있죠.
똑같은 이미지예요. 하나의 이미지를 지금 데이터 오그멘테이션이라는 거가 지금 아까 여기 레이어 이거예요.
이거 이거 시퀀셜이죠. 시퀀셜 알겠죠?

참석자 1 21:23
이거 이거예요. 예예. 이 레인 3개 레이어가 있는 거예요.
알겠어요 이거 이거 대신에 여러분 요 골그씨 대신에 이거 써도 되는 거야.
그쵸 이해돼요 여러분 그래서 걔를 이걸 이미지로 이제 입력을 준 거지.
그쵸 그러면은 이게 아까 세 가지 통과하는 거잖아요.
랜덤하게 플립하기도 하고 이렇게 하자 플립하기도 하고 졸리기도 하고 그쵸 석대를 축소하기도 하고 그렇게 해서 그림을 그렸어야지.
여러분 이해되죠 여러분 그림을 그렸어요. 이렇게 하나 첫 번째 이거는 그러니까 여기 여기 뭐야 이게 이게 이게 한 번 통과하고 두 번 두 번 한 번 첫 번째 통과한 게 똑같은 이미지 갖고 얘 이렇게 돌리고 그다음에 돌리고 돌리고 이렇게 한 거야.
알겠어요 여러분 세 번 세 가지를 적용한 거죠. 하나의 이미지다.

참석자 1 22:20
그래서 재미있는 게 이 이건 축소가 됐고 약간 약간 이건 늘어났네.
얘 늘어났네. 이거 늘어났고 이거 추가 축소하려고 그랬어요.
그쵸 그래서 여러분 보면 축소되는 게 좀 무서운 게 이게 남는 공간이 있잖아요.
남는 공간이 복사가 돼 이 발이 붙어 있는 거 보여요.
여러분 무섭죠? 안 무섭다. 나는 무섭던데 그랬어요.
알겠죠? 여러분 이렇게 만들었어요. 근데 어차피 걔가 옆에 있다고 생각해도 상관없으니까 알겠죠.
그냥 아무거나 채우지 않고 이렇게 복사해서 해놨어요.
알겠죠 그런 식으로 여러분 이거 실제로 해보면 이제 막 랜덤하게 하니까 어차피 딴 이미지가 또 튀어나오게 다 이렇게 되겠지.
그렇죠 그래요. 그다음에 이렇게 해가지고 이거를 지금

참석자 1 23:12
그리고 이제 여기 아까랑 다르게 뭐 했냐면 모델 장식도 하고 아까 데이터 보듯이 아까 여기 이게 들어간 게 아까 뭐가 들어갔는지 알겠죠?
여러분 시퀀셜 한 다음에 착착착착 넣은 거예요. 지 세 가지 다 똑같아요.
그러니까 이거 그대로 써도 되는데 이거 그냥 그대로 써도 돼요.
그렇죠? 이해되죠? 여러분 여기 레이어드 레이트 넣어도 돼요.
알겠죠? 맨 처음에 이미지 들어올 때 이미지가 뭔가 랜덤하게 변형되는 거지.
그쵸 그래가지고 또 재미있는 게 여기 지금 마지막에 플랙튼하고 드라바웃도 넣었어요.
드라바웃 좀 더 잘하게 알겠죠 사실 드라마 넣는 것도 굉장히 잘 되는 편인데 드라마도 그렇고 드라마를 넣으면은 폭을 늘려줘야 돼요.
여러분 왜냐하면은 이게 모델이 조금씩밖에 훈련을 못하니까 더 많이 훈련시켜야 되거든요.
알겠죠? 여러분 그래서 애 폭을 지금 얼마나 했냐면 교과서에서 아까는 한 서너 번 했나 보지 여기는 지금 100으로 했어요.

참석자 1 24:10
애 폭을 그래서 애 폭을 100으로 했는데 아까 얼마였나

참석자 1 24:22
아까 얼마라 했는지 제가 안 보고 넘어갔죠. 그쵸 아까는 아까는 폭이 얼마였어요?
핏 30 맞네 30이었는데 이번에는 3배로 해줬어요.
알겠죠? 3배는 아니 3배 3.3배지 그래서 하고 했더니 그림 구경 구경 여러분 그림이 폭이 약간은 사실 30까지니까 여기서 끊기긴 했는데 30 그쵸 나아질 기미가 안 보였는데 여기 계속 나아질 수 있었잖아요.
그쵸 여기서 끊으면 안 되는 거지. 그쵸 이해돼요 여러분 이게 계속 나아지고 있는데 끊으면 곤란하잖아.
그렇죠 아까는 안 나아지니까 더 할 필요도 없었지 여기는 보면은 나아지고 있지 않잖아 그쵸 3심이 여기서부터 벌써 여기서 안 나아지니까 더 의 볼 필요가 없었죠 그쵸 이해되죠 그림을 이렇게 그린 이유도 이제 여기 어차피 해봤자 여기 잘 안 보이잖아요.
여기가 너무 조그매지니까. 그쵸 그래 여기 30까지만 해준 거고 이해되죠?

참석자 1 25:19
여러분 여기는 이제 계속 잘 되고 있으면 더 해줘야 될 거 아니야 그쵸 그래서 100으로 해준 거고 100으로 해줬더니 여기 한 이쯤에서 좀 잘 안 되겠지 진도 안 하기 시작하네.
그렇죠 여기도 그렇죠 지나서 그렇죠 어쨌든 나중에 테스트해봤더니 로드 마드 이거 저장을 했나 보죠?
언제 세이브도 하고 이제 로딩을 해가지고 테스트 리얼이 실제 코드 보면 여러분 세이브하는 게 있을 거예요.
알겠죠? 세이브 하는 거 뭐든 하는 거 다 알아야 된다죠 여러분 여러분이 숙제할 때도 이제 세이브 시켜놔야겠지.
자꾸 처음부터 다시 하려고 그러지 말고 그쵸 나중에 한 5분 동안 여러분 잠깐 딴 짓하고 전화 통화하고 나면 이제 그거 다 끊기는 거 알아요.
여러분 콜에 콜 다 끊기잖아 여러분 우리 서버도 지금 제 뒀는데 우리 서버도 시간 지나면 끊어버려요.
알겠죠 그러니까 저장해놔야 돼.

참석자 1 26:11
그쵸 알겠죠 그래서 로딩 해가지고 이멀리티 하니까 83.5% 너무 감동 약간 아까 60 70%도 안 되는데 좋잖아 그렇죠 그러니까 그래요.
그래서 데이터 어그멘테이션만 해도 상당히 잘 된다라는 걸 보여주고 있고요.
그다음에 드디어 드디어 이거 여러분 숙제 숙제에 해당하는 사전 훈련용 모델 이거 봅시다.
여러분 프리트레인드 모델 프리트레인드 영어로 이거 프리트레인드 모델이라고 불러요.
여러분 프리 트레인드 모델을 활용하는 거를 트랜스포 러닝이라고 부른다고 그랬죠 여기 교과서에는 안 나오지만 여러분 이거 제가 식을 낸다고 그랬죠 그쵸?
트랜스퍼 러닝 전이 학습 알겠죠? 교과서에 안 나오지만 정말 중요한 용어예요.
알겠죠? 언니 학습 그리고 이거 83.1절에 8.3점 8절 3절에 지금 챕터가 2개가 있는데 311쪽에 31쪽 311쪽 볼까요?

참석자 1 27:18
311쪽에 밑에 여기 진한 글씨로 특성 추출 미세 적혀 있죠 그쵸 보이죠 여러분 요게 하부절 요게 하부절이에요.
영어 이거 영어도 중요해 이거 외워야 돼. 그러니까 트랜스퍼 러닝에서 두 가지 방법이 있는데 피치 액스트렉션과 파인 튜닝이에요.
영어로 더 많이 쓴 사람들이 용어를 그러니까 미세 조정이라는 말 잘 안 쓰고 파인트리닝이라는 말을 훨씬 많이 쓴다.
파인트리닝이 중요하다는데 이러면서 그쵸 그리고 피치 익스트랙션 알겠죠 지금 NLL 같은 거 쓰고 하는 거 전부 파일 필링 하는 거가 중요한 거지 피치 익스트랙션만 할 수도 있고 그러니까 이게 약간 두 개가 둘이 같이 쓸 수도 있고 좀 더 어드벤스트 한 게 파이트닝이야 알겠죠 이게 8.3.1절이 피치 익스트랙션이고 8.3.2절이 미세 조정이에요.
알겠죠 가봅시다. 8.1조 보면은 패선 추출은 이제 피치 스트렉션 영어로 여러분 외우세요.
피치 스트렉션 그래서 그림 보면은 요거

참석자 1 28:24
이거 보면은 원래 우리가 여기 합성 고기 보면 항상 컴버넷이 쭉 나오다가 마지막에 플래트 내가지고 드라아웃 해가지고 댄스 하는 거 봤죠 여러분 항상 그 패턴이에요.
커피는 열심히 하다가 막판에는 댄스 쓰죠 그쵸 얘는 알파고도 계속 어 여러 알파고까지도 알겠죠 이게 기본 모양인데 기본 모양인데 이거를 p2x 이게 나중에 트랜스퍼 러닝하는 방식이 기본적으로 피처 스트레이션 하는 거는 어쨌든 이게 분류하는 건 여러분 이게 문제에 맞춰서 분류하는 거잖아요.
10가지를 분류하는 거면 아까 아까 우리는 지금 시그오리 썼던 거잖아요 그쵸 이해되죠 시그 오리한테 하는 거였잖아 그쵸 여기가 댄스라고 그래서 마지막에 하나였잖아 뉴런이 근데 엠리스트 같은 경우에 지금 10개 분리하는 거고 그렇죠 근데 여기 이렇게 뭔가 이렇게 뭔가 이 피처 익스텍션 합성급에서는 뭔가 특성을 배우는 거잖아요.

참석자 1 29:26
거기는 뭔가 이제 숫자를 잘 분류해도 아라비아 숫자를 잘 분류해도 가나다 분류의 곡일 수도 있고 그다음에 여기는 그냥 여기 교과서 나온 대로 하면 뭐냐면은 교과서에 나오는 것대로 하면은 여기 지금 보면 오픈 소스랑 오픈 웨이트로 웨이트가 공개되어 있는 게 이런 이런 뉴럴 네트워크 이런 네트워크가 굉장히 구할 수가 있어요.
우리가 오픈 소스로 비터브 찾으면 다 나와요. 유명한 것들 얘네들이 다 웨이트까지 다 있어요.
데이터셋이 뭐냐? 데이터셋은 데이터셋 안 적혀 있나 데이터셋은 어디 있지?
이미지 넷 같은 거 떴는데 여기다 오른쪽에 33쪽에 나와 있어.
33쪽에 이미지 넷이라는 말 나오죠. 여러분 이미지 넷 두 번째 단락에 이미지 넷이라는 말 나오죠.
이미지 넷 이미지 넷이 이미지 넷이 굉장히 이미지가 많아요.
온갖 꽃도 있고 고양이 음식도 있고 온갖 게 다 있어요.

참석자 1 30:37
여러분 그래서 한 천 가지를 분류하는 천 가지를 분류하는 이미지들이 분류하는 거고 맨날 이거 말고 또 시트하라고 그래가지고 그런 여러 가지가 이미지들이 모아놓은 이 데이터 집합들이 있어요.
아까 내가 안 죽였겠지 요거 요거 요기 얘도 이미지 넷일걸 이미지 넷에 있는 이미지들이 쭉쭉 들어오는 거야 알겠죠?
이거 보면 여러분 이거 우리 보면 알지 이거 뭐예요?
배에다 하고 막 이거 배다 이거 알잖아 그쵸? 그런 것들이에요.
그런 걸로 훈련을 시켜놓은 웨이트들이 있는 거예요.
저렇게 강아지 고양이 배 이런 거 잘 구분하던 강아지 고양이 말고 말이랑 아까 차 이런 거 구분 잘하던 거나 개랑 고양이 구분하는 거라 더 쉬운 문제지 사실은 그렇죠 어떻게 보면은 그래서 사실 교과서가 조금 별로긴 한데 교과서도 스스로 인정하고 있는데 좀 더 어려운 문제를 풀던지 뭐 좀 이러면 좋은데 뭐 그랬어요.
근데 이제 그냥 어쩌다 보면 이렇게 됐대요.

참석자 1 31:33
알겠죠 그래서 지금 이게 얘기하고 싶은 거는 요 요거 훈련된 이게 도대체 어디 있는 거냐 훈련된 분류기 이게 원래 아까 이 VGG니 아까 교과서 312쪽에 나오는 거 미안하다 다 써 있었네.
312쪽에 여기는 1400만 개나 있었네. 여기 두 번째 단락에 미안해요.
여러분 실수했네. 보면은 1400 여기 페이지 이미지 넷이 여기 나왔네요.
여기 교과서에 311쪽에 두 번째 단락에 이미지 넷이라는 게 1400만 개의 레이블이고 이미지가 그러니까 이게 데이터가 1400만 개가 있는 거지 이 클래스가 천 개가 있는 거지 이해되죠 여러분 아까 걔 그런 거였어 이해 되지?
뭔지 감이 오죠. 여러분 여기 스파게티드프가 그래요.
여러분 진짜 서플라워도 있고 출입도 있고 별개 다 있어요.

참석자 1 32:30
근데 세상에 이제 천 개만 구분할 줄 알면 여러분 이거 한 한 세 살 그래서 걔네들 말은 못해도 두 사람이니까 그렇죠 그 정도지 그렇죠 지능이 그렇죠 그런 거 그런 애들이 이제 강아지랑 고양이랑 동방을 다 하게 되는 살 그걸 가지고 천 가지 구분할 수 있는 걸 그런 능력을 가지고 그래서 아까 여기 이 팀이 원래 여기는 여기 원래 원래 아까 피에 붙여놓은 거 있잖아요.
이에 가고 그럼 여기는 여기 생 분위기가 마지막 데스 매제 어떻게 생겼겠어요?
그러면 그럼 이제 댄스의 마지막 출력이 몇 개겠어요?
천이겠지 천 천 소프트 맨스겠지 그쵸 근데 우리는 이제 그거를 원 소급 시그 오이드로 바꿔야 되는 거지 그쵸 이해돼요.
그러니까 이렇게 이걸 얘를 이제 여기 어차피 강아지 여러 가지 튤립이랑 플라워랑 스파이트 구분하는 능력을 있는 건 그대로 살려놓고 마지막에 분류기는 우리 거에 맞춰서 바꿔야 되잖아요.

참석자 1 33:30
그래서 여기를 동결하고 이걸 그대로 쓰는데 여기 마지막에 분리하는 것만 학습시키면 잘하지 않겠냐 이런 거지.
근데 그래도 잘 되는 경우가 많이 있다는 거지 이해돼요.
여러분 이해돼요. 여러분 그게 여러분 실제로 LLM도 마찬가지인 게 LLM에 원래 이제 일반적인 말 잘하던 거를 우리 뭔가 예를 들어서 우리 학교 규정 규칙 같은 거 적용하려면 교칙에 새로운 뭔가 쓰는 용어들 있잖아요.
그쵸 우리만 쓰는 용어 그런 거를 적용해서 여기서 뭐 학사라고 그러면은 학사 규정 이런 거 있잖아요.
그런 학종지라고 그러면 여기서 우리 사는 학종지라고 그러면 학생 종합 제일 좋다는 거 우리는 알죠.
다른 학교도 학종지라는 말 안 써요. 여러분 정종설이라고 그러면 우리는 뭐예요?
정보통신 종합설인 우리 알잖아. 근데 다른 데서 또 그렇게 안 쓰잖아요.
다른 데 당연히 우리 거만 그렇지 그런 것만 이제 여기 분류기 쪽에 이제 학습시켜 놓으면 여기 다 똑같이 된 걸 써도 괜찮은 거지.

참석자 1 34:33
그러다가 다음 줄에 나오는 거는 여기서 약간 좀 튜닝 시키는 거를 하는 거예요.
알겠어요 여러분 여기 튜닝 시키는 건 좀 더 이제 비용이 많이 들거든 여러분 뭐냐면은 메모리가 많이 필요해 여기까지 다 올려가지고 훈련시키려고 그러면 여기는 여러분 얕잖아요.
은근히 얕아. 그러니까 백 프라보게이션 쉬운데 여기는 왠지 깊을 거 아니에요 그쵸 백프라보게이션 할 때 돈이 많이 들어요.
돈이 많이 든다는 게 구체적으로 얘기하면은 여러분이 딥시크가 지금 웨이트가 공개됐잖아요.
그걸 받아왔어. 나도 이제 뭔가 개를 갖고 훈련시켜보고 싶어.
근데 이렇게만 여기 마지막 쪽만 훈련시키는 거 있죠.
거의 바깥쪽 레이어 디코딩 쪽인데 거기에 이렇게 레이어만 훈련시켜도 충분히 쓸 만하거든요.
그래서 좀 바보같이 될 때가 있어. 그리고 여기서 다 전체 빨리 중 하고 싶어 이런 생각이 들잖아요.

참석자 1 35:23
근데 여러분 컴퓨터는 안 된다 노트북 컴퓨터로 비싼 거 사야 되나요?
결국 GPU가 4070 갖고는 안 되는구나. 4080으로 가야 되는구나 4070으로 가야 돼 이렇게 되기 시작한다는 거죠.
램이 모자라 이렇게 이렇게 된다는 이해돼. 파이트리닝 하는데도 레이 부족해 다 가려고 그러면 그래서 그런 것 때문에 이것 때문에 여기에 이제 원래 거에다가 뭔가 차이스만 학습시키는 게 로라라는 게 유명한 게 있어요.
실제로 알아서 하셔 거기까지는 내가 못 나갈 것 뜻인 거야.
알겠죠? 교과서에도 없고 일단 이게 무슨 개념인지 알고 있어야 되는 거지.
그쵸 여러분 여러분 실제로 할 때는 로라 같은 거 써야 돼요.
그러니까 여기 조합 설계나 졸업 논문이나 할 때는 사실 이거 지금 4학년 1학기 하는 게 아니라 3학년에 다 내려갈 수는 없으니까 좀 빨리빨리 배워야긴 해야 되는데 진짜 그래요.

참석자 1 36:12
지금 그래서 교과서에서 첫 번째 피처 익스프레션 하듯이 하는 거는 3 13쪽에 지금 보면은 여러 가지 유명한 모델들이 있는데 이렇게 쭉 있잖아요.
이런 것들이 이제 오피소스를 많이 있는 거예요. 유명한 컨볼레션 라이트가 네트웍이 그래서 레즈넷 되게 유명하고 여기서 진짜 힘든 게 스패션이 제일 먼저 나와 있잖아요.
이거는 저자가 만든 거야. 그가 첫 번째 있어요. 사실은 잘 안 쓰이고요.
이게 1등 보통 이게 댄스 대 이런 게 많이 쓰여야겠죠.
레지넷이랑 가벼우면서도 되게 잘 되거든요. 레즈넷이 이게 정말 잘 돼요.
이게 이게 레지듀얼 넷이라고 그래서 스키 커넥션 뒤에 나올 거예요.
알겠죠 그런데 이게 제일 심플한 게 교회가 이제 교과서 시대를 이제 VG 16이라는 걸 쓴다고 그랬잖아요.
그쵸 얘가 VG 16 어떻게 생긴 모양인지 나오는데 그냥 합시다.

참석자 1 37:11
일단은 이런 거 써서 할 건데 이걸 쓰고 시키면은 이거 워낙 유명하니까 이제 댄스 플로우는 자기 거 잘 쓰라고 이렇게 케라스의 애플리케이션지라는 패키지가 있고요.
VG 16 아예 이게 들어 있어 너무 유명하니까 여기 적혀 있는 거 있죠 엑셉션 내지넷 모바일넷 인피션트 리텐스 이거 있죠 다 있어 알겠죠?
이거 말고도 더 있는데 그러니까 오픈 소스 구할 필요도 없어요.
지금은 그렇죠 이렇게 유명한 것들 그래서 거기에 이제 이렇게 써가지고 웨이츠도 이제 뭐가 박습한 거냐 이미지 네가 박습한 거야.
여기 다른 것도 있을 수 있겠죠 지금 이제 우리 쓰는 앱만 써야 일단 알겠죠.
이것도 이것도 여러분 계속 바뀔 수 있는 거죠. 계속 그렇죠 인지 말고 저로 새로운 게 들어오는 게 1파 같은 것도 있고 막 이런 18 16 18 이런 게 있고 그 그다음에 인쿠루드 탑이 이게 뭐냐면은 댄스 네트워크를 의미하는 거예요.
당연히 폴스로 하는 게 낫겠죠.

참석자 1 38:03
여러분 그쵸 댄스 네트워크 내가 갈아치울 거니까 그래가지고 인풋 쉐입을 이렇게 인풋 쉐입은 18 183으로 내가 정해줬으니까 그치 우리 그렇게 하고 있잖아요.
자 지비로 해가지고 이렇게 만들어주는 거 이해되죠?
여러분 그러면은 컴브 베이스가 이게 학습된 베이트를 학습시킨 상태로 로딩이 되는 거예요.
그래서 그러니까 초기 값을 정해주는 거지 학습된 걸로 이해되죠 여러분 이게 지금 전이 학습하는 거예요.
그래서 이렇게 만들고 그리고 콤브 베이스 점 서머리를 해버리면은 이렇게 파라미터가 쭉 있고 이게 다 지금 사실 옛날에는 이게 다 비어 있는 거였잖아요.
여러분 다 이게 다 값이 예쁘게 들어 있다는 거지 이미지 넷을 구분할 이미지 넷을 천 가지를 구분할 수 있는 능력을 가진 그런 놈으로 된다는 거야 이해되죠?
여러분

참석자 1 38:54
그래요. 그리고 그리고 여러분 그래도 한번 이동을 시켜줄게요.
VDG가 322쪽 341쪽 한번 볼래요 3441쪽 341쪽 342

참석자 1 39:16
내가 저으라고 시 보여요. 여러분 341쪽에 그림 98 VG 16 보이죠 여러분 이런 거예요.
이런 거 생긴 게 원래는 2 2 4 2 24 곱하기 24를 받아가지고 마지막에 천 빨간색으로 돼 있는 거 보이죠.
여러분 소프트맥스 이 아이사 있나 잠깐만요. 여러분 이게 9장이 있어요.
9장 9장에 한번 볼게요.

참석자 1 39:54
이거 여기서 보면은 우리는 지금 180 곱하기 180 곱하기 180 이미지를 기대하고 있는데 여기 224 224가 들어오는 걸 기대하고 있어요.
여기는 얘는 그렇게 이미지 넷이 그렇게 돼 있다는 거죠.
알겠죠 알겠죠 그다음에 그래서 점점 이렇게 보면 여기도 VGD가 이게 16이 16개 계층이 아마 있을 거예요.
이게 전부 다 아마 16개일 거예요. 계층이 vt 30에는 이게 32개고 이런 식이에요.
여러분 이름 붙이는 게 그런 식으로 심플하게 돼 있어요.
알겠죠 마지막에 천이라는 게 왜 천이겠어요? 여러분 소프트 미스라니까 그쵸?
이렇게 이렇게 돼 있는 건 다 이 댄스라는 거죠. 댄스 댄스 댄스 그쵸?
이해되죠? 여러분

참석자 1 40:37
밀집 연결 플러스 렐루 거지 이해되죠? 여러분 밀집 연결이 사실 리니어지 저지 리니어 리니어의 액티베이션 벤스 액티베이션 없으면 리니어잖아요.
그래서 내가 사실 파이지가 좋은 점 교육적으로 좋은 게 거기는 댄스라는 용어를 안 쓰고 그냥 리뉴얼하고 해놨어.
리뉴얼 레이어가 있고 렐로 레이어가 있어요. 리뉴얼 레이어 다음에 렐로 붙이는 걸로 해놨어요.
댄스를 여러분은 약간 좀 파라처럼 들어가니까 마치 댄스가 뭔가 좀 그쵸 딴 느낌이잖아 리니얼 제가 모르잖아 그쵸 리니어 맞잖아 그거 그렇죠 리뉴얼 이 미친 연결이 리뉴얼 옐로지 그래요 이렇게 해주고 있는 거고 여기 색깔 다르게 한 거는 구분 잘 되라고 색깔 다르게 한 것뿐이고 사실은 알겠죠 그런 게 128개가 여러 개 있구나.
그래요. 그다음에 이게 아마 세대 플링이구먼 이거 잘 되는 게 아니라 미안해요.

참석자 1 41:35
렉스트 플링 했구먼 흰색이 맥스 플링했네. 맥스 플링 한 번에 쫙 줄어들고 그쵸 공이 잘 되는 게 아니라 맥스플링 했구나 맥스플링 줄인 다음에 또 이렇게 아까 배운 거랑 비슷하다.
그쵸 원래 이제 뽑아내고 맥스프링하고 뽑아내고 맥스프링하고 그렇죠 이해되죠?
여러분

참석자 1 41:56
그래요. 그다음에

참석자 1 42:03
그런 거를 가지고 우리가 정말 열심히 훈련시켜 놓은 걸 가지고 예쁘게 웨이트가 있는 걸 가지고 옷도 쓸 수가 있는 거지.
LNN 쪽에서는 딥시크도 예쁘겠는데 그렇죠 좋지 사실은 굉장히 잘하는 똑똑하던데 그래요.
그래서 이제 여기 뭐 웬만한 건 할 수 있는 거 사실 우리도 우리 학과 사무실을 위한 뭔가 근데 그러려고 그러면 이제 파인 튜닝 당연 당연히 다시 훈련시켜야 될 거 아니야 트랜스포 러닝 해야겠지 그렇죠 처음부터 훈련시키는 건 절대로 훈련이 안 돼요.
여러분 보면은 훈련이 안 돼요. 막 이런 경우가 많거든요.
종합설계 같은 거 할 때 그냥 처음부터 맨 땅에서 얘기를 하니까 그런 경우가 많지 맨땅 전 하지 맙시다.
이제는 그렇죠 그래야 되는 거죠. 그렇죠 그래요.
당연히 갖고 와줘야지

참석자 1 42:54
다 했고 내가 지금 교과서 지금 여기까지 했고 그다음에 두 가지 방식이 가능합니다.
여기 나오는 그다음에 이제 315쪽에 두 가지 방식이 또 가능하다고 돼 있는데 이 지점에서 다시 해볼게 우리가 여기는 어차피 이 8.3.1절에서는 피트 스트레이션만 하기로 했잖아요.
그쵸? 컨볼루션은 뉴럴 레트워그는 몸통은 훈련 안 시키는 파인 튜닝 안 해요.
그쵸 파인 튜닝 안 하더라도 이렇게 두 가지 방식이 있어요.
정말 돈이 없으면 첫 번째 방식으로 할 수 있다는 건데 이거는 그냥 정말 피처 익스트랙션만 해가지고 그냥 프린트만 해가지고 합성 검사는 그냥 프린트만 한 번 해서 한 번만 실행되게 하는 거 방식이고 두 번째 방식은 이거 데이터 제시를 안 사용하는 거고 그래서 어디 갔지 교과서에 지금 315쪽에 분리 2개 있는 거 보이죠.
여러분 분리 첫 번째는 데이터 인식 안 사용하는 거 두 번째는 데이터 인식 사용하는 거를 얘기하고 있어요.

참석자 1 43:58
그래서 첫 번째 거는 그냥 프린트가 써가지고 그냥 진짜 원래 네트워크 그대로 사용해서 하는 거고 두 번째는 네트워크 약간 새로 만들어서 하는 거고 그러는 거예요.
그래서 레이어를 새로 쌓아서 보면은 3미 16조 볼래요?
한 16조 316쪽에 8 202 아까 얘기했던 데이터 정식 안 사용하는 거고 818쪽 이 88쪽이 이제 까만 글씨 데이터 등 기식을 사용하고서 지금 적혀 있죠 그렇죠 그렇게 하는 거가 있고 그렇죠 이제 첫 번째 게 제일 여러분 진짜 메모리 부족하고 돈 없으면 하는 식이고 점점점 이제 나오는 방식이 메모리 좀 맞고 컴퓨터가 좋으면 할 수 있는 방식이에요.
알겠죠 근데 어쨌든 뭐 메모리가 안 돼도 돌려볼 수가 있으니까 좋잖아 그쵸 첫 번째 거는 그냥 백타 포기 그러니까 아예 그냥 프린트만 하겠다는 거야 볼게요.
여러분 8 20은 원래 그냥 데이터셋 갖고 프린트만 했죠 여러분 플리트 그쵸 모델도 정의 안 하고 그쵸 프린트를 해버리면 어떻게 되겠어요?

참석자 1 45:05
여러분 컴브 베이스에서 프린트 하니까 그 피처액만 주 튀어나오겠죠 피처액만 원래 데이터에 대해서 이해돼요.
여러분 정말 이거 인퍼런스는 되게 가볍거든요. 여러분 그래가지고 그냥 프린트만 튀어나오게 그렇게 만들어서 이걸 지금 저장을 다 시키는 피처라는 거 이제 피처랑 레이블이랑 저장을 시켜가지고 이거 지금 넌 파일을 컴퓨터네이트 시켜가지고 지금 뭐 하고 있냐면은 피처를 쌓아놓은 거 출력 각 다 뭐야 아까 강아지 고양이 데이터에 대해서 이 피처를 다 뽑아낸 거예요.
컨버레이션 통과한 결과를 이 상태에서 그래서 이제 이게 뭐냐면은 이거 개 피처의 라벨 트레인 데이터셋 밸리데이션 테스트 데이터셋에 대해서 이렇게 다 저장을 시켜놓는 거지 그러니까 진짜 완전히 그냥 원래 있는 컨볼루션 뉴럴 네트워크를 그냥 사용해서 프린트만 시켜놓은 거예요.
여러분 이건 별로 레모리가 없어도 되거든요. 그래요.

참석자 1 46:08
그런 다음에 그리고 이제 미치 이게 이게 이제 여기 코드 317쪽에서 아까 그 피처 나온 거를 기반으로 해가지고 이렇게 마델을 그냥 여기 보면 모델이 정말 썰렁하게 이제 500가 500c가 나오게 돼 있잖아요.
이게 되게 뭐냐면은 결국 아까 그 VGG 네트워크 있잖아요.
그러니까 마지막에 5 50cd가 튀어나와요. 보여줄게요.
왜 5 5 50이냐 이게 77이면 여기 50이었죠.
여기는 근데 우리는 5 500cd가 된 게 5 5012가 된 게 왜 그러냐면은 입력 사이즈가 달라서 그래요.
여러분 입력 사이즈가 달라서 입력 사이즈가 여기 224였잖아요.
우리 180이라서 결과가 달라질 수 있죠. 여러분 이해되죠?
그래요. 그래서 우리는 이거 이거

참석자 1 47:11
이거 보면은 앞에 34쪽 다시 볼래요 34쪽 34쪽에서 내가 지나가는데 34쪽에서 보면은 서머리 하면 우리 180일 인풋을 줬으니까 180일 이제 우리가 인풋을 쉐입을 줬잖아요.
그쵸 그러니까 마지막에 결과가 5 5012가 나왔잖아요.
그쵸 5 512가 이제 입력 피처만 뽑아낸 게 이렇다는 거죠.
그쵸 이해되죠 여러분 이게 여기 앞에 180 180이 아니라 다른 걸로 달라질 수 있는 거야.
그쵸 왜 이렇게 되는지 여러분 배워서 알지 그쵸 그렇지 그래요.
그래서 여기 지금 피처럼 포바에서 하는 거가 정말 돈 없을 때 하는 돈 없을 게 아니라 컴퓨터가 너무 성능이 안 좋을 때 하는 방식이 여기 있어.
코드 지금 49분이네 시간이 없네. 그래서 지금 317쪽에 9시 21 8 21에 쉐이블 5도 15 11 하고 그쵸 거기 있는 거보다 그 플레이트 시킨 다음에 이제 하는 거야.
알겠죠?

참석자 1 48:26
그다음에 플레이트 시켜가지고 댄스 시켜서 저거 시키고 이 시킨 다음에 약간 그 데스 안에 넣어줬어요.
중간에 그래도 그렇죠 좀 약간 훈련시킬 것 좀 해주려고 이 딥러닝 좀 시키려고 알겠죠.
그런 다음에 이제 마델 만들어서 컴파일 시켜가지고 하면은 여기 있는 웨이트만 학습이 되겠죠.
이 대열은 요 웨이트가 핵습이 되겠지 이해되지 돼요.
마르 체크 포인트 만들었으니까 이거 여기다 저장이 돼 있고 또 그렇죠.
아까 세이브 와델 같은 거 없어도 이것 때문에 저장이 된다고 그랬죠.
마르 체크 포인트 해놓으면은 콜백으로 이해되죠.
여러분 그래요. 50분이구나. 그래서 시간이 많은 줄 알았는데 금방 다 끝나네.
그래서 하면 어떻게 되느냐 아까보다는 좀 그리고 굉장히 시간이 10p 에포크에 2초 굉장히 별거 없으니까 학습시키는 게 별로 없잖아요.
그쵸 겨우 됐을 때 2개인데 그쵸? 벤스 넷이야 댄스 얘가 두 개잖아요.
엄청 금방 학습돼요.

참석자 1 49:32
그러니까 내 말은 이게 여러분 LLM도 이런 식으로 갖고 놀 수가 있다는 거지 이런 것까지 나오니까 저 제일 좋은 국가어요.
여러분들 진짜 돈 없을 때 할 수 있잖아요. 여러분이 딥시크 갖고 와서 이런 거 할 수 있잖아.
우리가 b LT 코드 쪽에 조그마한 이 키는 거니까 된다는 거지.
정말 돈 없어도 우리가 뭐 쓸 수 있는 걸 만들 수 있는 종합 설계에서도 이 얘기 있죠.
여러분 그리고 나중에 이거 비교해서 돈이 많으면 잘할 수 있어 이런 것들 충분히 되잖아요.
그쵸 그래서 아까보다 아니 좋아 근데 은근히 덜 되는데 잘 안 돼.
그래서 이제 대표 지식까지 하면서 좋으니까 뒤에 나오는 게 이제 나오는데 지금 52분이라 가지고 시간이 없어요.
그쵸 다음 시간에 진도를 좀 더 빨리 나가보도록 할게요.
됐죠 여러분.


clovanote.naver.com