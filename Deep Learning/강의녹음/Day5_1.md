딥러닝 day5_1
2025.03.19 수 오전 10:02 ・ 55분 46초
심승환


참석자 1 00:00
하는데 이거 안 바뀌었구나 여기 퍼포먼스 헤드릭에 사실 지난 시간에 이제 퍼랑스 에드릭 었죠.
여러 가지가 있을 수 있는데 일단 제일 근본적인 게 이제 사실 리뉴얼 이그렉션, 라디에스티 이그렉션 두 가지인데 어쨌든 중요한 건 이제 문제 자체가 컨티뉴스 밸류를 프디션하는 거랑 바이너리 클래시피케이션 하는 거 그 두 문제가 제일 근본적인 문제죠.
여러분 그렇죠 우리가 풀어야 돼. 그쵸 보통 예측하려고 그러면은 어떤 입력 값으로부터 뭔가 어떤 연속적인 값들 중에 뭐냐 이렇게 예측하는 거랑 그렇죠 되게 여기 사실 여기도 이것도 이거 여러분 기억나 AI 카 이거 있잖아요 기억나죠 여러분 설명했죠.
제가 여기서도 여러분 이제 출력물이 출력이 2개잖아요.
뭐였어요 어느 방향으로 하느냐 그렇죠 마이너스 1부터 1까지 컴트랜스 밸류 중에서 그렇죠 그리고 또 이제 속도 속도도 컨텐스페이잖아요.
그쵸 그렇게 할 수 있지 그치 그리고 다른 하나는 바이너리 플레시피케이션 문제 있잖아요.

참석자 1 01:01
그쵸 마이클 스필오드는 예스 노잖아 예스 노 예스 노 근본이죠.
그쵸 사실 얘도 예스 노로 바꾸어서 되기도 해요. 그러니까 뭐냐면은 오투터냐 왼쪽 터냐 그다음에 또 오투 터을 때는 또 그 안에서 또 45도 이상 치냐 45도 6 지냐 계속 이렇게 나눠서 얘기할 수 있기 때문에 뭔가 또 이진 분류가 제일 근본이에요.
그쵸 어쨌든 그거에 대해 그거에 대해서 이제 이번 우리가 오차에 대한 개념이 있어야 되잖아요.
그쵸 그래서 여기는 목차가 에러가 rlse랑 mse로 했다는 거 설명했어요.
지난 시간에 그쵸 맞죠 이거 다 이해되죠 여러분 너무 잘 그렇죠 이게 되는 거지 왜 이거 했어요 내가 안 했나 안 했어 이게 너무 헷갈려 AI 종합 개발해서 했거든 여러분 이거 잠깐만 내가 이거 여러분 지나 씨 이건 했어요.

참석자 2 01:50
영상으로 보여주세요.

참석자 1 01:53
저 뒤에 슬라이드 하셨으니 여기서부터 안 했어. 이것도 하셨어요 이것도 했어 확인은 하긴 했지 일단 했다.
여러분 색깔 왜냐하면 슬라이드를 막 띄워놔가지고 교과서 안에 거 지금 맡겨 넣어가지고 사실은 이게 1학년한테 쉽게 설명하는 걸 넣다 보니까 아니 여러분들도 사실 쉽게 설명하는 게 필요한데 그래서 그냥 막 넣었어요.
말로 떼우다가 자꾸 넣었어요. 이게 쓰기 좋잖아요.
사실은 없어 없어 이거 다 아는 거라고 생각할 수도 있지만 이거 여러분 다시 강조하면은 원래 기본적으로 오차는 얘야 얘 그쵸 요거 요거 뭐냐면 예측 값이라 해서 원래 정답 값을 뺀 거지 이게 차이잖아요.
절대값이 맞지 왜냐면 플러스 마이너스가 실제로 진짜는 진짜 고차 값은 이건데 이거를 뭔가 전체적인 여러 샘플에 대해서 하려고 그러면 절대 값을 더 하든지 제곱을 더 하든지 그런 것밖에 없죠.
두 가지 방법이 대표적이지 그쵸 다양 다 똑같이 오차를 쌓아야 되니까 이해되죠.
여러분 상쇄해버리면 곤란하잖아.

참석자 1 02:49
그렇죠 그래서 이렇게 두 가지가 있고 그래서 RMS MA가 두 가지가 있어요.
그렇죠 이거 외워야 되는데 그렇죠 제가 사실 옛날에 말로는 그냥 막 떼웠을걸 이거 하여튼 용어를 RMS MA 이런 거 있잖아요.
용어를 외우시기 그죠 유명한 용어고 루트민스케어는 귀찮으면 그냥 루트 안 씌워도 되잖아.
그쵸 루트 안 씌우면 여러분 옛날에 원래 보통 통계에서 분산을 쓰잖아요.
표준 절차 안 쓰고 휴대표처럼 무사하면 제곱 쓰고 찌고 안 쓰고 차이잖아요.
그쵸 분사만 얘기해도 되잖아. 사실 그렇죠 본 것처럼 여기도 알 루드를 떼버릴 스 루트 저것도 지금 안 할 수도 있는 거지 그리고 미니 스케어 에러라고 했는데 미니는 너무 당연하니까 평균 내는 거 당연하니까 그것도 귀찮아서 그냥 스케어해라 그렇게 얘기해요.
그쵸? 그럴 수 있죠 여러분 이해되죠 사람들이 그러니까 루트 미인 스퀘어 에러 이렇게 얘기할 수도 있지만 그냥 미인 스퀘어 에러라고도 얘기해도 똑같아요.

참석자 1 03:43
결과적으로 어차피 저 둘이 왔다 갔다 하니까 그렇죠 이해됐죠?
여러분 미인 거 너무 당연해 평균 내는 건 당연한 거 아니야 그쵸?
안 내봤자 상관도 없어 어차피 이거 줄이면 되는 거니까 n분의 1 안 하면 평균이 아닌 거잖아요.
그쵸 그냥 합친 거지 그쵸 n분의 1 안 해도 별 문제없잖아요.
오차 값이 보이는 거는 어차피 이게 0이 돼버리면 0도 돼버릴 텐데 n분의 1 나누건 말건 민이라는 건 순전히 n분의 1 나누는 거잖아요.
그쵸 됐죠 그래가지고 우리 옛날에 그 앞에서 여러분한테 지도 학습이 뭐가 있는 주 보여줬대.
여기에 이제 제가 리뉴 리그렉션은 콘텐츠 밸리 피딕션을 위한 대표적인 방법인데 여기에 이제 리스트 스퀘어 리그레이션이 적혀 있다 그랬잖아요.

참석자 1 04:26
리스트 스퀘어가 이 스퀘어가 어디서 온 말이냐 아까 그 루트 민 스퀘어 에러에서 제일 근본적인 거 스퀘어만 남는 거고 오차를 스케어해버린다고 이해되죠 여러분 에러를 스퀘어한 거 그거 그거를 리스트 최소화시켜야 되니까 그쵸 그래서 이거 사실 제가 지난 시간에도 책으로 보여줬었는데 책으로 책에 사실 책 가지 보여줄 필요 없어 내가 이거를 1학년들을 위해서 이 책을 가지고 1학년들한테 이 책을 보여주고 들이밀 수는 없잖아요.
그래가지고 1학년들은 교과서도 없이 그냥 하고 있는데

참석자 1 05:02
여기 띄워놨지 내가 여기 여러분 강의 자료 제가 월요일에 새로 올렸었는데 진도 못 나갔지만 요거 요거 요거 올렸었거든요.
봤어요. 여러분 트레이닝만 제가 따로 올렸거든요.
이것도 사실은 1학년을 위해 새로 만들었다가 열어봤으면 좋을 것 같아서 그냥 그렇게 하려고 했어요.
그래요. 그리고 어차피 제대로 들어가기 전에 앞에 거는 확실히 아는 게 더 좋으니까 여기 보면은 원래 교과서에 있는 거랑 사실 비슷한 그림인데 어쨌든 원래 지도 학습이 이렇게 한다고 제가 보여줬잖아요.
옛날에 그쵸 근데 이거를 좀 더 자세히 그리면 입력 데이터가 들어가서 프로그램이 뭔가 이제 결괏값을 내는 건데 근데 이제 이 프로그램이 사실은 이렇게 노란 부분이 이렇게 부분으로 나눠지고 사실은 트레이닝을 위해서 이게 전에 제가 미리 얘기하느라고 이게 아까 그 리스트 스키어에서 얘기하면서 얘기하려고 하는 거예요.

참석자 1 06:01
요게 이게 원래 이제 이 부분이 순전히 트레이닝을 위해 있는 거라 그랬지.
그쵸? 트레이닝 안 할 때는 이 빨간 부분 없어요. 그쵸 알고 있죠?
여러분 트레이닝을 할 때만 빨간 부분이 쓰이는 거지 나중에 이제 인퍼런스 할 때는 안 쓰이는 거고 그쵸 지금 내가 이걸 이걸 왜 강조하냐면 여러분이 사실 머신러닝 러닝을 하는 거가 목적이 나중에 이제 다 웨이트 가중치 제대로 만들어 놓은 다음에는 그냥 쓰려고 하는 거잖아요.
그래서 그 프로그램이 차이가 뭔지 좀 아는 게 필요하잖아요.
그쵸 그래서 지금 제가 강조하는 거예요. 이거는 이 그림은 전부 트레이닝이 들어간 건데 트레이닝에 해당하는 부분만 제가 지금 빨간색을 쳐놓은 거예요.
알겠어요 이해되죠 여러분 여러 가지 목표에서 보여주고 있지 그쵸 여기도 여기도 트레이닝에 해당하는 부분이 뭐냐면은 그렇게 된 부분이 트레이닝 해당 트레이닝에 해당하는 부분이에요.
트레이닝 안 할 때는 이게 없는 거지 빨간색 톤이 아래에서 이해돼요.

참석자 1 07:02
이거 그치 그렇거든요. 체인이 안 할 때는 저 빨간색 부분이 없어 뭐 알겠죠?
여러분 그래서 여러분이 그냥 딥러닝을 쓴다고 할 때 트레이닝을 위해 쓰는 건지 그냥 가져와서 쓰는 건지 이게 다르다고 그거를 명확히 잘 모르겠네요.
알겠죠 어쨌든 그 트레이닝을 할 때는 여기 지금 sl이라고 적었잖아요.
내가요 그쵸? sl 이게 뭐냐면 여기 제가 슈퍼마이드 러닝 sl이라고 여기 지금 약자를 해놨어요.
그걸 말하는 거예요. 알겠죠 됐어요. 그러니까 이게 슈퍼 이즈 러닝을 하는 뭔가 함수가 있다는 거지.
이게 뭐냐면은 결국은 예측하는 함수지 이게 함수가 뉴럴 네트워크일 수도 있는 거예요.
여러분 이해돼요. 여러분 뉴럴 네트워크 함 함수는 뭐죠?
입력이 나와 출력이 나오는 함수라고 알겠어요.

참석자 1 07:54
여러분 근데 그 함수의 파라미터 값을 이제 우리가 가중치 값인데 그거를 업데이트하는 거지 그쵸 알겠죠 어쨌든 여기 지금 이거 빨간색 이거 친 거는 이제 지금 다시 그거 아니야 딥러닝 아니 아니 저기 트레이닝하고 상관없는 내용이 있어요.
이 이건 트레이닝을 하건 말건 쓰는 거지. 그쵸 색깔 잠깐 해야 돼 지금 그래서 중요한 건 여기서 보고 싶은 거는 원래 이제 트레이닝 안 할 때는 이게 등장하지 않지만 트레이닝 할 때는 반드시 이게 좀 계산이 돼야 돼.
그쵸 이게 중요하다고 트레이닝을 할 때는 반드시 출력 데이터로 끝나는 게 아니라 손실함수를 통과해요.
항상 그래서 뭐라고 하면 손실 함수가 뜻이 통신 지난 시간에 얘기했지만 로스랑 에러랑 코스 같은 말이라는 거 교과서 할 때 했었죠.
프로젝트 안 되는 줄 알고 막 조를 했을 때 그렇죠 기억나죠.
여러분 손실 함수를 통과해 이게 중요하다고 알겠죠.

참석자 1 08:48
실제로 트레이닝 할 때는 그래서 우리가 관점을 얘 이 함수에 생긴 모양을 보느냐 이 함수에 생긴 모양을 보느냐에 따라서 말을 얘를 이제 이게 뉴럴 네트워크일 수도 있고 아니면은 그냥 리뉴얼 리그렉션이나 라지스트리그렉션으로 끝날 수도 있잖아요.
라지스트 리그렉션 리뉴얼 리그렉션 이런 것들 전부 다 뭐예요?
여러분 그들 걔도 다 러닝이야 그쵸 그러니까 걔도 손실함수 통과하겠지 이해돼요.
여러분 손실함수를 뭘 쓰느냐가 되게 중요하겠죠.
실제 손실 하려고 그러면 손실 압수를 보고 가중치를 업데이트하는 거잖아요.
손실 값수에 나온 결과값을 보고 손실 값을 보고 그렇죠 이거를 잘 써야지 잘될 거 아니에요 그렇죠 주면에서 호시라 잘 만들어야 돼.
그쵸 잘 적용해서 그래서 머신러닝이나 딥러닝에서 훈수하면서 뭐 쓰는지 당연히 묻겠지.
그걸 이상하게 쓰면은 니가 의도를 가지고 그렇게 쓰는 줄 알고 시킨다고 그냥 알겠죠.
그 문제의 말씀 되게 잘 해야 돼요.

참석자 1 09:44
그래서 계속 얘기하면은 아까 이게 리뉴얼 리그레션이면 만약에 이게 제가 딥러닝일 수도 있고 그 딥뉴얼 네트워크일 수도 있고 그냥 퍼셉티션 뉴럴 네트워크일 수도 있고 라디스 리그레이션일 수도 있고 리뉴얼 리그레이션일 수도 있다고요?
이해됐어요. 여러분 이해되지 이게 이 셀 함수다 이해돼요.
여러분 무슨 말인지 여기 손실 함수는 만약에 여기가 라스 리니어 리그레션이라고 쳐봐 여기가 s 함수가 그럼 여기는 뭐가 와야 되냐 손실함수가 뭐가 와야 된다는 거예요?
루드미 스퀘어 에러라든지 루드미 스퀘어 에러 이해돼요.
여러분 또 뭐 아까 민 앱솔루트에요. 그쵸? RMS MA 이런 게 들어온다고 여기 손실 함수로 알겠어요 여러분 됐어요.
지금 요 너무 쉬운 건 아닌 것 같아요.

참석자 1 10:35
배 거 보니까 그렇죠 됐죠 그다음에 이름 지을지 얘가 이제 리니어 리그레션을 하면은 여기 sl 함수가 리니어 리그레션을 하면 무조건 여기는 항상 어떻게 되냐 스퀘어 에러를 계산하니까 스퀘어 리그레션이라고 부르기 시작했다고 사람들이 결국 이거 이거 이거 갖고 맨날 훈련시키니까 리니어 리그렉션은 스퀘어 리그렉션이구나 이렇게 리스트 스퀘어 리그레이구나 리스트 스퀘어 값이 제일 스퀘어 값이 제일 작아지게끔 훈련시키는 거니까 리그렉션이 뭔가 글로 만들겠다는 뜻이잖아.
손 모양으로 만들겠다. 여기는 이제 리스트 스퀘어를 만들겠다 이거잖아 이거를 그래서 이렇게 같은 같이 막 섞여서 본다는 거지.
이 사람들이 여기 이름이 여기서 리스트 스케어즈라고 부르는 이유가 얘는 아까 에셀 함수를 리뉴얼로 만들겠다.
이거는 손실 함수를 리스크 스퀘어로 만들겠다 이런 거였다고 알겠죠 이해되지 그런 용어가 있단 말이야.

참석자 1 11:31
그럼 여기 뒤에 적혀 있는 것도 뭔가 다 그런 뭔가 맥락이 있겠지 아지스티랑 시그모이드는 그냥 슬라이드 지금 서로 올려놓고 같은 말이다.
이런 것들이 나중에 일단 이거 이거 있잖아 이거는 일단 나중에 할게요.
여기까지 일단 했어요. 여기까지 다 이해했어요.
여러분 그렇죠 이해됐죠 여러분 그러면은 어쨌든 지금 성능 지표로 쓰는 거가 이제 에러 성능 지표가 저 에러가 작게 만드는 거지 이거를 rmsa나 MA를 작게 만드는 거예요.
그쵸 이해됐어요. 여러분 그리고 rmsa랑 MA랑 다 장단점이 있어요.
RMS는 여러분 제곱을 만드니까 좀 큰 오차는 더 뻥 튀게 되겠지 여러분 또 그런 면이 있죠.
그거 그러니까 MA는 크로차라고 해서 더 막 커지진 않죠.
그렇잖아요.

참석자 1 12:30
큰 오차를 막 되게 굉장히 민감하게 반응하고 싶어 그러면 RMS로 쏴야 되고 너무 오차가 큰 거는 확 줄여야 되고 이거 MA는 그런 거 차려주지 않아야 되는 그런 문제라면 MA를 쓰고 그러는 거예요.
여러분 문제 성격에 따라서 알겠죠 그래요. 그다음에 그래서 여기 이렇게 여기 이제 퍼포먼스 애트리스폼 컨트리스 밸리피리터랑 여기 퍼포먼스 애트리스폼 다음에 근본적인 문제가 이제 바이너리 클래스케이션이니까 그거에 대한 퍼포먼스를 쭉 얘기하는 거예요.
알겠죠 그리고 이건 사실 머신러닝 수업 시간에 배워요.
여러분이 배울 수밖에 없지. 그래서 아 모르면 이제 아예 논문이고 하는 사람들이 얘기하는 거 못 알아들으니까 기본적인 거예요.
이것도 알겠죠? 얘가 이제 여기 보면 제가 쭉 적어놨잖아요.
이런 것들 있죠. 내가 이거를 열심히 여기서 강의하는 게 좀 별로인 것 같아.
왜냐하면 그 수업 시간에 할 테니까 겹치잖아요. 제가 이거 운영 체제 시간에 컴퓨터로 강의할 수는 없잖아요.

참석자 1 13:25
그거는 시간 하는데 여기 있는 거 모르면 안 돼. 다 안다고 보고 진행하겠다는 거예요.
알겠죠 그냥 순순히 시간을 아끼기 위해서 알겠죠.
당연 안 한다. 알겠죠 근데 안 하는데 다 알아봐 알겠죠 알겠죠 다 안다고 보고 대수 시험 문제 이게 막 섞여 나와도 틀리면 틀리는 거다.
왜냐하면 더 근본을 모르는 건 더 안 좋죠. 여러분 애플을 쓰는데 애플에 있는 키가 키인지 모르면 안 된다는 거지 알겠어요 여러분 그래요.
여기 막 제가 열심히 적어놨는데 모르면서 질문하셔 알겠죠?
알겠죠? 여기 이렇게 저거 해놨어요. 여러분 제가 이거는 그냥 신문 기사에도 막 나와요.
여기 신문 기사 신문 기사에도 지금 코로나 바이러스 진단기가 나왔는데 이번에는 재연도가 얼마고 특이도가 얼마예요 막 이러면서 진짜로 진짜 그래요.
그리고 필스전 리콜이랑 아로스 커브라는 것도 유명하고요.
그래요.

참석자 1 14:20
근데 약간 얘기하고 싶은 거는 프리시즌 니콜이랑 라로시 코드가 전부 다 이게 서로 프리시전이랑 리콜이라 이게 지금 프리시전이 이제 정확도가 아니라 정밀도고 리콜이 재연도라는 건데요.
그 사람의 하는데 이게 서로 같이 도와주기 힘들어서 하나 좋게 하면 다른데 아까의 모양이 있어요.
그래서 그거를 두 개 동시에 보기 위해서 이런 그래프를 보통 다 그려요.
진짜 잘하는지 못하는지 보라고 그러면 알겠죠. 그래서 이거 둘 다 그러니까 이게 뭐냐 프리시전을 높이면 리콜이 나빠지고 이래가지고 리콜을 높이면 프리시전이 나빠져서 적정한 부분을 찾아서 이제 진단 시 출자하는 거지 누가 그러니까 이콜이 말을 하려면 너무 길어져 그만할게요.
여러분 나 강의하려면 되게 강의하면 되게 오래해 그래서 그냥 그만하고 이거는 요즘 머신러닝 수업 걸로 하세요.
그래서 딥러닝 해야지 그다음에 이거는 제가 여러분 강의 자료에 좀 틀리게 돼 있어서 내가 새로 올렸어요.

참석자 1 15:19
멀티 클래스에 대한 컴플리 매트릭스인데 기본적으로 이제 용어가 에큐로시랑 프리시전 리콜 이런 게 있거든요.
여러분 에큐르시 영어로도 알고 우리나라 말도 알아야 돼요.
에큐로시 정확도라는 거고요. 프리스트하고 이크는 아까 PR 카드 얘기했잖아요.
그렇죠 액트리스는 기본적으로 데이터 셋 전체에 대해서 얘기하는 건데 에큐시는 여러분 좀 계속하세요.
여기도 맨날 계산할 때 에큐시 계산하고 이러거든요.
그러니까 딥러닝 이제 훈련시킬 때도 무슨 딥뉴럴 에토 콜린 에큐로시 맨날 나와요.
근데 그 에큐로시의 정체가 뭐냐면은 그러니까 프레시전이나 리콜 이런 거는 이제 안 나와 그래요.
여러분 알겠죠 근데 에큐르시 나올 때 에큐르시는 전체 뭔가 맞춘 것 중에 전체 뭔가 예측한 것 중에서 진짜로 맞는 것의 비율인데 이거는 데이터셋이 전체에 대해서 얘기하는 건데 실제로 분류할 때 이제 아까 지금 다이어리 플리피케이션이라는 멀티플레스 있잖아요.
여러 개 세 가지를 분류하는 거죠.

참석자 1 16:16
예를 들어서 이런 거 할 때 프리지랑 리콜은 클래스별로 하는 건데 클래스별로 그러니까 애플에 대한 프리스 전 애플 리콜 옳은지에 대한 표시 리콜 이런 식으로 얘기한다고 이해돼요.
여러분 내 말 뭔지 알겠어요? 여러분 그렇게 한다는 거지 근데 이거를 제가 그냥 얘도 애플이랑 오 이거 프레시안 리콜도 지금 방금 보내준 슬라이드에서는 그냥 데이터셋 와이드한 걸로 전체 데이터에 대한 걸로 해서 다 똑같은 걸로 해놨거든요.
제가 프리시전 리코이랑 에큐레이션이 같다고 여기 여기서는 이렇게 하면 그쵸?
그쵸? 그렇게 적혀 있다는 슬라이드에 여러분 원래 슬라이드는 아니면 중요하지 않고 이게 중요한 거예요.
실제로는 에트로시는 데이터셋 전체에 대해서 하고 플래시지랑 리콜은 데이터셋 전체 하는 게 아니라 알겠죠 이거 사실 인터넷 찾아보면은 트랙이 나오는 게 하도 많아가지고 내가 사 피트티 물어봐가지고 피티트가 맞는 것 같아요.

참석자 1 17:11
이렇게 얘기해 주더라고 왜 이거 이렇게 다르냐 그랬더니 사실 이게 합리적이다.
이게 공부 잘한 것 같아 돈 내고 사면 더 괜찮더라고요.
돈 내고 뭔가 좀 사람 피드백이 들어갔나 튼 그런 게 있어요.
여러분 그래서 이게 맞는 말이야 내 말이 이해하는 이게 맞는 것 같아요.
플래스 별로 하는 게 맞아요. 얘는 아무 의미가 없거든 이렇게 했으면 무슨 의미가 있어요?
그쵸 그래요. 여러분 그래서 이제 진짜로 그 채팅 피트맵도 얘기해 주는데 나 논문 안 찾아봤는데 사이트도 다 있어 그래가지고 아는 말지 이런 거는 이게 애플이랑 오렌지랑 망고랑 세개 구분하려고 그래 애플이랑 망고를 구분하는 애플을 잘 구분하는 게 되게 중요하다.
그럼 얘의 프리시전 이런 걸 높여야 되고 여기 지금 망고는 별로 잘못 별로 상관없나 봐요.
여기 되게 나쁘잖아요. 아니 뭐 이래도 참는 게 애플 애플이라고 하는 게 되게 중요하다는 거지.

참석자 1 17:57
여기서는 그럴 때 이제 여기 이게 이렇게 이거 전부 다 세 개를 다 보는 게 아니라 이제 특별히 중요한 클래스에 대해서 좀 집중한다 이런 거지.
사실 얘도 이제 바이너리 이거 컴퓨존 매트리스는 전부 다 뭔지 설명도 안 했는데 설명 안 했는데 좀 좋긴 한가 여러분 이거 첨 봐요.
머신러닝이랑 같이 듣는 사람은 처음 보지 그치 그냥 공부하셔 이 사람들은 할 수 있어요.
이 별로 어렵지 않아요. 알겠죠 하세요. 복습 학과요 그거 그래요.
그다음에 이거 이것만 강의하면 진짜 또 1시간을 써요.
내가 내가 재미있거든 이게 여지 그래요. 그래서 그냥 갑니다.
저기 1학년과 거기서 열심히 설명해야지 그다음에 그다음에 여기 요거 지금 저 그래서 지금 끝났잖아요.
저거 저거 이 슬라이드는 보세요. 강의 자료가 지금 3개 올라와 있는데 이거 지금 요거 요거 요거 방금 끝났어요.

참석자 1 19:00
이거 원래 여기 뒷부분이 여러분 사실은 사실 더 있죠 막 제가 원래 처음에 올린 버전에서는 베리언스 바이어스 트 트리더투 이런 거 다 있죠 그죠?
제가 그걸 여기다가 그냥 다 넣으려고 그러다가 그냥 이거 만들면서 여기서 빼버렸어요.
이쪽으로 그냥 그랬어요. 알겠죠 그래서 여러분 공부할 때 사실 막 승마가 바뀌면 되게 짜증 나잖아요.
미안해요. 미안한데 도저히 참을 수가 없어가지고 왜냐하면 이거 이거 만들었는데 또 아까워서 그냥 했어요.
여러분 어차피 이거 안 줄 수도 있는 건데 준다고 생각하시고 여러분 알아서 하셔 알겠죠 안 줄 수도 있는데 준다고 생각하세요.
필기로 다 끝낼 수도 있어. 사실 근데 주는 거라고 생각하시고 용서해 줘요.
여러분 그래요. 그다음에 가면 모든 학생을 만족시킬 수는 없죠.

참석자 1 19:46
어떤 애는 어떤 옛날의 강의 평가에는 슬라이드를 하나에서 계속 처음부터 쭉같이 체크하면 좋겠는데 자꾸 앞에서 또 이거 나왔다고 설명하고 다른 슬라이드 왔다 갔다 하고 하는 것 때문에 짜증 나 죽겠다고 적어놨더라고요.
난 상관 안 개의치 않아서 그렇게 하겠어 이게 왜냐하면 그렇게 설명 안 하면 복습이 잘 안 된다고 생각해서 그러니까 사람마다 다르다는 목소리 큰 사람만은 무조건 듣는 건 아니라서 그게 이제 딥러닝에서 나와 나도 학습을 하는데 학생들은 그때 그걸 갖고 이게 아웃라이어거든 아웃라이어 아웃라이어가 뭐냐면은 사실은 정규 분포에서 벗어난 건데 그 사람 목소리가 크다고 그 사람 말 들으면 곤란한 거거든요.
우리나라 정책이라 하면 구급자가 사실은 좀 무섭고 대부분 사람은 그렇게 생각 안 하는데 목소리가 너무 커 또 그렇죠 알지 못했던 그지 무조건 사람을 막 죽이려고 그러면 안 되잖아요.

참석자 1 20:31
그렇죠 죽은 거라서 선을 넘었지. 아니 그러니까 그렇다고 해서 그 사람 말을 다 들어줘야 되냐 그런 건 아니잖아요.
미안 그 사람이 그 친구가 그런 건 아닌데 어쨌든 그래요.
어쨌든 여기 이거 트레이닝 들어가서 할게요. 여러분 트레이닝 베이리스 이거는 훈련에 대한 건 제가 따로 뺐어요.
아까 사실 퍼포먼스 매트리스도 사실 훈련에 대한 내용이지만 워낙에 기본적인 거니까 베이지에 넣어놨고요.
여기서부터 약간 좀 트레이닝만 모아놨어요. 트레이닝 할 때 이거부터 하는 게 좋을 것 같아서 헬로우 월드라는 말 쓰잖아요.
항상 여러분 신하고 파이썬이나 맨 처음에 그것처럼 이쪽에서는 이제 항상 근본이 데이터 셋이 넷리스트 데이터베이스라는 거예요.
그냥 이거는 이 마디파이드 내셔널리 재밌잖아요.
그렇죠 이름이 이런 거예요. 완전히 그냥 무슨 기관 이름이에요?
그쵸? 표준 표준 기관이야 표준 이거 그리고 재밌는 게 미국은 자기 말 안 붙여 우리는 맨날 뭐 만들 때 카이스트도 코리아 겹치잖아.

참석자 1 21:37
MIT도 없었으니까 거기는 대전이라고 붙이는 게 좋았을 텐데 어쨌든 여기는 미국이라는 말 붙이지도 않아 us라는 말 붙이지도 않아 있죠.
미국 사람들은 그냥 세계 대표 거라서 그 거기 여러분 여기 있죠 안cc라는 것도 있잖아요.
여러분들 아직 들어가 모르겠다. c 언어도 사실 여러분 그냥 내셔널 스탠다드 아니 그러니까 국가 표준을 잃어버린다고 우리도 내셔널 스탠 리치 이래버리 가 이제 국가로 끝내버려 그렇게 버려 재밌잖아.
어쨌든 앤 리스트는 별게 아니라 마디파이드 왜냐하면 이 플레임은 전혀 외울 필요 없어요.
여러분 그쵸 근데 외우지 말라고 그러면 또 외우지 어쨌든 그쵸 알겠죠?
엔리스트가 그런 거였고요. 거기서 여러 개의 데이터베이스를 만드는데 핸드레이트 비지트를 수집했어요.
일단 우체국에서 우체국에서 이제 우편 배달할 때 우편번호 잘 구별하는 게 되게 좋잖아요.
우편 번호 갖고 구분하면 좋잖아요. 싸움할 때를 그래서 사람들이 쓴 글자들을 모은 거예요.

참석자 1 22:34
숫자 숫자만 그래서 7만 개나 있어요. 7만 개나 모아놨어요.
그래서 데이터를 이렇게 이미지로 다 모아놓은 거니까 굉장히 좋지.
그리고 패션 리스트라는 거는 이게 엔리스트 하도 유명하니까 사실 이거랑 전혀 상관없는데 그냥 옷만 이렇게 모아놓은 거 알겠죠 여기도 똑같이 10가지 여기까지 물어주세요.
그리고 슈퍼바이즈드 러닝에서 이제 데이터셋이 되게 중요하잖아요.
그렇죠 여러분 데이터 집합이죠. 그쵸 아까 계속 강조하지만 여기도 계속 보면은 이제 정답 이게 이게 이게 데이터지 데이터 이게 데이터셋이야 그쵸 입력 데이터만 있는 것뿐만 아니라 정답도 있어야 돼.
그쵸 이게 제가 여기 이거 여기 이걸 여기다 적고 이걸 여기다 적었어요.
제가 일부러 그쵸 정답은 사실 이쯤에 들어가거든 그쵸 손실 함수에만 들어가잖아.
그쵸 계산할 된다. 알겠지 그래요.

참석자 1 23:32
어쨌든 제가 강조하고 싶은 거는 내가 그림을 이렇게 그렸지만 밑에서는 이거 말고 밑에서는 이렇게 그렸지만 이것도 안 그래 그랬지만 얘랑 얘랑 합쳐서 데이터인 거야.
그렇죠 이해되죠 슈퍼 바지죠 이서 슈퍼 바지 지도 학습에서는 지도해야 되니까 정답 값을 알려줘야 되는 지도 안 하면 필요 없지 그쵸 그러니까 어느 슈퍼바이즈 러닝에서는 그냥 데이터만 있어도 돼.
그쵸 근데 슈퍼바이즈 러닝에서는 라벨을 붙여야 돼.
그쵸 전에 얘기했나 알바 있다고 알바 있었다고 옛날에 이미지 전파운데 는 그런 안 했나 이거 안 했어요?
안 했어 옛날에 바운드 박스 치고 그거 못하고 표시하는 알바가 있었어요.
한 번 하면 80원 한 장에 하나 표시하는 80원 받았거든요.
되게 한때 한 여러분 한 10년 전에 되게 유행했었어요.
진짜 2005년 2015년쯤이잖아. 그때 엄청나게 데이터 모으느라고 많이 했었거든요.

참석자 1 24:34
지금 기어가 없어졌지 지금 다 모였으니까 알겠죠 여러분 세만 이렇게 바운딩 박스 치고 이런 알바 있을 때 성공이 망가지는 알바지만 소스 받아 지 얼마나 벌어 가지겠어요 어쨌든 되게 성년 되면 되게 잘한다고 그러는 그런 옛날에 블로그에는 이거 잘하는 거 몇 하도 말아서 말하면서 어쨌든 중요한 건 이제 지금 내가 말하고 싶으면은 옛날에 기본적으로 초보드 러닝이 성행이 기본이고 여기 데이터셋이 인슈파이 저닝이랑 또 썼네.
오픈하는 슬라이드가 데이터 셋 한 다음에 인슈파이 저닝이라고 살라고 그러다가 군대 다 버렸네.
무슨 얘기인지 알겠죠 여러분 SRS 주스 고 그리고 여기서 기본적으로 이제 트레이닝 셋이랑 테스트 셋이 있어요.
여러분 항상 보통 데이터 나눌 때 그러니까 이게 트레이닝이랑 테스트를 나눠야 되는 게 데이터 셋이 있다고 전부 다 트레이닝 써버리면은 얘가 진짜로 좋은지 알 수가 없잖아요.

참석자 1 25:29
그래서 항상 테스트 셋을 따로 빼놔야 돼. 그러니까 지금 여러분 용어 제가 전에 했었죠.
여기 이거 내가 트레이닝 적어놨잖아요. 그쵸 트레이닝 말고 뭐가 있다고요?
테스팅이 있잖아 그쵸 테스팅을 실제로 해봐야지.
이게 성능이 좋은지 안 좋은지 알 수 있을 거 아니에요 프로세티브 그래서 다 데이터셋을 다 트리니 쓰는 게 아니라 보통 테스트 셋트 따로 빼놔야 된다.
퍼머 프렉티스는 보통 80%를 트레이닝 하는 데 쓰고 20%를 테스팅하는 데 써요.
보통 그냥 보통 이라는 거지 항상 이렇다는 건 아니고 그래요.
왜냐면은 데이터가 많아야지 뭔가 훈련이 잘 되니까 그래요.
그래서 이제 다시 또 똑같은 말인데 데이터는 트레이닝 데이터 테스트 데이터다 알겠죠.
너무 중요하니까 여러 번 강조했어요. 그리고 또 약간 더 얘기하면은 트레이닝 데이터의 사실은 보통 트레이닝을 할 때도 사실 문제가 이게 트레이닝을 어디까지 해야 되는지 문제가 좀 있잖아요.

참석자 1 26:22
그쵸 트레이닝이 내가 하고 있는 거 여기 지금 보면은 손실 함수를 줄여다가 손실 함수에서 손실이 줄어들게끔 얘를 업데이트할 거 아니에요 그쵸 근데 이거 업데이트하는데 원래 이제 지금 입력 데이터로 준 거 있잖아요.
이게 트레이닝 데이터잖아요. 트레이닝 데이터에 대해서 오스트 값을 최소화시키게만 훈련을 시키면 그러면 이제 무슨 문제가 있냐면은 너무 트레인 데이터에만 잘하는 거야.
그 시험 문제만 잘 푸는 거지 실제로 안 본 데이터에 대해서 사람을 봐야 될 거 아니에요 그래서 그걸 테스트 데이터에 해놨잖아요.
테스트 데이터 뽑아놨는데 그렇게 해놓으니까 다 너무 이제 훈련이 끝까지 이거 이거 만약에 현실 함수가 0이 나오게끔 결과가 그러니까 우리 에러가 전연 0이 되게 해버리면은 완전히 그냥 그 문제만 잘 풀고 이상하게 학습이 되는 거야.

참석자 1 27:14
문제는 또 여러분 교과서에도 되게 좋은데 이거 이 교과서가 부거운데 뭐가 있냐면은 숫자 있잖아요.
레이블 그러니까 여기 예를 들면 여러 거 이런 거 있잖아요.
여러분 요거 이런 거 보면은 이게 이거 4지 4 4 4인 거 보이나요?
여러분 산인데 라벨이 9 붙어 있는 것도 있고 막 그래요.
그리고 진짜 이상한 거 있어요. 9라고 하는 건 여러분 들어볼 수도 있는 것 같은데 여기 명확하게 6인데 실수를 이거를 7 이렇게 적어놓은 사람도 있다니까 라벨을 잘 없을 때고 사람이 실수할 수 있잖아요.
그렇게 근데 훈련을 여러분이 뭐냐면 시험 문제랑 교과서에 틀린 거 있는데 그냥 이것만 듣다 외우는 거 이거 공부하면 안 되잖아.
실제로 문제는 데이터는 항상 틀렸다니까 여러분 교과서가 항상 틀렸어.
교과서 여기서 지금 여기는 번역 잘못된 것도 많고 어쨌든 항상 그렇잖아요.
여러분 그러니까 여러분이 항상 뭐냐면은 너무 과대 적합이라고 그래요.

참석자 1 28:03
과대 접합 트레이닝 데이터에 대해서 과대 적합자라는 게 좀 곤란해서 사람들이 요즘에는 또 어떻게 하고 있냐면은 3M 데이터를 또 쪼개가지고 2로 트레이닝 데이터랑 밸리데이션 데이터를 나눠놔요.
밸리데이션 밸리데이션 데이터는 뭐 하는 거냐면은 이게 이제 홀드 아웃 밸리데이션 따로 그지 홀드아웃이 따로 빼놨다는 뜻이잖아요.
따로 빼놔서 이 이 홀드아웃 셋이라는 게 밸리데이션 셋인데 또는 디벨로먼트 셋이라고 그러는데 내부 셋이라고 그래요.
개발하기 위해서 쓰는 거 이거는 훈련을 어디까지 할지를 보기 위해서 실제로 나중에 이제 로스 값을 트레이닝 데이터에 대해서 계속 오차 값을 계산해서 줄이는 걸 하지만 실제로 훈련을 어디까지 할지는 밸리데이션 데이터 갖고 얘가 지금 이 정도 잘하는 정도다를 판단한다는 거죠.

참석자 1 28:53
공부 어디까지 해야 돼 하산 라인 그런 거 있잖아 하산하는 거 원래 이제 공부하던 그거 말고 안 보던 문제 갖고 계속 한 번도 했던 공부하지 않았던 거 정답 안 알려주는 거지 걔 갖고 너가 지금 이 정도 하고 있구나 이렇게 한다는 거죠.
그리고 나서 진짜 테스트해 갖고 또 해보고 이해되죠.
여러분 밸리데이션 데이터도 문제 밸리데이션 데이터도 이것도 문제가 나중에 또 밸리데이션 데이터가 잘하면 또 그 3년을 멈추기 때문에 또 밸리데이션 데이터에 바이어스 되는 경향도 좀 있어요.
그래요. 여러분 됐죠 그리고 참고로 얘기하면은 밸리데이션이라는 말은 우리나라 말로 아니 검증이 우리나라만의 곡이에요.
그분 밸리데이션이 맞아요 아니잖아요 그쵸 검증이 우리나라 별로 좋은데 베리피케이션이야 그쵸 소프트웨어 공학 이어 그렇습니다.
그래가지고 이 베리피케이션 데이터라고 해야 돼요.

참석자 1 29:52
사실은 그렇게 안 보이고 밸리데이션 밸리데이션은 뭔가 뭔가 밸리데이션은 유효화 이런 뜻이잖아요.
유효하게 만들기 이런 뜻이잖아요. 아니면 또 확인하는 거 확인 확인하는 거잖아.
밸리데 항상 확인하는 거지 괜찮게 지금 훈련하고 있나 근데 그래서 밸리케이션은 사실은 맨날 테스트랑 비슷한 거예요.
마지막에 괜찮냐고 뭔가 이 검증하는 거거든. 어쨌든 검증이라는 느낌이 들어서 검증이라고 번역을 해놨는데 우리나라에서는 모든 책이 다 검증이라고 해요.
근데 영어랑은 안 맞다고 그냥 알아두라고 내가 여러분 항상 그렇게 생각을 하는 게 좋잖아요.
그쵸 이거 검증이라고 공부했는데 검증은 영어 베리피케이션이에요.
진짜로 진짜로 그렇잖아. 밸리데이션은 뭔가 확인하는 거야 이게 제대로 됐는지 그쵸 검증이라는 밸리데이션이랑 밸리핏이 좀 다르거든요.

참석자 1 30:43
사실은 근데 사실은 데리피케이션 하는 느낌도 좀 있어가지고 뭔가 약간 좀 애매하긴 해요.
그쵸 어느 게 더 맞다고 여러분 어쨌든 다르다는 것만 알고 계세요.
일단은 알겠죠 그래요. 그다음에 그래서 이제 또 케이폴드 프로스 밸리데이션이라는 것도 있는데 이거는 교과서에 나중에 나올 거예요.
이게 하도 이제 또 이 밸리데이션도 이제 아까 밸리데이 데이터에 너무 적합 되는 게 별로라고 그랬잖아요.
이해되죠? 여러분 무슨 말인지 밸리데이션 데이터 그것만 또 이렇게 넘어색했던 게 별로니까 밸리데이션 데이터를 여러 개 또 만들어 여러 개 만들어서 아예 훈련을 다 다르게 해.
여러 개 밸리린 데이터랑 테스트 트레이닝 데이터 나눠 프로그램 이거 나누는 방법을 여러 개 해가지고 테이번 해가지고 1번 한 것 중에서 실제로 제일 잘 되는 여기에 이제 밸리데이션 데이터들이나 검증 점수가 나올 거 아니에요 아까 RMSD 같은 거 그거를 평균 내서 그냥 괜찮은지 보면서 계속 수정한다는 거죠.

참석자 1 31:39
특정 밸리데이션 데이터에 대해서 너무 막 바이러스 데이터가 밸리데이션 데이터가 여러분 틀렸어 가 또 문제가 그런 거잖아요.
아까 테스트 데이터가 틀린 문제가 아니라 이번엔 밸리에 데이터가 틀릴 수도 있잖아.
또 이상한 선생님이 와가지고 그러니까 이게 되게 사실 실제로도 그래요.
그러면 한 사람만 여러분 리뷰하는 거 별로잖아 우리 안 그래요 여러분 이 사람이 틀릴 수도 있잖아.
그래요. 머신러닝이 되게 인간 세계랑 비슷해요.
사고도 많이 하고 그리고 되게 고차원적인 일을 하는데 믿을 수 없어 항상 그렇다고 해요.
알겠죠 여러분 그래서 이 짓을 하고 있어요. 알겠죠 그래요.
그래서 트레이닝 타임은 그래서 이제 전부 다 케이번이나 다 나눠서 해야 되니까 케이별로 늘어나는 거지만 데이터가 별로 없으면 이런 짓을 하고 있어요.
알겠죠 교과서에도 있어요. 그림이 있는데 그냥 안 갔다 놓고 그래요.

참석자 1 32:28
그래서 그다음에 이제 이런 게 트레이닝 셋 에러는 굉장히 작은데 텍스트 셋 에러가 높다 이거 이거 왜냐하면 이거 기출 문제 잘 풀었는데 맨날 시험을 못 봐 이런 경우 있잖아요.
그렇죠 어떻게 된 거냐 이거는 보통 오 피팅 오 피팅이 우리나라 말로 봐 대적합이라고 그러거든요.
이거 계속 나올 거예요. 여러분 알티스 쓰는데 한두 번 나오는 게 아니라 과대 적합이라고 불러요.
과대 적합 적합 과대 적합 obt OB이라가 없어요.

참석자 1 33:04
그리고 이제 제너럴라이제이션 에러 아웃오브 샘플 에러 이런 걸 쓰는데 제노라이제이션 일반화가 잘 안 된 거지.
그러니까 오브 피팅이랑 제너널라이제이션은 반대 말이에요.
오브 피팅이 되는 것은 반대 말이에요. 제너레이션이 잘 되면 오브 피팅이 안 일어나는 거고 오브 피팅이 잘못 오피팅이면은 제너레이션이 아닌 건데 막 섞어 쓰고 있어 반대로 그렇죠 이해돼요.
여러분 반대 말이야 오피팅이랑 제너레이션 반댓말이라고 일반적인 걸 잘 잘해야 되는데 이 기출 문제만 잘한다고 기출 문제 같은 경우는 드레인 데이터가 알겠죠 그래요.
그리고 마지막에 아우드브 스템프라라고 적혀 있잖아요.
아웃오브 샘플 아웃오브 샘플은 뭔가 원래 이상한 문제에 대해서 뭔가 잘 풀었다는 거지.
기출 문제 중에서 좀 이상한 문제 있잖아요.

참석자 1 33:49
여러분 정말 더러운 문제 요즘 좋은 게 인간이 많아졌는데 인간에서 계속 제가 웃긴 거라고 아까 애들이 보여서 봤는데 이 문제 이렇게 더러워 이제 이러면서 정승재 그 사람 막 그런 거 봤는데 진짜 그런 게 필요해요.
여러분 이 문제를 풀지 않는 게 나은 게 있어요. 여러분 문제가 너무 지저분하거나 정답이 그러니까 괜히 그걸 안 공부하는 게 나은 게 있어요.
일반화적인 공부를 하기 위해서 그런 거죠. 아도브 샘플이라는 게 그런 거는요.
뭐냐면은 진짜로 일반적이지 않은 그런 샘플 가지고 공부를 해가지고 그 문제만 잘 풀어 실제로는 시험에도 나오지도 않는 문제들 실제 데이터는 그런 거 없다는 거지 여기 있다.
잠깐만

참석자 1 34:37
여기를 안 갖다 놨나 보네. 교과서에 있는데

참석자 1 35:00
이거 말고 이것도 해야겠다.

참석자 1 35:12
뭐냐면은 전에 이거 아무거나 보여주고 싶은데 내가 준비 안 하고 여러분이

참석자 1 35:23
이거 봅시다. 여러분 여기 일반적으로 이런 거 아까 내 그림에도 있기 때문에 이런 거 대충 하면 되는데 진짜 잠깐만요.
미안해요. 여기 가서 할게요. 여기 원래 이윤 리그레이션 할 때 여기 요거 하면 이걸 하면 여기 만약에 여기서 갑자기 샘플 중에 이 데이터가 지금 몇 개 있어요?
여러분 데이터가 개수가 몇 개예요? 이거는 내가 이걸 설명했나 이거 설명했죠 했었어 슬라이드 새로 추가된 건데 그냥 이거 데이터가 몇 개 있는지 알아요.
여러분 여기 지금 이거 이 그림도 전혀 이해 못하나 딱 보면 이제 알아야 돼요.
여러분 이거 데이터가 몇 개인 거야? 9개 잘 썼어요.
그쵸? 여기에 이런 데이터가 있는 거 없잖아 여기 갑자기 이런 데이터가 있는 거예요.
이거에 대해서 선을 해결하려고 해봐요.

참석자 1 36:19
이상해질 거 아니야 안 되는 거야 이 이건 무시해야 되는 거지 그쵸 이상한 거는 제껴야 된다고 그렇죠 잘못된 거지 뭔가 다른 이유가 있는 거예요.
여기서 그냥 형 말고 뭔가 다른 사유가 있는 건데 데이터에 들어왔잖아요.
일반적이지 않은 거잖아. 제너레이즈는 에러 아웃오브 샘플 에러 이렇게 부르는 게 이해가 됐지 오 피팅 에요.
이거를 하려고 여러분 서정 일기를 안 쓰고 머신러닝을 썼어요.
그래서 머신러닝을 썼더니 이거를 위해서 이렇게 만들었습니다.
이렇게 만들었어요. 그러면 여러분 별로잖아. 이게 이게 낫지 이해돼요.
여러분 그거예요. 알겠죠? 됐지 여러분 무슨 얘기인지 그러니까 여러분 이상한 교수를 만나서 그 교수 상태로 시각화되면은 안 되는 거야.
그 교수가 이상한 소리하는 건 제껴야 돼. 나 몰라 알겠죠 무슨 얘기인지 알겠죠 여러분 일반적으로 잘해야 된다고 알겠죠.
가끔 저도 헛소리하니까 여러분 알아서 벌어들이시고 알겠죠.
진짜로 일반적인 걸 잘해야 돼요.

참석자 1 37:16
그렇죠 이상하게 뭔가 사람들이 가끔 그러니까 이제 밸리데이션 에서 그렇게 하는 거죠.
밸리데이션도 여러 가지 하는 거죠. 한 사람 말을 믿으면 안 돼요.
여러분 누구를 마음대로 실천을 신고하면 절대로 안 된다는 거죠.
그래요 네 그래요. 옛날에 트레이닝 하고 있죠 뭐 하는 거예요?
그래서 트레이닝 이렇게 해서 가면은 여기 그래서 여기 여기까지 이제 트레이닝 기본적인 내용이고 여기는 이제 진짜 약간 여기 이거는 사실 앞에 내용에 있었던 내용인데 제가 여기 슬라이드 따로 만들려다가 하다가 여기 냅뒀는데 체인이 쪽이니까 이거 원래 이거는 앞에 나왔던 내용이죠.
여러분 이거 원래 리뉴얼 리그렉션에서 리뉴얼 리그렉션에서 그리고 심플 유니베이션을 한 거는 이제 값이 하나인 경우 내가 왜냐면은 입력 값이 하나인 거예요.
여러분 유니 베넷이라는 거는 입력 값이 하나 입력 값이 여러 개일 수도 있잖아요.

참석자 1 38:19
여러분 입력하신 키다 아까 얘기한 청수 같이 하나인 경우 그럴 경우에 어떻게 운영하는지 먼저 일단 얘기해 주는 거예요.
그냥 가져주라고 하고 이것도 아마 거기서 할 거야.
또 머신러닝에서 할 거예요. 그런데 머신러닝에서 똑같은 방법이 있는데 그래도 이거 보려고 그래도 이해가 안 돼서 이거는 좀 너무 중요해서 하려고 해요.
강의를 그래요. 어쨌든 이게 포스트 펑션이라는 것도 여러분 다 제가 봤으니까 알잖아.
포스트 펑션이 손실 함수랑 똑같은 말이죠. 그쵸 그리고 뭘 쓴다고요?
민스퀘어 에러시죠? 그쵸 RMC에 쓸 수도 있고 ms를 쓸 수도 있지 그냥 ST를 쓸 수도 있어 이해돼요.
여러분 여기 봐봐요. 이게 지금 여기 이 그림은 지금 RMST 이해 안 했어 그 안 씌웠거든 이해돼요.
여러분 그리고 여기 이거 2n 분의 1 있잖아요. 이거 안 나눠도 되잖아.

참석자 1 39:14
평균 한다고 m으로 나눴는데 이까지 붙인 거는 여러분 제곱이라고도 이까지 붙여놨는데 그런 거 필요 없어 없어.
이거 없어도 상관없잖아요. 전혀 EN 분의 1로 나눈 거 말고는 상관없잖아요.
그쵸 실제 로스 줄이는 데는 그래서 어쨌든 여기는 민 스퀘어 에러라고 적혀 있는데 어쨌든 예측 여기 보면은 여기 예측값을 여기 보면 이게 이게 지금 i는 1부터 m까지 적혀 있잖아요.
그쵸 이게 MB 데이터를 의미하는 거예요. 아까 그 하우징 있죠 빅 디플에서 몇 개의 m이 m이 얼마예요 아까 여러분 했잖아 x 표시 몇 개야 9개 그러니까 첫 번째 데이터부터 아홉 번째 데이터까지 전부 오차를 이게 이제 예측 값이고 이게 진짜 값인 거예요.
예측 값을 요거 x 축이 아까 평수 같은 게 XI겠지 샘플 표시할 때 이렇게 아 이렇게 위에 많이 붙여요.

참석자 1 40:15
여러분 표현하려고 그렇죠 이해되죠 그리고이게 4가 씌우면 이제 뭔가 예측했다는 느낌이 드니까 예측 값으로 이렇게 제가 적어놨죠.
실제 값 실제 값이 정답 값 알겠죠? 여러분 그래가지고 이렇게 해놓은 거야 이해되죠?
여러분 그리고 이제 이거 예측 값은 사실은 이 함수 XI에 대해서 뭔가 예측 함수를 통과시킨 거니까 이렇게 쓴다는 거지 그쵸 YI를 이렇게 예측 값을 이렇게 표시한 거지 예측 값이 예측 값을 이렇게 표시한 거 이해되죠?
여러분 이해되지 그래요. 그냥 산수예요. 산수 알겠죠?
수학인가 그래요. 그렇죠 됐어요. 그래요.

참석자 1 40:58
그리고 여기 지금 하이퍼스 펑션이 이게 이거 미리 이거 아까 하이퍼서3라고 h 붙였는데 세타에 대해서 약간 의미를 해 주면은 세타가 보통 파라미터를 여러분 x로 볼 수도 있지만 이게 지금 데이터가 x로 들어오기 때문에 우리가 우리가 학습시켜야 되는 파라미터는 뭔가 다른 알파벳 말고 다른 거 쓰는 게 좋아서 세타를 많이 써요.
사람들이 진짜 여러분 알파고 논문 봐도 세터 나와요.
거기도 뉴럴 네트워크를 그냥 f의 세타라고 나온다고 학습시켜야 될 프라미스 세타가 아주 이런 게 여러분 뭐냐면은 어떤 이제 업계에 들어왔죠 중간중간 업계에 들어오면 이제 세탈은 전부 다 파라미터야겠죠.
모든 논문에서도 세타를 써야지 이거를 다른 이상한 알파벳 변수를 쓴다 그러면 이상한 놈인 거야 알겠죠?
세터를 써요. 보통 알겠죠 그래요. 그리고 리니어 리그레션에서는 이렇게 세타 제로와 세타 원이 있는 거지 그러니까 웨이트가 2개인 거죠.
이해되죠? 여러분 하나는 세타 제로가 바이어스 세타 원이 뭐예요?

참석자 1 41:57
원래 입력 들어온 거에다 곱하는 거 기울기지 기울기 페타 제로가 이제 0에서의 바이러스 값인 거 알겠죠?
x가 0일 때 이해되죠 그래요. 그리고 사실 이게 이게 무조건 1에다가 하고 세타 제로 곱한 걸로 볼 수도 있지.
1에다가 세타 제로 곱하고 세타 1에다가 x 곱하고 그쵸 이런 식으로 입력 값에서 입력값 중에 1도 있다고 보고 1은 세타대로 곱하고 x에다가 세타 원 곱하고 이런 식으로 그쵸 그렇게 볼 수도 있지.
어쨌든 그러면 이제 이게 선형 회기 선형 대수는 비슷해 선형 대수는 되잖아.
선형 대수 매트리스 현상 두 개 세타 매트리스랑 엑스 매트리스인데 일하고 x 있는 거 그 말로 떼면 뒤에 나올 거예요.
또 얘기를 하면은 그다음에 중요한 건 이제 뭐냐면은 얘랑 세트대로 세타 원을 우리가 학습을 시켜야 되잖아요.
그렇죠 학습을 시키는데 이 오차 값이 이게 최소화 되게 학습시켜야 되잖아요.
그쵸 근데 이것도 당연한데 결국은 얘가 이게 오차가 어쨌든 0이 되게 만들어야 돼.

참석자 1 43:03
그렇치 최소화되게 최소화되게 만드는 건 원래 여러분 맨날 최소한 변화율을 계속 보는 거잖아요.
변화율을 변화율이 오차의 변화율이 결국은 계속 오차를 줄이도록 우리가 이 세타 값을 변화시켜야 되잖아요.
각 세타에 대해서 각 세타에 대해서 세타의 변화 세터를 변화시키는 니까 세타의 변화율이 필요하잖아요.
이게 세타 이게 이게 느니 어사인먼트고요. 여러분 이게 세타 제가 얘 다음 세타 값이죠.
섹터 j가 섹터 j가 이 j가 i랑 0이 있어요. 이해되죠 여러분 여기서는 이해돼요.
여러분 이해돼 아까 바이러스랑 각각 곱하는 거야.
그게 다음 세타 값이랑 지금 세타 값이 있는 거잖아.
그래서 이거 이렇게 사실 식이 사실 세타 제로 델타 세타 j 는 하고 이거 이렇게 있는 거랑 똑같은 거 아니래요 너무 대충 설명하나 다시 할게요.
여러분 여러분 이거 산수가 아니라 수확이지 수확이지 여기 보면은 이거 있잖아.
요거 요거 요거 요거 왼쪽 오른쪽 거를 자변으로 넘기면은 세터 제만이 세터 j잖아.

참석자 1 44:12
근데 여러분 이거 우리가 알지만 산수 원래 수학에서 수학 2%로 우리가 프로그래밍에서 어사이먼트 할 때 왼쪽 값은 나중 값이고 오른쪽 값은 먼저 있는 값이잖아요.
사실 왼쪽 세트짜리는 미래 값이고 변화 값이고 저 세타 제 오른쪽 세터 j는 지금 현재 값이잖아.
저거를 왼쪽으로 옮기면 세타 j 미래 값 빼기 세타 제 현재 값이잖아요.
그래서 그게 변화율이라고 변화 값이잖아. 변화값 이해돼요 여러분 그래서 저거를 계속 어떻게 만들고 싶으냐는 오차가 최소가 되게 쟤도 변화시키고 싶은 거잖아.
우리가 얼마큼 변화시키냐의 문제인데 패터 제로 업데이트해야 되잖아.
우리가 그래서 원래 이상한 값이 있었는데 계속 원래 제대로 만들어서 로스를 잡게 만들어야 되잖아요.
그러기 위해서 원래 이제 이 오차 있죠.

참석자 1 44:56
오차 오차 이 오차 오차를 지금 제로 오차가 아니라 정확히 말하면 이제 퍼포먼스 te으로 MSD 같은 거지 mse가 이 섹터에 대해서 얼마큼 이 섹터 j에 대해서 얼마큼 변하느냐 변화율 변화율만큼 이제 변화시키려고 하는 거예요.
얘가 이만큼 세터가 이만큼 변하고 쟤도 이만큼 변하니까 쟤도 변화시키려고 한다고 근데 오차를 감소시키기 위해서 뭔가 원래 값에서 이제 빼는 식으로 감소를 시키는 거예요.
빼는 식으로 줄이려고 그런데 거기다가 그냥 안 하고 알파를 곱했죠.
또 알파를 그냥 오차만큼 빼버리면 너무 이상해질 수 있어서 약간 조금 조금씩 제가 담보면서 줄여나가는 건데 그 이유가 뭐냐면은 사실 이게 이 그림이 여기 오른쪽에 있는데 이게 세타 원에 대해서 지금 이게 x 축이 세타 세타 원이고 y축이 세타 세타 뭐예요?
j 값이에요. 제 값 오차 값인데 이게 여러분 보면 이거 보이지만 이렇게 돼 있는데 여기 이게 들어가잖아요.
그러면 이거 사실 2차 함수잖아요.

참석자 1 46:03
이제 제곱 하니까 그래서 세타 제로에 대해서 이게 2차 함수로 변하겠지 오차가 이렇게 생겼겠지 당연히 뭐야 이거 다 산수예요.
수확이야 수학 알겠죠. 그래서 이렇게 그린 거야.
그림이 실제로 이렇게 생겼어요. 이렇게 생겼다고 우리 차가 세타 원이 지금 초기 값이 여기 있었어.
만약에 그러면 여기로 와야 될 거 아니야 이래야지 오차가 최소화 될 거 아니에요 여기까지 보내 내기 위해서 얘는 조금 조금씩 뭔가 로 이동해야 되잖아.
여기 있었으면 일로 와야 되고 여기서 쓰면 만약에 여기서 으면은 이쪽으로 와야 되잖아.
여기까지 그래서 이렇게 와야 되는데 그래서 이제 여기서 그냥 이만큼 이렇게 이렇게 와야 되잖아요.
그쵸. 그래서 그거를 이제 얘가 변화율이 변화율 있죠.
변화율을 보고 변화율을 에다가 마이너스 변화율을 빼서 변화시키겠다는 거지.

참석자 1 46:52
그리고 이거 다 보면 여러분 다 맞는 말이고 어쨌든 그래서 지금 중요한 거는 그냥 또 하면 안 되고 이게 만약에 이것만 봐도 알 수 있는 게 여러분이 이 조금 조금 안 하고 만약에 크게 했으면은 맨 처음에 여기 있었는데 맨 처음에는 여기 있었는데 여기 있었다고 했냐 고는 맨 처음에 여기 있었어요.
만약에 여기 값이 그런데 일로 오른쪽으로 가야 되잖아요.
오른쪽으로 이만큼씩 가서 가야 되는데 여기까지.
그래서 나중에 이제 이제 진짜 얘가 아무리 변해도 거의 이제 변하지 않는다는 걸 느끼는 거지.
쟤가 별 변화율이 없으면 이제 여러분 사실 최소잖아.
그쵸 보통 여러분 손실 함수는 다 이렇게 미분 가능하고 변화율이 없는 지점이 여러분 어떻게 돼요?
변화율이 없잖아. 여기는 지금 제가 변화율이 없잖아.
얘 세타 원이도 변하지가 않잖아. 그게 최소라고 알겠어요.
근데 만약에 이렇게 만약에 그냥 오차 나오는 대로 이렇게 매니저로 확 넘겨 이쪽으로 가버리면은 계속 또 변화율이 크잖아.

참석자 1 47:46
또 너무 한꺼번에 많이 변화시키면 안 되는 거지 찔끔찔끔 해야 된다고 그래서 알파 값을 좀 작게 해서 곱하는 거를 변화율에 비례해서 하는데 찌끌찌끌 다는 게 좋다는 거죠.
너무 찔끈찔끓이면 오래 걸릴 거고 너무 이게 크면은 힘들 거고.
그쵸. 그래서 이거를 러닝 레이트라고 해서 학습률이라고 불러요.
여러분 그런 거죠. 내가 여러분 자전거 타는데 자꾸 오른쪽으로 넘어가서 왼쪽으로 왼쪽으로 해야 되는데 왼쪽으로 확 가버리면 여러분 왼쪽으로 넘어가겠죠.
그러면 오른쪽으로 하면 또 오른쪽으로 가면서 계속 막 미치겠는 거죠.
그렇죠 계속 어떻게 하라는 거야 됐죠 그쵸 이해되죠 여러분 그 러닝 레이트를 너무 피하면 안 되고 찔끔찔끔해야 돼.
그쵸. 오른쪽으로 왼쪽으로 넘어지길래 오른쪽으로 쭉 가라고 그랬더니 오른쪽으로 갔더니 좀 덜 넘어지네.

참석자 1 48:26
또 계속 오른쪽으로 가서 그렇죠 그러다가 이제 어느 순간 에러의 변화율이 없으면 이런 식으로 해야지.
그쵸. 저로 넘어가 버리면 곤란해. 그쵸 그래서 러닝 레이트를 적당히 해야 돼.
그렇죠 근데 이제 그런 것도 있지. 러닝 레이트 너무 작아 계속 왼쪽으로 넘어지면서 조금 나아지고 있는 것 같은데 들어서 어느 세월에 그렇죠 100년 걸릴지 모르겠어요.
이런 거죠. 그쵸 자전거 못 타는 거죠. 그쵸 이해되죠 여러분 그래요.
그렇게 이해하시고 그다음에 이걸 왜 했느냐면은 아까 이거는 심플 유니베이션라는데 이게 x 값이 이거 하나 진짜 하나로 생각하면 파란 건데 아까 총수 총수고 이건 뭐야 집값 이런 거였잖아요.
숫자나 스칼라 근데 사실은 입력 값이 하나가 아니라 되게 많을 수 있잖아요.
그쵸. 그러니까 집값을 할 때 사실은 평수만 넣는 것보다 뭐가 있는 게 좋냐면은 범죄율 편의시설 비율 이런 거 되게 있으면 중요하잖아요.

참석자 1 49:21
사실은 그렇죠 실제로 되게 온갖 파라미터를 막 저는 보스턴 집값 예측이라는 게 굉장히 유명한 또 문제인데 에니스처럼 걔는 13개가 있어요.
여러분 피처가 되게 은근히 잘 맞아요. 옛날 거지만 이해됐죠 여러분 지금도 우리나라도 뭔가 할 때 학군 이런 거 있잖아 넣으면 비율 그런 거 넣으면 범죄율이 있으면 확 내려가버리고 그런 거 있잖아.
그쵸 알겠죠 그래서 이게 여러 개가 있는 게 정상이에요.
보통 많아요. 알겠죠 그리고 아까 앱 리스트 있잖아요.
엠 리스트 걔도 사실 얘가 달라지는 거 이것도 피처가 이 입력 값이 그림인데 그림이잖아요.
그림 픽셀로 이게 흰색은 0이고 검은색은 1인 거예요.
255 또는 50부터 2505까지면은 근데 그런 개가 몇 개 있냐면은 가로 28개 세로 28개 있는 점들로 이루어진 거예요.
28 곱하기 28개의 데이터가 있는 거예요. 이해돼요.
여러분 이 피처가 2828 30도 되는 이 사람이 있는데 그 28,198 알겠죠?

참석자 1 50:21
엄청나게 많은 피가 그래서 이렇게 일반적으로 만들 수가 있고 그쵸 예측 값은 그래서 각각의 피처에 대해서가 다 뭔가 세터를 곱하는 거라고 맨 앞에 1로 x 제로가 있는 걸로 보고 x 1부터 시작하지만 데이터 처가 입력 피처가 x n개가 있으면 이건 여러분 데이터랑 여러분 헷갈리는 게 조심해야 되는 거야.
그 데이터의 개수는 여기랑 상관없어요. 그쵸 지금 입력 필처의 개수라고 보스턴의 집값의 특성 알겠어요 여러분 거기다 그거만큼 더 세타가 존재한다고 파라미터가 각각에 대해서 얼마큼 곱해야지 뭔가 되는지 보는 거야.
집값이 만약 집값 같은 경우에는 아까 보스턴 같은 경우에는 범죄율에 대해서 엄청 마이너스를 곱해야 될 것이고 평수에 대해서는 플러스를 곱하라도 얼마큼 곱하면 적당히 나온다는 거죠.
손용액으로 잘 되거든요. 이제 이해가 이해돼요.
여러분 그러다가 사실 선행 얘기도 안 되는 구간이 발생하는데 그걸 이제 딥러닝하면 너무 잘 되지 이렇게.

참석자 1 51:17
근데 집값이 너무 범죄율이 또 너무 막 이제 어느 정도 이상 된다고 더 집값이 좋아지지는 않거든 뭔가 다 항상 이렇게 뭔가 세츄레이트 하는 구간이 있거든요.
그래서 딥러닝이 더 잘 돼 대충은 맞고 딥러닝 이런 건데 우리 3분이구나 10분 쉬었다가 11시 3분에 합시다.
여러분

참석자 1 51:49
지각 회사 있어요. 지각

참석자 2 52:04
지각 지각이네.

참석자 1 52:27
이게 2시간짜리 수업에서는 그냥 죽어버리기 싫어서 이때 나타난 친구들은 그냥 출석을 해 주고 있어요.

참석자 3 52:39
그 세타 하나마다 그러면 특성이라고 생각하면 되는 건가요?
여기서

참석자 1 52:44
세트 하나마다 특성 x 하나마다 특성이지 x가 x 1부터 세타는 파라미터 특성에다 곱할 거

참석자 3 52:50
그러면 세터가 웨이트 값이라고

참석자 1 52:51
그렇지 메타 메이트 값

참석자 3 52:52
그러면 x가 특성이고 그거 그러면 x 제로는 왜

참석자 1 52:56
1이야 1 무조건 바

참석자 3 52:57
여기부터 이제 시작하는 건가요?

참석자 1 52:59
바이어스 아까도 앞에도 봐봐요. 여기도 세터 제로에다 1 곱한 거라고 생각하면 돼요.
x 제로가 있다고 생각해도 된다. 네 x 제로는 무조건 1이다.

참석자 3 53:08
네 그렇게 해도

참석자 1 53:09
없는 건데 사실 일로 무조건 보고

참석자 3 53:11
네 알겠습니다. 그리고 데이터가 이제 그냥

참석자 1 53:15
수업시간 중에 뭐라도 더 좋은데

참석자 3 53:16
그래 그래 방해될까 봐 데이터가 그러면 3개로 나눠진다고 생각하면 되는 거예요.
트레인 테스트 밸리데이션 근데 테스트랑 밸리데이션이랑 차이가 뭐예요?

참석자 1 53:24
테스트랑 밸리데이션이랑 차이니까 그 밸리데이션 기능이 기능이 뭐예요?
밸리데이션을 훈련하는 중이에요. 어디까지 훈련을 멈춰야 될 거 아니야 언제까지 계속 끝까지 할 수는 없고 그 밸리데이션 그 점수 보면서 훈련을 계속 멈출까 할까 이렇게 하고 있는 거고 네 테스트 근데 밸리데이션이고 이제 다 끝난 다음에 진짜 니 점수는 이러면서 진짜 쉬워 그러면 비율 밸리데이션은 약간 모의고사.

참석자 3 53:46
그러면 전체적인 비율이 어떻게 돼요? 테스트 트레인이

참석자 1 53:49
또 그 안에서도 또 이제 80대 20대 이렇게 하니까 많이

참석자 3 53:52
8 2 1이에요.

참석자 1 53:54
여기 안에서도 실제 여기서 팔 이 20% 안에서 밸리데이션 데이터를 또 20%를 쓴다.

참석자 3 54:00
아 안에서 또 20분 알겠습니다. 감사합니다.

참석자 2 54:21
쭉 가야 된다. 아니야 그게 나쁘지 않아 이런 식으로 보면.


clovanote.naver.com