딥러닝 day5_1
2025.03.19 수 오전 10:02 ・ 55분 46초
심승환


참석자 1 00:00
하는데 이거 안 바뀌었구나 여기 퍼포먼스 헤드릭에 사실 지난 시간에 이제 퍼랑스 에드릭 었죠.
여러 가지가 있을 수 있는데 일단 제일 근본적인 게 이제 사실 리뉴얼 이그렉션, 라디에스티 이그렉션 두 가지인데 어쨌든 중요한 건 이제 문제 자체가 컨티뉴스 밸류를 프디션하는 거랑 바이너리 클래시피케이션 하는 거 그 두 문제가 제일 근본적인 문제죠.
여러분 그렇죠 우리가 풀어야 돼. 그쵸 보통 예측하려고 그러면은 어떤 입력 값으로부터 뭔가 어떤 연속적인 값들 중에 뭐냐 이렇게 예측하는 거랑 그렇죠 되게 여기 사실 여기도 이것도 이거 여러분 기억나 AI 카 이거 있잖아요 기억나죠 여러분 설명했죠.
제가 여기서도 여러분 이제 출력물이 출력이 2개잖아요.
뭐였어요 어느 방향으로 하느냐 그렇죠 마이너스 1부터 1까지 컴트랜스 밸류 중에서 그렇죠 그리고 또 이제 속도 속도도 컨텐스페이잖아요.
그쵸 그렇게 할 수 있지 그치 그리고 다른 하나는 바이너리 플레시피케이션 문제 있잖아요.

참석자 1 01:01
그쵸 마이클 스필오드는 예스 노잖아 예스 노 예스 노 근본이죠.
그쵸 사실 얘도 예스 노로 바꾸어서 되기도 해요. 그러니까 뭐냐면은 오투터냐 왼쪽 터냐 그다음에 또 오투 터을 때는 또 그 안에서 또 45도 이상 치냐 45도 6 지냐 계속 이렇게 나눠서 얘기할 수 있기 때문에 뭔가 또 이진 분류가 제일 근본이에요.
그쵸 어쨌든 그거에 대해 그거에 대해서 이제 이번 우리가 오차에 대한 개념이 있어야 되잖아요.
그쵸 그래서 여기는 목차가 에러가 rlse랑 mse로 했다는 거 설명했어요.
지난 시간에 그쵸 맞죠 이거 다 이해되죠 여러분 너무 잘 그렇죠 이게 되는 거지 왜 이거 했어요 내가 안 했나 안 했어 이게 너무 헷갈려 AI 종합 개발해서 했거든 여러분 이거 잠깐만 내가 이거 여러분 지나 씨 이건 했어요.

참석자 2 01:50
영상으로 보여주세요.

참석자 1 01:53
저 뒤에 슬라이드 하셨으니 여기서부터 안 했어. 이것도 하셨어요 이것도 했어 확인은 하긴 했지 일단 했다.
여러분 색깔 왜냐하면 슬라이드를 막 띄워놔가지고 교과서 안에 거 지금 맡겨 넣어가지고 사실은 이게 1학년한테 쉽게 설명하는 걸 넣다 보니까 아니 여러분들도 사실 쉽게 설명하는 게 필요한데 그래서 그냥 막 넣었어요.
말로 떼우다가 자꾸 넣었어요. 이게 쓰기 좋잖아요.
사실은 없어 없어 이거 다 아는 거라고 생각할 수도 있지만 이거 여러분 다시 강조하면은 원래 기본적으로 오차는 얘야 얘 그쵸 요거 요거 뭐냐면 예측 값이라 해서 원래 정답 값을 뺀 거지 이게 차이잖아요.
절대값이 맞지 왜냐면 플러스 마이너스가 실제로 진짜는 진짜 고차 값은 이건데 이거를 뭔가 전체적인 여러 샘플에 대해서 하려고 그러면 절대 값을 더 하든지 제곱을 더 하든지 그런 것밖에 없죠.
두 가지 방법이 대표적이지 그쵸 다양 다 똑같이 오차를 쌓아야 되니까 이해되죠.
여러분 상쇄해버리면 곤란하잖아.

참석자 1 02:49
그렇죠 그래서 이렇게 두 가지가 있고 그래서 RMS MA가 두 가지가 있어요.
그렇죠 이거 외워야 되는데 그렇죠 제가 사실 옛날에 말로는 그냥 막 떼웠을걸 이거 하여튼 용어를 RMS MA 이런 거 있잖아요.
용어를 외우시기 그죠 유명한 용어고 루트민스케어는 귀찮으면 그냥 루트 안 씌워도 되잖아.
그쵸 루트 안 씌우면 여러분 옛날에 원래 보통 통계에서 분산을 쓰잖아요.
표준 절차 안 쓰고 휴대표처럼 무사하면 제곱 쓰고 찌고 안 쓰고 차이잖아요.
그쵸 분사만 얘기해도 되잖아. 사실 그렇죠 본 것처럼 여기도 알 루드를 떼버릴 스 루트 저것도 지금 안 할 수도 있는 거지 그리고 미니 스케어 에러라고 했는데 미니는 너무 당연하니까 평균 내는 거 당연하니까 그것도 귀찮아서 그냥 스케어해라 그렇게 얘기해요.
그쵸? 그럴 수 있죠 여러분 이해되죠 사람들이 그러니까 루트 미인 스퀘어 에러 이렇게 얘기할 수도 있지만 그냥 미인 스퀘어 에러라고도 얘기해도 똑같아요.

참석자 1 03:43
결과적으로 어차피 저 둘이 왔다 갔다 하니까 그렇죠 이해됐죠?
여러분 미인 거 너무 당연해 평균 내는 건 당연한 거 아니야 그쵸?
안 내봤자 상관도 없어 어차피 이거 줄이면 되는 거니까 n분의 1 안 하면 평균이 아닌 거잖아요.
그쵸 그냥 합친 거지 그쵸 n분의 1 안 해도 별 문제없잖아요.
오차 값이 보이는 거는 어차피 이게 0이 돼버리면 0도 돼버릴 텐데 n분의 1 나누건 말건 민이라는 건 순전히 n분의 1 나누는 거잖아요.
그쵸 됐죠 그래가지고 우리 옛날에 그 앞에서 여러분한테 지도 학습이 뭐가 있는 주 보여줬대.
여기에 이제 제가 리뉴 리그렉션은 콘텐츠 밸리 피딕션을 위한 대표적인 방법인데 여기에 이제 리스트 스퀘어 리그레이션이 적혀 있다 그랬잖아요.

참석자 1 04:26
리스트 스퀘어가 이 스퀘어가 어디서 온 말이냐 아까 그 루트 민 스퀘어 에러에서 제일 근본적인 거 스퀘어만 남는 거고 오차를 스케어해버린다고 이해되죠 여러분 에러를 스퀘어한 거 그거 그거를 리스트 최소화시켜야 되니까 그쵸 그래서 이거 사실 제가 지난 시간에도 책으로 보여줬었는데 책으로 책에 사실 책 가지 보여줄 필요 없어 내가 이거를 1학년들을 위해서 이 책을 가지고 1학년들한테 이 책을 보여주고 들이밀 수는 없잖아요.
그래가지고 1학년들은 교과서도 없이 그냥 하고 있는데

참석자 1 05:02
여기 띄워놨지 내가 여기 여러분 강의 자료 제가 월요일에 새로 올렸었는데 진도 못 나갔지만 요거 요거 요거 올렸었거든요.
봤어요. 여러분 트레이닝만 제가 따로 올렸거든요.
이것도 사실은 1학년을 위해 새로 만들었다가 열어봤으면 좋을 것 같아서 그냥 그렇게 하려고 했어요.
그래요. 그리고 어차피 제대로 들어가기 전에 앞에 거는 확실히 아는 게 더 좋으니까 여기 보면은 원래 교과서에 있는 거랑 사실 비슷한 그림인데 어쨌든 원래 지도 학습이 이렇게 한다고 제가 보여줬잖아요.
옛날에 그쵸 근데 이거를 좀 더 자세히 그리면 입력 데이터가 들어가서 프로그램이 뭔가 이제 결괏값을 내는 건데 근데 이제 이 프로그램이 사실은 이렇게 노란 부분이 이렇게 부분으로 나눠지고 사실은 트레이닝을 위해서 이게 전에 제가 미리 얘기하느라고 이게 아까 그 리스트 스키어에서 얘기하면서 얘기하려고 하는 거예요.

참석자 1 06:01
요게 이게 원래 이제 이 부분이 순전히 트레이닝을 위해 있는 거라 그랬지.
그쵸? 트레이닝 안 할 때는 이 빨간 부분 없어요. 그쵸 알고 있죠?
여러분 트레이닝을 할 때만 빨간 부분이 쓰이는 거지 나중에 이제 인퍼런스 할 때는 안 쓰이는 거고 그쵸 지금 내가 이걸 이걸 왜 강조하냐면 여러분이 사실 머신러닝 러닝을 하는 거가 목적이 나중에 이제 다 웨이트 가중치 제대로 만들어 놓은 다음에는 그냥 쓰려고 하는 거잖아요.
그래서 그 프로그램이 차이가 뭔지 좀 아는 게 필요하잖아요.
그쵸 그래서 지금 제가 강조하는 거예요. 이거는 이 그림은 전부 트레이닝이 들어간 건데 트레이닝에 해당하는 부분만 제가 지금 빨간색을 쳐놓은 거예요.
알겠어요 이해되죠 여러분 여러 가지 목표에서 보여주고 있지 그쵸 여기도 여기도 트레이닝에 해당하는 부분이 뭐냐면은 그렇게 된 부분이 트레이닝 해당 트레이닝에 해당하는 부분이에요.
트레이닝 안 할 때는 이게 없는 거지 빨간색 톤이 아래에서 이해돼요.

참석자 1 07:02
이거 그치 그렇거든요. 체인이 안 할 때는 저 빨간색 부분이 없어 뭐 알겠죠?
여러분 그래서 여러분이 그냥 딥러닝을 쓴다고 할 때 트레이닝을 위해 쓰는 건지 그냥 가져와서 쓰는 건지 이게 다르다고 그거를 명확히 잘 모르겠네요.
알겠죠 어쨌든 그 트레이닝을 할 때는 여기 지금 sl이라고 적었잖아요.
내가요 그쵸? sl 이게 뭐냐면 여기 제가 슈퍼마이드 러닝 sl이라고 여기 지금 약자를 해놨어요.
그걸 말하는 거예요. 알겠죠 됐어요. 그러니까 이게 슈퍼 이즈 러닝을 하는 뭔가 함수가 있다는 거지.
이게 뭐냐면은 결국은 예측하는 함수지 이게 함수가 뉴럴 네트워크일 수도 있는 거예요.
여러분 이해돼요. 여러분 뉴럴 네트워크 함 함수는 뭐죠?
입력이 나와 출력이 나오는 함수라고 알겠어요.

참석자 1 07:54
여러분 근데 그 함수의 파라미터 값을 이제 우리가 가중치 값인데 그거를 업데이트하는 거지 그쵸 알겠죠 어쨌든 여기 지금 이거 빨간색 이거 친 거는 이제 지금 다시 그거 아니야 딥러닝 아니 아니 저기 트레이닝하고 상관없는 내용이 있어요.
이 이건 트레이닝을 하건 말건 쓰는 거지. 그쵸 색깔 잠깐 해야 돼 지금 그래서 중요한 건 여기서 보고 싶은 거는 원래 이제 트레이닝 안 할 때는 이게 등장하지 않지만 트레이닝 할 때는 반드시 이게 좀 계산이 돼야 돼.
그쵸 이게 중요하다고 트레이닝을 할 때는 반드시 출력 데이터로 끝나는 게 아니라 손실함수를 통과해요.
항상 그래서 뭐라고 하면 손실 함수가 뜻이 통신 지난 시간에 얘기했지만 로스랑 에러랑 코스 같은 말이라는 거 교과서 할 때 했었죠.
프로젝트 안 되는 줄 알고 막 조를 했을 때 그렇죠 기억나죠.
여러분 손실 함수를 통과해 이게 중요하다고 알겠죠.

참석자 1 08:48
실제로 트레이닝 할 때는 그래서 우리가 관점을 얘 이 함수에 생긴 모양을 보느냐 이 함수에 생긴 모양을 보느냐에 따라서 말을 얘를 이제 이게 뉴럴 네트워크일 수도 있고 아니면은 그냥 리뉴얼 리그렉션이나 라지스트리그렉션으로 끝날 수도 있잖아요.
라지스트 리그렉션 리뉴얼 리그렉션 이런 것들 전부 다 뭐예요?
여러분 그들 걔도 다 러닝이야 그쵸 그러니까 걔도 손실함수 통과하겠지 이해돼요.
여러분 손실함수를 뭘 쓰느냐가 되게 중요하겠죠.
실제 손실 하려고 그러면 손실 압수를 보고 가중치를 업데이트하는 거잖아요.
손실 값수에 나온 결과값을 보고 손실 값을 보고 그렇죠 이거를 잘 써야지 잘될 거 아니에요 그렇죠 주면에서 호시라 잘 만들어야 돼.
그쵸 잘 적용해서 그래서 머신러닝이나 딥러닝에서 훈수하면서 뭐 쓰는지 당연히 묻겠지.
그걸 이상하게 쓰면은 니가 의도를 가지고 그렇게 쓰는 줄 알고 시킨다고 그냥 알겠죠.
그 문제의 말씀 되게 잘 해야 돼요.

참석자 1 09:44
그래서 계속 얘기하면은 아까 이게 리뉴얼 리그레션이면 만약에 이게 제가 딥러닝일 수도 있고 그 딥뉴얼 네트워크일 수도 있고 그냥 퍼셉티션 뉴럴 네트워크일 수도 있고 라디스 리그레이션일 수도 있고 리뉴얼 리그레이션일 수도 있다고요?
이해됐어요. 여러분 이해되지 이게 이 셀 함수다 이해돼요.
여러분 무슨 말인지 여기 손실 함수는 만약에 여기가 라스 리니어 리그레션이라고 쳐봐 여기가 s 함수가 그럼 여기는 뭐가 와야 되냐 손실함수가 뭐가 와야 된다는 거예요?
루드미 스퀘어 에러라든지 루드미 스퀘어 에러 이해돼요.
여러분 또 뭐 아까 민 앱솔루트에요. 그쵸? RMS MA 이런 게 들어온다고 여기 손실 함수로 알겠어요 여러분 됐어요.
지금 요 너무 쉬운 건 아닌 것 같아요.

참석자 1 10:35
배 거 보니까 그렇죠 됐죠 그다음에 이름 지을지 얘가 이제 리니어 리그레션을 하면은 여기 sl 함수가 리니어 리그레션을 하면 무조건 여기는 항상 어떻게 되냐 스퀘어 에러를 계산하니까 스퀘어 리그레션이라고 부르기 시작했다고 사람들이 결국 이거 이거 이거 갖고 맨날 훈련시키니까 리니어 리그렉션은 스퀘어 리그렉션이구나 이렇게 리스트 스퀘어 리그레이구나 리스트 스퀘어 값이 제일 스퀘어 값이 제일 작아지게끔 훈련시키는 거니까 리그렉션이 뭔가 글로 만들겠다는 뜻이잖아.
손 모양으로 만들겠다. 여기는 이제 리스트 스퀘어를 만들겠다 이거잖아 이거를 그래서 이렇게 같은 같이 막 섞여서 본다는 거지.
이 사람들이 여기 이름이 여기서 리스트 스케어즈라고 부르는 이유가 얘는 아까 에셀 함수를 리뉴얼로 만들겠다.
이거는 손실 함수를 리스크 스퀘어로 만들겠다 이런 거였다고 알겠죠 이해되지 그런 용어가 있단 말이야.

참석자 1 11:31
그럼 여기 뒤에 적혀 있는 것도 뭔가 다 그런 뭔가 맥락이 있겠지 아지스티랑 시그모이드는 그냥 슬라이드 지금 서로 올려놓고 같은 말이다.
이런 것들이 나중에 일단 이거 이거 있잖아 이거는 일단 나중에 할게요.
여기까지 일단 했어요. 여기까지 다 이해했어요.
여러분 그렇죠 이해됐죠 여러분 그러면은 어쨌든 지금 성능 지표로 쓰는 거가 이제 에러 성능 지표가 저 에러가 작게 만드는 거지 이거를 rmsa나 MA를 작게 만드는 거예요.
그쵸 이해됐어요. 여러분 그리고 rmsa랑 MA랑 다 장단점이 있어요.
RMS는 여러분 제곱을 만드니까 좀 큰 오차는 더 뻥 튀게 되겠지 여러분 또 그런 면이 있죠.
그거 그러니까 MA는 크로차라고 해서 더 막 커지진 않죠.
그렇잖아요.

참석자 1 12:30
큰 오차를 막 되게 굉장히 민감하게 반응하고 싶어 그러면 RMS로 쏴야 되고 너무 오차가 큰 거는 확 줄여야 되고 이거 MA는 그런 거 차려주지 않아야 되는 그런 문제라면 MA를 쓰고 그러는 거예요.
여러분 문제 성격에 따라서 알겠죠 그래요. 그다음에 그래서 여기 이렇게 여기 이제 퍼포먼스 애트리스폼 컨트리스 밸리피리터랑 여기 퍼포먼스 애트리스폼 다음에 근본적인 문제가 이제 바이너리 클래스케이션이니까 그거에 대한 퍼포먼스를 쭉 얘기하는 거예요.
알겠죠 그리고 이건 사실 머신러닝 수업 시간에 배워요.
여러분이 배울 수밖에 없지. 그래서 아 모르면 이제 아예 논문이고 하는 사람들이 얘기하는 거 못 알아들으니까 기본적인 거예요.
이것도 알겠죠? 얘가 이제 여기 보면 제가 쭉 적어놨잖아요.
이런 것들 있죠. 내가 이거를 열심히 여기서 강의하는 게 좀 별로인 것 같아.
왜냐하면 그 수업 시간에 할 테니까 겹치잖아요. 제가 이거 운영 체제 시간에 컴퓨터로 강의할 수는 없잖아요.

참석자 1 13:25
그거는 시간 하는데 여기 있는 거 모르면 안 돼. 다 안다고 보고 진행하겠다는 거예요.
알겠죠 그냥 순순히 시간을 아끼기 위해서 알겠죠.
당연 안 한다. 알겠죠 근데 안 하는데 다 알아봐 알겠죠 알겠죠 다 안다고 보고 대수 시험 문제 이게 막 섞여 나와도 틀리면 틀리는 거다.
왜냐하면 더 근본을 모르는 건 더 안 좋죠. 여러분 애플을 쓰는데 애플에 있는 키가 키인지 모르면 안 된다는 거지 알겠어요 여러분 그래요.
여기 막 제가 열심히 적어놨는데 모르면서 질문하셔 알겠죠?
알겠죠? 여기 이렇게 저거 해놨어요. 여러분 제가 이거는 그냥 신문 기사에도 막 나와요.
여기 신문 기사 신문 기사에도 지금 코로나 바이러스 진단기가 나왔는데 이번에는 재연도가 얼마고 특이도가 얼마예요 막 이러면서 진짜로 진짜 그래요.
그리고 필스전 리콜이랑 아로스 커브라는 것도 유명하고요.
그래요.

참석자 1 14:20
근데 약간 얘기하고 싶은 거는 프리시즌 니콜이랑 라로시 코드가 전부 다 이게 서로 프리시전이랑 리콜이라 이게 지금 프리시전이 이제 정확도가 아니라 정밀도고 리콜이 재연도라는 건데요.
그 사람의 하는데 이게 서로 같이 도와주기 힘들어서 하나 좋게 하면 다른데 아까의 모양이 있어요.
그래서 그거를 두 개 동시에 보기 위해서 이런 그래프를 보통 다 그려요.
진짜 잘하는지 못하는지 보라고 그러면 알겠죠. 그래서 이거 둘 다 그러니까 이게 뭐냐 프리시전을 높이면 리콜이 나빠지고 이래가지고 리콜을 높이면 프리시전이 나빠져서 적정한 부분을 찾아서 이제 진단 시 출자하는 거지 누가 그러니까 이콜이 말을 하려면 너무 길어져 그만할게요.
여러분 나 강의하려면 되게 강의하면 되게 오래해 그래서 그냥 그만하고 이거는 요즘 머신러닝 수업 걸로 하세요.
그래서 딥러닝 해야지 그다음에 이거는 제가 여러분 강의 자료에 좀 틀리게 돼 있어서 내가 새로 올렸어요.

참석자 1 15:19
멀티 클래스에 대한 컴플리 매트릭스인데 기본적으로 이제 용어가 에큐로시랑 프리시전 리콜 이런 게 있거든요.
여러분 에큐르시 영어로도 알고 우리나라 말도 알아야 돼요.
에큐로시 정확도라는 거고요. 프리스트하고 이크는 아까 PR 카드 얘기했잖아요.
그렇죠 액트리스는 기본적으로 데이터 셋 전체에 대해서 얘기하는 건데 에큐시는 여러분 좀 계속하세요.
여기도 맨날 계산할 때 에큐시 계산하고 이러거든요.
그러니까 딥러닝 이제 훈련시킬 때도 무슨 딥뉴럴 에토 콜린 에큐로시 맨날 나와요.
근데 그 에큐로시의 정체가 뭐냐면은 그러니까 프레시전이나 리콜 이런 거는 이제 안 나와 그래요.
여러분 알겠죠 근데 에큐르시 나올 때 에큐르시는 전체 뭔가 맞춘 것 중에 전체 뭔가 예측한 것 중에서 진짜로 맞는 것의 비율인데 이거는 데이터셋이 전체에 대해서 얘기하는 건데 실제로 분류할 때 이제 아까 지금 다이어리 플리피케이션이라는 멀티플레스 있잖아요.
여러 개 세 가지를 분류하는 거죠.

참석자 1 16:16
예를 들어서 이런 거 할 때 프리지랑 리콜은 클래스별로 하는 건데 클래스별로 그러니까 애플에 대한 프리스 전 애플 리콜 옳은지에 대한 표시 리콜 이런 식으로 얘기한다고 이해돼요.
여러분 내 말 뭔지 알겠어요? 여러분 그렇게 한다는 거지 근데 이거를 제가 그냥 얘도 애플이랑 오 이거 프레시안 리콜도 지금 방금 보내준 슬라이드에서는 그냥 데이터셋 와이드한 걸로 전체 데이터에 대한 걸로 해서 다 똑같은 걸로 해놨거든요.
제가 프리시전 리코이랑 에큐레이션이 같다고 여기 여기서는 이렇게 하면 그쵸?
그쵸? 그렇게 적혀 있다는 슬라이드에 여러분 원래 슬라이드는 아니면 중요하지 않고 이게 중요한 거예요.
실제로는 에트로시는 데이터셋 전체에 대해서 하고 플래시지랑 리콜은 데이터셋 전체 하는 게 아니라 알겠죠 이거 사실 인터넷 찾아보면은 트랙이 나오는 게 하도 많아가지고 내가 사 피트티 물어봐가지고 피티트가 맞는 것 같아요.

참석자 1 17:11
이렇게 얘기해 주더라고 왜 이거 이렇게 다르냐 그랬더니 사실 이게 합리적이다.
이게 공부 잘한 것 같아 돈 내고 사면 더 괜찮더라고요.
돈 내고 뭔가 좀 사람 피드백이 들어갔나 튼 그런 게 있어요.
여러분 그래서 이게 맞는 말이야 내 말이 이해하는 이게 맞는 것 같아요.
플래스 별로 하는 게 맞아요. 얘는 아무 의미가 없거든 이렇게 했으면 무슨 의미가 있어요?
그쵸 그래요. 여러분 그래서 이제 진짜로 그 채팅 피트맵도 얘기해 주는데 나 논문 안 찾아봤는데 사이트도 다 있어 그래가지고 아는 말지 이런 거는 이게 애플이랑 오렌지랑 망고랑 세개 구분하려고 그래 애플이랑 망고를 구분하는 애플을 잘 구분하는 게 되게 중요하다.
그럼 얘의 프리시전 이런 걸 높여야 되고 여기 지금 망고는 별로 잘못 별로 상관없나 봐요.
여기 되게 나쁘잖아요. 아니 뭐 이래도 참는 게 애플 애플이라고 하는 게 되게 중요하다는 거지.

참석자 1 17:57
여기서는 그럴 때 이제 여기 이게 이렇게 이거 전부 다 세 개를 다 보는 게 아니라 이제 특별히 중요한 클래스에 대해서 좀 집중한다 이런 거지.
사실 얘도 이제 바이너리 이거 컴퓨존 매트리스는 전부 다 뭔지 설명도 안 했는데 설명 안 했는데 좀 좋긴 한가 여러분 이거 첨 봐요.
머신러닝이랑 같이 듣는 사람은 처음 보지 그치 그냥 공부하셔 이 사람들은 할 수 있어요.
이 별로 어렵지 않아요. 알겠죠 하세요. 복습 학과요 그거 그래요.
그다음에 이거 이것만 강의하면 진짜 또 1시간을 써요.
내가 내가 재미있거든 이게 여지 그래요. 그래서 그냥 갑니다.
저기 1학년과 거기서 열심히 설명해야지 그다음에 그다음에 여기 요거 지금 저 그래서 지금 끝났잖아요.
저거 저거 이 슬라이드는 보세요. 강의 자료가 지금 3개 올라와 있는데 이거 지금 요거 요거 요거 방금 끝났어요.

참석자 1 19:00
이거 원래 여기 뒷부분이 여러분 사실은 사실 더 있죠 막 제가 원래 처음에 올린 버전에서는 베리언스 바이어스 트 트리더투 이런 거 다 있죠 그죠?
제가 그걸 여기다가 그냥 다 넣으려고 그러다가 그냥 이거 만들면서 여기서 빼버렸어요.
이쪽으로 그냥 그랬어요. 알겠죠 그래서 여러분 공부할 때 사실 막 승마가 바뀌면 되게 짜증 나잖아요.
미안해요. 미안한데 도저히 참을 수가 없어가지고 왜냐하면 이거 이거 만들었는데 또 아까워서 그냥 했어요.
여러분 어차피 이거 안 줄 수도 있는 건데 준다고 생각하시고 여러분 알아서 하셔 알겠죠 안 줄 수도 있는데 준다고 생각하세요.
필기로 다 끝낼 수도 있어. 사실 근데 주는 거라고 생각하시고 용서해 줘요.
여러분 그래요. 그다음에 가면 모든 학생을 만족시킬 수는 없죠.

참석자 1 19:46
어떤 애는 어떤 옛날의 강의 평가에는 슬라이드를 하나에서 계속 처음부터 쭉같이 체크하면 좋겠는데 자꾸 앞에서 또 이거 나왔다고 설명하고 다른 슬라이드 왔다 갔다 하고 하는 것 때문에 짜증 나 죽겠다고 적어놨더라고요.
난 상관 안 개의치 않아서 그렇게 하겠어 이게 왜냐하면 그렇게 설명 안 하면 복습이 잘 안 된다고 생각해서 그러니까 사람마다 다르다는 목소리 큰 사람만은 무조건 듣는 건 아니라서 그게 이제 딥러닝에서 나와 나도 학습을 하는데 학생들은 그때 그걸 갖고 이게 아웃라이어거든 아웃라이어 아웃라이어가 뭐냐면은 사실은 정규 분포에서 벗어난 건데 그 사람 목소리가 크다고 그 사람 말 들으면 곤란한 거거든요.
우리나라 정책이라 하면 구급자가 사실은 좀 무섭고 대부분 사람은 그렇게 생각 안 하는데 목소리가 너무 커 또 그렇죠 알지 못했던 그지 무조건 사람을 막 죽이려고 그러면 안 되잖아요.

참석자 1 20:31
그렇죠 죽은 거라서 선을 넘었지. 아니 그러니까 그렇다고 해서 그 사람 말을 다 들어줘야 되냐 그런 건 아니잖아요.
미안 그 사람이 그 친구가 그런 건 아닌데 어쨌든 그래요.
어쨌든 여기 이거 트레이닝 들어가서 할게요. 여러분 트레이닝 베이리스 이거는 훈련에 대한 건 제가 따로 뺐어요.
아까 사실 퍼포먼스 매트리스도 사실 훈련에 대한 내용이지만 워낙에 기본적인 거니까 베이지에 넣어놨고요.
여기서부터 약간 좀 트레이닝만 모아놨어요. 트레이닝 할 때 이거부터 하는 게 좋을 것 같아서 헬로우 월드라는 말 쓰잖아요.
항상 여러분 신하고 파이썬이나 맨 처음에 그것처럼 이쪽에서는 이제 항상 근본이 데이터 셋이 넷리스트 데이터베이스라는 거예요.
그냥 이거는 이 마디파이드 내셔널리 재밌잖아요.
그렇죠 이름이 이런 거예요. 완전히 그냥 무슨 기관 이름이에요?
그쵸? 표준 표준 기관이야 표준 이거 그리고 재밌는 게 미국은 자기 말 안 붙여 우리는 맨날 뭐 만들 때 카이스트도 코리아 겹치잖아.

참석자 1 21:37
MIT도 없었으니까 거기는 대전이라고 붙이는 게 좋았을 텐데 어쨌든 여기는 미국이라는 말 붙이지도 않아 us라는 말 붙이지도 않아 있죠.
미국 사람들은 그냥 세계 대표 거라서 그 거기 여러분 여기 있죠 안cc라는 것도 있잖아요.
여러분들 아직 들어가 모르겠다. c 언어도 사실 여러분 그냥 내셔널 스탠다드 아니 그러니까 국가 표준을 잃어버린다고 우리도 내셔널 스탠 리치 이래버리 가 이제 국가로 끝내버려 그렇게 버려 재밌잖아.
어쨌든 앤 리스트는 별게 아니라 마디파이드 왜냐하면 이 플레임은 전혀 외울 필요 없어요.
여러분 그쵸 근데 외우지 말라고 그러면 또 외우지 어쨌든 그쵸 알겠죠?
엔리스트가 그런 거였고요. 거기서 여러 개의 데이터베이스를 만드는데 핸드레이트 비지트를 수집했어요.
일단 우체국에서 우체국에서 이제 우편 배달할 때 우편번호 잘 구별하는 게 되게 좋잖아요.
우편 번호 갖고 구분하면 좋잖아요. 싸움할 때를 그래서 사람들이 쓴 글자들을 모은 거예요.

참석자 1 22:34
숫자 숫자만 그래서 7만 개나 있어요. 7만 개나 모아놨어요.
그래서 데이터를 이렇게 이미지로 다 모아놓은 거니까 굉장히 좋지.
그리고 패션 리스트라는 거는 이게 엔리스트 하도 유명하니까 사실 이거랑 전혀 상관없는데 그냥 옷만 이렇게 모아놓은 거 알겠죠 여기도 똑같이 10가지 여기까지 물어주세요.
그리고 슈퍼바이즈드 러닝에서 이제 데이터셋이 되게 중요하잖아요.
그렇죠 여러분 데이터 집합이죠. 그쵸 아까 계속 강조하지만 여기도 계속 보면은 이제 정답 이게 이게 이게 데이터지 데이터 이게 데이터셋이야 그쵸 입력 데이터만 있는 것뿐만 아니라 정답도 있어야 돼.
그쵸 이게 제가 여기 이거 여기 이걸 여기다 적고 이걸 여기다 적었어요.
제가 일부러 그쵸 정답은 사실 이쯤에 들어가거든 그쵸 손실 함수에만 들어가잖아.
그쵸 계산할 된다. 알겠지 그래요.

참석자 1 23:32
어쨌든 제가 강조하고 싶은 거는 내가 그림을 이렇게 그렸지만 밑에서는 이거 말고 밑에서는 이렇게 그렸지만 이것도 안 그래 그랬지만 얘랑 얘랑 합쳐서 데이터인 거야.
그렇죠 이해되죠 슈퍼 바지죠 이서 슈퍼 바지 지도 학습에서는 지도해야 되니까 정답 값을 알려줘야 되는 지도 안 하면 필요 없지 그쵸 그러니까 어느 슈퍼바이즈 러닝에서는 그냥 데이터만 있어도 돼.
그쵸 근데 슈퍼바이즈 러닝에서는 라벨을 붙여야 돼.
그쵸 전에 얘기했나 알바 있다고 알바 있었다고 옛날에 이미지 전파운데 는 그런 안 했나 이거 안 했어요?
안 했어 옛날에 바운드 박스 치고 그거 못하고 표시하는 알바가 있었어요.
한 번 하면 80원 한 장에 하나 표시하는 80원 받았거든요.
되게 한때 한 여러분 한 10년 전에 되게 유행했었어요.
진짜 2005년 2015년쯤이잖아. 그때 엄청나게 데이터 모으느라고 많이 했었거든요.

참석자 1 24:34
지금 기어가 없어졌지 지금 다 모였으니까 알겠죠 여러분 세만 이렇게 바운딩 박스 치고 이런 알바 있을 때 성공이 망가지는 알바지만 소스 받아 지 얼마나 벌어 가지겠어요 어쨌든 되게 성년 되면 되게 잘한다고 그러는 그런 옛날에 블로그에는 이거 잘하는 거 몇 하도 말아서 말하면서 어쨌든 중요한 건 이제 지금 내가 말하고 싶으면은 옛날에 기본적으로 초보드 러닝이 성행이 기본이고 여기 데이터셋이 인슈파이 저닝이랑 또 썼네.
오픈하는 슬라이드가 데이터 셋 한 다음에 인슈파이 저닝이라고 살라고 그러다가 군대 다 버렸네.
무슨 얘기인지 알겠죠 여러분 SRS 주스 고 그리고 여기서 기본적으로 이제 트레이닝 셋이랑 테스트 셋이 있어요.
여러분 항상 보통 데이터 나눌 때 그러니까 이게 트레이닝이랑 테스트를 나눠야 되는 게 데이터 셋이 있다고 전부 다 트레이닝 써버리면은 얘가 진짜로 좋은지 알 수가 없잖아요.

참석자 1 25:29
그래서 항상 테스트 셋을 따로 빼놔야 돼. 그러니까 지금 여러분 용어 제가 전에 했었죠.
여기 이거 내가 트레이닝 적어놨잖아요. 그쵸 트레이닝 말고 뭐가 있다고요?
테스팅이 있잖아 그쵸 테스팅을 실제로 해봐야지.
이게 성능이 좋은지 안 좋은지 알 수 있을 거 아니에요 프로세티브 그래서 다 데이터셋을 다 트리니 쓰는 게 아니라 보통 테스트 셋트 따로 빼놔야 된다.
퍼머 프렉티스는 보통 80%를 트레이닝 하는 데 쓰고 20%를 테스팅하는 데 써요.
보통 그냥 보통 이라는 거지 항상 이렇다는 건 아니고 그래요.
왜냐면은 데이터가 많아야지 뭔가 훈련이 잘 되니까 그래요.
그래서 이제 다시 또 똑같은 말인데 데이터는 트레이닝 데이터 테스트 데이터다 알겠죠.
너무 중요하니까 여러 번 강조했어요. 그리고 또 약간 더 얘기하면은 트레이닝 데이터의 사실은 보통 트레이닝을 할 때도 사실 문제가 이게 트레이닝을 어디까지 해야 되는지 문제가 좀 있잖아요.

참석자 1 26:22
그쵸 트레이닝이 내가 하고 있는 거 여기 지금 보면은 손실 함수를 줄여다가 손실 함수에서 손실이 줄어들게끔 얘를 업데이트할 거 아니에요 그쵸 근데 이거 업데이트하는데 원래 이제 지금 입력 데이터로 준 거 있잖아요.
이게 트레이닝 데이터잖아요. 트레이닝 데이터에 대해서 오스트 값을 최소화시키게만 훈련을 시키면 그러면 이제 무슨 문제가 있냐면은 너무 트레인 데이터에만 잘하는 거야.
그 시험 문제만 잘 푸는 거지 실제로 안 본 데이터에 대해서 사람을 봐야 될 거 아니에요 그래서 그걸 테스트 데이터에 해놨잖아요.
테스트 데이터 뽑아놨는데 그렇게 해놓으니까 다 너무 이제 훈련이 끝까지 이거 이거 만약에 현실 함수가 0이 나오게끔 결과가 그러니까 우리 에러가 전연 0이 되게 해버리면은 완전히 그냥 그 문제만 잘 풀고 이상하게 학습이 되는 거야.

참석자 1 27:14
문제는 또 여러분 교과서에도 되게 좋은데 이거 이 교과서가 부거운데 뭐가 있냐면은 숫자 있잖아요.
레이블 그러니까 여기 예를 들면 여러 거 이런 거 있잖아요.
여러분 요거 이런 거 보면은 이게 이거 4지 4 4 4인 거 보이나요?
여러분 산인데 라벨이 9 붙어 있는 것도 있고 막 그래요.
그리고 진짜 이상한 거 있어요. 9라고 하는 건 여러분 들어볼 수도 있는 것 같은데 여기 명확하게 6인데 실수를 이거를 7 이렇게 적어놓은 사람도 있다니까 라벨을 잘 없을 때고 사람이 실수할 수 있잖아요.
그렇게 근데 훈련을 여러분이 뭐냐면 시험 문제랑 교과서에 틀린 거 있는데 그냥 이것만 듣다 외우는 거 이거 공부하면 안 되잖아.
실제로 문제는 데이터는 항상 틀렸다니까 여러분 교과서가 항상 틀렸어.
교과서 여기서 지금 여기는 번역 잘못된 것도 많고 어쨌든 항상 그렇잖아요.
여러분 그러니까 여러분이 항상 뭐냐면은 너무 과대 적합이라고 그래요.

참석자 1 28:03
과대 접합 트레이닝 데이터에 대해서 과대 적합자라는 게 좀 곤란해서 사람들이 요즘에는 또 어떻게 하고 있냐면은 3M 데이터를 또 쪼개가지고 2로 트레이닝 데이터랑 밸리데이션 데이터를 나눠놔요.
밸리데이션 밸리데이션 데이터는 뭐 하는 거냐면은 이게 이제 홀드 아웃 밸리데이션 따로 그지 홀드아웃이 따로 빼놨다는 뜻이잖아요.
따로 빼놔서 이 이 홀드아웃 셋이라는 게 밸리데이션 셋인데 또는 디벨로먼트 셋이라고 그러는데 내부 셋이라고 그래요.
개발하기 위해서 쓰는 거 이거는 훈련을 어디까지 할지를 보기 위해서 실제로 나중에 이제 로스 값을 트레이닝 데이터에 대해서 계속 오차 값을 계산해서 줄이는 걸 하지만 실제로 훈련을 어디까지 할지는 밸리데이션 데이터 갖고 얘가 지금 이 정도 잘하는 정도다를 판단한다는 거죠.

참석자 1 28:53
공부 어디까지 해야 돼 하산 라인 그런 거 있잖아 하산하는 거 원래 이제 공부하던 그거 말고 안 보던 문제 갖고 계속 한 번도 했던 공부하지 않았던 거 정답 안 알려주는 거지 걔 갖고 너가 지금 이 정도 하고 있구나 이렇게 한다는 거죠.
그리고 나서 진짜 테스트해 갖고 또 해보고 이해되죠.
여러분 밸리데이션 데이터도 문제 밸리데이션 데이터도 이것도 문제가 나중에 또 밸리데이션 데이터가 잘하면 또 그 3년을 멈추기 때문에 또 밸리데이션 데이터에 바이어스 되는 경향도 좀 있어요.
그래요. 여러분 됐죠 그리고 참고로 얘기하면은 밸리데이션이라는 말은 우리나라 말로 아니 검증이 우리나라만의 곡이에요.
그분 밸리데이션이 맞아요 아니잖아요 그쵸 검증이 우리나라 별로 좋은데 베리피케이션이야 그쵸 소프트웨어 공학 이어 그렇습니다.
그래가지고 이 베리피케이션 데이터라고 해야 돼요.

참석자 1 29:52
사실은 그렇게 안 보이고 밸리데이션 밸리데이션은 뭔가 뭔가 밸리데이션은 유효화 이런 뜻이잖아요.
유효하게 만들기 이런 뜻이잖아요. 아니면 또 확인하는 거 확인 확인하는 거잖아.
밸리데 항상 확인하는 거지 괜찮게 지금 훈련하고 있나 근데 그래서 밸리케이션은 사실은 맨날 테스트랑 비슷한 거예요.
마지막에 괜찮냐고 뭔가 이 검증하는 거거든. 어쨌든 검증이라는 느낌이 들어서 검증이라고 번역을 해놨는데 우리나라에서는 모든 책이 다 검증이라고 해요.
근데 영어랑은 안 맞다고 그냥 알아두라고 내가 여러분 항상 그렇게 생각을 하는 게 좋잖아요.
그쵸 이거 검증이라고 공부했는데 검증은 영어 베리피케이션이에요.
진짜로 진짜로 그렇잖아. 밸리데이션은 뭔가 확인하는 거야 이게 제대로 됐는지 그쵸 검증이라는 밸리데이션이랑 밸리핏이 좀 다르거든요.

참석자 1 30:43
사실은 근데 사실은 데리피케이션 하는 느낌도 좀 있어가지고 뭔가 약간 좀 애매하긴 해요.
그쵸 어느 게 더 맞다고 여러분 어쨌든 다르다는 것만 알고 계세요.
일단은 알겠죠 그래요. 그다음에 그래서 이제 또 케이폴드 프로스 밸리데이션이라는 것도 있는데 이거는 교과서에 나중에 나올 거예요.
이게 하도 이제 또 이 밸리데이션도 이제 아까 밸리데이 데이터에 너무 적합 되는 게 별로라고 그랬잖아요.
이해되죠? 여러분 무슨 말인지 밸리데이션 데이터 그것만 또 이렇게 넘어색했던 게 별로니까 밸리데이션 데이터를 여러 개 또 만들어 여러 개 만들어서 아예 훈련을 다 다르게 해.
여러 개 밸리린 데이터랑 테스트 트레이닝 데이터 나눠 프로그램 이거 나누는 방법을 여러 개 해가지고 테이번 해가지고 1번 한 것 중에서 실제로 제일 잘 되는 여기에 이제 밸리데이션 데이터들이나 검증 점수가 나올 거 아니에요 아까 RMSD 같은 거 그거를 평균 내서 그냥 괜찮은지 보면서 계속 수정한다는 거죠.

참석자 1 31:39
특정 밸리데이션 데이터에 대해서 너무 막 바이러스 데이터가 밸리데이션 데이터가 여러분 틀렸어 가 또 문제가 그런 거잖아요.
아까 테스트 데이터가 틀린 문제가 아니라 이번엔 밸리에 데이터가 틀릴 수도 있잖아.
또 이상한 선생님이 와가지고 그러니까 이게 되게 사실 실제로도 그래요.
그러면 한 사람만 여러분 리뷰하는 거 별로잖아 우리 안 그래요 여러분 이 사람이 틀릴 수도 있잖아.
그래요. 머신러닝이 되게 인간 세계랑 비슷해요.
사고도 많이 하고 그리고 되게 고차원적인 일을 하는데 믿을 수 없어 항상 그렇다고 해요.
알겠죠 여러분 그래서 이 짓을 하고 있어요. 알겠죠 그래요.
그래서 트레이닝 타임은 그래서 이제 전부 다 케이번이나 다 나눠서 해야 되니까 케이별로 늘어나는 거지만 데이터가 별로 없으면 이런 짓을 하고 있어요.
알겠죠 교과서에도 있어요. 그림이 있는데 그냥 안 갔다 놓고 그래요.

참석자 1 32:28
그래서 그다음에 이제 이런 게 트레이닝 셋 에러는 굉장히 작은데 텍스트 셋 에러가 높다 이거 이거 왜냐하면 이거 기출 문제 잘 풀었는데 맨날 시험을 못 봐 이런 경우 있잖아요.
그렇죠 어떻게 된 거냐 이거는 보통 오 피팅 오 피팅이 우리나라 말로 봐 대적합이라고 그러거든요.
이거 계속 나올 거예요. 여러분 알티스 쓰는데 한두 번 나오는 게 아니라 과대 적합이라고 불러요.
과대 적합 적합 과대 적합 obt OB이라가 없어요.

참석자 1 33:04
그리고 이제 제너럴라이제이션 에러 아웃오브 샘플 에러 이런 걸 쓰는데 제노라이제이션 일반화가 잘 안 된 거지.
그러니까 오브 피팅이랑 제너널라이제이션은 반대 말이에요.
오브 피팅이 되는 것은 반대 말이에요. 제너레이션이 잘 되면 오브 피팅이 안 일어나는 거고 오브 피팅이 잘못 오피팅이면은 제너레이션이 아닌 건데 막 섞어 쓰고 있어 반대로 그렇죠 이해돼요.
여러분 반대 말이야 오피팅이랑 제너레이션 반댓말이라고 일반적인 걸 잘 잘해야 되는데 이 기출 문제만 잘한다고 기출 문제 같은 경우는 드레인 데이터가 알겠죠 그래요.
그리고 마지막에 아우드브 스템프라라고 적혀 있잖아요.
아웃오브 샘플 아웃오브 샘플은 뭔가 원래 이상한 문제에 대해서 뭔가 잘 풀었다는 거지.
기출 문제 중에서 좀 이상한 문제 있잖아요.

참석자 1 33:49
여러분 정말 더러운 문제 요즘 좋은 게 인간이 많아졌는데 인간에서 계속 제가 웃긴 거라고 아까 애들이 보여서 봤는데 이 문제 이렇게 더러워 이제 이러면서 정승재 그 사람 막 그런 거 봤는데 진짜 그런 게 필요해요.
여러분 이 문제를 풀지 않는 게 나은 게 있어요. 여러분 문제가 너무 지저분하거나 정답이 그러니까 괜히 그걸 안 공부하는 게 나은 게 있어요.
일반화적인 공부를 하기 위해서 그런 거죠. 아도브 샘플이라는 게 그런 거는요.
뭐냐면은 진짜로 일반적이지 않은 그런 샘플 가지고 공부를 해가지고 그 문제만 잘 풀어 실제로는 시험에도 나오지도 않는 문제들 실제 데이터는 그런 거 없다는 거지 여기 있다.
잠깐만

참석자 1 34:37
여기를 안 갖다 놨나 보네. 교과서에 있는데

참석자 1 35:00
이거 말고 이것도 해야겠다.

참석자 1 35:12
뭐냐면은 전에 이거 아무거나 보여주고 싶은데 내가 준비 안 하고 여러분이

참석자 1 35:23
이거 봅시다. 여러분 여기 일반적으로 이런 거 아까 내 그림에도 있기 때문에 이런 거 대충 하면 되는데 진짜 잠깐만요.
미안해요. 여기 가서 할게요. 여기 원래 이윤 리그레이션 할 때 여기 요거 하면 이걸 하면 여기 만약에 여기서 갑자기 샘플 중에 이 데이터가 지금 몇 개 있어요?
여러분 데이터가 개수가 몇 개예요? 이거는 내가 이걸 설명했나 이거 설명했죠 했었어 슬라이드 새로 추가된 건데 그냥 이거 데이터가 몇 개 있는지 알아요.
여러분 여기 지금 이거 이 그림도 전혀 이해 못하나 딱 보면 이제 알아야 돼요.
여러분 이거 데이터가 몇 개인 거야? 9개 잘 썼어요.
그쵸? 여기에 이런 데이터가 있는 거 없잖아 여기 갑자기 이런 데이터가 있는 거예요.
이거에 대해서 선을 해결하려고 해봐요.

참석자 1 36:19
이상해질 거 아니야 안 되는 거야 이 이건 무시해야 되는 거지 그쵸 이상한 거는 제껴야 된다고 그렇죠 잘못된 거지 뭔가 다른 이유가 있는 거예요.
여기서 그냥 형 말고 뭔가 다른 사유가 있는 건데 데이터에 들어왔잖아요.
일반적이지 않은 거잖아. 제너레이즈는 에러 아웃오브 샘플 에러 이렇게 부르는 게 이해가 됐지 오 피팅 에요.
이거를 하려고 여러분 서정 일기를 안 쓰고 머신러닝을 썼어요.
그래서 머신러닝을 썼더니 이거를 위해서 이렇게 만들었습니다.
이렇게 만들었어요. 그러면 여러분 별로잖아. 이게 이게 낫지 이해돼요.
여러분 그거예요. 알겠죠? 됐지 여러분 무슨 얘기인지 그러니까 여러분 이상한 교수를 만나서 그 교수 상태로 시각화되면은 안 되는 거야.
그 교수가 이상한 소리하는 건 제껴야 돼. 나 몰라 알겠죠 무슨 얘기인지 알겠죠 여러분 일반적으로 잘해야 된다고 알겠죠.
가끔 저도 헛소리하니까 여러분 알아서 벌어들이시고 알겠죠.
진짜로 일반적인 걸 잘해야 돼요.

참석자 1 37:16
그렇죠 이상하게 뭔가 사람들이 가끔 그러니까 이제 밸리데이션 에서 그렇게 하는 거죠.
밸리데이션도 여러 가지 하는 거죠. 한 사람 말을 믿으면 안 돼요.
여러분 누구를 마음대로 실천을 신고하면 절대로 안 된다는 거죠.
그래요 네 그래요. 옛날에 트레이닝 하고 있죠 뭐 하는 거예요?
그래서 트레이닝 이렇게 해서 가면은 여기 그래서 여기 여기까지 이제 트레이닝 기본적인 내용이고 여기는 이제 진짜 약간 여기 이거는 사실 앞에 내용에 있었던 내용인데 제가 여기 슬라이드 따로 만들려다가 하다가 여기 냅뒀는데 체인이 쪽이니까 이거 원래 이거는 앞에 나왔던 내용이죠.
여러분 이거 원래 리뉴얼 리그렉션에서 리뉴얼 리그렉션에서 그리고 심플 유니베이션을 한 거는 이제 값이 하나인 경우 내가 왜냐면은 입력 값이 하나인 거예요.
여러분 유니 베넷이라는 거는 입력 값이 하나 입력 값이 여러 개일 수도 있잖아요.

참석자 1 38:19
여러분 입력하신 키다 아까 얘기한 청수 같이 하나인 경우 그럴 경우에 어떻게 운영하는지 먼저 일단 얘기해 주는 거예요.
그냥 가져주라고 하고 이것도 아마 거기서 할 거야.
또 머신러닝에서 할 거예요. 그런데 머신러닝에서 똑같은 방법이 있는데 그래도 이거 보려고 그래도 이해가 안 돼서 이거는 좀 너무 중요해서 하려고 해요.
강의를 그래요. 어쨌든 이게 포스트 펑션이라는 것도 여러분 다 제가 봤으니까 알잖아.
포스트 펑션이 손실 함수랑 똑같은 말이죠. 그쵸 그리고 뭘 쓴다고요?
민스퀘어 에러시죠? 그쵸 RMC에 쓸 수도 있고 ms를 쓸 수도 있지 그냥 ST를 쓸 수도 있어 이해돼요.
여러분 여기 봐봐요. 이게 지금 여기 이 그림은 지금 RMST 이해 안 했어 그 안 씌웠거든 이해돼요.
여러분 그리고 여기 이거 2n 분의 1 있잖아요. 이거 안 나눠도 되잖아.

참석자 1 39:14
평균 한다고 m으로 나눴는데 이까지 붙인 거는 여러분 제곱이라고도 이까지 붙여놨는데 그런 거 필요 없어 없어.
이거 없어도 상관없잖아요. 전혀 EN 분의 1로 나눈 거 말고는 상관없잖아요.
그쵸 실제 로스 줄이는 데는 그래서 어쨌든 여기는 민 스퀘어 에러라고 적혀 있는데 어쨌든 예측 여기 보면은 여기 예측값을 여기 보면 이게 이게 지금 i는 1부터 m까지 적혀 있잖아요.
그쵸 이게 MB 데이터를 의미하는 거예요. 아까 그 하우징 있죠 빅 디플에서 몇 개의 m이 m이 얼마예요 아까 여러분 했잖아 x 표시 몇 개야 9개 그러니까 첫 번째 데이터부터 아홉 번째 데이터까지 전부 오차를 이게 이제 예측 값이고 이게 진짜 값인 거예요.
예측 값을 요거 x 축이 아까 평수 같은 게 XI겠지 샘플 표시할 때 이렇게 아 이렇게 위에 많이 붙여요.

참석자 1 40:15
여러분 표현하려고 그렇죠 이해되죠 그리고이게 4가 씌우면 이제 뭔가 예측했다는 느낌이 드니까 예측 값으로 이렇게 제가 적어놨죠.
실제 값 실제 값이 정답 값 알겠죠? 여러분 그래가지고 이렇게 해놓은 거야 이해되죠?
여러분 그리고 이제 이거 예측 값은 사실은 이 함수 XI에 대해서 뭔가 예측 함수를 통과시킨 거니까 이렇게 쓴다는 거지 그쵸 YI를 이렇게 예측 값을 이렇게 표시한 거지 예측 값이 예측 값을 이렇게 표시한 거 이해되죠?
여러분 이해되지 그래요. 그냥 산수예요. 산수 알겠죠?
수학인가 그래요. 그렇죠 됐어요. 그래요.

참석자 1 40:58
그리고 여기 지금 하이퍼스 펑션이 이게 이거 미리 이거 아까 하이퍼서3라고 h 붙였는데 세타에 대해서 약간 의미를 해 주면은 세타가 보통 파라미터를 여러분 x로 볼 수도 있지만 이게 지금 데이터가 x로 들어오기 때문에 우리가 우리가 학습시켜야 되는 파라미터는 뭔가 다른 알파벳 말고 다른 거 쓰는 게 좋아서 세타를 많이 써요.
사람들이 진짜 여러분 알파고 논문 봐도 세터 나와요.
거기도 뉴럴 네트워크를 그냥 f의 세타라고 나온다고 학습시켜야 될 프라미스 세타가 아주 이런 게 여러분 뭐냐면은 어떤 이제 업계에 들어왔죠 중간중간 업계에 들어오면 이제 세탈은 전부 다 파라미터야겠죠.
모든 논문에서도 세타를 써야지 이거를 다른 이상한 알파벳 변수를 쓴다 그러면 이상한 놈인 거야 알겠죠?
세터를 써요. 보통 알겠죠 그래요. 그리고 리니어 리그레션에서는 이렇게 세타 제로와 세타 원이 있는 거지 그러니까 웨이트가 2개인 거죠.
이해되죠? 여러분 하나는 세타 제로가 바이어스 세타 원이 뭐예요?

참석자 1 41:57
원래 입력 들어온 거에다 곱하는 거 기울기지 기울기 페타 제로가 이제 0에서의 바이러스 값인 거 알겠죠?
x가 0일 때 이해되죠 그래요. 그리고 사실 이게 이게 무조건 1에다가 하고 세타 제로 곱한 걸로 볼 수도 있지.
1에다가 세타 제로 곱하고 세타 1에다가 x 곱하고 그쵸 이런 식으로 입력 값에서 입력값 중에 1도 있다고 보고 1은 세타대로 곱하고 x에다가 세타 원 곱하고 이런 식으로 그쵸 그렇게 볼 수도 있지.
어쨌든 그러면 이제 이게 선형 회기 선형 대수는 비슷해 선형 대수는 되잖아.
선형 대수 매트리스 현상 두 개 세타 매트리스랑 엑스 매트리스인데 일하고 x 있는 거 그 말로 떼면 뒤에 나올 거예요.
또 얘기를 하면은 그다음에 중요한 건 이제 뭐냐면은 얘랑 세트대로 세타 원을 우리가 학습을 시켜야 되잖아요.
그렇죠 학습을 시키는데 이 오차 값이 이게 최소화 되게 학습시켜야 되잖아요.
그쵸 근데 이것도 당연한데 결국은 얘가 이게 오차가 어쨌든 0이 되게 만들어야 돼.

참석자 1 43:03
그렇치 최소화되게 최소화되게 만드는 건 원래 여러분 맨날 최소한 변화율을 계속 보는 거잖아요.
변화율을 변화율이 오차의 변화율이 결국은 계속 오차를 줄이도록 우리가 이 세타 값을 변화시켜야 되잖아요.
각 세타에 대해서 각 세타에 대해서 세타의 변화 세터를 변화시키는 니까 세타의 변화율이 필요하잖아요.
이게 세타 이게 이게 느니 어사인먼트고요. 여러분 이게 세타 제가 얘 다음 세타 값이죠.
섹터 j가 섹터 j가 이 j가 i랑 0이 있어요. 이해되죠 여러분 여기서는 이해돼요.
여러분 이해돼 아까 바이러스랑 각각 곱하는 거야.
그게 다음 세타 값이랑 지금 세타 값이 있는 거잖아.
그래서 이거 이렇게 사실 식이 사실 세타 제로 델타 세타 j 는 하고 이거 이렇게 있는 거랑 똑같은 거 아니래요 너무 대충 설명하나 다시 할게요.
여러분 여러분 이거 산수가 아니라 수확이지 수확이지 여기 보면은 이거 있잖아.
요거 요거 요거 요거 왼쪽 오른쪽 거를 자변으로 넘기면은 세터 제만이 세터 j잖아.

참석자 1 44:12
근데 여러분 이거 우리가 알지만 산수 원래 수학에서 수학 2%로 우리가 프로그래밍에서 어사이먼트 할 때 왼쪽 값은 나중 값이고 오른쪽 값은 먼저 있는 값이잖아요.
사실 왼쪽 세트짜리는 미래 값이고 변화 값이고 저 세타 제 오른쪽 세터 j는 지금 현재 값이잖아.
저거를 왼쪽으로 옮기면 세타 j 미래 값 빼기 세타 제 현재 값이잖아요.
그래서 그게 변화율이라고 변화 값이잖아. 변화값 이해돼요 여러분 그래서 저거를 계속 어떻게 만들고 싶으냐는 오차가 최소가 되게 쟤도 변화시키고 싶은 거잖아.
우리가 얼마큼 변화시키냐의 문제인데 패터 제로 업데이트해야 되잖아.
우리가 그래서 원래 이상한 값이 있었는데 계속 원래 제대로 만들어서 로스를 잡게 만들어야 되잖아요.
그러기 위해서 원래 이제 이 오차 있죠.

참석자 1 44:56
오차 오차 이 오차 오차를 지금 제로 오차가 아니라 정확히 말하면 이제 퍼포먼스 te으로 MSD 같은 거지 mse가 이 섹터에 대해서 얼마큼 이 섹터 j에 대해서 얼마큼 변하느냐 변화율 변화율만큼 이제 변화시키려고 하는 거예요.
얘가 이만큼 세터가 이만큼 변하고 쟤도 이만큼 변하니까 쟤도 변화시키려고 한다고 근데 오차를 감소시키기 위해서 뭔가 원래 값에서 이제 빼는 식으로 감소를 시키는 거예요.
빼는 식으로 줄이려고 그런데 거기다가 그냥 안 하고 알파를 곱했죠.
또 알파를 그냥 오차만큼 빼버리면 너무 이상해질 수 있어서 약간 조금 조금씩 제가 담보면서 줄여나가는 건데 그 이유가 뭐냐면은 사실 이게 이 그림이 여기 오른쪽에 있는데 이게 세타 원에 대해서 지금 이게 x 축이 세타 세타 원이고 y축이 세타 세타 뭐예요?
j 값이에요. 제 값 오차 값인데 이게 여러분 보면 이거 보이지만 이렇게 돼 있는데 여기 이게 들어가잖아요.
그러면 이거 사실 2차 함수잖아요.

참석자 1 46:03
이제 제곱 하니까 그래서 세타 제로에 대해서 이게 2차 함수로 변하겠지 오차가 이렇게 생겼겠지 당연히 뭐야 이거 다 산수예요.
수확이야 수학 알겠죠. 그래서 이렇게 그린 거야.
그림이 실제로 이렇게 생겼어요. 이렇게 생겼다고 우리 차가 세타 원이 지금 초기 값이 여기 있었어.
만약에 그러면 여기로 와야 될 거 아니야 이래야지 오차가 최소화 될 거 아니에요 여기까지 보내 내기 위해서 얘는 조금 조금씩 뭔가 로 이동해야 되잖아.
여기 있었으면 일로 와야 되고 여기서 쓰면 만약에 여기서 으면은 이쪽으로 와야 되잖아.
여기까지 그래서 이렇게 와야 되는데 그래서 이제 여기서 그냥 이만큼 이렇게 이렇게 와야 되잖아요.
그쵸. 그래서 그거를 이제 얘가 변화율이 변화율 있죠.
변화율을 보고 변화율을 에다가 마이너스 변화율을 빼서 변화시키겠다는 거지.

참석자 1 46:52
그리고 이거 다 보면 여러분 다 맞는 말이고 어쨌든 그래서 지금 중요한 거는 그냥 또 하면 안 되고 이게 만약에 이것만 봐도 알 수 있는 게 여러분이 이 조금 조금 안 하고 만약에 크게 했으면은 맨 처음에 여기 있었는데 맨 처음에는 여기 있었는데 여기 있었다고 했냐 고는 맨 처음에 여기 있었어요.
만약에 여기 값이 그런데 일로 오른쪽으로 가야 되잖아요.
오른쪽으로 이만큼씩 가서 가야 되는데 여기까지.
그래서 나중에 이제 이제 진짜 얘가 아무리 변해도 거의 이제 변하지 않는다는 걸 느끼는 거지.
쟤가 별 변화율이 없으면 이제 여러분 사실 최소잖아.
그쵸 보통 여러분 손실 함수는 다 이렇게 미분 가능하고 변화율이 없는 지점이 여러분 어떻게 돼요?
변화율이 없잖아. 여기는 지금 제가 변화율이 없잖아.
얘 세타 원이도 변하지가 않잖아. 그게 최소라고 알겠어요.
근데 만약에 이렇게 만약에 그냥 오차 나오는 대로 이렇게 매니저로 확 넘겨 이쪽으로 가버리면은 계속 또 변화율이 크잖아.

참석자 1 47:46
또 너무 한꺼번에 많이 변화시키면 안 되는 거지 찔끔찔끔 해야 된다고 그래서 알파 값을 좀 작게 해서 곱하는 거를 변화율에 비례해서 하는데 찌끌찌끌 다는 게 좋다는 거죠.
너무 찔끈찔끓이면 오래 걸릴 거고 너무 이게 크면은 힘들 거고.
그쵸. 그래서 이거를 러닝 레이트라고 해서 학습률이라고 불러요.
여러분 그런 거죠. 내가 여러분 자전거 타는데 자꾸 오른쪽으로 넘어가서 왼쪽으로 왼쪽으로 해야 되는데 왼쪽으로 확 가버리면 여러분 왼쪽으로 넘어가겠죠.
그러면 오른쪽으로 하면 또 오른쪽으로 가면서 계속 막 미치겠는 거죠.
그렇죠 계속 어떻게 하라는 거야 됐죠 그쵸 이해되죠 여러분 그 러닝 레이트를 너무 피하면 안 되고 찔끔찔끔해야 돼.
그쵸. 오른쪽으로 왼쪽으로 넘어지길래 오른쪽으로 쭉 가라고 그랬더니 오른쪽으로 갔더니 좀 덜 넘어지네.

참석자 1 48:26
또 계속 오른쪽으로 가서 그렇죠 그러다가 이제 어느 순간 에러의 변화율이 없으면 이런 식으로 해야지.
그쵸. 저로 넘어가 버리면 곤란해. 그쵸 그래서 러닝 레이트를 적당히 해야 돼.
그렇죠 근데 이제 그런 것도 있지. 러닝 레이트 너무 작아 계속 왼쪽으로 넘어지면서 조금 나아지고 있는 것 같은데 들어서 어느 세월에 그렇죠 100년 걸릴지 모르겠어요.
이런 거죠. 그쵸 자전거 못 타는 거죠. 그쵸 이해되죠 여러분 그래요.
그렇게 이해하시고 그다음에 이걸 왜 했느냐면은 아까 이거는 심플 유니베이션라는데 이게 x 값이 이거 하나 진짜 하나로 생각하면 파란 건데 아까 총수 총수고 이건 뭐야 집값 이런 거였잖아요.
숫자나 스칼라 근데 사실은 입력 값이 하나가 아니라 되게 많을 수 있잖아요.
그쵸. 그러니까 집값을 할 때 사실은 평수만 넣는 것보다 뭐가 있는 게 좋냐면은 범죄율 편의시설 비율 이런 거 되게 있으면 중요하잖아요.

참석자 1 49:21
사실은 그렇죠 실제로 되게 온갖 파라미터를 막 저는 보스턴 집값 예측이라는 게 굉장히 유명한 또 문제인데 에니스처럼 걔는 13개가 있어요.
여러분 피처가 되게 은근히 잘 맞아요. 옛날 거지만 이해됐죠 여러분 지금도 우리나라도 뭔가 할 때 학군 이런 거 있잖아 넣으면 비율 그런 거 넣으면 범죄율이 있으면 확 내려가버리고 그런 거 있잖아.
그쵸 알겠죠 그래서 이게 여러 개가 있는 게 정상이에요.
보통 많아요. 알겠죠 그리고 아까 앱 리스트 있잖아요.
엠 리스트 걔도 사실 얘가 달라지는 거 이것도 피처가 이 입력 값이 그림인데 그림이잖아요.
그림 픽셀로 이게 흰색은 0이고 검은색은 1인 거예요.
255 또는 50부터 2505까지면은 근데 그런 개가 몇 개 있냐면은 가로 28개 세로 28개 있는 점들로 이루어진 거예요.
28 곱하기 28개의 데이터가 있는 거예요. 이해돼요.
여러분 이 피처가 2828 30도 되는 이 사람이 있는데 그 28,198 알겠죠?

참석자 1 50:21
엄청나게 많은 피가 그래서 이렇게 일반적으로 만들 수가 있고 그쵸 예측 값은 그래서 각각의 피처에 대해서가 다 뭔가 세터를 곱하는 거라고 맨 앞에 1로 x 제로가 있는 걸로 보고 x 1부터 시작하지만 데이터 처가 입력 피처가 x n개가 있으면 이건 여러분 데이터랑 여러분 헷갈리는 게 조심해야 되는 거야.
그 데이터의 개수는 여기랑 상관없어요. 그쵸 지금 입력 필처의 개수라고 보스턴의 집값의 특성 알겠어요 여러분 거기다 그거만큼 더 세타가 존재한다고 파라미터가 각각에 대해서 얼마큼 곱해야지 뭔가 되는지 보는 거야.
집값이 만약 집값 같은 경우에는 아까 보스턴 같은 경우에는 범죄율에 대해서 엄청 마이너스를 곱해야 될 것이고 평수에 대해서는 플러스를 곱하라도 얼마큼 곱하면 적당히 나온다는 거죠.
손용액으로 잘 되거든요. 이제 이해가 이해돼요.
여러분 그러다가 사실 선행 얘기도 안 되는 구간이 발생하는데 그걸 이제 딥러닝하면 너무 잘 되지 이렇게.

참석자 1 51:17
근데 집값이 너무 범죄율이 또 너무 막 이제 어느 정도 이상 된다고 더 집값이 좋아지지는 않거든 뭔가 다 항상 이렇게 뭔가 세츄레이트 하는 구간이 있거든요.
그래서 딥러닝이 더 잘 돼 대충은 맞고 딥러닝 이런 건데 우리 3분이구나 10분 쉬었다가 11시 3분에 합시다.
여러분

참석자 1 51:49
지각 회사 있어요. 지각

참석자 2 52:04
지각 지각이네.

참석자 1 52:27
이게 2시간짜리 수업에서는 그냥 죽어버리기 싫어서 이때 나타난 친구들은 그냥 출석을 해 주고 있어요.

참석자 3 52:39
그 세타 하나마다 그러면 특성이라고 생각하면 되는 건가요?
여기서

참석자 1 52:44
세트 하나마다 특성 x 하나마다 특성이지 x가 x 1부터 세타는 파라미터 특성에다 곱할 거

참석자 3 52:50
그러면 세터가 웨이트 값이라고

참석자 1 52:51
그렇지 메타 메이트 값

참석자 3 52:52
그러면 x가 특성이고 그거 그러면 x 제로는 왜

참석자 1 52:56
1이야 1 무조건 바

참석자 3 52:57
여기부터 이제 시작하는 건가요?

참석자 1 52:59
바이어스 아까도 앞에도 봐봐요. 여기도 세터 제로에다 1 곱한 거라고 생각하면 돼요.
x 제로가 있다고 생각해도 된다. 네 x 제로는 무조건 1이다.

참석자 3 53:08
네 그렇게 해도

참석자 1 53:09
없는 건데 사실 일로 무조건 보고

참석자 3 53:11
네 알겠습니다. 그리고 데이터가 이제 그냥

참석자 1 53:15
수업시간 중에 뭐라도 더 좋은데

참석자 3 53:16
그래 그래 방해될까 봐 데이터가 그러면 3개로 나눠진다고 생각하면 되는 거예요.
트레인 테스트 밸리데이션 근데 테스트랑 밸리데이션이랑 차이가 뭐예요?

참석자 1 53:24
테스트랑 밸리데이션이랑 차이니까 그 밸리데이션 기능이 기능이 뭐예요?
밸리데이션을 훈련하는 중이에요. 어디까지 훈련을 멈춰야 될 거 아니야 언제까지 계속 끝까지 할 수는 없고 그 밸리데이션 그 점수 보면서 훈련을 계속 멈출까 할까 이렇게 하고 있는 거고 네 테스트 근데 밸리데이션이고 이제 다 끝난 다음에 진짜 니 점수는 이러면서 진짜 쉬워 그러면 비율 밸리데이션은 약간 모의고사.

참석자 3 53:46
그러면 전체적인 비율이 어떻게 돼요? 테스트 트레인이

참석자 1 53:49
또 그 안에서도 또 이제 80대 20대 이렇게 하니까 많이

참석자 3 53:52
8 2 1이에요.

참석자 1 53:54
여기 안에서도 실제 여기서 팔 이 20% 안에서 밸리데이션 데이터를 또 20%를 쓴다.

참석자 3 54:00
아 안에서 또 20분 알겠습니다. 감사합니다.

참석자 2 54:21
쭉 가야 된다. 아니야 그게 나쁘지 않아 이런 식으로 보면.


clovanote.naver.com

딥러닝 day5_2
2025.03.19 수 오전 11:04 ・ 46분 52초
심승환


참석자 1 00:00
그래가지고 우리가 이렇게 할게요. 여러분 이거 하고 있죠.
그래서 제가 이거 그냥 손으로만 썼는데 열심히 써서 예쁜 글씨 좀 알아볼 수 있잖아요.
그렇죠 어쨌든 그 핵심이 뭐냐면은 여기 센터 제로부터 센터 n까지가 있는데 이거는 사실 피처는 MD가 있는 건데 여러분 안 헷갈려요 괜찮아요.
여기서부터 좀 헷갈릴 수 있잖아요. 지금 샘플의 개수와 피처의 개수와 헷갈리기 시작하잖아요.
근데 이거를 미리 해놓는 게 낫더라고요. 여기서 진짜 되게 헷갈리거든 딥러닝 들어가면 더 헷갈려 그래서 지금 미리 해놓겠다는 거야.

참석자 1 00:35
알겠어요 이게 그냥 여기서 봤는데 내가 책 보면 선생님 설명해 봐 더 헷갈리니까 여기서 지금 제가 이렇게 미리 선수를 쳐놓는 거지 안 헷갈리 헷갈려 하면 여기서 헷갈리고 질문하시라 알겠죠 그리고 아까 질문 저도 쉬는 시간에 질문 잘했는데 여기도 아까 여기도 내가 그냥 지나가는데 쉬는 시간에 질문하는 게 이제 좀 방해할까 봐 그러는 거 이해되는데 어쨌든 여기 보면은 결국은 데이터가 트레이닝 데이터 테스트 데이터 밸리데이징 데이터 이렇게 나눠지게 되지.
왜냐하면 트루라는 말을 굳이 붙이는 게 사실은 그냥 테스트 트레이닝 데이터에 트레인 데이터에 사실은 트레인이랑 베리데이션 들어가는데 사람들이 귀찮으니까 그냥 어떨 때는 트레인 밸리데이션 테스트 이렇게 세 개 있다고 얘기하기도 하고 이해돼요.

참석자 1 01:16
여러분 이제 용어가 막 나올 거라고 막 이제 근데 사실은 트레인 데이터에 트루 트레인이랑 밸류에이션이 있으면 사람들이 어떻게 부르겠어 실제로 그냥 트레인 데이터랑 밸리데이트랑 테스트 데이터를 이렇게 부르겠지 그래서 여기 앞에는 트레인드 셋 테스 때 이렇게 나눴을 때 80% 20% 나눴잖아요.
그럼 만약에 3개 되면 어떻게 되냐고 질문을 했어요.
좋은 질문이에요. 그렇잖아요. 수업 시간 중에 하면 좋을 텐데 좀 그럴 수 있지 그래요.
실시간이니까 여러분은 그래서 저기 계속 얘기하면 좋은 질문이라니까 트레인 데이터가 80% 이제 맞고 테스트 데이터 트레이드 데이터 안에서 다시 팔리데이션 돼서 하는 거잖아요.
테스트 데이터 안에서가 아니라 내가 아까 잘못해 트레인 데이터 안에서 80% 안에서 다시 또 80%를 프리 트레이닝을 쓰고 밸리데이션은 20% 쓰고 이런 식으로 한다고 이해되죠 여러분 됐죠 근데 그것도 다 문제가 있으니까 케이 콜도 쓰고 막 온갖 절짓을 다 하는 거야.

참석자 1 02:09
그렇죠 그리고 그냥 재미있어서 얘기하면은 오늘 졸업 논문 쓰는 친구 중에 한 명이 학생 중에 한 명이 이런 저기 토파스는 알아요.
여러분 클라우드 데이트 컴퓨팅 하는 그런 플랫폼인데 그거를 머신러닝을 시작하고 이런 걸 하겠다고 하는데 오픈소스 소프트웨어를 조사해 왔는데 셀던이라는 게 있더라고 셀던 알아요.
셀던 모르지 이 문학을 좀 해야 되는데 아시모프 아이작 아시모프라는 사람 알아 파운데이션이라는 소설이 있는데 다시셨죠?
sf sf sf의 SFT 거기에 역사학자가 등장하는데 그게 웃긴 거야.
그 역사학자가 모든 걸 다 예측해가지고 그래서 그 사람 이름이 헤일리 셀든이거든 그 사람은 그래서 로드 예측 잘한다고 이제 셀든 코어라고 만든 거야 그런 거예요.
그래서 지금 셀든 코어라는 게 그래서 뭐 하는 노래냐면 머신러닝이 이렇게 잘 못 믿겠잖아.

참석자 1 03:00
진짜 아무리 테스트 데이터로 잘했다고 해도 그레이드 데이터를 잘했다고 해도 실제 데이터에서는 못 믿겠으니까 그 셀덴 코 한 놈이 뭐 하냐면은 피트 f 되고 여러분들이 실제로 잘 돌아가는 건 계속 공부 확장시키고 새로운 거 깔면서 조금 했다가 계속 이제 서비스가 잘 돼서 뭔가 지표가 좋아지면 계속 확장시키고 이런 거 하더라도 좋잖아.
그렇죠 실제로 그런 거 좋은데 어 아닙니다. 잘했다는 얘기예요.
팀장님 그래요. 그래서 아니 내 말은 여기서 얘기하고 싶은 거는 이게 사실은 여기 막 이렇게 여러 가지 하는데 이걸로 충분하지 않아서 사실은 실무로 가면은 별 짓 다 한다 사람들이 알겠죠.
근데 아까 재미있는 게 그 셀든이 재미없잖아. 쓸데없는 얘기긴 한데 너무 하고 싶은 사람은 아까 해리 셀든이 파운데이션 sf에서는 그리고 미리 예측 잘해가지고 잘 나가는 걸로 돼 있는 파운데이션이 그 사람이 예측대로 다 흘러가거든요.
그 사람이 쓰는 거 성전이고 살았으니까 그건 믿어 거의 성경이야.

참석자 1 03:52
그런데 최근에 나온 미키세븐이라는 세븐틴이라는 소설 영화도 있고 그렇거든요.
여러분 영화는 별로긴 해요. 공주 감독이 했는데 소설이 좋은데 거기에 역사학자가 세상에 쓸모가 없어가지고 완전 버려두는 거 다 아니까 입으로 그러는 거 같아.
그 사람들한테 모든 반전을 반전하는 개념입니다.
거기서는 역사학자가 완전히 바보로 나오는데 그 이유가 사실은 파운데이션에 너무 헤디 셀더는 뛰어나서 역사학자가 완전히 최고 체험인 것처럼 나오고 우리 거기 지금 니키 세브틴에서는 역사학자가 세상에 쓸모없는 걸로 나와서 결국은 그 사람이 그렇게 계속 이렇게 뭔가 대항되는 사람이 나오는데 이상한 얘기고 어쨌든 뭐든지 세상에는 뭔가 용어를 지으면 다 이유가 있다는 거지.
셀든포가 왜 나왔냐 미키 세븐이면은 미키 세븐은 세븐을 지급하라는 그래요.

참석자 1 04:43
계속 가면은 여기서 세타가 어쨌든 파라미터고 여기 지금 파라미터가 여기는 지금 피처 값이 피처 값이 몇 개야지냐면 여기서는 하나 그쵸 여러분 실제로 이거를 그냥 이렇게 외우면 절대로 기억이 안 나고 항상 예를 들어서 해야 돼.
스토리로 집값 예측하는 문제를 생각하시라고 너한테 그렇게 알 수 있잖아요.
그쵸 집값 어떻게 예측해? 평수 갖고 그쵸 들어올 때 피처 하나잖아 숫자 하나 그쵸 그쵸 그리고 여기 세터 제로는 사실 1이라는 입력 값이 있는 걸 생각해서 곱하는 걸로 생각하라고.
그렇죠 1이라는 입력 값이 있는 거야.

참석자 1 05:22
1이라는 항상 z 값이 있는 거라고 그쵸 여기에서 근데 만약에 집값도 예측하는데 제대로 된 집값이라면은 사실은 범죄율로 들어가야 되고 막 이렇다고 그렇죠 다 곱해야 된다고 일일이 그렇죠 그래서 여기 지금 ND가 있다는 거는 여기 얘는 아까 보스턴 집값 데이터가 피처가 13개가 있거든 거기는 범죄율이랑 학부모 이런 거 있다고 마트 수 이런 거 인근 인근 1km 이내에 마트 수 이런 거 되게 중요해요.
여러분 kg 인근 1kg도 아니고 한 200m 내에 마트가 있는 게 편의점이 있느냐가 되게 중요하잖아요.
사실은 집값에 많이 영향을 미쳐요. 그래서 그런 게 그런 게 이제 n개가 있는 거지 열쇠 13 같은 게 있는 거죠.
보스탕 같은 경우는 13 이해되죠? 여러분 그래요.
세타 제로는 항상 여기 세타 제로 이거 세타 원부터 하는데 세타 제 x 제로 붙여놓은 거는 입을 일부러 이제 예쁘라고 다 예로부터 다 얘기까지 가는 게 좋잖아.
근데 s 제로는 무조건 1이라는 거지.

참석자 1 06:13
그럼 이쁘잖아 그쵸 알겠어요 여러분 그래서 이게 바이어스지 사실은 그쵸 바이어스잖아 바이러스가 무슨 뜻인지 알잖아요.
그래서 그냥 상수로 들어가는 거는 입력이 뭐가 됐든 간에 착 들어가는 거잖아 값이 그게 약간의 편향 이런 느낌이죠.
절편 그쵸 그런 거지 그쵸 이해되죠 그러니까 이거 세트 들어왔어도 되는데 그냥 이렇게 썼다는 거지 알겠죠 그리고 여기 지금 x ji를 이렇게 따로 정리를 하면은 여기 j가 이제 피처 인덱스고 0부터 0까지 그쵸 사실 x 제로는 1인데 그쵸 이해되죠 여러분 그래요.
제가 여러분 필기하느라고 혹시 바쁠까 봐 다 적어놨잖아.
그러니까 그쵸 여기는 여기 인스턴스 여기 위에는 이제 인스턴스 아이디 있어요.
1부터 m까지 n m이 아까 거기서 아까 그 성수하는 데는 얼마였어요 9개 적어 이해돼요 여러분 9개 샘플에는 좀 웃긴 거죠.
사실은 무슨 9개 갖고 뭐 하겠어요 하시면 안 되지 그쵸 근데 그런 거지 실제로 아까 여기 보면 얘 봐요.

참석자 1 07:14
여기 m 리스트 같은 경우에는 7만 개가 있다고 7만 개 중에 80%도 테스트를 쓰고 그쵸 그 안에 또 80% 또 최초의 테스트 테스트 대 80%를 확 놨을 때 트레이닝으로 쓰고 그렇죠 20% 테스트를 쓰고 그렇죠 그런 식으로 한다고 그래서 그게 그러니까 m이 그러니까 예를 들어서 7만 개의 80%면 얼마야 78회 14,666만 4천 그쵸 6만 4천 아직 맞나 18에 얼마예요?
56 5만 6천 미안해요. 5만 6천 원이야 알겠죠 5만 6천 원 5만 6천 원인데 사실은 또 트레이닝 슬라 나누면 거기서 또 80%에서 와트 찾네 5만 6천원 합시다.
5만 6천 원 같은 거 알겠죠 이해되죠 뭔지 그럼 감을 잡으라고 여러분이 알겠죠.
그래서 지금 이거를 실제로 여러분 코딩을 하면은 어떻게 해야 되냐면은 다 어레이로 해야 될 거 아니야 넌 파이라고 해서 어레이 라이브러리가 있어요.
선형 배수해야지 선형 배수 선형 대수가 얼마나 좋은데 열린 박자씨 같은 선형 대수라고 얼마나 되게 고마운 거예요.

참석자 1 08:20
여러분 그렇게 편하게 만들어 놓은 거 선형 대수가 되게 이거 안 하면 얼마나 귀찮아 사서 너무 편해 진짜로 안 하면 더 귀찮아 진짜 그래가지고 이게 이렇게 많은 거 있잖아요.
이렇게 x 제로부터 x n까지 많은 거를 그냥 이렇게 벡터로 만들어버려 벡터로 그리고 세트하도 벡터 벡터를 만들어 버린 벡터라고 벡터 벡터라는 거 벡터가 근데 여러분 이렇게 행이 한 행위에다가 여러 개 있는 걸로 표현하잖아요.
보통 그 어레이는 어레이는 그렇잖아요. 여러분 근데 이제 성형 대사 하려고 그러면 앞에다 곱하는 거는 보통 이렇게 한 한 칼럼에다가 이렇게 행이 쫙 여러 개 있는 게 편하고 기하 높혀지잖아요.
몇 개 써요 사람이면 매트리스 선정 교수는 다 배웠나 안다고 칠게요.
그래서 이렇게 트랜스포드 시킨 거 앞에 있고 그냥 못 하면은 값이 하나 똑똑하이 튀어나오지 하나 값이 나오지 그쵸 이해되죠 여러분 됐어요.

참석자 1 09:20
그래서 그렇게 해서 이거 표현하려고 그러면 근데 만약에 근데 그래서 이것도 표현하면 이거 벡터로 만들어도 m 곱하기 1 벡터라고 할 수 있는 거지 이건 뭐냐면 이게 벡터를 또 만든 이유가 이게 여러 이거를 인증을 한 데이터에 대한 로고 한 데이터 다시 핸드페이트가 뭐냐면은 샘플 하나 x 하나 아까 7만 개가 있어 7만 개 중에 한 숫자 하시자.
이미지 보스턴 집값이 있으면 집값 중에 한 어디 보스턴 외곽에 있는 집 하나요?
이런 식으로 알겠어요. 여러분 그거 하나에 대해서 이렇게 계산되겠지 숫자 하나 툭 튀어나오게 됐어요.
근데 그게 또 여러 개 있잖아 사실은 그러면은 이렇게 차원이 하나 늘어나 그쵸?
mm m 개죠 이게 m 개가 이게 7만 개 이런 거 알겠어요 여러분 그리고 이거 선형 대수로 하면 이제 또 m 늘어나고 여기 x가 이제 ND에 있는 거니까 그리고 이제 n 플러스 1개의 피처가 있는 거잖아.
그쵸. 그리고 여기 또 거꾸로 적었어.

참석자 1 10:20
여기서 하나는 하나는 이렇게 적었는데 또 여기서 거꾸로 또 이렇게 n 플러스 1 곱하기 하나로 세터는 이해되죠 여러분 그럼 맞잖아요.
이거 둘이 만나면 사라지고 m 곱하기 1이 되잖아.
차원이 알겠죠 사현 대수도 잘 모르겠으면 비슷하셔요.
알겠죠 수용 대수를 고등학교 때 안 배운다며 그래 그럼 지금 알면 되지 그렇죠 그래요.
자꾸 필수를 많이 늘릴 수밖에 없어요. 내가 그래요.
필수를 안 해도 알아서 잘 하셔. 어쨌든 그래서 이렇게 해서 이것도 그냥 다 적어본 거예요.
여러분 이거 다 이 실제로 넌 파일 이렇게 챙겨 먹어야 돌아간다는 거 보여주고 있어요.
모르겠으면 질문해 주고 그다음에 이게 만약에 이게 왜 멀티 프리니 이그렉션이 왜 중요하냐면은 실제로 피처를 한 개만 있는 데이터는 없다는 거지.

참석자 1 11:08
아까 이미지도 사실 피처가 28 28개에 있는 거고 거의 보스턴 집값도 제대로 되려고 그러면 하나가 아니라 13개씩 있는 거고 그래서 이렇게 벡터나 이제 뭔가 행렬로 표현해야 되기 시작하는 거지.
그쵸 그렇죠 여러분이 나오는 데이터 바로 잡았어.
하나만 파는 게 어디 있어 세상에 그렇죠 그렇잖아요.
여러분 피처를 하나만 받아서 사는 게 어딨겠어 여러 개 다 하겠지 그래서 이거를 표현하면은 아까 여기 하나에 대해서는 값이 하나인 거라서 이렇게 딱 적었지만 이거 한꺼번에 표현하면 이것도 한 생렬로 표현할 수 있잖아.
한꺼번에 아까 세터 자로 나온 거 있잖아요. 이런 거 한꺼번에 이렇게 업데이트할 수 있겠지 그게 얼마나 편해 한꺼번에 적어주세요.
한꺼번에 여기 로스 값을 지금 그냥 로스를 MSD로 적어놓고 MSD로 적어놓고 그쵸 세타에 대한 세타에 대한 거 세타에 대한 거 이것도 다 이것도 다 행렬로 하나하나 엄청나게

참석자 1 12:03
그러네. 어떻게 돼? 스텝 사이즈를 이렇게 스텝 사이즈를 할 수도 있겠다.
그래요. 잠깐만요. 미안해요. 내가 혹시나 내가 혹시 뭐 실수했나 싶어서 터미널로 해서 이거 이거 여러분한테 보이게 할게요.
이거 저도 헷갈려서 정리해 놓은 건데 제가 지난 시간에 요 로스트 에러 코스에 열심히 얘기하는 것처럼 여기 보면은 러닝 레이트를 스텝 사이즈라고 많이 부르거든요.
여러분 러닝 레이드를 왜냐하면 보폭이라고 그래서 한 번 움직인다고 약간 뭔가 움직여 봐서 뭔가 업데이트한다고 러닝 레이트로 구하는 거를 스텝이라고 부르기도 하고 스프 사이즈라고 부르기도 한다고 보폭이라고 부르기도 하고 알겠죠?
스텝 사이즈가 우리나라 말로 보폭이지 뭐 그렇죠 한 발자국의 크기 그러니까 이게 옮겨가야 되잖아.
그렇죠 그런데 그걸 러닝 메이트라고 곱하는 거를 그렇게 부른다는 거죠.
레이트 사실 러닝 레이트가 정확한데 스텝 스텝 사이즈라고 부른다고요 스텝 값 스텝 사이즈 값 이렇게 한다고요.

참석자 1 12:54
근데 제가 지금 여기다가 한 이유가 뭐냐면은 알파만 쳐야 되는데 다 쳐버렸어.
그래서 지금 제가 이랬어요. 알겠죠? 미안해요.
여러분 고쳐 고쳐요. 어떻게 고쳐야 된다고 그 사람들이 이걸 이걸 스텝 사이즈라고 부르는 사람이 있어요.
실제로 내가 옛날에 내가 봤던 머티리얼에서는 이렇게 다 해서 스택 사이즈라고 부르는 거지 이해돼요.
여러분 근데 그렇게 부르는 사람도 있지만 대세는 알파를 스펙살이라고 부르니까 오닐레이트 스펙살이라고 부르니까 대세가 나 따라해야지 알겠죠?
여러분 나도 그냥 사실 스펙 사이즈가 전부 다 같기도 한데 어차피 변화비라서 근데 이거 알파고 스펙 사이즈로 쳐요.
여러분 알겠죠? 다른 말로 러닝 레이트로 그래요.
여러분 요거 요거 혹시 로테이션 모를까요? 이렇게 적어놨어요.
친절하게 변화율 이렇게 삼각형 삼각형으로 나이 적어서 알겠죠 해볼게요.

참석자 1 13:50
그리고 그 러닝 레이트에 대해서 또 적어놓은 게 밑에 또 다른 내가 궁금해서 찾아봤는데 한 방에 앞뒤 말하면 한 방에 딱 갈 수도 있겠잖아.
그렇죠 한 방에 운이 좋으면 그런 건 불가능하니까 조금씩 조금씩 가든지 그쵸 그리고 결국은 좀 값이 커도 이렇게 갈 수도 있어요.
사실은 눈이 눈이 나쁘지 않으면 너무 크면 이렇게 벗어나고 이해되죠?
여러분 러닝 레이트가 크면은 발산을 해버려 학습이 안 된다고 아까 제가 얘기했죠.
자전거 타는 거 얘기했잖아요. 그쵸 이해되지 여러분 너무 크면 안 되지만 한 번에 딱 배웠다.
이거는 거의 드물잖아. 사실 그렇죠 젓가락이 커도 어떻게 또 왔다리 갔다 하는지 배우기도 한다는 거지.
커도 무조건 발사 하수 설명했잖아. 내가 아까 그렇지는 않다고 알겠죠.
적당히 크면 되는 거예요. 그렇죠 그러니까 이게 좋을지 이게 좋을지 사실 비슷하게 돼.
이거에 대해서 논문을 우리가 정리해 놨어요. 어떤 경우에 이제 벗어나지 않느냐를 우리가 논문을 썼어요.

참석자 1 14:48
그렇죠 근데 이거를 여러분이 볼 필요는 없지 지금 알겠죠?
됐죠? 그래요. 근데 여러분이 이제 이거 때문에 감을 익혀야 되는 게 여러분이 데이터 생성에 따라서 실제로 여러분이 이제 앞으로 러닝 레이트 같은 것도 여러분 프런트로 지정할 수 있어요.
잘 모르겠으면 러닝 레이트만 바꿔봐야 한다고 이체 여러분이 나오는데 러닝 레이트 값 때문에 이제 학습이 아예 수렴이 안 되는 경우가 있어요.
데이터는 멀쩡한데 여러분이 러닝 레이트는 너무 크게 해주는 거야.
그러면 정말 해석이 불가능할 수 있잖아요. 너무 작게 지면 또 어떻게 해야 되겠어요?
더 기다려야 되는 거지. 한 4시간 기다려야 되는데 여러분이 2시간만 하고 아이고 안 되네 이러고 맞는다고 이해돼요.
여러분 적당한 걸 해줘야 되잖아요. 그쵸 그래서 보통 여러 개 돌려놓고 있죠 보통 이것저것 다 뜯어놓고 있죠.

참석자 1 15:33
기정도 여러 개 파가지고 만약에 프렌 같은 거 하면 이메일 주소 여러 개 다 주고 그래서 이거 누가 적용이 되는 거지 그렇죠 아니면 또 툴이 있어서 그냥 여러 개 동시에 돌리는 것도 있어요.
여러분 보통 컴퓨터가 하나 들리면 힘드니까 그렇게 힘들고 자동으로 막 여러 개 돌려놓고 뭐가 제일 잘 되나 이렇게 보는 거지 알파고 같은 그런 놈들은 진짜 어마어마하게 하지 제자라는 손들 이런 식으로 저번에 제가 여러분 제가 이것도 보여줬잖아요.
여기도 보면 너무 그래요. 여러분 이 그림이 생크리지 베이직에서 얘도 도대체 신경망을 몇 개나 만들어 이거 봐요 여기 지금 어마어마하게 많이 떼를 지어서 만들어졌기 때에 제일 잘하는 놈 이런 거 있잖아 뭔 말인지 알겠어요 여러분 얘도 학습시키는데 여러 가지 버전으로 하는 거지 그냥 알겠죠 그래요.
러닝메이트가 다른 걸 수도 있어야 되니까 이해되죠 여러분 운 좋게 감동을 고르라는 거지.
그런 거잖아. 사실은 그러니까 로스트 값이 작으면 좋은 거잖아.

참석자 1 16:29
그 근데 그게 뭐가 될지 모르니까 열어놓으면 돌려보는 거잖아.
그게 이게 감을 잡으라고 여러분 실제로 학습할 때 이런 감이 이렇게 된다면 이 그림이 좋다니까 그래서 이거 이렇게 될 수도 있어.
이렇게 되면 좋겠지만 이렇게 되는 놈이 있으면 좋은 거잖아.
이런 놈도 있을 거고 이런 놈도 있을 거고 이런 놈도 있을 거 아니에요 그쵸?
그런 감을 잡고 있어야지 여러분이 러닝 레이트를 조절할 수가 있잖아요.
뭔 얘기냐 이 그림이 좋다고 그래서 나는 알겠죠. 아까 머릿속에 약간 좀 세팅해놓으라고 그래요.
그래요. 그다음에 여기도 이제 용어가 또 나오는데 사실 지금 내가 설명할 때 제가 계속 설명할 때는 맨날 이런 식으로 설명했어요.
이런 식으로 항상 제가 그냥 여러분 설명할 때 배치 그래 세트로 설명해 버렸어요.
배치라는 게 뭐냐면은 일괄 처리죠. 일괄 처리 배치 운영 체제 시간에도 여러분들이 배치 물리 시스템이라는 걸 배웠어요.

참석자 1 17:19
그게 뭐냐면은 데이터 그러니까 한 사람당 한 사람 다 처리하고 다 또 처리하고 처리하고 이런 식으로 뭔가 기다리고 있는 사람들을 어쨌든 터치 카드 이런 거 주소에 있는 거 교과서에 나오는데 그렇죠 가르표 보는 사람들 일괄 처리하는 거야.
그리고 배치 파일이라는 배치 실행 파일이라는 것도 뭔가 하나씩 하나씩 수행하는 것들인데 어쨌든 배치는 원래 한꺼번에 처리한다는 뜻이 강해서 이 배치의 의미는 원래 데이터가 아까 7만 개가 있잖아요.
예를 들어서 트레이닝 하는 용으로 7만 개가 있어.
툴 트레이닝으로 7만 개에 대해서 전부 다 에러 값을 계산하는 거예요.
이거를 이거 이게 이게 여러분 다 이렇게 설명했잖아.
내가 아까 9개면 9개 다 쓰는 거지 그냥 저거를 아까 두고 이러지 않잖아요.
이해되죠 여러분 9개를 다 m이 아웃 두라는 거지 그렇게 하는 게 약간 정상적이잖아.

참석자 1 18:06
사실은 그런데 여기 모든 트레인 데이터를 활용하여 세터 업데이트하고 또 하고 또 하고 또 하고 이렇게 하는 거 있죠 그쵸 이해돼요.
여러분 이게 여러분 이게 여러분 벌써 여기서부터 답이 오는데 이게 한 번 가서 여기 해보고 또 한 번 가서 이렇게 하면서 이렇게 한 번에 끝날 수 없다는 느낌이 들죠.
이거를 하나하나하나하나를 이제 이걸 에이 에폭이라 부르기도 하고 에 그러니까 시기 이런 식으로 있죠 에퍼 이해되죠?
여러분 또 이거 스텝이라고 부를 수도 있지. 스텝 사이즈라고 그래서 이거 이것도 스텝이라고 부르기도 하지 한 스텝스 2 스텝 하면서 훈련한다고 알겠죠 이해되죠 여러분 스텝을 막 이걸 스텝이라고 부르기도 하고 스텝 사이즈라고 부르기도 하고 영어가 엉망 대창인데 알아먹어야 돼.
알겠죠 사람들이 진짜 그렇게 써요. 막 나도 여기 슬라이드 갈 게 막 여기다 이렇게 해놓고서 고가에서도 박사도 받고 막 이러잖아요.
근데 알아먹으라고 알겠죠 그래요.

참석자 1 18:57
그리고 혹시 이상한 거 있으면 나한테 또 누가 이러더라한테 알려주든지 이쪽 이쪽이 지금 완전 진흙탕이에요.
너무 돈 버는 돈 버는 학문이라 용어가 아주 이상해 브루식 동사 여러분 다 막을 수가 없잖아 고 트 렌트 지금 그거를 보드 보드 하라고 아우리 봐 들어요.
그렇죠 컴 케인 컴인데 그거 왜 컴이야 또 컴드라고 안 해 절대로 안돼.
그렇죠 막 쓰는 거 많이 쓰는 건 용어가 이상해져요.
여러분 내 말은 그래서 이 용어가 다 달라. 근데 알아먹어야 돼.
그렇죠 그래서 얘기하고 싶은 게 이게 한 번에 끝나는 게 아니라 한 스텝 한 스텝 한 스텝 이렇게 한다고 근데 이제 스텝 사이즈라고 부르기도 하고 이게 스텝 사이즈 가긴 하잖아.
진짜로 그 근데 그거를 또 사람들이 막 이베스트 사이즈라고 그러더라고요.
나는 뭘 따라야 되느냐 시험에 안 내겠어 시험에 안 내겠다고 그때 알아먹으라고 알겠죠.
알겠죠. 그 맥락상 알아두세요.

참석자 1 19:45
어쨌든 얘기하고 싶은 거는 그 스텝 하나 옮길 때마다 센터가 업데이트할 때 어떻게 할 거냐 모든 데이터 다 쓰는 거 m으로 다 쓰는 거야.
이해돼요 여러분 되지 그런데 사실 이거를 먼저 보여주기 전에 이거를 보여주고 싶은데 여기 누가 사오라는 분이 정말 잘 만들어놔가지고 여기는 몇 억 건이래 이분은 몇 억 건이래 아까 걔 7만 개는 양호하다 이거지 트레인 데이터가 몇 억 건이래 이렇게 이분은 데이터가 되게 많은가 봐 그래서 선박 지역교 때 몇 억분 계산하면 저 아까 아까 변화율 있잖아요.
그레디언트 그레디언트 계산하는데 시간이 많이 걸리는 거야.
2초씩 걸리고 막 이러는 거죠. 그러면 되게 오래 걸릴 거 아니야 그쵸 근데 그래서 그렇게 하는 것보다 그냥 여기 언어 천년인가 보다 이서는 어쨌든 오래 걸리잖아요.
그래가지고 이게 이게 지금 보통 얘가 배치라고 하는 게 풀 배치를 의미해요.
전부 다 학습 데이터 전부 다 쓰는 거 이해되죠 여러분 제가 이렇게 했잖아요.

참석자 1 20:47
근데 그거 말고 그냥 조금씩 조금씩 쓰자 조금씩만 샘플을 뽑아가지고 학습을 해도 그러니까 이게 여러분이 이런 거예요.
공부할 때 이게 여러분 옛날에 수능은 다 봤잖아 거의 수능은 안 보이어 모의고사도 봤잖아요.
여러분 그쵸? 아까 그 트레 데이터 배치 데이터 테스트 데이터 있잖아요.
트레인 데이터 밸리데이션 데이터 테스트 데이터 이런 거 그것도 트레이닝 데이터가 여러분이 그냥 연습 문제 푸는 거고 밸리데이션 데이터가 모의고사 푸는 거고 테스트 데이터가 수능이야 알겠어요 여러분 그렇게 느끼면 더 와닿잖아.
테스트 데이터가 존재하잖아요.

참석자 1 21:24
테스트 수능이 진짜 진정한 거지 그쵸 그리고 또 모의고사도 여러분 뭐냐면은 지자체 모의고사가 있고 평가 모의고사가 있다며 그럼 테스트 데이터를 평가 모의고사로 보고 지자체 모의고사 있고 그렇게 볼 수 있다고 그러면 그렇죠 서울시청에서 나오는 건 글치에서 나온 거 잘 푸는데 경기도청 거 잘 푸는데 이상하게 이상하게 평가 금 못 풀어 이러면 이제 그것도 법인에 취득하는 거지 그 간접하건 이지 그런 식이고 그다음에 또 얘기하고 싶은 게 이제 여기서 마찬가지로 얘기하면은 여러분이 저 기출 문제 풀 배치라는 거 기출 문제를 다 풀어 기출 문제가 있다고 하는 게 여러분 어마어마하게 많잖아요.
그거 다 풀고 점수를 매겨서 내가 이렇게 풀렸구나 이렇게 공부하는 게 좋아요.
아니면 조금조금씩 풀고 하는 게 좋아요. 조금 조금씩 풀고 하는 게 낫잖아.
조금 조금씩 풀고 내 머리를 바꾸는 게 낫잖아. 그거예요.

참석자 1 22:07
조금 조금씩 풀고 조금 조금씩 풀고 여러분 머리를 바꿔야지.
저 배치를 하는 순간 저 아까 뭐라 하고 있어요? 여러분 머리를 바꾸는 거잖아 세터를 바꾼다는 건 머리를 바꾸는 거잖아요.
학습을 하는 거잖아. 잘될 때까지 그쵸 로그 정보 잡힐 때까지 하는 거잖아.
근데 그걸 바로 푸는 거 아니고 좀 있다가 풀어야 될 거 아니야 그렇죠 그리고 언제까지 내가 공부를 계속해야 되냐 국어를 국어 영어 수학 할 거 많은데 그렇죠 저 밸리이션 데이터 모의고사 잡을 때까지 하는 거지 평가하는 게 아니라 모여서 봤더니 국어로 잘 나와 그럼 이제는 수학해야지 이러다 망하는 소리 하지만 그렇죠 이해되죠.
여러분 인 것 같으니까 이해되죠. 여러분 그래서 미니 배치라는 게 뭐냐 데이터의 일부만 쓴다고 근데 다 쓰긴 써요.

참석자 1 22:50
이거 다 여러 번 해야 되니까 알겠죠 이거를 계속 돌리는 이거 다 계산해서 계속 머리를 바꾸는 게 아니라 조금씩만 바꾸면 바꾸고 이런 식으로 하는 게 더 잘 되더라 라는 걸 사람들이 발견했는데 실제로 여러분 공부할 때도 그렇잖아 시간이 이게 훨씬 적게 될 거 아니야 계산할 때 이해돼요 안 돼요.
이게 많이 걸린다는 거 이해돼요. 안 돼 아까 너무 데이터가 작아서 여러분 답이 안 오지만 사실은 지금 7만 개만 해도 많아요.
지금 계산이 하나 평균 계산하는데 한 0.5초 걸려도 여러분들 되게 오래 걸려 하다 보면은 여러 번 해야 되니까 여기 그림에서는 간단하게 이렇게 이렇게 하면 되는 것 같지만 사실은 이게 한 최소한 한 20번에서 여러 개 하는데 보통은 트레이닝 할 때는 이거를 며칠씩 하게 하고 그랬는데 이해되죠 여러분 여러분 공부할 때 다 풀고 하면 안 되잖아.
점수를 매기는 거 좀 하나씩 풀고 하는 건 좀 그렇지만 그렇죠 하나씩 풀고 하는 것도 있어.

참석자 1 23:45
여기 스토캐스틱 그레이디세트라고 이니에 하나 이미 하나 그러니까 하나만 정답 풀고 계속 다시 바로 매기는 거야.
그럴 수도 있지. 오히려 왜냐하면 여러분 나는 이것도 좋아하는데 한 개만 왜냐하면 내가 이거 잘못 바로바로 그냥 이 문제에 답이 뭔지 확인하는 건 괜찮잖아요.
진단 그쵸 근데 좀 오히려 좀 너무 속도가 느릴 수도 있겠지 또 바로바로 업데이트하고 있으니까 좀 약간 공부 이만큼 한 페이지에 풀고 하는 게 더 효율적일 수 있잖아요.
지금 비유하는데 정확히 맞아 사실 그게 이해되죠 여러분 이해돼요.
근데 이제 문제 한 페이지를 푸는데 요거 풀다가 한참 있다가 또 풀고 막 이러면 또 이따가 한꺼번에 할 수 있는 건 다 점수 매기고 지나가야지 그냥 하면 안 돼요.
사실은 그렇죠 어쨌든 그거는 이거는 스토캐스틱 그레이트트를 에스인이라고 부르는 거는 이미 하나를 하는 거고 알겠죠 여러분 그러고 이제 사실 미니 배치가 아까 봤던 거죠.

참석자 1 24:43
일부를 그쵸 근데 중요한 거는 뭐냐 이거 지금 아까 얘기했지만 진흙탕이라 그랬죠.
용어가 사람들이 막 쓰니까 이게 잘 되잖아요. 이게 잘 돼 이게 잘 되니까 미니배치 그랜드 센트라고 막 사람들이 미니 배치 그랜드 센트 새 보야 이랬는데 이름이 길죠.
SGD는 플레임이 이제 SGD라는 말을 또 사람들이 이거 잘 된다고 막 얘기해 사람들이 SGD가 스토캐스틱하게 근데 미니 매치를 할 때 약간 좀 섞어서 하거든요.
랜덤하게 샘플링 해가지고 할 때 그러니까 공부할 때 여러분이 이 친구들 풀 때 옛날 거 1980년대 거 풀고 81년 풀고 이렇게 안 하잖아요.
막 섞어가지고 나온 걸 풀잖아 보통 그런 식으로 하면 이게 미니 비치가 되잖아요.
근데 그게 약간 스토캐스틱한 느낌이 들잖아. 콩물주고 똑같은 거니까 기초 물리고 섞인 거 이해돼요 여러분 기치

참석자 2 25:33
그럼 미니 배치 브리디언트 디센트를 쓰면은 하나의 미니 배치를 끝나면은 이제 그다음에 밸리데이션 데이터를 이용해서

참석자 1 25:43
아니 밸리데이션 데이터를 바로 그 점수를 매기는 거 있죠.
그것도 하기 나름이야 할 수도 있고 안 할 수도 있고 그래요.
그렇게 해도 되고 안 해도 되고 근데 여기 원래 보통 API들은 보통 업데이트하자마자 밸리데이션 해요.
그게 더 느리지. 근데 그것도 밸리데이션 일부러 얼마 만에 하기로 하는 거 설정할 수도 있고 사실 생각할 수 있는 모든 건 다 가능해 이해되죠 디폴트가 뭐냐가 중요한데 디폴트는 바로 그 센터가 업데이트하자마자 밸리데이션 만약에 데이터를 줬으면 밸리데이션을 해요.
되게 느려 성능이 너무 귀찮으니까 가끔씩 하게 막 시키고 그래요.
이해돼요. 보통 바로바로 모의고사 팀이 잘 풀지도 못하는 것처럼 맨날 시키니까 그렇죠

참석자 2 26:23
미니 배치가 다 끝나면은 그러면 밸리데이션 데이터를 이용해서 업데이트를 하는 게 더 빨라서 그런 쪽으로

참석자 1 26:29
그렇죠 왜냐하면 바로바로 하는 게 좀 너무 느리니까 시간 걸리잖아요.
이 터가 베리드 데이터도 작긴 하지만 미니 배치보다는 더 작게 하기도 하는데 알겠죠 미니 배치 사이즈를 줘요.
여러분들한테 미니 배치 레이디 센트 할 때는 배치 사이즈를 지정해줘.
얼마큼 한꺼번에 포스시킬지 이거를 많이 하면 할수록 더 학습이 잘 되는 경향도 있지만 너 느리지 아예 이게 적당한 값이 필요해요.
그쵸 이것도 잘 모르겠지 사실은 그렇죠 이분이잖아요.
사실은 그렇죠 그리고 이 미니 매치할 때 샘플링을 잘하는 게 되게 중요해요.
또 그러니까 이게 뭐냐면은 아니 여러분이 내가 머리를 바꾸려고 했는데 머리를 바꾸는데 너무 편하게 문제 풀어서 머리를 바꾸면 별로잖아.
좀 골고루 좀 뭔가 공부한 다음에 머리를 바꾸는 게 좋잖아.
경험을 다양하게 해서 그러니까 다양하게 이렇게 하겠다.
정책을 바꾸려고 그러는데 내 머리를 그거를 단 한 개의 샘플 갖고 하는 건 별로잖아요.

참석자 1 27:21
사실은 그게 이제 2 s고 이거고 그쵸 하나 갖고 한 문제 풀고 이렇구나 이러는 것보다 좀 여러 개 한꺼번에 하면서 하는 게 더 낫지.
근데 그거를 여러 개 한꺼번에 할 때 좀 골고루 하는 게 좋지 골고루 데이터를 복습하는 게 좋잖아요.
완전히 완전히 골고루가 배치고 첫 번째 나온 거고 이해돼요.
여러분 저거 진짜 다 모든 게 다 쓰는 거니까 완전히 골고루 다 학습한 다음에 하는데 저건 너무 오래 걸린다고 지금 항상 얘기하는 게 학습의 효율성이 중요해 공부를 아까 빨리빨리 잘해서 뭔가 잘하고 싶은 거잖아요.
학습을 좀 빨리 하는 게 중요하니까 학습을 빨리 잘해야 돼.
그렇죠 요즘 여러분 항상 패스트 러너가 중요해요.
그쵸 페스트 러너 어차피 내가 새로 공부해야 되는데 그쵸 그러기 위해서 어떻게 해야 되느냐의 문제라고요.

참석자 1 28:07
미니 배치도 사람들이 문제는 스토키스트 에스틴이라고 그냥 불러 저걸 미니 배치를 사람들이 지금은 왜냐하면 저거 미니 배치 자체를 스토캐스틱하게 랜덤하게 샘플링하거든 몇 차시는 어떻게 하고 있냐면 데이터를 그냥 랜덤하게 섞어 골고루 잘 근데 그것도 잘 못 섞일 수 있어서 잘 섞는 게 되게 중요해요.
예를 들어서 m 리스트 데이터가 0 데이터가 쫙 있고 1 데이터가 쭉 있고 이 데이터가 쭉 있어 봐요.
예를 들어서 7만 개 중에 7천 개가 0이고 그다음 뭐야 그다음 701개부터 만 399개까지가 그쵸?
이해되죠? 내가 뭘 하는지 1 2위인데 영어 열심히 공부한 다음에 그다음에 이 열심히 공부하고 이러면 무슨 소용이야 그쵸?
어떻게 해야 되겠어 다시 골고루 섞어서 0 1 2 3 4 5 7 89 같이 공부한 다음에 또 공부하고 또 공부 이래야 좋잖아요.
이해돼요 여러분 그게 그럴라게 하려면 약간 스토캐스틱한 면이 있잖아.

참석자 1 29:00
좀 랜덤하게 그래서 미니 배치할 때도 스토캐스틱이 들어가기 때문에 사람들이 생각이 그렇게 돼서 이제 SDD를 그냥 미니 배치를 쓰는 용어로 썼다 쓰기 시작했다고 사람들이 그리고 중요한 게 API가 있죠 텐서 플로우나 파이프치에서 그냥 막 계속 써요.
그게 이제 걔네들이 글 쓰는데 어떡하겠어 그렇죠 근데 이게 이게 이게 원래 어쨌든 원래는 근본은 이랬는데 다 이유는 있지 그쵸 근데 어쨌든 SGD 미니 벤치 그레디세트를 SGD라고 부른다고 사람들이 이걸 SSD라고 부르는 사람들 있으라고 갖고 적어놨잖아.
그렇죠 알겠죠 그리고 이제 이게 어쨌든 미니 배치가 이게 그레디젠트가 제일 뭔가 모든 걸 다 커버하는 걸 볼 수 있죠.
이 배치 사이즈를 풀로 하면 배치가 되고 이 배치 사이즈를 1로 하면 스토리 스틱이 되니까 다 커버하잖아 이게 여러분 이해되죠?
돼요 안 돼 되죠 이해돼요 안 돼요.

참석자 1 30:00
그지 배치 사이즈를 1로 하면 배치 그래세스가 되고 이렇게 돼 1로 하면 스토케 스틱이 되고 풀로 하면 되잖아요.
그쵸? 이게 이걸로 하나 부르는데 이 이름이 미니 배치 그리스 별로니까 SGD라고 부르기 시작해서 SGD가 그냥 만능이 돼서 SGD 같고 다 해 그냥 다 해.
이 세 가지를 알겠어요. 여러분 API가 SPC 화면이 다 되는 거야.
계측 사이 설정하는 거에 따라서 디폴트 값은 위니 배치 돼 있고 보통 이제 30인 이런 정도로 해요.
30이 32개 하는 포트야 텐서 플로우는 이해돼요.
여러분 근데 이걸 학습이 너무 잘 안 되면 이걸 좀 늘리면 잘 되거든 약간 느려져도 보통 32개만 갖고는 잘 안 되는 경향이 있어요.
디폴트는 32개라고 하는데 아니면 그냥 그런 거 알고 있으라고요.
알겠죠? 그래요.

참석자 1 30:43
근데 느리게 느려지겠지 약간 이게 한 스텝 지날 때마다 좀 느려질 거 아니에요 그래야 나중에 결과가 좋은 게 좋으려고 그러면 나중에 이제 풀 배치하는 경우는 잘 되는 경우도 있죠.
데이터가 정말로 이상하게 섞여 있으면은 아무리 스캐스팅 해도 막 이상하게 공부를 하잖아요.
그럼 그냥 이렇게 하는 수도 있는데 데이터 작으면 알겠죠 여러분 이런 것들이 여기 책에 잔뜩 적혀 있는데 제가 책 안 가고 그냥 막 떠들고 있어요.
책 공부해가지고 지금 알겠죠 여러분 됐죠 그래요.
그래서 다 있지 배치 베디센트 배치 브레디센트는 전부 다 하는 걸 얘기하는 거고 이거 보 다시 MSA가 이렇게 MSA가 실이 원래 이거니까 이거를 실제로 미분하면은 이렇게 되니까 적어놓은 거예요.
여러분 미분하면 해결이 이렇게 되거든요. 제곱하는 게 이렇게 t 만들어서 제곱하면 되니까 이게 산수예요.
수확이에요. 수확 알겠죠? 넘어갈게요.

참석자 1 31:37
이런 거 사실 이제 딥러닝이 잘한다네. 아니 그러니까 딥러닝이 잘 못하기도 해.
여러분 저기 어쨌든 넘어갈게요. 여러분 여러분 시험은 오픈북과 클로드 북을 섞어서 낼 거예요.
그러니까 맨 처음에 클로즈드 북을 보고 그다음 오픈북 또 봐야지 2시간짜리 수업에서 하나 이 시간에 이해돼요.
여러분을 잘 관리 테스트를 하고 싶어서 그래서 이제 외워야 될 것도 있잖아요.
팀장님 내가 외워야 된다고 맨날 얘기하는 것들에 이 못 알아 몰라야 된다고 그러고 막 이런 거 있잖아요.
너무 나는 그런 거죠. 여러분이 너무 과적합되는 게 싫은 거지 센 단어로 외우는 게 너무 싫은 거예요.
찾아보면 되는 것들이 있잖아요. 이런 거 찾아볼 수 있는 거예요.
찾아볼 수 있어요.

참석자 1 32:19
나중에 찾아보면서 살 수 있으면 된다고 이거 내가 어떻게 다 숙달해서 하는데 돈이 필요한 세상이 아니야 오픈북을 보겠다는 거야.
알겠어요? 여러분 근데 외워야 되는 것도 있지 러닝 레이트가 뭔지는 알아야 될 거 아니야 이중에서 러닝 레이트가 뭐냐 그러면 이걸 알파밖에 될 수가 없잖아요.
그런 거는 뭐야 클로즈도 그리고 이거 수시 전개했는데 이렇게 되는 거는 여러분이 내가 무슨 수학 가르치는 사람도 아니고 이거는 그리고 사실은 물어보면 제가 이해만 하면 잘 아니까 높은 북으로 보겠다는 거예요.
이런 거 만약에 시험이 낸다면 알겠어요. 여러분 됐죠 얘가 이렇게 막 지나가는 거는 이해하세요?
하는 건 오픈북 외워 하는 거는 프로 북이에요. 다 필요해요.
사실은 근데 내 게 체화되지 않고서는 또 아직 나아갈 수 없는 게 있기 때문에 배치 SGD 이런 용어 배치 사이즈 이런 용어에 대해서는 여러분 모르겠어요.

참석자 1 33:07
프로드 이지 이 개념 모르면 안 돼 알겠어요 여러분 그리고 맨날 찾아서 어떻게 해 그거를 머릿속에 없으면 알겠어요.
그래요. 여기 지금 설명한 거 이것도 지금 이 사람이 워낙 슬라이드를 잘 만들어서 그냥 무단으로 가져왔는데 여기 처럼 적어놨죠.
그렇죠. 여러분 이사할 거예요. 알겠죠? 내 거 아니에요?
보면은 이게 이게 지금 로스 펑션이 요런 어떤 파라미터에 대해서 이런 식으로 생길 수도 있잖아요.
그쵸 이해되죠 여러분 그래요. 그러면 여기서 시작해가지고 쭉 내려가서 이렇게 되는 건데 이게 지금 점점점 보이는 데 너무 오래 걸리는 게 보이죠.
여러분 그렇죠 그래서 이렇게 하는 거가 이제 이게 좀 약간 부당하다고 보이는 거고 어쨌든 웨이트 업데이트하는 거는 애를 낮추는 방향으로 해가지고 이게 지금 변화를 적어 놨지 봤어요.
한 발자국 크기 러닝 웨이트 그쵸 스텝 사이즈로 적어놨지.
이 사람이 보니까 한 발자국 크기 스텝 사이즈잖아요.

참석자 1 34:02
그쵸 한 지점의 기후의 변화율이지 변화율이라는 게 루스 펑션의 변화율이에요.
용어가 용어가 변화율이라는 게 여러분 이게 변화율이 되게 여기 크죠.
여기 변화율이 크다. 여기 여기 여기 변화율이 0이지 변화율이 작아지게끔 해야 돼.
그치 그죠? 변화율이 크다가 점점 작아지지 않도록 한다.
이런 책인 거예요. 그죠? 이해돼요. 여러분 내 자리의 기울기에서 반대 방향으로 가야 되고 상 그래서 마이너스를 붙이고 디스텐트라는 게 에러를 낮춘다고 마이너스로 가는 것들 이걸 하는 반복해서 간다는 거예요.
그렇죠 그래요. 그다음에 데이터를 쓸 때 어떻게 쓸 것이냐에 대한 문제 알겠죠?
미니벤치 그쵸 스토캐스틱하게 그렇죠 스토캐스틱의 중요성도 여러분 알겠죠 0만 학습하고 1만 학습하면 안 돼.

참석자 1 34:52
숫자 숫자 배우는데 0 1, 2, 3, 4 5 7, 8 9가 다 한꺼번에 보여야지 뭐가 다른지 알 거 아니야 여러분 공부할 때 그래서 아니 발표할 때도 여러분이 연구를 발표해 여러분이 종합 설계 같은 거 발표해 4학년이 많잖아요.
3학년도 이제 내년에 해야 되는데 그때 내 연구를 발표할 때 그냥 내 거 열심히 설명하면 절대로 안 되고 기존에 이런 연구가 있었는데 나는 이만큼 더 하고 뭐가 다르다 이렇게 설명해야지 다 알아먹지 안 그래요.
여러분 항상 그렇다고 그러니까 뭐냐면은 데이터 공부시킬 때 항상 다른 거 여러 개 잘 모아가지고 가르쳐야지.
같은 것만 공부하면은 잘 모른다고 0하고 구하고 구분해야 되는데 영하고 열심히 배웠다가 나중에 또 구만 열심히 배웠다 이러지 말고 골고루 섞어서 배운 다음에 해야지.
그쵸 그래야 차이점을 알 거 아니야 저는 탑다운으로 공부해야 돼.

참석자 1 35:38
그럼 탑다운으로 메 탁 다운이 필요하냐면은 전체가 뭐가 있는지 안 상태에서 난 이거 지금 공부하고 있지 이렇게 알아야 되거든.
그래스 퍼스트 서치가 좀 필요하고 그다음 내가 지금 어디를 공부하는지 아는 상태에서 데스트 퍼스트를 봐야 돼.
알겠어요 여러분 전체 맥락을 알아야지. 그쵸 굉장히 여러분 슬라이드 만들고 할 때도 여러분 졸업 논문 제가 주도해서 항상 느끼는 건데 도대체 뭔 소리하는지 모르게 만드는 사람이 있어.
왜냐하면 그게 공부 잘하는 애들이 있잖아. 아 왜냐하면 얘네들이 디테일이 강하거든.
거기만 열심히 얘기하고 0은 열심히 설명 이렇게 열심히 설명해 그러지 말고 0 1 2 3으로 집에 누가 있는데 다른 거 이렇게 다르다고 설명하는 게 낫잖아.
이해돼요. 여러분 근데 그거를 못한다고 너무 공부 잘하는 애들이 0만 설명한다니까 그러면 안 되지 그러면 이제 나중에 입사가 안 돼.
그렇죠. 진짜로 이해돼요. 왜냐하면 우리서 뭔 소리 하는 건지 모르겠어요.

참석자 1 36:24
그렇죠 다른 거랑 비교 설명해야 되는데 그쵸 그러니까 이게 지금 제가 여기 보면 굳이 이것만 열심히 설명하는 것보다는 같이 설명해야 좋은 거 아니야 비칠렌데스 이거라고 설명한다고 한참 뒤에 이건 이랬지요.
이렇게 나오는 게 좋아요. 한 거로 설명해야지. 안 그래요 여러분 이거 설명해.
근데 이거는 이거 이런 게 있다고 설명하지 않고 이게 제가 설명할 때는 이거는 사실은 이거를 다 포괄한다고 설명했잖아요.
이런 거 설명 안 하면 이상한 거야. 그쵸 배치 사이즈가 이기면 얘고 뿌리면 얘라는 거 있잖아요.
설명 안 하면은 소통이 안 돼. 알겠어요 여러분 믿지 진짜로 중요하잖아요.
여러분 그거를 설명하는데 지금 배치가 왜 중요하냐 공부할 때도 그렇게 애들 공부 그렇게 시켜야 된다고 여러분 얘 공부시킬 때도 데이터 몰아서 주면 안 돼.
골고루 섞어줘야 돼. 알겠죠 네 비교할 수 있는 거 줘야 된다고 비교할 수 있는 거 줘야 돼.
그래요.

참석자 1 37:23
그래서 SGD의 컨셉 스토에서 그리스 느린 암벽보다는 조그마 호트 보고 빨리 빨리 가봅시다가 진짜 중요하다고 이해되죠.
여러분 그래 그래서 여러분이 질문을 못하는 것도 있지.
교수님이 이제 발표하고 있는데 좀 이따 설명하겠지 자꾸 중간에 끼어들면 이제 진행이 안 되겠지.
약간 이런 것 때문에도 있는 거긴 한데 그러면 gd gd가 여러분 gd가 여기 이 사람 만든 데에서는 gd가 이제 브랜드스트가 이제 DGD를 의미하는 배치 그랜디스트 전부 다 하는 거 알겠죠 여러분 그리고 미니 렉시를 하는 거 이해되죠 여러분 그래요.
보면은 여기 검은색이 이제 gd로 이렇게 목도 쪽까지 가는데 이렇게 예쁘게 가는 거고 BGD는 이렇게 이렇게 가는 걸 좀 아까 왔다리 갔다리 하죠.
그쵸. 근데 여기서 여기 하나 가는데 여기 1시간씩 걸리고 있는 거고 이 까만색은 요 점선으로 된 거는 한 1초 만에 가고 있다는 거야.
알겠어요 여러분 무슨 얘기인지 감이 와요.

참석자 1 38:22
그래서 이게 여기는 아까 내가 2시간이라고 그랬나 대충 2시간이라 그랬으면 2시간 하나 둘 셋 넷 다 명 12시간이 걸렸고 이거는 1 2 3 4 5 6 7 8 94 10초 만에 나왔다고 무슨 얘기인지 알겠어요.
여러분들 극단적으로 얘기했는데 됐어요. 여러분 그래서 지금 배치 그랜드 센트는 데이터가 정말 작을 때만 한다고 알겠어요.
여러분 여기 이 슬라이드에는 배치 그랜드 세트는 그냥 또 그랜드 센트로 적어놨어요.
그냥. 근데 좀 이상하지 이상하지만 이거는 또 이렇게 적어놨어요.
알겠죠 그럼 요거가 막 되게 사람들 말이 다른 거 이해돼요.
여러분 볼게요. 그다음에 이것도 이 사람 되게 이게 다른 논문에 나오는 걸 읽으니까 뺏기신 것 같은데 어쨌든 이게 로스 이게 파라미터가 두 개 있을 때 로스 펑션을 같이 그리면 이렇게 그려지는 거 이해돼요.

참석자 1 39:10
여러분 그러니까 이거 리니리베이션 이렇게 표현할 수 있지 사실은 사실 이렇게 진짜 생기지 않고 이거는 비슷하게 생겼는데 이럴 수 있잖아요.
여러분이 실제로 그래서 여기서 물수가 작은 데로 가려고 그러면 욜로 가야 되는 거잖아요.
그쵸 이제 가야 되는데 이거 찾아가기 위해서는 얘가 방향성이 둘 다 있잖아요.
세타 제로랑 세트 원이 둘 다 방향성이 있어요. 그쵸 그쵸 잘 가야 돼.
그렇죠 그래서 이게 전부 다 이제 기울기가 낮아지는 기울기가 점점 줄어드는 쪽으로 가는 거예요.
그쵸 그래서 다 들고 그다음에 그다음에 조심할 게 뭐냐면은 실제로 로스 펑션이 생긴 모양이 그러니까 얘는 진짜로 펑션이 이거 리뉴얼 이그레션 쪽에서 할 때는 이렇게 생겼거든요.
진짜로 리뉴얼 이그레션이 이렇게 생겼어요. 그러니까 이노리메레이션에서는 그냥 nse로 끝나니까 진짜 이렇게 생겼으니까 어떻게 봐도 그냥 여기 도달하면 끝이잖아.
그렇죠. 기울기가 0 인데 도달하면 끝이에요.

참석자 1 40:06
그런데 여기는 실제 로스포션이 이렇게 생길 수가 있어요.
여러분 라니어 쪽으로 가면은 여기서 줄어들었는데 사실은 다시 올라가는 걸로 안 줄 알았더니 더 이걸 지나가면 더 좋은 데가 있는 거야.
더 작은 로스가 가능할 수 있다는 거죠. 이해돼요.
여러분 근데 이거는 진짜 여기서 시작 출발점이 여기서 있으면은 절대로 여기서 벗어나지 못해요.
이해되나 여기서 그냥 이거 더 이상 여기 가면 증가하니까 여기서 멈추지 뭐 그렇죠 이러고 말지 그러면 이거 글로벌한 걸 못 찾잖아요.
그래서 결국은 어떻게 해야 되느냐 초기 값을 여러 개 주고 여기서 시작하는 거기서 시작하는 거 이런 거 여러 개 만들어서 해볼 수밖에 없잖아요.
이런 경우에는 그래서 리얼 네트워크 여러 개 만들어서 볼 수밖에 없는 거죠.
이해돼요. 여러분 테이크도 다양하게 주면서 그런 거 있잖아요.
이해되죠. 망상불이 잘 되는 것도 원인 중에 하나이기도 해.

참석자 1 40:52
문제가 지저분하고 그럴수록 이제 굉장히 복잡하니까 다양한 시도를 해볼 필요가 있는 거지.
그렇죠 여러분도 항상 이게 진짜 다른 길인가요? 가면 돼 이러면서 가도 여기까지 올라가지 못하는 건데 그렇죠 뭔가 벽을 부수고 나와서 여기 다 들어야 되는 거잖아요.
그렇죠 이해되죠 그래요. 선호에 비해서는 그렇지만 다른 데에서는 비선형 솔이 조금 들어가는 데서는 안 그렇다는 거지 알겠죠?
그래요. 그래서 이 그림이 뭘 의미하는지 알겠죠?
여러분 세타 j 포스트 그렇죠 이게 뭔지 알겠죠? 제 번째 파라미터 값이지 그렇죠 35분이네 이거 이거 하나만 더 얘기하면 아까 질문했던 거 기억나는데 여기 이 센터가 웨이트야 웨이트 알겠어요?
여러분 웨이트랑 파라미터랑 제가 많이 섞어 쓰고 있어요.
웨이트랑 파라미터랑 같은 말이야 알겠어요? 여러분 웨이트 곱하는 값을 웨이트를 딥뉴럴 네트워크도 이렇게 생겼어요.
여러분 나중에 거기다가 액티베이션 공전 통과시키고 여러 개 있는 건 다르잖아요.

참석자 1 41:50
알겠어요 여러분 액티베이션 성 통과하는 것 때문에 비정형성이 생기기 때문에 이렇게 되면 복잡해진다고 했어요.
진짜로 진짜 웃기세요. 그래요. 그다음에 지금 지금 용어가 제가 특수성이라는 용어랑 피이랑 같은 말인지 알겠죠?
여러분 그래요. 스케일링이라는 말을 조절하는 거예요.
여러분 인금 조정이라는 거 알죠? 스케일이 여러분 뜻이 규모 이런 뜻도 있고 그렇죠 스케일 러브 하면 확장 가능하다 이런 뜻이 있고 근데 사실은 뭐냐하면 스케일이 원래 뜻이 저울 있어요.
저울 스케일링 하면은 누구 조정하는 거야? 저울에 이제 0점 조정해야 되잖아요.
여러분 맨 처음에 아무것도 안 오르는데 5kg 이렇게 가르키고 있을 수도 있잖아요.
그쵸 그럼 0으로 맞죠? 0점 조정해야 되잖아 그쵸 그런 거에서 용하는 게 이제 피처 스필링인데 피처들이 이제 문제는 이제 하나 이 피처가 기본적으로 하나가 아니고 여러 개 들어온다고 그랬잖아요.

참석자 1 42:47
아까 여러 개 값이 근데 레인지가 되게 다르다고 그러니까 예를 들어서 키 하나면 상관없는데 아까 보스턴 집값 같은 거 하면 어떤 데는 어떤 데 어떤 거 예를 들어 무슨 뒷값을 검지율 같은 거는 퍼센트로 돼 있고 어떤 데는 진짜 그리고 아까 편의점 비율 이런 거는 소수 1 0하고 1 사이에 다 다를 수 있잖아요.
근데 그걸 같이 학습시키면 되게 이상하겠죠. 같이 웨이트를 곱해서 하라고 그러면은 좀 알아서 될 수도 있지만 좀 힘들 거 아니에요 웬만한 피처들이 다 비슷한 게 좋지 레이지가 안 그래요 여러분 그래서 여기 그림이 있는데 뒤에도 미리 보면 해.
이게 피치 스케일링 안 된 놈이 이게 피트 된 놈이거든요.
그러니까 이게 어떤 세터는 되게 커야 되는 거지 피처가 이 레인지가 되게 작으면 디자인이 커야 될 거 아니야 그쵸 포라미터 값이 이해되죠 여러분 그래서 이렇게 만들어놓고 하는 게 좋잖아 잘 돼 학습이 잘 된다는 거지.

참석자 1 43:42
그러니까 이게 세타 제로랑 세타 2의 레인지가 비슷하게 만든다는 거죠.
세타 원의 레인지가 너무 작았어. 그럼 좀 늘려서 지금 만든 거죠.
사실 이게 그쵸 이게 뭐냐면 여러분 왠지 왜 돌아보겠다 이거 사실 3차원 그림이에요.
세터와 세타 2가 있고 로스트 값이 지금 지금 그림이 이렇게 돼 있지만 사실은 여기 내려가는 거야.
등고선처럼 이게 가운데 오타 진 게 아니라 내려가는 거야 이 그림이야 이게 사실 이 그림 이거 이거는 그게 하나 있는 거고 이게 거꾸로 이 부분이 그린 거예요.
이해돼요. 여러분 이 부분 여기는 지금 그리지도 않았고 여기 그렸어요.
이해돼요. 여러분 그런 걸 여러분 모르는 질문해야 돼요.
내가 지금 당연히 아는 게 아니지.

참석자 1 44:21
근데 원래 여기서 시작했으면 센터 지원의 형태대로 여기 있었으면 찾아가서 일로 가야 될 거 아니야 그렇죠 근데 이렇게 가려고 그러면 이게 만약에 이게 너무 이제 이 보폭이 얘네들이 너무 두께가 다르면 자는 데 힘들 거 아니에요 비슷하게 해야지 쭉쭉쭉 잘 간다 이거지 알겠죠?
그래요. 그래서 피치 스케일링이라는 거를 보통 어떻게 하냐면은 보통 미믹스 스케일링이 제일 쉬워요.
일심적으로 무조건 원래 값에서 민 값을 빼고 맥스하고 맥스 마이너스 2로 나누면 0하고 일자로 되겠지.
어떤 숫자에 어떤 값이 100에서 천까지 있으면 원래 100을 빼버린 다음에 그렇죠 그다음에 이제 원래 레인지 값이 나가버리는 하고 이사가 되잖아요.
그쵸? 이해되죠? 여러분 당연히 이렇게 하겠지.
상식적으로 사요. 예 쉽지 그리고 그것보다도 뭔가 좀 더 스탠다 정규 분포 같은 거를 만들고 싶으면은 평균을 빼가지고 분산으로 표준 표차를 나누는 거지 이렇게 할 수도 있죠.

참석자 1 45:17
그쵸 그러면 바오 시 한 번도 비슷하게 만들어 그러면 이제 이게 좋은 점이 이 상치가 있을 때 이 상치가 좀 걸러지는 경향이 있어요.
얘는 이상치가 계속해서 살아남아 있는데 얘는 이제 바우처 범표가 찍혀 스탠다드 디피션이 나가버리기 때문에 분포가 이제 너무 막 커지는 거는 좀 다시 줄어드는 거지 좀 좋은 점이 있어야 되겠죠.
이상치가 있을 경우에도 이상치가 좀 골라지는 경향이 있어요.
영향성이 어쨌든 얘는 이제 항상 평균이 0이고 이렇게 안 되면 평균이 0이고 그다음에 베이러스가 1로 만들어버리는 거지 이런 식으로 하면 그렇죠 표준 기차가 1이야 알죠 여러분 표준 표준 분포라는 거 알잖아요.
표준 분포를 매칭시키는 거지 원래 여러분 세상에 대부분 데이터는 표준 분포가 따르니까 공유 분포로 따르니까 여러분 데이터가 정리 표잖아요.
여러분 아닌 것도 있지. 근데 정기 정표가 많아 사실은 여러분이 있잖아요.

참석자 1 46:10
정기 정표가 많다는 게 뭐냐면은 여러분 생긴 모양이 여러분 다 사람을 우리가 알아먹잖아.
나도 사람이 어디 있어 그렇죠 그리고 눈이 여기 있으면 다 눈이 여기 찔리고 여기서 벗어나지 않아.
별로 평균이 있고 차가 있다고요. 갑자기 눈이 여기 달린 거가 있을 수 있잖아요.
아니면 삼지압 아니면 또는 김열의 칼날 이런 거 있잖아요.
김열 칼라 계속 미안해. 내가 그거 봐가지고 그쵸.
근데 보면은 그런 거는 이제 아웃라이어지 상 편차 많이 벗어나잖아요.
사실은 그렇죠 그런 거가 별로 지는 경향이 있어요.
잘 되는 게 사실 우리 5분 여기까지 할게요. 여러분 여기까지.


clovanote.naver.com