딥러닝 day8
2025.03.31 월 오전 10:02 ・ 48분 37초
심승환


참석자 1 00:00
큰 문제는 안 되는데 이 사람들이 이제 인터넷에 이런 게 많다는 거죠.
사실은 이제 어떤 데는 세로 가로 가로 행렬 이런 게 막 바뀐다는 거지.
근데 그럼 문제는 뭐냐면은 제가 이걸 주고 요 계산 있잖아요.
여기 프리시전 리콜 계산하는데 폴리스티시탈 계산하는데 이거 제가 귀찮아가지고 치티 피터 시켰거든요.
이미지 주면서 걔는 이제 당연히 정상적인 건 줄 알고 이 글씨를 보고도 그래서 계산이 반대로 돼 있었어요.
여러분 행렬이 거 이것도 그러니까 프리스틴하고 리콜이 바뀌어 있었어요.
고쳤어요. 그래서 알겠죠 여러분 근데 이제 세상에 그런 게 많다는 거죠.
여러분 조심하세요. 그래서 일부러 이거를 샘플을 갈아버릴까 하다가 근데 사실 세상은 그러니까 조심하면 참을 때 놔두기로 했어요.
알겠죠? 여러분 그래요. 그러면은 여러분 시험에 이걸 낼 거야 아니면 딥러닝 수업인데 여러분 시험에 안 나오니까 별 관심이 없을지 모르겠지만 그냥 그렇다고 알겠죠.
여러분 강의자도 바꿨어요.

참석자 1 00:59
그래서 피스 피티한테 내가 또 물어봐도 또 틀려요.
또 틀려 계속 틀리다 나한테도 나 자꾸 틀리고 이건 무슨 얘기냐면은 이거 프린트 라 트루랑 써 있는데도 불구하고 자꾸 여기를 나는 그냥 당연히 여기가 트 클래스고 여기가 프린트에 존재한다고요 뭔 얘기인지 알겠죠?
여러분 세트 PT 할 때 한 번 물어서 그다음에 물으면 이제 제대로 된 걸로 계산해 드렸는데 또 틀리게 하고 틀리고 계속 그게 나도 그렇잖아 사람이랑 비슷해지는 게 못 믿으러 이 나는 믿으러 이 사실 이제 조심해야 되고 사람이 그렇죠 그렇다고요 네 재밌죠.
인터넷은 왜 이렇게 이게 반대로 된 게 많아 문제는 이게 이렇게 쓰기도 하고 이렇게 쓰기도 하고 다 막 그러고 있거든요.
여기 달아놓으면 편하죠. 그렇죠 조심하세요. 여러분 그래도 어쨌든 틀린 건 틀린 거니까 그다음에 우리 교과서로 진행하고 있었잖아요.
그쵸 저기 수요일에 여러분 예비군 몇 명이나 가는지 손 들어보실래요?
진짜 진짜 많네 거의 다음에 알겠어요. 알겠어요

참석자 1 02:12
이거 어떻게 해야 돼? 녹화를 해서 올리는 게 맞는 것 같아요.
그렇죠 녹화를 수요일 수업을 그러면은 녹화를 내가 내일 다 할 수 있을지 모르겠는데 녹화하는 게 사실 좀 찝찝해요.
왜냐하면 그래도 녹화를 해야지 녹화를 하지 뭐 녹화를 해서 제가 수요일 수업이 끝날 때까지가 몇 시지 수요일 6시까지 이거 내가 말 많이 했는데 녹화를 해야지 녹화를 하고 토요일 수업을 여기서 안 하고 그냥 녹화 녹화를 끝낼 거예요.
여기가 안 운로드 됐지 얼른 왜요? 그리고 듣는 수당은 수강일은 예비 훈련 때문에 못 듣는 건데 다음 한 금요일까지 이렇게 할게요.
여러분 금요일 정도까지 강의 올리는 거는 제가 딥러닝 수업 수요일 저녁 수요일 한 저녁까지 8시까지 올리고 알겠어요 여러분 출석 체크는 일단 제가 학교에 안 올리면은 뭔가 자꾸 문자가 나와서 귀찮아서 모두 수석으로 바꿨다 해놨다가 나중에 싹 바꿀게요.
원래 정상적으로 알겠죠 여러분 문자 나가도 귀찮아서 그래요.

참석자 1 03:22
계속 너 왜 안 했어 이러면서 날라와서 그냥 전부 다 출석을 했다가 나중에 제대로 바꿀 거예요.
알겠죠 그거는 비리는 아니니까 그래요. 그래서 여기다 봉지를 올리는 게 좋겠지 안 오는 학생들이 많아가지고 그래요.
나중에 이제 까먹고 공지를 안 올리면 골치 아프니까 공지 사항에 수요일 이것도 학교에 계속 뭔가 학생회에서 뭔가 요즘 열심히 하는 건데 학생회에서 예비군 훈련하는 학생들이 불이익이 없도록 무슨 조치를 취할 수 있습니까?
이런 메일을 먼저 보내야 돼. 이제 학교 끝날 때마다 그래서 답장해야지.
수요일 예비군 군 훈련 수업 라이 경영 금요일 수요일 저녁 6시까지 올라오는데 수요일 6시 6시 라 혹시 못했지 모어 금요일 밤 12시까지 시청하기 까지 시간 되면 이렇게 해놓고 여기도 그냥 똑같이

참석자 1 04:48
됐어요.

참석자 1 04:53
너무 멍이라고 하는 복잡해서 이렇게 진짜 어떻게

참석자 1 05:13
여러분 그래도 미리미리 알려줘서 고마워요. 되게 당황했을 것 같아요.
진짜 아무도 없어가지고 그래요. 그다음에 우리 진도 나가던 거 교과서는 그래서 지금 약간 고민되는 게 여러분 있잖아요.
제가 이거 녹화를 올리는데 원래 여러분 이거 이걸 녹화하면 안 된다고 돼 있어요.
이거 강의 자료를 온라인 동영상에 요즘에 지금 당장하던데 어쨌든 그래서 비밀이에요.
여러분 그러니까 지금 동영상을 어디다가 퍼날리든지 공유한다든지 그러면 여러분 문제가 될 수 있어요.
저작권은 알겠죠 무슨 문제가 참 궁금하긴 한데 돈을 돌아 가 아니 어떻게 될지 모르겠어요.
어쨌든 문제가 된대요. 그러니까 여러분 절대로 비밀로 하셔야 되겠죠.
실시간 동영상 녹화한 거 근데 다른 거 해야 되겠어 다른 거 구매하고 있으면 다른 거 해야 되나 교과서 가면 안 되고 기 그냥 그거 코렛 갖고만 해야 될지도 모르겠어요.
그렇게 그게 또 그것도 괜찮을 것 같기도 해요.

참석자 1 06:16
콜렛 책은 그냥 목적 그러면서 그래도 도움이 될 것 같긴 해요.
해볼게요. 어쨌든 여러분 동영상 할 때 제가 책을 이걸 못 올릴 수 있어 안 올리 겠죠 좀 찝찝 찝찝해서 안 올리는 게 좋아서 책을 몇 쪽 보라고 그러면서 제가 코르다 팔 수 있어요.
알겠죠 저 책을 그래서 혹시 집에 없다고 또 골치 아플 수 있으니까 책이 있어야 돼.
책이 이거를 그 당시 교과서 비료 교과서가 없으면 공부 제대로 할 수가 없어요.
알겠죠? 교과서 학습기 비료 혹시 책을 안 가져가서 또 이상하게 몇 쪽 보라 그러는데 아무것도 없으면 곤란하잖아요.
그러니까 근데 이게 사실 여러분 나도 이게 약간 좀 느껴지는 게 옛날에 내가 만든 강의 자료를 어떤 교수님이 자기 이름으로 그냥 막 배포하더라고요.
기분 나쁘더라고요. 진짜 내가 옛날 박사 과정 때 열심히 만들어서 내가 홈페이지에 올려놓은 거를 자기 이름으로 그냥 내가 자기 발문처럼 발표하고 있어서 내가 기가 막혀가지고 내가 다시 상정 안 해도 되고 있어요.

참석자 1 07:15
지금 벌써 은퇴하셨지만 어쨌든 무슨 얘기인지 알겠죠 여러분 되게 기분 나빠요.
진짜로 그렇지 않겠어요? 여러분 내 거를 그냥 아니 나는 얘기를 해도 사이트에도 그게 충분한 게 아니라 약간 이거 책 파는 건데 좀 그렇게 문제가 있어요.
여러분 알겠죠 우리나라 사람 아니지만 그래요. 어쨌든 그런 문제가 있어서 하지 말라는 걸 구조선을 진짜 어떻게 사이트도 안 하고 내가 나를 볼 수가 있을까 나는 인간적으로 진짜 맞는다 고 최소한 추추라도 적어놔야지 그래요.
그래서 여러분 지난 시간에 어디까지 했었냐면은

참석자 1 07:55
이거 사실 너무 책을 다 뺏겨줘가지고 너무 많아서 제가 쓱쓱 넘어가고 있는데 이렇게 해가지고 이게 여기 하다가 제가 이거 다시 또 이게 몇 쪽이냐면은 이니까 계속 1.1절이구나 다 보지는 않을 건데 말이 많아가지고 그림을 다 훑고 지나가려고 그러거든요.
그림은 다 열심히 그린 거라서 이거를 하다가 이게 무슨 의미를 갖는지 제가 텐서 플로우 플레이 그라운드 갖고 설명하면서 끝났었어요.
지난 시간에 그랬었어요. 파라미터 개수가 몇 개냐 이런 얘기까지 하면서 왜냐하면은 이제 계속 나오겠지만 추상적으로 먼저 안 다음에 코드를 보는 게 훨씬 더 좋거든요.
코드가 보면 또 디테일하게 빠져가지고 이게 뭔 건지 몰라서 여러분이 그 펜스 플로우 플레이 그라운드가 진짜 좋아요.
여러분 거기에 시각화가 돼 있잖아요. 그쵸 진짜 웨이트가 정체가 뭔지 그쵸 이거 몇 쪽이냐면 37쪽이에요.
37쪽 교과서 37쪽인데 여러분 이거 텐서 플로우 플레이 그라운드로 생각해 보시라는 거죠.

참석자 1 08:57
알겠죠 그래도 확실하게 하는 게 좋아서 패스 플러스 플레이그라운드로 가면 여기 여기 그냥 링크가 이렇게 강의 자료에다가 서 플로우 플레이그라고 그려놨죠.
이거 니 것도 아니야 여기 가면 이렇게 나와 있는데 여기는 가로 세로가 반대예요.
그쵸 그리고 되게 좋은 질문인데 여기 출력 계층이 입력 기준 출력 계층에 대해 설명을 제대로 안 했나 봐요.
제가 이게 입력 계층이고요. 여러분 그렇죠? 이게 출력 계층이에요.
출력 계층이 여기 있어요. 여러분 출력 계층이 생략된 게 아니라 있어요.
출력값이 하나예요. 이거는 알겠죠? 여러분 수력 계층도 뉴런으로 표현이 되네요.
여러분 알겠죠? 그래서 여기도 웨이트가 있어요.
알겠죠? 여러분 취약 계층에도 마찬가지로 액티베이션 펑션을 쓸 수 있어요.
여기는 지금 마이너스 하고 1로 나오게 되어 있는 거예요.
사실상 마이너스 1 1하고 사이로 그 사실 1하고 0으로 그냥 바꿔버렸죠?
그쵸? 사실 시그모이드로 나왔다고 생각해도 돼요.

참석자 1 10:10
시그모이드로 확률을 바꿔버린 거죠. 마지막에 막 그냥 숫자가 막 나올 텐데 액티비션 펑션을 시그모인으로 통과시키면 어떻게 돼요?
무조건 모든 마이너스 무한대에서 무한대까지의 숫자가 다 어떻게 돼요?
0하고 1 차이로 바뀌지. 그래서 1이면은 여기 플러스 1 해놓고 마이너스 0이면은 마이너스 1로 해놓은 거지 들여놓은 거예요.
그림 시각화는 또 시각화도 다른 문제니까 이해돼요.
그리고 그리고 여러분 다시 강조하고 싶은 게 제가 머신러닝 하고 대충 넘어간 그 개념 있죠 라젯이라는 개념 있잖아요.
이게 0하고 1 사이로 변하기 전에 여기 실수가 엄청 나올 거 아니에요?
아이레스 버츠 버드까지 숫자 그거를 라짓이라고 불러요.
라이 그냥 확률로 변하기 직전에 실수를 의미해요.
확률로 표현하고 변화시키고 싶은 알겠죠 라짓이 제대로 나와야지 이제 시그모이드가 확률이 제대로 나올 거 아니야 그쵸?
다음에 아요.

참석자 1 11:07
여러분 나도 이거 처음에는 되게 뭔 말인가 했더라고 아저씨들도 되게 헷갈렸었어요.
환경이 여러분들 알겠죠? 헷갈리면 물어보세요.
알겠죠? 라짓하고 확률하고 차이점 알겠어요 여러분 라짓은 확률로 변하기 전에 실수 값들 결국은 라이 확률이나 마찬가지가 돼.
결과적으로 라짓에서 양수면은 1이고 음수면은 마이너스 사실 우리 나이 플러스면 0.5 넘는 거죠.
확률은 그렇죠 0.5 양 쓰면 0.5 넘는 거고 확률이 수면 라지 음수면은 0.5 안 되는 걸로 이렇게 매핑해야 된다고 지금 얘기 그렇죠 원래 그렇게 하고 있지 지금 라지스틱 함수가 아직 스타트 생긴 모양 그렇잖아요 여러분 안 보여줘도 되지 그쵸 오히려 그림 보여주면서 거기만 한 벌 돼서 말로 하는 게 나은 것 같아요.
이런 거는 됐어요. 감 잡아요.

참석자 1 12:00
라즙 다스를 결국은 지금 뽑아내고 결국은 마지막에 이제 신으로 바꾼 것뿐이지 어쨌든 이게 출력 함수 출력 출력 함수 출력의 출혈 이게 입력층이에요.
그쵸 여기 지금 히든 층이 두 개 있는 거예요. 그쵸 그리고 파라미터가 다 어떻게 된다는 것도 그렇죠 피라미터 요기 요 선에 있어요.
그쵸 선 그쵸 사실 여기도 있는데 알겠어요 여러분 여기도 여기도 위트 있잖아 됐어요 여러분 그래요.
그래서 그래서 사실 요 그림상으로 하면은 가중치가 딱 딱 두 개 두 군데 있잖아요.
여기는 가중치가 몇 군데 있어요? 하나 둘 세 군데 있잖아요.
그러면 사실 이렇게 해야지 가중치가 두 군데 그냥 나왔네 무조건 이거 써야 되네.
어쨌든 아예 한 군데 두 군데 없앨 수가 없구나 이것도 이것도 없앨 수가 있었는데 왜 안 되지 여기 있구나 여기에 이렇게 해야 단수가 두 개다.
그렇죠 여러분 됐어요. 진짜예요 여러분 뻥이 아니라고 알겠어요 그래요.

참석자 1 13:10
그다음에 그래서 여기 지금 교과서 교과서 그림에서 여기 지금 입력 층에서 여기 입력이 이게 피처들의 개수예요.
피처들 개수만큼 피처에가 만약에 n 게 있으면은 입력이 사실은 1 플러스 n으로 있는 거 보면 된다고 하나는 1까지 무조건 들어오는 거죠.
바이러스를 위해서 알겠죠 그렇죠 여러분 이렇게 먼저 그냥 바로 들어가지 말고 코드로 들어가지 말고 먼저 수상적으로 알아야지 제대로 이해한다고 알겠어요 됐어요.
여러분 무슨 말인지 아까 저기 x1 x2 있었으면 사실 1도 있다고 직접 PC에 들어가면 1도 있어.
사실은 그렇죠 집값이 아니지 거기서는 평수였지 평수가 피치로 들어오면 사실 1이라는 숫자도 하나 더 있다고 입력해 바이어스 치로 생각해서 알겠죠.
그다음에 여기 층이 누런이죠 그쵸 누런 누런 들리죠 누런 한 개가 아니라 여러 개 있을 수 있어요.
그쵸 보통 한 개 안 하죠 그렇죠 여러 개 하죠 그쵸 그리고 거기 각 입력에다가 그 뉴런하고 연결된 게 가중치예요.

참석자 1 14:16
그렇죠 그림 요거 요거 요거 요거 알겠죠 그리고 여기 입력에 얘가 1이 있다고 하면 사실 여기 1 표시 안 돼 있지만 여기 이렇게 누르냐 이렇게 표시돼 있어요.
여기서는 1 요거 요거 여러분 까먹지 마요. 여기 여기 일이 이렇게 연결된 거나 마찬가지 여기 가상의 일이 이렇게 연결된 거랑 마찬가지라고요.
알겠죠 여기도 지금 여기가 웨이트가 있잖아요.

참석자 1 14:43
그다음에 여기 이렇게 층을 통과시킨다. 계속 무식하게 입력 들어온 거를 전부 다 가중치를 다 곱해 입력 하나하나에 대해서 그렇죠 그렇죠 무식하게 봐봐요.
다시 이 그림을 그 여기 입력 들어온 거 있잖아요. x1 2 여기다가 가중치를 여기 다 곱하지 다 곱해 다 곱해 가중치가 얼마만큼 곱하는 거냐면은 여기 지금 이 다음 층에 있는 뉴런의 개수만큼 곱하는 거예요.
한 개가 아니라 4개로 해놨지 여기서는 뭔가 더 잘 표현해 보려고 한 거예요.
여기를 한 개만 할 수도 있어요. 그 개 같으면 리뉴얼 이그레이션 같은 거 한 개만 있는 거지 딱 여기 여기다 하나씩 하나씩 곱하는 거잖아요.
근데 이거 늘리면 또 하나 배고파지 그래서 더 많이 해보려는 거예요.
더 많은 표현을 민요리 글을 여러 번을 만드는 거지 이 선을 여러 개 만들어가지고 닦고 시키려고 하는 거죠.
그쵸?

참석자 1 15:48
그리고 여기 지금 액티리지 펑션도 보통 디폴트가 탄젠트네 탄젠트 하이퍼블리 탄젠트인데 얘도 이제 마이너스 1하고 1 사이로 이렇게 생긴 놈이고 마이너스 1하고 1 사이에요.
여러분 매워요 렐루는 뭐라고요? 여러분 생긴 게 보여줬지 0 이하면 0으로 만든 음수는 전부 다 0으로 만들어버려 음수를 다 잘라버리는 거예요.
이게 낫니니얼 펑션의 핵심은 뭐냐 하면 리뉴얼 구간이 있는데 리뉴얼은 반드시 있어야 돼요.
그냥 앞에 거 전에 거를 그대로 통과시키는 부분이 반드시 있어야 얘네들이 얘 리니어 리니어지 얘네들이 난리 난 리니언인데 리니어 아니면 난 리니어지 뭐 그렇죠 언리어라고 안 하고 난민이라고 하면 이제 미녀라고 해요.
여러분 렐루는 어떻게 생겼는지 여러분 알죠? 봤죠?
이 슬라이드 있잖아요 0 이하는 전부 다 0으로 만들어버려요.
그쵸? 음수는 다 0으로 만들어 음수를 잘라버려.
그쵸? 음수 통과 안 해 음수는 통과 안 하는 거예요.
그쵸?

참석자 1 16:43
얘는 탄젠트는 전부 다 마이너스 1하고 이 사이로 만들어버려요.
그리고 마이너스 1이하고 얘도 괜찮잖아요. 여러분 좀 잘라버려도 마이너스 1하고 이 사이로 다 매핑하는 거고 얘는 시그모이드는 영화을 이 사이에 매핑하죠.
그쵸 그쵸 여러분 그쵸 그래요. 더 정확하게 잠깐만 이거 여러분 내가 내가 했어요.
잠깐만 현대통상 생긴 거 보여줄게요. 여러분 몇 시간 30분 나중에 태풍 진도를 나가려고 이제 지금도 잠깐만요.
어디 갔지? 여기에 DNA 이센스 있어 안드이니 때문에 삼수 생일이 뭐야 지금 내가 말로 떠들고 있는데 액티레이션 컴퓨트 내용 갖다 놨잖아 내가 지웠나 여기

참석자 1 17:39
뭐 라지 여기 여기 있잖아 여기 여기 여기 다시 여기 스텝 펑션은 여러분 안 써요 실제로 옛날에 썼었거든요.
옛날에 맨 처음에 포스트롬 나왔을 때 썼었어요. 근데 이거는 지금 미분이 안 되게 생겼잖아요.
여기 바위가 그쵸 미분이 안 되게 생겼잖아요. 너무 그래서 안 쓰고 사실 렐루도 미분이 안 되는데 얘는 렐루도 여기서 미분 안 되잖아요.
사실은 그래서 약간 문제 삼아서 또 바꾼 것도 있어.
셀로니 이렇게 해서 약간 미분대 부드럽게 만들어놓은 거 있어.
스무드하게 근데 별 문제없어서 그냥 다 써요. 알겠죠?
옐로 많이 쓰고 이거 이게 그래프가 x축 y축이 달라가지고 여기는 2 4 이렇게 돼 있고 여기는 지금 1 1 마이너스 1만 나와 있어가지고 생긴 모양이 약간 더 이렇게 좀 이렇게 이쪽으로 돼 있어요.

참석자 1 18:21
원래 이렇게 똑바로 45도로 가야 되는데 이게 지금 45도가 아니라 70 도도 같이 보이잖아요.
그죠? 뭔 얘기인지 알겠어요 그 x 스케일이 달라서 그래 알겠어요 여러분 됐죠 이거 이게 사실 그냥 그대로 통과시키는 거예요.
여러분 멜론은 알겠어요 탄젠트 그리고 시글 모이드 먼저 해 시글 모이드는 영화하고 1 사이로 바꿔버리죠 그쵸 그쵸 0하고 1 사이로 바꿔버리잖아요.
그런데 이게 그 숫자 보시면 약간 감을 잡으면 음수는 0.5보다 작은 숫자 양수는 0.5보다 큰 숫자 이러겠다.
그쵸 그쵸 이거 보이죠 여러분 아까 감 잡으라고 감 탄젠트는 하이퍼볼리 탄젠트 있죠 요거 요거 탄젠트 있지 얘는 음수는 전부 다 음수를 그대로 냅두는데 마이너스 1이 최하야 그쵸 무한대로 안 하고 여기서부터 한 마이너스 2부터는 그냥 아무리 더 여기 거의 리니언데 그쵸 그렇잖아요.
여기 아까 마이너스 1으로 무조건 바꿔버려요.

참석자 1 19:23
그쵸 마이너스 1도 아무리 작아도 신고 이드는 0으로 바꾸는 반면에 그쵸 그쵸 여러분 여기도 현재 이쪽도 음수 양수는 무조건 일로 해버려요.
그쵸 아무리 커도 현재 탈폴로 컨텐트가 알겠어요.
여러분 스텝 펑션 비슷한 놈이지 사실은 그렇죠 감이 오죠.
여러분 그런 놈이야 지금 그래서 이거 진짜 3개를 제일 많이 써요.
시그모이드랑 타네트라 엘로를 나중에 이제 배우겠지만 보통은 렐로를 쓰고 맨 마지막 출력단에 확률 하고 싶은 시그모드를 쓰고 확률 바꾸고 싶을 때 근데 이제 맨 마지막 출력 단위 이제 확률이 아니라 뭔가 어떤 바운드 시키고 뭔가 너무 이제 멜로는 커지는 경향이 있으니까 나중에 이제 RNN이라는 게 있어요.
리콜시브 뉴럴 네트웍이라고 수납 네트웍이라고 계속 계속 뉴런 한율론에서 연사를 반복시키는 게 있거든요.
근데 그게 또 이제 트랜스포머 나오면서 별로라고 그랬다가 다시 또 뜨고 있어요.

참석자 1 20:25
딥시크 이전 이후에 나온 것들도 뭔가 생각 다시 해봐 다시 해 다시 해봐 다시 해봐 리딩크라 그래가지고 뭔가 좀 하드웨어 적게 쓰면서 추론의 성능을 향상시키기 위한 방법으로 그냥 RNN을 내부적으로 쓰는 게 또 유행하고 있어요.
어쨌든 그렇게 할 때는 타결트를 또 많이 써요. 왜냐하면 너무 커지니까 같은 거를 계속 돌리려고 그러면 레로 하면 너무 이제 발사하는 경향이 있으니까 리커시브하게 연산이 들어가기 시작하면 탄진 하나 써요.
이런 경향이 있어요. 여러분 미리 얘기해 놓으면 나중에 좋지 않을까 싶어서 이렇게 얘기해 놓는 거야 알겠죠?
됐어요 그래요. 어쨌든 얘가 얘네들이 세 가지가 많이 쓰이고 그래서 다시 계속 하면은 이거를 다시 보여주면 얘네들이 지금 이렇게 하나로 해서 어쨌든 무조건 작업할 수 있고 그리고 그냥 다 더하는 거죠.
여러분 그렇죠 원래 다 더하잖아 더한 다음에 액티베이션 통해 통스통만 시키는 거잖아요.

참석자 1 21:27
또 하나 더 했어 오늘 더 더 했어 그러면 여기 또 똑같은 곳으로 하는 거잖아요.
그렇지 선이 다른 선이 나오겠지 만약에 선이 이렇게 제가 딥러닝 여기 할 때도 앞에서 때 이렇게 생긴 모양은 도저히 안 되잖아요.
근데 이제 이게 하나 둘 셋 넷 다섯 개 됐잖아요. 이렇게 연결 붙었죠.
이거 지금 누런이 5개면 이렇게 만들 수 있는 거예요.
감이 와요. 여러분 이런 감을 잡아야 되는데 책이 잘 안 나오잖아요.
책이 안 나오는 이 동영상에 나오지 이 동영상이 제가 왜 봤겠어요?
여러분 사람들 좋다고 하고 그러니까 저도 봤겠지 이렇게 설명해야 되는데 이러면서 그러니까 이렇게 설명했어야 되는 거 좋잖아 제대로 이해되지 않아요.

참석자 1 22:11
여러분 그러니까 여기 무려 5개 표현하려고 그러는 거지 알겠죠 여러분 그래서 이게 지금 난 니니를 이렇게 한 거 잘라버리고 이렇게 한 거 사실 이렇게 한 걸로 잘라버리는 게 아니라 이렇게 사실 이거랑 이거랑 합쳐가지고 뭔가 이렇게 상쇄될 수도 있고 막 그래서 복잡하긴 한데 원래 리니어 온 것들이 잘린 게 있고 또 여기도 여기도 와서 잘리고 또 일로 합쳐져 가지고 이렇게 되고 막 이래야 될 거 아니야 복잡하죠.
사실은 얘는 여기 계속 갈 거 아니야 여기 보세요. 여기 여기서 얘가 인진으로 잘렸다고 해도 얘는 이쪽에서 잘랐으면 이쪽으로 가잖아요.
여러분 그쵸 그래서 랠리를 했으면 한쪽은 가야 되잖아.
한쪽만 잘리고 그럼 두 개가 합쳐져 가지고 이게 만들어지고 그렇다고 그리고 얘는 또 잘리고 이런 식으로 하는 거 한 쪽만 자르잖아요.
일본은 다 자르지 못하니까 그런 그렇다는 거죠.

참석자 1 23:03
뭐 그래요 그래서 어쨌든 두 개 합치고 두 개 합치면 어쨌든 더 꺾은 게 또 하나 더 생길 수 있는 거고 그렇죠 그런 식으로 방향을 잡으세요.
여러분 저번에 어쨌든 얘네들 다 더하고 그러니까 여기 여기서 각각에서 온 거 다 리뉴얼 펑션 하나 더 생긴 것 같아요.
또 생기고 이해되죠 여러분 그리고 어떻게 되느냐 얘네들이 다 결괏값을 내잖아요.
그쵸? 액티베이션 포지션 통과해서 통과시킨 거 선이 나오지 사실 레티 선이 나오는데 걔네들은 또다음에 그 짓을 하는 거지 여기에 뉴런이 또 있으면 그렇죠 그런데 없어도 출력 계층에서도 어쨌든 뭔가 또 곱해가지고 다 마지막에 한다는 거지.
여기도 액티베이션 폭력이 숨겨져 있는데 사실 출력 계층에도 액티베이션 통전 통과시킬 수 있어요.

참석자 1 23:48
여러분 알겠어요 숨겨져 있지만 여기는 사실 여기 지금 그래서 여기 문제 자체에 대해서 문제 성격에 따라서 프라블럼 타입에 따라서 사실은 마지막 출력 계층에 프레시피케이션이면은 아마 시구 모듈로 돼 있을 거고 리그레션이면은 그냥 원래 값으로 돼 있을 거예요.
그냥 그냥 아무 애플레션 형제 없는 걸로 리뉴얼 돼 있을 거예요.
진짜예요. 여러분 알겠어요 그래서 여기 아이고 그래프로 얘기하면 여기 가중치 가중치 되어 있고 층 되어 있는 게 층이 하나하나 여러 뉴런이 있는 거고 그 뉴런의 뉴런과 입력과 연결된 게 다 가중치예요.
그렇죠 바이러스도 있고 그렇죠 그리고 또 유럽의 개수만큼 입력으로 변해요.
사실 얘 입장에서는 이 입력이 들어온 이 친구 입장에서는 이게 앞에 들어오는 게 그냥 입력이랑 똑같다고 그게 없어요.
그렇죠 뭔지 몰라 그냥 기계적으로 하는 거예요. 기계적으로 완전 기계적이죠.
이해되죠 여러분 그래서 잡으면 돼요.

참석자 1 24:53
마지막에 이제 예측 값이 나오는 거죠. 그쵸? 예측값이 확률일 수도 있고 그쵸 그냥 어떤 숫자값 숫자일 수도 있고 그렇죠 이런 식으로 돼요.
그래서 어쨌든 이게 이제 여기 바로 신경망이 나왔지만 머신러닝은 어쨌든 가중치의 정확한 값을 찾는 거고 그쵸?
됐죠? 됐어요. 그렇죠 그다음에 여기로 또 교과서에 다음 페이지 넘어가는 거는 옛날에도 얘기했지만 이게 이거 슈퍼바이즈으로 왔네요.
그렇죠 지도 학습에서 예측값 말고 여기 실제로 여기로 끝나는 여기까지 끝나는 게 원래 이제 테스팅 단계에서는 이렇게 되는데 가중치가 멀쩡하면 실제로 이제 러닝이라는 과정을 이 트레이드한 과정은 진짜 타겟을 주고 그쵸 손실 환수를 통과해 손실을 가지고 다음 페이지에 여기 다 밑에 있지.
손실 여기 옵티마이저는 신경 안 써도 되지만 어쨌든 가중치를 업데이트한다는 게 중요하지.

참석자 1 25:51
그쵸 그렇죠 여러분 가중치를 업데이트하는 거 가중치를 바꿔야 될 거 아니야 제대로 치를 그쵸 여러분 자전거 타다 오른쪽으로 넘어지면은 왼쪽으로 바꿔야 된다고 왼쪽으로 넘어질수록 오른쪽으로 가라고 해야 된다고 했죠.
그쵸 근데 옵티마이저라는 건 뭐냐 옵티마이저라는 거 어느 정도 얼마큼 움직일지를 좀 조정하는 거예요.
사실 안 배웠지만 여러분 대충 있다 그랬잖아요. 아담 같은 거 있다고 제가 보여준 적이 있었어요.
여러 가지 많은 거 가속 가속화시키는 거 내가 보여준 거 이거 여기 다른 슬라이드에서는 이거였어요.
여기 앞에 트레이닝이 있었구나. 여기에 옵티마이저 정체가 뭐냐면은 워낙에 파라미터가 많으니까 파라미터를 한꺼번에 뭔가 확실하게 다이런트 한 걸 조정해야 될 거 아니에요?
그쵸 걔가 뭔가 오류를 많이 줄이고 있으면 걔를 많이 조정해야 될 거 아니에요 그래서 이거 안 된다 이렇게 여러 가지 이렇게 뭔가 저기 뭐 NG 아다그라드 마다 이런 거 있잖아요.

참석자 1 26:52
이런 거 이런 게 이제 원래 SCD가 스토캐스틱 그레덴티센트 있잖아요.
사실은 배치예요. 사실 이거 되게 천천히 가는데 여기서는 그냥 무식하게 하나씩 하나씩 그냥 원래 알파 뭐야 러닝 메이트를 고정시키면서 하는 건데 여기서는 특별히 이제 손실을 줄이는 그런 파라미터를 훨씬 많이 업데이트해가지고 여러분이 사실은 이렇게 제가 자전거 탈 때 왼쪽으로 움직이는 건 단순하겠지만 앞으로 가는 것도 있고 뒤로 가는 것도 있고 약간 이상한 온갖 방향으로 내가 조절할 수 있잖아요.
왼쪽으로 가는 게 다미언트 한 거지 여기서는 만약에 뒤로 넘어지면 거기를 많이 조정하게 해야지 빨리빨리 배울 거 아니에요?
그쵸 그런 거를 조정하는 거 있잖아. 여러 가지 파라미터 중에서 특정 러닝 메이트 특정 파라미터의 러닝메이트를 더 가속화시키는 거 있잖아.
업데이트를 그런 게 아티마이저예요. 여러분 이해돼요.
약간 선생님 같은 것도 이쪽으로 가봐요. 이쪽으로 가봐요.
있죠 그렇죠 그렇게 얼마큼 해봐요.

참석자 1 27:46
이런 거 있죠 근데 이제 제가 자세히 안 한 거는 거기서 뭐 있다는 것만 알아두라는 거지 그거 갖고 더 하지는 않을 거라는 거죠.
뭔지는 알아야지 그래도 그거 바꿔볼 필요가 있는 거죠.
나중에 또 할게요. 여러분 그래서 아티마이저가 여기 왜 껴 있냐 가중치를 업데이트하는데 어느 가중치를 얼마만큼 업데이트할지 얘가 좀 조정해 주는 거야.
알겠죠? 그거에 따라 학습 속도가 달라지겠다. 그쵸 앞뒤 바지가 별로 안 좋으면 되게 느리게 배울 거고요.
아니면 학습이 안 되든지 이럴 수 있죠 그렇죠 이해됐어요.
여러분 대충 대충 이 정도로 이해하고 그다음에 됐지 그림 말 하면서 그림 말고 교과서에 진하게 적혀 있는 거 찍으려고 그러는데 그래서 이제 여기 지금 38페이지 설명 다 했고 39페이지에 지금까지 딥러닝 성과 이런 거 있잖아요.
이거 이거는 뭐 여러분 그리고 넘어갈게요. 여러분 당연히 여러분 이거 딥러닝 너무 좋은 거 알죠?
여러분 그렇죠 지금 넘어갈게요.

참석자 1 28:48
그리고 1.2절이 딥러닝 이전에 머신러닝 강략한 역사 있잖아요.
요거 요거 보면은 이거는 여러분 좀 알 필요가 있는 게 지금도 온갖 과목들이 있어요.
저기 보면은 통계학 쪽 있잖아요. 우리 우리과 통계학 우리 통계학과 있잖아요.
법인은 옛날부터 자기들이 머신러닝 해왔다고 주장할 수밖에 없어요.
진짜로 최초의 머신러닝은 다 거기서 했거든 지금도 여러분 한명 초통 모델링은 너무너무 유용해요.
지금 선거 예측 같은 거 너무너무 잘 되잖아요. 그것도 확률 모델링이 그거 여기 보면 43쪽 확률적 모델링 있잖아요.
여기 사실상 확률적 의견이 이거는 지금도 유효하고 이건 수학적으로 잘하는 거잖아요.
그렇죠 이게 원래 이거는 지금도 살아 있어요. 그러니까 계속 강조하지만 제가 인밸리 시스템 쪽도 하지만 운영 체제도 얘기하지만 운영 체제를 왜 배우는데 지금도 운영실에 나와 있는데 뭔가 근본적인 거 하려면 거기를 흔들어야 돼요.

참석자 1 29:40
그쵸 그러니까 여러분 미사일이나 로케트나 이런 거 분타에서는 여기서 이기려면 연체제 잘해야 돼.
진짜로 그래야지 싸게 미사일 많이 쏠 수 있거든요.
비싸지 않게 한 번 쏘는데 네 발 몇백만 원 쏘는 게 아니라 한 번 쏘는데 천 원씩 쓰면 얼마나 좋아 진짜로 그러기 위해서는 근본을 뭔가 해야 되잖아요.
싸게 만들고 뭔가 이렇게 뭔가 우리가 잘하려고 그러면 원래 근본으로 돌아가야 되는데 근본이 확률이라고요.
여러분 알겠어요. 여러분 그래서 옛날부터 맨 처음에 하는 머신러닝 확률적 모델링이고 그 제너럴한 통계학과 쪽에서 수학과가 여러분 잘 뜨는 이유는 사실 다 통계 때문이에요.
알겠죠 여러분 그래서 통계치 모델링은 옛날부터 원래 머신러닝이 근본이고 지금은 중요하다.
통계학과도 관심 있어서 하던 거라 거기도 뭔가 통계학과 머신러닝 이런 거 많이 있거든요.
그게 근본이거든요. 사실은 그다음에 신경망이 초창기 신경망이 83페이지에 나와 있잖아요.

참석자 1 30:28
그쵸 뭐 있었대 그쵸 제가 얘기했지 맨 처음에는 미분 이런 거 잘 신경 못 쓰고 이제 식구 만들었었고 그리고 다음 페이지 244쪽 넘어가면은 르넷이라는 부르는 신경막이 있었대요.
워낙 유명해가지고 벨령구소라는 것도 있었어요.
189년에 벨연구소가 여러분 옛날이 아니라 여기는 여전히 좋죠.
이런 데서 먼치를 했었고 그다음에 1.2.3에 커널 방법이라고 적혀 있잖아요.
커널 방법 커널 방법 커널 방법이라고 혀 있는데 이게 조심할 게 커널이 커널 방법 여러분 커널이라는 용어는 저한테서 배운 적 있지 않아요 커널 운영 체제에서 제가 운영 체제의 좁은 의미는 커널이고 넓은 의미는 그쵸 층도 들어가고 이런다고 설명해 줬잖아요.
근데 여기는 코널이란 말을 써요. 그리고 문제는 이제 이게 NVIDIA에 투다 라이브러리에서도 커널이라는 개념이 나와요.
또 커널이라는 말은 또 이쪽에서 또 많이 너무 많이 쓰기 시작했어요.

참석자 1 31:33
그래서 진짜로 헷갈려 이제는 왜냐하면 운영체제와 커널이 운영 체제의 커널과 이 딥러닝에서의 커널이 동시에 쓰이기 시작했거든.
이 세상에는 왜냐하면 딥러닝이 너무 중요해졌잖아요.
하드웨어 아트메이션 너무너무 중요해지고 그렇다고요.
그래서 여기 또 있는데 이 코너는 또 그 코너는 또 아닌데 어쨌든 커널이라는 말을 사람들 되게 좋아해요.
그러니까 근버전인 거잖아 코어라는 말로 너무 좋아해 그래가지고 코는 방법은 원래 뭐였냐면은 약간 알고 있으라는 거죠.
그냥 터널 메소드라는 게 있었고요. 걔는 sv에 적혀 있죠.
여러분 제가 보였죠. SVL이 뭔가 고의로 저차원을 고차원으로 바꿔가지고 평면 같은 거 만들어가지고 분류하는 그런 놈인데 어쨌든 바이너리 분류하는 게 모든 거의 근본이잖아요.
여러분 뭔가 이진 분리하는 게 모든 거의 근본이죠.
그 전부 다 예스 노 예스 노랑 다르잖아요.

참석자 1 32:17
그래서 서포트리트 머신이라는 게 있었고 그래서 지금 보면은 초평면 이런 게 있는데 마진치에 대한 이런 거 있는데 제가 이거 이거 다 저기 머신러닝에서 해야지 사실 그렇죠 이런 게 있어요.
여러분 굉장히 그리고 옛날에 이 몇 년도에 90년대 이때는 거의 이게 대박이었어요.
여러분 너무 뭐가 잘 풀려 안 되던 게 그래서 SVM을 여전히 좋아하시는 분들이 많아요.
신공자 근데 사실은 딥러닝이 더 잘해. 사실은 이거보다는 SVM이 딥러닝의 서브 셋이에요.
비슷해 사실은 그 증명한 논문도 많이 나왔어요. 2천년대에 SVM이 딥러닝 서비셋이라는 게 근데 별로 주목 안 받다가 나중에 이제 딥러닝이 막 뜨니까 여러분 잘 모르는 것 같아서 얘기해 주면은 2011년도에 서울대학교 컴퓨터 공학과 미달이었던 거 알아요.

참석자 1 33:12
여러분 2011년도 서울대 컴퓨터 공학과 미달 외대도 그래서 컴퓨터 공학과가 너무 위험해서 컴퓨터 전자 시스템 공학부로 이름 바꿨고 모르죠 여러분 너무 옛날 일 같지 2015년 그렇게 옛날 아니잖아요.
여러분 살아 있을 때잖아요. 그렇죠 그렇죠 그런데 지금 어때요?
컴퓨터가 우리보다 있길래 좋아 그렇죠 기분 많이 나았대.
우리 거 전복 뺏어가려고 막 난리 치고 전복 전보 일을 달라고 사실 서울대는 전기정보공학부잖아요.
저기 옛날에 컴퓨터랑 합쳤어요. 컴퓨터 하도 사람들이 안 오니까 거기 나오면 실업자 된다고 근데 딥러닝 때문에 이렇게 됐지 사실은 사실 그렇죠 여러분 알 수 없어.
그렇죠 은근히 그래요. 그래서 어쨌든 내 말은 SVM도 SVM도 막 약간 그런 면이 있었어요.
SVM 최고다 이러다가 딥러닝 나오고 나서 약간 여전히 SVM이 최고인 줄 아는 사람들이 있다는 게 문제지.

참석자 1 34:06
그러니까 여러분이 항상 남의 말 그대로 듣지 말고 내 말도 그대로 듣지 말고 항상 여러분 지금 어떻게 될지 모른다니까 사실 그렇지 세상에 이해돼요.
여러분 여러분 잘 모르잖아 폭풍 아과 기다렸다는 거 그렇잖아요.
여러분 서울대도 기다렸었어요. 서울대 미달인 것도 되게 드물잖아요.
여러분 지금은 의대 안 가고 하기도 하잖아. 아이가 모르잖아요.
여러분 잘 그래요. 저 때는 의대보다 우리가 내가 높았는데 전기전자가 그전에 화학과도 높았다는데 알 수 없잖아.
내가 나 정 수서가 팔았는데 이렇게 했는지 여기 왔는지 모르잖아.
어쨌든 무슨 얘기인지 알겠어요. 여러분 그러니까 이게 세상이 변하다.
여기서 보면서 용어 이게 역사를 약간은 알 필요가 있다고요 여러분 알겠죠 그래서 당연히 그런 거지 그래야 더 이상 얘기하지 맙시다.
그래요.

참석자 1 34:51
그래서 그래서 이게 여기서 코널이라는 용어가 그렇게 여기 쓰이고 있다는 거 알고 있어요.
여러분 알겠죠 코널이라는 용어는 맥락에 따라 달라질 수 있다는 것도 알고 계세요.
여러분 그다음에 SVM에서도 SVM이 코너 방법의 일종이었고 커널 리소드라는 거에 여전히 사이클런이라는 물은 머신러닝 쪽에서는 여전히 유용하지만 가벼워서 좋긴 해요.
여러분 만약에 이걸로 돌릴 수 있으면 머신러닝보다 훨씬 머신러닝이 딥러닝보다 가벼우니까 좋아요.
알겠죠 근데 대부분의 복제 성문제는 얘보다는 머신러닝이 그다음에 디시전 트리 랜덤 프로젝트가 45쪽에 적혀 있죠.
45쪽 45쪽 이런 게 있어요. 여기 요거 요런 거 보여줬죠 그렇죠 이것도 굉장히 유용하고 지금도 많이 쓰여요.
이것도 이거 진짜 많이 쓰여요.

참석자 1 35:36
지금 너무 간단하게 가지기 때문에 그래서 이제 신경망으로 넘어와서 42쪽에 적혀 있고 여기 다 적혀 있는 거 이거 다 읽어봐도 기억도 안 날 테니까 넘어가고 그다음에 48페이지 딥러닝의 특징 이제 날림으로 넘어갈 수밖에 없어요.
여러분 이제 뒤에 할 게 많아서 딥러닝의 특징 48쪽에 딥러닝 특징 적혀 있죠.
38분 딥러닝 숙제 딥러닝 수지 너무 많아 정말 말이 많죠 그쵸 층 거치면서 한다는 거 그쵸 층 거치면서 여기 여기 적혀 있는 거 여기 보면은 교과서에 진한 색으로 나와 있는 건 중요하지 진한 색으로 나와 있는 거는 다 봅시다.
여기 시나식으로 나와 있는 거예요. 여기 특성 공학이라는 거 적혀 있죠.
특성 공학 맨 처음에 특성 공학 교과서에 지금 48쪽에 특성화가 적혀 있잖아요.
그쵸?

참석자 1 36:37
특수 공학 특성 공학이 뭐냐면은 데이터의 조표를 수동으로 만든다는 거잖아요.
그쵸? 이해되죠 여러분 적혀 있죠. 피처 엔지니어링 여러분 피처 엔지니어링 중에 하나가 뭐였냐면은 여러분 저한테 배운 것 중에 하나 있었어요.
피처 스탠다드 이제이션 하는 거 있었잖아요. 왜 영화 맥스민으로 막 바꾸고 그러는 거 피처가 어떤 값을 치고 어떤 값을 자꾸 이러는 거 변화 범죄율은 퍼센트로 돼 있고 예를 들어서 마트 비율 같은 거는 소수점으로 돼 있고 영감이 그러면 이거 특성이 값이 다르니까 비슷하게 맞춰야 될 거 아니야 그것도 특성 공감의 일종이에요.
여러분 특성 공학이 뭐라고 적혀 있어요? 여러분 데이터의 좋은 표현을 수동으로 만드는 걸 특성 공학이라고 한다잖아요.
그쵸? 보여요. 여러분 알파 특성고가 여기 바로 교과서에 특성화가 따랗게 적혀 있잖아요.
진하게 그 앞에 뭐라고 적혀 있어요? 데이터의 좋은 표현을 수동으로 만든다고 돼 있잖아.

참석자 1 37:27
그러니까 이게 뭔가 학습할 때 먼저 데이터를 가공해서 준다는 거지 이해돼요.
여러분 이해되냐고 데이터 좋은 표현을 만든다고 가짜로 일부러 전에 얘기했듯이 우리가 민 맥스 만들고 하는 거 있잖아요.
또는 정규 분포 바꿔서 만드는 거 그런 게 다 투성공하의 일종이라고 저희 보여줬잖아요.
여러분 뭔 말인지 모르는구나 그래요. 자기가 보여줄게요.
여러분 이게 오프라인 수업인데 어디 갔어? 여기 트레이닝 베이스에 제가 여기 이런 거 이렇게 이렇게 돼 있는 걸 이렇게 바꿔주고 하는 거 있잖아요.
네 다 비슷한 레인지로 세타도 비슷하게 만들어지는 거지 입력이 비슷하게 만들어서 이해돼요.
여러분 그래서 여기 어디 갔어? 여기 여기 여기 나는

참석자 1 38:23
피처 스케일링이라는 게 피처 스케일링도 피처 스케일링도 피처 엔지니어링의 일종이라는 거지 피처 스케일링이라고 표현 스케일링이 뭔지 알죠?
여러분 값을 비슷하게 만들어주는 거 그쵸? 기억나요?
여러분 그것도 특성 공항이 이쪽이라고 알겠어요 그래요.
그리고 여기도 설명을 좀 더 해야지. 특성 공학이 여기서 나오고 더 이상 안 나오니까 특성 공학이 여러분 여기서 꽂히지 마시라는 얘기를 하는 거예요.
딥러닝에서 필요가 없어요. 그러니까 여러분이 공부를 이제 특성학을 하겠다.
이러면은 시대 붙이는 사람이야 알겠어요 여러분 SVM을 하겠다.
이것도 이상하게 보이는 거지 리뉴얼 리그레이션을 하겠다.
이것도 이상한 거라고 지금 세상에 약간 알려주는 거예요.
여러분 알겠어요? 디시전 트리나 랜덤 프리스트는 여전히 유용한데 또 아까 확률 쪽 모델링 잘하는 것도 유용하다고 여전히 약간 그런 면이 있어요.
알겠어요 여러분 완전히 서비스만 하는 거는 하는 게 별로라고 그쵸?

참석자 1 39:14
이해돼요 여러분 그래서 그런데 제가 이렇게 얘기해도 만약에 SVM이 딥러닝보다 훨씬 좋은 거를 발견해서 여러분 하면 그것도 논문이야.
상사 내가 얘기하는 거를 뭔가 반대로 하잖아요. 직관에 반대되는 거 그러면 논문이 탄생하는 거예요.
진짜로 좋은 거지 그렇죠 그러니까 삐딱한 사람이면 잘하는 거예요.
연구를 위해서 진짜로 나도 삐딱하고 날짜는 알겠어요 여러분 그래요.
그래서 계속 얘기하면은 뭐야 여기 이거 이거 텍스 플로우 플레임을 하는 요거 여기서 특성화를 한다고 하는 거는 예를 들어서 이렇게 생긴 데이터 있죠 요거 요거는 여러분 보면 어때요?
이거는 여러분 원점에서의 제곱 있죠 원점에서의 거리가 제곱으로 표현되잖아요.
사실은 x 1 제곱 y 1 제곱 그렇잖아 절대값이 중요한 게 아니라 그렇지 보이죠.
여러분 원점에서 거리 재고로 표현될 수 있잖아. 그래서 이걸 넣어주면 훨씬 진짜 잘 되겠지.

참석자 1 40:08
이거 리뉴얼로 해도 돼 이거는 심지어 이걸 넣어주면 그러니까 이거 보여요.
여러분 이걸 이게 특수 공학이라고 특수 공학이 특성만 잘 지어주면은 뭔가 학습이 돼버리는 거 있잖아요.
아무리 웨이트가 작고 계산량이 적어도 잘 되는 거예요.
이해돼요. 여러분 그런 게 직선 모양이에요. 느낌이 와요.
여러분 이런 건데 딥러닝에서는 약간 좀 덜 필요해졌어요.
왜냐하면 배치 노멀레이션 하나 생겼고 그래서 딥러닝에서는 덜 필요해 알겠지 상대적으로 나중에 또 나오긴 하겠다.
그래서 그렇게 놓고 있고 그다음에 여기 또 16페이지 여기 있지 48페이지의 중간에 중간에 탐욕적 방법이 아니라 이렇게 적혀 있죠.
탐욕적 여러분 그리디 그리디 그림이라는 거 알고리즘에서 브리디 알고리즘 이런 거 배웠나 그리디 알고리즘은 다 뒤지는 걸 말하는 거죠.

참석자 1 41:03
그쵸 디다 디즘에서도 사실은 또 최적은 아닐 수 있잖아요.
그리디 알고리즘이 그런 것도 알죠 여러분 하다가 그냥 이거 괜찮은 것 같아요.
갖고 끝나는 거 이런 거죠. 이 탐욕적 알고리즘이 맞대요 아니래요.
딥러닝은 아니에요. 아니에요. 그쵸 그게 탄력적 알고리즘이 아니라는 거가 무슨 얘기냐면은 이걸 또 설명하면은 얘네들이 이렇게 지금 3개를 중간에 만들었잖아요.
그쵸 얘네들을 다 다 쓰려고 하는 게 아니라 잘 되는 거를 그냥 살리는 약간 그런 시켜 보다가 웨이트를 크게 하는 게 그냥 잘 살리는 거잖아요.
사실은 다 뭔가 중요하게 생각하는 게 아니라 추상적으로 뭔가 바꿔서 잘 되는 놈으로 가는 거야.
학습을 그래서 여러분 이게 딥러닝도 하나 쓰는 게 아니라 여러 개 쓰는 게 더 좋아요.
딥러닝 뉴럴 네트워크도 얘도 지금 한 개씩 쓰는 게 아니라 여러 개 쓰는 집단 지성이 아니지 내부적으로 집단 지성이라니까요.

참석자 1 41:52
지금 내부적으로 여기서 여러분 팀워크가 중요한 누가 누가 어떤 데이터에 대해서 누가 다를지 몰라 데이터가 다 계속 보드 데이터가 새로 들어오고 이러니까 새로운 문제는 항상 여러분 항상 여러 문제 해결 능력이 중요한 게 계속 여러분 대학원이 왜 있고 공학이 왜 이렇게 많냐 사람 많이 필요한 이유가 계속 계속 예측하지 못한 문제가 새로운 문제가 생겼을 때 잘 대응하려고 하면 누가 말할지 모른다니까 그쵸 또라이 같은 그러니까 지금 이 문제 뒤에 못 풀어도 이 문제 잘 푸는 다른 유연이 생길 수 있고 이렇다고요 여러분 사실 지금 이 뜨는 것 중에 믹스트 베스트 퍼스트라는 게 있어요.
아키텍처가 동러닝에 그거는 모든 뉴런이 똑같은 일을 하는 게 아니라 어떤 뉴런은 어떤 도메인에 대해서 어떤 뉴런은 어떤 도메인 서 이렇게 했다가 동시에 더 돌려서 저기서 제일 잘하는 놈으로 가는 거야.

참석자 1 42:40
그리고 나 지금 그런 게 유행이에요. 여러분 근데 아까 그 철학을 이해할 필요가 있다고 여기서 그리디가 아니라는 거 딥러닝 리디가 아니에요.
그리디가 아니라는 거는 여러 개가 어떤 게 다 뭘 잘할지 모르는데 잘 되는 놈으로 그냥 가는 거예요.
알겠어요. 그러면 그러니까 여러분 숫자 인식할 때는 1 2 3 4 5 7 8 9 27 89 있잖아.
거기서 어떤 유로는 되게 치만 잘 인식하고 있고 모서리만 잘 인식하고 있고 어떤 유로는 동그라미만 잘 인식하고 있어.
그래서 팔이라는 걸 인식할 때는 다른 일로 일을 하고 있고 사실 그러고 있다고 지금 시장 예식할 때 또 다른 이런이 일어나 그래서 결과적으로 판단을 한다는 거지 뭐 그런 식으로 이바 그리리하게 다 같이 일하는 게 아니라 돈에 쓰이고 있는 것들이 계산을 추상적으로 잘 되는 돈으로 동시에 공동으로 학습해가지고 공동 특성 학습 능력 자동으로 전화하면서 그래요.
파격적으로 하는 게 아니에요.

참석자 1 43:31
알겠죠 탐욕적이에요. 그리디가 아니라는 거지 그래서 여기서 맨 마지막에 적혀 있는 거 한번 여러분 다시 한번 봅시다.
층을 거치면서 점진적으로도 복잡한 표현이 만들어진다.
점진적으로 그리고 점진적인 중간 표현이 공동으로 학습된다.
추상적인 걸 계속 만들어서 추상적인 거 갖고 공부를 계속 하고 있어요.
그렇게 해놓고 나중에 점점 더 봅시다. 여러분 서로 점점 직권이 느는 중이라서 그다음에 49쪽에서 머신러닝 최근 동향에 적혀 있는 거 보면은 여기 이거 옛날 거잖아.
사실은 그렇죠 케라스가 제일 많이 쓰인다고 주장하고 있는 게 이 책이 케라스니까 그렇고요.
넘어갑시다. 여러분 그렇죠 화이트지 뭐 이런 것들 여러분 이런 이거 사실 설문조사 이런 거 되게 명태 교육 난리 나는 거 있잖아요.
여러분 알고 있죠 명태 조사 알면 설문조사 어떻게 하냐에 따라서 달라지거든 사실은 테라스가 있더니 그것도 사실은 명태 규식일 수 있다는 거거든 알겠어요 여러분 그래요.

참석자 1 44:30
그다음에 50쪽에도 과학 분야의 사용 도구가 이제 사이킬런이 1등인데 이게 분명히 맞는 측면도 있지만 사실은 또 아 그래요.
사이클론이 뭔지 모르죠. 여러분 머신러닝 수업 시간 중에 써보세요.
알겠죠 컴퓨터 매트릭스 같은 것도 사이클론이 다 있어요.
웬만한 건 딥러닝 빼고는 그래요. 그다음에 교과서 1.3절로 넘어가서 이만하면 다 끝내려고 하는 애들 거는 지금 제대로 다 하는지 파악을 좀 할게요.
왜 딥러닝이냐 그리고 여기 지금 적혀 있는 게 하드웨어 데이터셋 벤치마크 알고리즘에서 이렇게 적혀 있잖아요.
그쵸 알고는 있어야 돼요. 여러분 알겠어요. 사실은 이게 이렇게 넘어가서 하드웨어로 넘어가야 돼요.
하드웨어가 이게 또 웃긴 게 하려면 너무 말이 많지만 하드웨어가 지금 잘 나와서 GPU가 달리기 시작해서 그런 거죠.
여러분 알파고가 처음에 잘했던 게 GPU 갖고 와서 그런 거잖아요.

참석자 1 45:26
그래요. 여러분 mbda가 같이 NVIDIA 지분석을 잘하는 거죠.
그렇죠 그리고 나중에 지금은 이제 딥러닝만을 위한 뭔가가 많이 나오고 있죠.
그래서 지금 여러분 싸우고 있어요. 계속 회사들은 이게 삼대전자도 마찬가지예요.
GPU 갖고 잘할까 mbda 갖고 잘할까 아니면 나만의 하드웨어를 나만의 반도체 만들까 현대전차가 미국의 우리나라에서 지금 수출을 미국에 제일 많이 하는 건지 알고 있었어요.
여러분 그러니까 이렇게 사람을 많이 뽑지 고맙잖아.
그렇죠 여러분 선배들이 많이 가더라도 별로 안 미안 진짜 그냥 그냥 별 볼 일 없어 보여서 다 잘 가더라고 그랬는데 이게 어떻게 가능하냐 여러분 어떻게 가능하냐면 사람을 많이 수출을 많이 해야지 가는 거예요.
그쵸? 그렇죠 여러분 그런 게 왜 가능하냐 보면은 이게 아니 그러니까 지금 그렇게 하는 이유가 이제 학교 전자차에서 싸게 잘 만들어서 그래 거든요.
거기 미국에서 비싸게 만들었어요.

참석자 1 46:21
코드 같은 게 거의 안 팔리고 현대차가 되게 가성비가 좋거든 아 그래요 근데 그렇게 하기 위해서는 하드웨어가 좋아야 된다고 하드웨어 상대 전체에 반도체 팀이 있어요.
여러분 돈 붙였지 따로 있다고 저기 역산역에 지하에 저도 다 다 받아먹었어요.
막 밥 사주더라고요. 그것도 열심히 하다 보면서 그러니까 그런데 거기 지금 왜 한대전차가 사업도 짓고 있는 사업지에다가 막 갈아타고 있는 거 알아요 여러분 하나의 사옥을 만들겠다고 비싼 삼천 땅에다가 막 거기다가 맨 처음에 높게 짓는다 그랬더니 안 된다 그랬더니 다시 또 3개 만든다고 그랬다가 또 안 된다고 그랬다가 2개 만든다 그랬다가 계속 그분은 참 짠해요.
진짜 열심히 하고 있잖아요. 그쵸 잘 좀 해주지 백악관에도 처음 초청됐다 왔잖아요.
정의선 회장이 알고 있구나. 여러분이 제일 많이 취업할 수 있는 데 중에 하나인데 거기가 잘 돼야지 여러분이 취업할 때가 많은 게 그런 게 있어요.
여러분 이 정치랑 우리랑 상관이 없는 게 아니라고요.

참석자 1 47:13
그래야지 사실 우리 가도 잘 되고 그런 게 있어요. 사실은 그래요.
그래서 하드웨어라는 게 지금 이게 여러분이 여기 나오는 말도 있지만 사실 핵심이 뭐냐면은 여기는 GPU가 잘돼서 원래 썼는데 나중에는 전용 하드웨어를 만드는 거 있잖아요.
자동차는 여러분 비싸게 더 만들었으면 안 팔리지 자율주행도 잘 되는데 싸야 되잖아요.
반도체가 쌓여야 된다고 자동차의 반도체 테슬라는 반도체가 하나가 대빵으로 크게 들어가는데 엄청 비싸지 현대자동차 반도체를 조그만 게 많이 넣어가지고 싸게 만들었어요.
근데 그러면 이제 자율주행이 잘 안 되니까 거기 전용 반도체를 계속 열심히 만들고 있는 중이에요.
근데 그것도 접었다가 접으라고 그랬다가 말하려고 그랬다가 왔다 갔다 하고 있어요.
계속 맨날 한다 그랬다 안 한다고 그랬다가 작년에도 몇 번 발표를 지냈는지 그러고 있어요.

참석자 1 47:58
여러분 어쨌든 하드웨어가 되게 상관이 많아요. 알겠죠?
그리고 반도체 기술도 되게 상관이 많고 용인 용인 클러스터 만들어도 상관이 많아요.
여러분 근데 반도체랑 우리랑 상관이 없느냐 상관이 있어 시스템 소프트웨어 거기도 사실은 컴퓨터 구조 운영 체제라고 사실은 이게 딥러닝이랑 다 완전히 쫙 내려가는 거야 알겠죠?
계속해줄게요. 그러면 그래서 다음 시간에는 좀 차분하게 녹화를 해놓을게요.
코드를 좀 보여 줄 것 같아요.

참석자 1 48:29
일에 그래서 여기 안 와야 돼.


clovanote.naver.com