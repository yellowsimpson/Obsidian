딥러닝 day19
2025.05.21 수 오전 10:01 ・ 48분 45초
심승환


참석자 1 00:00
어쨌든 지금 혹시나 보통 애들이 요즘에 늦게 오길래 다음 시간에 이거 저한테 할게요.
그러면 일단 트랜스포러닝은 진도 잘 나가고 나가고 지금 새로 지금 우리가 어디 하고 있냐면 부장하고 있잖아요.
그쵸 부정하고 있는 거 맞나 맞지 부정하면서 내가 그 뭐냐면은 요거 이미지 분할은 일단 보류하겠다고 그랬잖아요.
그렇죠 프랜스포 필요하니까 근데 어쨌든 지금 숙제로 하는 거에서는 최신 컴브레 대처 요 요거 좀 들어가야 되고 그래서 이거 우리도 살점 안 하려고 그러는데 이서가 이렇게 들어가라고 그러는데 트랜스크 러닝은 여기 지금 다른 거 제가 하나 더 갖다 놨거든요.
왜냐하면 숙제할 때 여러분이 TFDS를 써서 해야 되는데 텐서 플로우 데이터셋이라고 해서 데이터셋이 우리가 원래 이제 어디 보통 있는 데이터 센들은 파일만 쭉 있거든요.

참석자 1 00:56
파일들만 이미지 같은 것들 보통 지금 이미지 갖고 하는 게 제일 여러분이 웃기기 좋으니까 이미지 갖고 하는데 그게 이제 디렉토리를 이제 또 만들어 가지고 거기다가 이름 이름 바꿔야 되고 이런 거 필요하거든요.
그런 거 전에 그 강아지 고양이 케그에서 하는 거 봤잖아요.
그런 식으로 해야 되는데 그렇게 하는 거야 여러분이 알아서 하시고 나중에 숙제해서 부과할 필요 없을 것 같아서 그냥 TFDS 있는 거 골라서 하게 시키려고 그러거든요.
그래서 그냥 이거 근데 이 샘플은 지금 코드가 교과서에 없으니까 이거 보면 좋아서 제가 갖다 놨어요.
알겠죠 그러면 이제 예를 들어서 플라워 쪽에 받아서 하는 거는 여기 코드가 있거든요.
이렇게 그리고 코드가 약간씩 형태가 좀 다른데 프리프레스하고 셔플하고 이런 것들이 있는데 이거는 해도 되고 안 해도 되고 사실 상관없는데 하면 더 빠르다고 이미 알려져 있어요.
갖다 놨거든요.

참석자 1 01:47
설명을 일일이 다 할 필요 없이 채찍 피트는 물어보세.
여러분 다 알려줄 거예요. 알겠죠 왜 하는지 쓰면 빨라져요.
알겠죠 프리 패치에서 메모리 로딩하고 설명 다 일일이 알려도 될 것 같아요.
어쨌든 그리고 이제 교과서대로 진도를 나갈게요.
어쨌든 여러분 지금 그리고 이제 혹시 두 번째 시간에 시작할 때 제가 까먹고 숙제를 안 내주면 숙제 왜 안 내냐고 뭐라 해봐요 다 보면 제가 까먹어서 두 번째 시작할 때 사람들 많이 모였을 때 숙제를 내줄게요.
팀도 잘라고 팀 팀 짜는 건 순전히 이제 뭔가 자료를 공유하기 위해서 그러는 거예요.
여러분 같이 얘기 나눌 사람을 좀 말해 놓는 게 좋아서 강제로라도 팀원이 잘 안 한다고 여러 피해 가는 거 없다고 이렇게 강조했죠.

참석자 1 02:31
그래서 어쨌든 지난 시간부터 9.2절이 그래서 완전히 안 들어가는 건 아니고 여기 이미지 세그멘테이션에 사실은 이렇게 스매틱 세그먼테이션 인스턴스 세그먼테이션 개념이 있다는 걸 여러분 알아야 된다고 그랬지 그쵸 그것도 알아야 돼요.
이 개념이 있는 건 그리고 이게 되게 훌륭한 게 예를 들어서 이제 이렇게 고양이의 이미지가 이제 지금 여기 교과서는 이제 시멘트 세그멘테이션만 하는데 그러니까 고양이랑 고양이 아닌 거랑 색깔 다르게 픽셀을 칠하려고 하는 거예요.
그래서 각 픽셀별로 예측을 하려고 하는 거예요. 세상에 그런 거 되게 대단하잖아요.
여러분 근데 욜로 이런 건 다 그런 거거든요. 욜로 같은 것도 전부 다 픽셀별로 다 뭔가 값이 나오는 거예요.
예측 값이 다 그래서 원래 이미지 원래 입력 원래 입력 이미지 크기랑 출력의 나중에 나오는 그 결과 값이랑 똑같은 크기가 나와요.
욜로도 그렇거든요.

참석자 1 03:31
멋있잖아 그런 거 하는 거를 가능하게 하는 게 강의를 그래도 해봐야지 하면 안 되지 여러분 근데 이게 저는 이거 막 지금 되게 하고 싶은 욕구가 생기잖아요.
이건 근데 그런 게 여러분 결국 성적이 나빠지거든요.
그러면 그래서 여러분 성적이 4.0 넘는 사람보다 3.5 원저리가 더 연구를 잘하는 경향이 있어요.
성적이 아주 좋은 사람이 그렇게 연구를 잘 못해 왜냐하면 자꾸 뭔가 가지치기를 해버리거든 뭐 해야 될 궁금한 거를 근데 원래 이제 약간 비절제로 뭔가 하는 사람이 나중에 몰입을 잘하는 사람이 나중에 뭔가 큰일을 많이 벌이기 때문에 여러분 성적이 좋은 사람은 성적 좋은 대로 회피하게 사시고 성적이 조금 나빠도 그냥 나는 좀 몰입을 잘해서 그렇구나 너무 잘 빠지는 거지.
남자애들끼리 여자애들은 왜 끼 그런 거 빠지는 것도 나쁜 성적 성격이 아니라는 거지 내 말은 알겠죠.

참석자 1 04:27
뭐 그냥 그렇다고 그래서 여러분 성적 3.0만 넣으면 되고 3.5 언저리가 좋고 알겠죠 그래요.
성적을 제가 좋은 사람만 좋아하지는 않아요. 여러분 그래요.
어쨌든 그리고 어쨌든 팀으로 하는 것도 되게 필요한 게 강조하지만 사람이 너무 비슷한 사람끼리만 모여 있는 게 좋지는 않아서 비슷한 게 이 친구랑 하게 되겠지만 어쨌든 좀 다양한 견해를 보게 얘기해 보는 게 좋아요.
여러분 그래요. 그다음에 그래서 이렇게 지금 보면은 얘는 이제 지금 개만 예측하는 거예요.
얘는 그렇죠 개 그렇죠 개만 이렇게 채면 안죠. 고양이만 하는 게 아니라 개만 했네 이거 그렇죠 그래서 이게 이런 걸 학습시켰던 거지 진짜 이렇게 나오는 게 신기하죠.
이 잘못 예측한 거죠. 이거 걔 아닌 데로 계약이 돼 있다고 이런 이런 이상한 그쵸 어쨌든 그런 거였고 그다음에 이제 이거 부자 사장 처음 들어가는 건데 9.3절은

참석자 1 05:30
최신 컴브넷 아키텍처라는 건데 컨볼루션 뉴럴 네트워크 댄스는 사실 은근히 구조가 간단했잖아요.
여러분 사실은 스텐트 쌓아서 사실은 컨볼루션 뉴럴 네트워크도 마지막으로 항상 댄스 있는 거 봤죠 여러분 지금 컨볼루션 쪽에서 아트처가 이렇게 지금 여러분 약간 아플레이션이 좀 특이하긴 했잖아요.
그쵸? 맨 처음에 이렇게 터널 컴퓨터 사프 하는 게 거기 잘 되는 게 유명한 것들이 있는 거예요.
이거 맨날 경진대회를 해가지고 경진대회에서 탄생하는 거예요.
사실은 그래가지고 직관을 따라서 뭔가 직관을 기르는 건 좋은데 직관이 전부는 아니라는 걸 여러분 항상 강조하잖아요.
공학에서는 그래서 사람이 많이 필요하고 실험도 많이 해야 되고 연구소가 있는 거지 그쵸 과잉 투자해야 되는 거고 그렇죠 과잉 투자라는 거는 쓸데없이 자꾸 해봐야지 뭔가 나오지 그렇죠 그런데 그 중에서 모범 사례가 많이 있죠.

참석자 1 06:17
과학도 사실 마찬가지야. 페실리 이런 것도 사실 여러분 실수에서 발견하고 그랬잖아요.
여러분 다 실수 안 했으면 몰랐고 그런 거 많죠. 그래가지고 어쨌든 약간 정리하면 모듈화하는 게 좋고 매칭을 하는 게 좋고 재사용하는 게 좋다고 그러는데 모듈화라는 거는 뭔가 반복되는 그 기반이 되는 그 덩어리를 의미하고 이제 컴포넌트라는 말을 보통 많이 쓰는데 이제 여기서는 이란 말을 썼고 그리고 이제 지금 우리가 모델이라고 하는 거 있잖아요.
맨날 우리가 만들 때 모델하고 핏하고 이런 거 있잖아요.
그쵸 근데 사실 그 모델도 여러 개의 모델이 막 모여서 될 수도 있잖아요.
사실 앙상블도 그렇고 그래서 이제 사실 파이푸치는 아예 모듈이라는 용어를 쓰거든요.
모델이라고 안 부르고 모델도 그냥 모듈이라고 불러 어차피 모듈들이 모여서 모델이 되니까 모델들이 모여서 모델이 되니까 좀 다르게 차별화하려고 모델이라고 안 부르고 모듈이라고 부르거든요.

참석자 1 07:09
요 어쨌든 나중에 보여줄게요. 여러분 그거야 이거 한 다음에 그거는 한번 하고 넘어갈게요.
파이토치가 워낙 오픈 소스가 많으니까 어쨌든 그냥 계층화하는 거가 위상 체계가 있는 거죠.
추상화도 다시 또 뭔가 다시 또 모아서 추상화하고 추상화가 굉장히 중요한 거죠.
지금 제가 이렇게 막 떠돌고 있는 것도 다 추상적으로 뭔가 설명해서 여러분이 뭔가 진짜 여러 가지 경우를 알려고 하는 거니까 재사용해서 하는 거고 어쨌든 그래서 MHR 이런 거 모듈레르티 하이어라키클 하이어라키 리유즈 이렇게 세 가지 공식이라고 하는데 이거는 저는 다른 데서 잘 안 쓰니까 그냥 넘어갑시다.
그러니까 이런 게 있어요. 알겠죠 그래요. 아키텍처라는 게 여러분 항상 뭔가 구조를 만드는 거고 원래 이제 이게 아키텍처가 컴퓨터 구조에서 쓰던 용어죠.

참석자 1 07:57
그쵸 그래서 그 서울대에 제가 이제 옛날에 운영 체제 강의하던 교수님이 있었는데 교수님이 김미환 교수님 저희 지도 교수님이 지도 교수님이셔가지고 그 교수님이 재밌는 게 원래 건축에 너무 관심 있어 가지고 건축 수업 듣다가 원래 전차관인데 알고 보니까 아키텍처라는 게 당연히 건축인 줄 알고 들었는데 전혀 CSO에서 성행하지 않을 때 이제 옛날이죠.
알고 보니까 그게 컴퓨터 구조였던 거야. 황당하죠.
영어도 잘 못하는 이유가 여학 가가지고 어쨌든 그래서 컴퓨터 사이언스 처음으로 우리나라에서 하게 돼서 그것도 여러분 실수잖아요.
그렇죠 사람이 엄밀하지 않으니까 그런 짓을 하는 거지.
그쵸 그래가지고 우리나라에 CST를 도입해서 운영 체제책도 쓰시고 처음에 우리 지도 교수님도 기르시고 저도 다 탄생하고 이렇게 된 거지 재밌잖아요.
여러분 어쨌든 재미있다. 어쨌든 아키텍처라는 용어가 원래 건축에서 나오는 용어인데 지금은 소프트웨어 하드웨어 다 쓰는 용어예요.

참석자 1 08:47
그쵸 아키텍트라는 말이 건축가를 의미하는 게 아니라 지금 온대 건데 쓰이고 있죠.
지금 건축가가 진짜 이름이 안 바뀌는 것 같아요. 그렇죠 고기 들게서 안 바뀌어.
원래 옛날부터 집 짓는 일이니까 다른 데는 이름 다 바뀌어가지고 그렇죠 어쨌든 아키텍처에서 여기 보면은 여기 건축 아키텍처 하면 이제 건물 나오는 게 제일 이해가 쉬우니까 보면은 이게 반복되고 있어.
패턴이 요기 요기 요기 똑같은 게 반복되고 반복되고 얘가 또 여기 반복되고 그럼 보이죠.
여러분 이게 뭐 하고 있냐면 모듈들이 모여서 다시 큰 모듈을 이루고 큰 모듈이 다시 또 반복되고 그렇죠 여기 안에 안에 하나도 여기도 반복 반복 반복 그쵸 이게 마인크래프트 같은 데서 되게 많이 일어나는 일이고 이거를 일일이 손으로 짓는 게 아니라 카피해서 짓는 게 맞지 마인크래프트 할 때도 명령어 배워서 마이크로프트 여러분 아시나 그거 참 훌륭한 소프트웨어지 거기도 여러분 미친 듯이 이거 다 짓고 있어 봐 미쳤지 카피해야지.

참석자 1 09:38
스트 카피한 페스트는 도덕적인 거야. 알겠죠? 그걸 왜 일일이 만들고 있어요?
근데 그거를 만든다는 게 리우즈 하는 게 중요한 거예요.
여러분 그러니까 리우즈 리우즈 재사용을 할 줄 알아야지 계속 똑같이 반복하고 있어요.
여러분 너무 성실하게 좋지가 않은 거지. 그렇지 성실하면 그걸 계속하고 있거든.
근데 문제점을 느끼는 그게 성실하지 않고 약간 게을러야지 이거 도저히 못하겠다면서 그렇죠 뭔가 해야 내잖아요.
그쵸 이거 이거 반복되는 거. 그렇죠 반복돼 사랑도 마찬가지예요.
이게 보면 인체에도 결국은 전부 다 똑같은 구조가 다 변형되고 있잖아요.
그쵸 그렇군요. 그래요. 보은 손가락 발가락 구도 비슷하고 약간씩 다르고 그렇죠 그래요.

참석자 1 10:22
그다음에 우리가 소프트웨어 엔지니어라면 소프트웨어 보면서 여러분 굉장히 제발 소프트웨어 공학 소프트웨어 엔지니어 소프트웨어 공학에서 나오는 이고 지금 잘 알고 있을 걸 잘 모를 걸 여러분 아직은 그래스트 이거 거기 나올 거고 거기 소프트 거기 나오는 내용들이에요.
여러분 알겠죠 기본 원리들 공간 때 적용되는 거고 그래서 여기 이거 여러분 저번에 파인 튜닝하고 피처 익스텍션 했던 것도 얘 갖고 있었어요.
vt 16 갖고 그쵸? 기억나죠 여러분 여러분 숙제도 이거 비슷한 거 활용할 거고 그래서 거기 보면 이렇게 다섯죠.
여러분 했죠. 여기도 반복되는 거지 사실은 이게 보면은 맥스 플링하고 맥스 플링 이렇게 컨볼루션 뉴럴렛하고 넥스플린 컨볼루션 키시 컴브하고 맥스 플린 이런 게 반복되고 있죠?
여러분 그쵸? 여기 이렇게 반복되고 있고 그렇죠 보이죠.
여러분이 반복되고 있는 게 보여요.

참석자 1 11:15
그런 걸 보여주고 있고 그다음에 이제 절제 연구라는 게 우리 교과서가 어디 있냐면은 이거 233 측에 주황색 버스가 있어요.
저 주황색 버스 보면은 진한 글씨로 나오는 게 여러분 중간에 아래쪽에 절제 연구하고 어블레이션 스테디라고 적혀 있는 거 보여요.
여러분 가운데 아래쪽 아래에서 두 번째 단락에 주황색 박스에 어블레이션 스터디라고 적혀 있는 거 보이죠 여러분 이거 여러분 되게 중요한데 어블레이션 스터디라는 거는 여러분 외워요.
여러분 어블레이션 어블레이션의 뜻이 절제 뭔가 못하게 하는 거예요.
레귤라이제이션 비슷한데 오블릭 없애는 거 잘 안 쓰던 건데 여러분 이어 이영어 그쵸 근데 이게 딥러닝 연구할 때는 좀 필요한데 어떤 뭔가 새로운 뭔가 아키텍처를 약간 도입을 해 중간에 모듈이나 근데 근거가 막 복잡하게 섞여 있어요.

참석자 1 12:11
걔 때문에 잘 되는지는 사실 걔를 없애보고도 잘 되는지 봐야 될 거 아니야 그쵸 그래서 어블레이션 스터디라는 게 보통 딥러닝 앞에서 제시할 때 논문에 항상 있어야 돼요.
이게 없으면은 모든 논문이 사실 우리 쪽도 마찬가지로 딥러닝 쪽에 저 시스템 쪽에서 하는데 시스템 쪽도 어블레이션을 안 한 연구가 퍼포먼스 나중에 이밸류이션 할 때 어블레이션을 하면서 뭔가 내가 제안한 것들을 없애고도 똑같은 성능이 나는 정말 그것 때문에 좋아지는지를 보여줘야 된다고 이해돼요.
여러분 알겠죠 뭔 얘기인지 잘 알겠지 내가 제안한 게 세 가지 요소야.
그리고 세 가지 요소를 하나씩 하나씩 제거하면서도 어떻게 했는지 보여주는 게 필요하지.
제가 최근에 논문이 하나 억세트 돼가지고 자랑해야지 DNN 타입 이거 지금 검색이 잘 안 되는구나.

참석자 1 13:00
아직 DNN 파일인데 아니 다음 시간에 보여줄게 아직은 온라인 출판만 돼가지고 제대로 안 보이는데 지금 잘못 봤구나 검색을 잘 안 했네 이거 나온다 그래요.
여기 이거 이 논문이에요. 저기 제가 코스타닝 루스트 여기 이 친구가 여기 여러분 석사 여기서 우리 학교 출신이에요.
외대 전덕호 친구 아까 첫 번째 호스죠. 그렇죠 지금 서울대 박사 박사 과정 하고 있는데 어쨌든 여기 보면은 오리 스토리 보여주고 싶은 게 여기 보면은 보여주고 싶은 게 익스피리멘탈 이밸류에이션이 있잖아요.
보면은 익스페리멘탈 이밸류에이션에 어블레이션 스토디가 뭔지 잠깐 보여주면은 여기 보면 원래 제안한 게 이제 DNN 파입이라는 건데 DNN 파입이라고 여기 있잖아요.
그렇죠 여러분 제안한 건데 여기 여기 wro가 위드 아웃이라는 뜻이에요.
위드 아웃 제안한 거 없을 때 없을 때를 보여주고 있잖아요.
이렇게 제안한 게 이건데 제안한 게 없을 때는 이렇게 된다.

참석자 1 14:03
제안한 게 세 가지 두 가지인데 하나 없을 때 하나 없을 때 보여주고 완전히 다 없을 때 이렇게 된다 이런 걸 보여주고 있는 거죠.
시간이 많이 y축이 시간이 많이 걸리는 거거든요.
이해돼요. 여러분 그래서 우리가 제안한 거 하나만 적용해도 이렇게 되고 하나씩 이렇게 적용해도 이렇게 좋아지고 다 적용하면 이렇게 된다 이거죠.
근데 딥러닝도 이게 이게 지금 제 논문도 항상 이런 식으로 하잖아요.
여러분이 뭔가 항상 모든 일을 할 때 내가 제안한 게 세 가지면 두 가지나 이러면 원래는 이랬는데 기존 연구 그렇죠 내가 제안한 거를 추가 추가했을 때 다 추가했을 때 이렇게 된다는 걸 보여줘야지.
그쵸.

참석자 1 14:38
그래서 지금 교과서에서 얘기하는 것도 어블레이션 스터디라는 것도 이제 뭐 무슨 얘기를 하는 거냐면은 막 복잡하게 해놓고 그게 그냥 이것 때문에 잘 들려요.
이렇게 얘기하는 건 말도 안 된다는 거지 사실 거기 쓸데없는 게 되게 많을 수 있다는 거지 저렇게 보여야 된다 이런 얘기야 알겠죠 알겠어요.
됐어요. 됐지 그래요. 그래서 9.3.2도로 들어가면 이제 유명한 것들이 여기서 검증된 것들이 나오는데 이건 확실히 좋은 거네.
버블링 스터디가 완료된 것들이지. 여기 보면은 2.3.2절이 잔차 연결인데 이거는 영어로 사실은 영어로 잔차 연결이 여기 343쪽에 진한 글씨로 잔차 연결 적혀 있는 거 보이죠.
여러분 오른쪽에 다 그렇죠 보이죠. 잔차 영어로 뭐라고 돼 있어요?
영어가 중요해요. 항상 근데 뭐래요? 여러분 레지 듀얼 커넥션이라고 그래요.

참석자 1 15:36
레지 듀얼 커넥션 레지 듀얼 커넥션 잔차가 레지 듀얼 커넥션이라고 하는 거고 그래서 유명한 네트워크 중에 레즈넷이라는 게 있는데 이 레즈디얼 레커넥션이라는 걸 도입한 연구예요.
이게 뭐냐 하면 별게 아니고 이 그림 보면은 원래 이제 우리가 항상 여기 블록이라고 돼 있는 게 여러분 CNN 같은 것들이에요.
CNN 스크린 이런 거 섞여 있는 거를 블록 반복되는 거 있죠 그걸 블록이라고 부르고 블록이라 부르고 이제 사람들이 이제 이걸 마두이라고 부르기 시작했어요.
많이 블록이라는 말 대신에 블록이라 불러도 마두리라고 불러도 다 똑같은 말이야 알겠어요 여러분 뭐가 마주냐면 여기 봐봐요.
여기 다시 보르면은 이런 게 이런 게 모듈이에요. 반복되잖아요.
그쵸 요런 거 있죠 여기 요거 요 모듈 요렇게 맥스 풀 여기 션 컨볼루션 두 번 하고 맥스 플링 두 번하고 맥스플링 있죠 여기도 세 번 하고 맥스프린 세 번 하고 스프링 이런 거 있죠 이 하나하나가 다 뭔가 반복적인 패턴이잖아요.

참석자 1 16:29
여기 하나가 바들 바들 바들이라고 부르기도 하고 블록블록 블록이라고 부르기도 하고 알겠죠 여러분 그래요.
그래서 그런 것들이 있는데 그렇게 하고 나서 뭔가 추상화로 했잖아요.
사실 정보가 줄어드는 건데 보통 이제 아까 보면 줄어들잖아요.
클레인 크기가 가로 세로가 그쵸 그런데 그러면 입력에 대한 뭔가 원래 오리지널 정보를 잃어버리잖아요.
추상화시켰으니까 그냥 추상화 일부러 시켜서 보려고 하는 건데 추상화된 정보를 그대로 넘겨주면서 추가로 원래 입력도 다시 넘겨주는 거야.
원래 이렇게 생겼어요 하고 알려주는 거야. 다음에 학습할 때 그걸 더 해 더 한다는 건 쫙 열어 붙인다는 거예요.
그냥 다시 원래 것도 잘 붙여 그래서 출력을 내보내겠다는 거예요.
좀 신기하죠 여러분 그쵸 근데 원래 원본도 알려주면서 계속 이제 학습을 하는 게 좋다는 거야.

참석자 1 17:18
이게 레지 듀얼 네트워크인데 커넥션인데 이해되죠 여러분 여기 정보가 사라지잖아.
원래 이렇게 생겼다는 것도 알려주는 거야. 추상화정 정보랑 원래 정보랑 같이 알려주는 거야.
추상화하기 전에 정보랑 알겠어요 그게 엔지니어 커넥션이고 이거 지금 안 하는 데가 없어요.
다 해 이거 다 해야지 좋아. 지금은 거의 100% 하고 있어요.
알겠어요 모든 데 다 하고 있어. 알겠죠? CNN을 써도 이걸 하고 있다고 항상 이걸 하기 위해서는 사실 보면 보면 여기 전체 연결 코드가 있는데 원래 입력이 들어오는데 이거를 미리 입력한 거를 레지듀얼이라는 변수에 저장을 해놨어요.
그리고 이제 그 블록이 이제 여러분 원래 이제 여러 개 시퀀셜로 있는 걸 수도 있고 여러 구여 해놓은 거를 모듈이죠.
그쵸? 를 모델이지 사실 일종의 이것도 하나의 걔를 통과시키면서 원래 변수로 보통 많이 해버리잖아요.

참석자 1 18:08
필요 없으니까 그런 그리고 이제 나중에 다시 다음 거 넘겨줄 거를 에드 해가지고 두 개를 더 해버려가지고 넘겨준다는 거예요.
두 개 더한다는 게 여러분 차곡차곡 연결하는 거 마저 값을 더하는 걸 수도 있고 이제 저기 더하려고 그러면 원래 이게 두 개가 사이즈가 맞아야 더 할 수 있잖아요.
여러분 사이즈를 이 블록이 원래 사이즈를 유지하게끔 세인 패딩 같은 거 많이 하겠죠.
그쵸 알겠죠 여러분 원래 줄어들면 안 되잖아 이렇게 하려고 그러면 그렇죠 정보가 그래야 더 할 수 있잖아요.
그렇죠 원래 거랑 사이즈가 똑같아야지 더 할 수 있으니까 그래요.
그래서 여기 봐 전류 설명이 교과서에 다 돼 있지

참석자 1 18:53
여기 보면은 다음에 344쪽에 344쪽에 출력 크기와 입력이 같아야 한다는 것을 의미합니다가 맨 첫 줄에 적혀 있죠.
출력 크기와 입력 크기가 같아야 된다는 것을 344쪽에 맨 위에 출력 크기와 입력이 같아야 된다라는 거 적혀 있죠.
여러분 맨 첫 줄에 그렇죠 그래요. 그래서 패딩을 세임을 써야 된다는 것도 바로 이제 코드 9 2 위에 패딩은 두 번째 줄에 있죠.
패딩은 세임을 사용해야 된다고요. 지금 옆에 있죠 이해되죠?
여러분 네 그렇게 만들어야 돼야 되고 실제로 지금 전체 블록을 쓰는 예가 나와 있는데 여기 지금 원래 인풋을 받았는데 이제 불 한번 통과했고 여기 냅뒀는데 통과한 거를 이제 다시 다음에 또 컨버레이션 여기 원래 이게 생긴 모양 컴투디 컴퓨트 두 번 하잖아요.
그쵸 근데 이거를 여기 컴퓨트디 나온 거를 저장해 놓는 거예요.

참석자 1 19:50
다음에 넣을 때는 이걸 그대로 넘겨주려고 하는 거지 레 드디오를 그래가지고 컴브 2D 여기는 지금 여기는 지금 세임 안 했죠.
패딩 패딩 세임 안 했죠. 근데 여기는 패딩을 세임을 넣어줬잖아요.
그쵸. 그런 게 이제 레지디어를 만들었으니까 다음에 똑같은 크기를 유지하기 위해서 만들어지는 거고 그리고 레지 듀얼도 지금 레지듀얼도 근데 두 개 더 하기 위해서는 필터가 지금 크기가 여기 지금 보면은 이게 이게 지금 x 통과하고 나면은 x가 64개로 변했죠.
필터 크기가 플레인이어서 64개죠. 근데 이제 두 개 더 해주려고 그러면은 이제 어쨌든 얘는 지금 원래 레지디얼 들어온 건 이제 32개 플랜이었죠 그쵸 두 개가 또 안 맞잖아요.
맞춰주게 하기 위해서 지금 이 필터를 64개로 바꾸고 필터가 여기가 재미있는 게 이게 1이죠.
여러분 그쵸?

참석자 1 20:50
처음 보는 건데 여러분 3 곱하기 3 제재한 게 3 곱하기 3이고 5 곱하기 이런 거 많이 봤는데 일자리 있었잖아요.
그쵸? 이거 원 곱하기 1 컴플레이션이라는 건 도대체 이게 뭐 하는 놈이야 점검이잖아요.
사실 그렇죠 근데 얘가 얘 1이라 그래도 사실은 필터 이게 커널이 1이라고 그래도 원래 앞에 그 채널의 크기만큼 뻥 뻥 튀게 되잖아요.
여러분 원래 원래 여러분 커널이 사실 필터가 되면은 앞에 채널 크기만 펑 늘어나는 거 알죠 사실 그래서 다 곱하기 다시 앞에 거랑 하는 거야.
이해돼요 여러분 64개 여기 입력이 들어오는 게 지금 레지 듀얼 요거 들어오잖아요.
원래 여기 들어오는 거에다가 피터하고 24잖아요.
그쵸 64개고 사실은 그 앞에 채널이 64개 얘가 뭐야 32였잖아요.
32개만큼 하나의 필터가 메이트가 있는 거지 그쵸 바이스까지 있으면 더하기 1도 있는 거고 그쵸 이해돼요.
여러분 어쨌든 원 곱하기 원 컨볼루션이에요.

참석자 1 21:52
이게 처음 보는 건데 여러분이 원고 파기용 컨볼루션이라는 거 처음 보는데 이게 뭐 하는 거냐면 이걸 어디다 쓰냐면은 주로 실제로 이제 뭔가 공간적으로 이렇게 압축하려고 하는 게 아니라 그냥 원래 그 정보를 뭔가 뎁스 정보를 필터링 뎁스 정보별로 뭔가 웨이트를 줘서 값을 크기를 변형시키려고 할 때 써요.
그런 필터가 여기 비싸게 있는 거니까 여기 하나 여기 보면은 64개가 튀어나올 거 아니야 이제 이렇게 하고 나면

참석자 1 22:26
제가 옛날 슬라이드 보여줄게요. 잠깐만 CNN 이제 이거 아니 이거 아닌가 내가 따로 만들었지 잠깐만요.
미안해요. CNN 이겠지

참석자 1 22:47
이거보다 이게 더 낫죠 여기 보면은 이게 지금 3 곱하기 3 커널이죠.
그쵸 앞에 플레인이 3개예요. 그쵸 그러니까 이게 아웃풋 채널이 개 여기 인풋 채널이 3개인 경우지 그쵸 그러면은 필터는 이거 다 합친 거예요.
그렇죠 그래서 얘가 지금 필터가 필터는 3개 플레인이라고 앞에 입력 채널의 크기가 똑같이.
근데 이게 이게 1이라는 거잖아. 지금 이게 하나라는 거죠.
1 곱하기 1이라는 거고 하나만 있는 거잖아. 그쵸 이해돼요.
하나만 있어도 이렇게 앞에 채널 크기만큼 뻥축이 돼서 다 늘어난다고 그랬죠 다시 어떻게 하지?
여기 지금 커널이 지금 이거는 지금 여기가 3 5 곱하기 5짜리인데 그쵸 어쨌든 여기 데스는 얘네들의 뎁스는 여기가 3이잖아.
3이랑 유지 똑같이 돼야 된다고 7평 1짜리를 해도 여기가 하나 하나는 여기 3짜리라고 웨이트가 3개씩 있다고 그리고 더하기 1이 있지.
바이어스 때문에 1 곱하기 1이 그냥 숫자 똑같은 거 곱하는 게 플랜별로 다른 거 곱한다는 거야.

참석자 1 24:02
그치 앞에다가 이게 무슨 얘기냐면 여기 여기서 여기서는 여기 해볼게요.
얘는 얘 1 곱하기 1이죠. 그렇죠 근데 여기가 64개가 있는 거죠.
64개를 만들고 싶어요. 그쵸 74개 64번 하기 필터를 적용하는 거야.
근데 한 필터는 필터 웨이트는 몇 개냐를 지금 얘기하고 있는 거예요.
이게 지금 레지 도어를 곱하잖아요. 여기다 곱하지 그쵸 얘가 이거잖아요.
그치 얘가 중요하지 그치 얘는 사실 결과에 영향을 미치지 않잖아.
나오는 플레이 32개가 튀어나오잖아요. 그쵸 32개 플레이 이게 32개라고 그쵸 플레인이 그쵸 그러면은 요 일자리 일자리 커널에서의 필터의 메이트s 크기는 32개가 플레인이 있는 거니까 32개의 점이 있는 거라고 베스트 그리고 바이스 때문에 하나 더 있고 이런 게 64개가 있는 거지.
그쵸 필터가 여기서 64개니까 또 그쵸 이해되죠 그리고 세일으로 해놨으니까 클리가 그대로 유지되면서 나오는 거지.

참석자 1 25:05
사실 원래 근데 1 곱하기 1로 하면은 사지도 않아.
여기 여기 팩이 셈이구나. 그게 아니네. 여기 세이딩 세임 여기 여기 세임 안 했네.
세일 필요도 없어. 여기 레즈 디어리 할 때 여기 패딩 세임 넣지도 않았죠.
넣을 필요가 없지 1 곱하기 1로 하면 안 사라져요.
주면 나중에 뭐 커버 못하는 게 없잖아요. 그쵸 PD cm을 넣나 안 나나 똑같겠죠 그쵸 이해되죠.
그래서 결국 나오는 건 여기가 사이즈가 얘도 얘 하고 나면 64개 플랜이 나오는 거잖아요.
그쵸 그리고 여기 원래 이제 요 x랑 페이딩 세일즈 하니까 똑같은 크기일 거 아니야 가로 세로가 그쵸 뭐가 됐든 간에 입력이 그쵸 얘도 똑같은 크기로 나오는 거지.
64개로 나오고 그쵸 24개의 플레인이 있으면서 그쵸 원래 가로세로 똑같이 나온다고 그래가지고 이렇게 더할 수가 있다고 더하는 게 완전히 그냥 하나씩 하나씩 더 하는 거 똑같은 게 대응 똑같은 자리에 똑같은 자리에 대응해서 똑같은 크기가 나오게 돼요.

참석자 1 26:08
크 같은 걸 더하면은 똑같은 그러니까 예를 들어서 여기 얘의 쉐입이랑 얘의 쉐입이 똑같아서 여기 똑같은 쉐입이 나오는 거지 이게 이 이 입이랑 똑같은 게 나오는 거지 알겠어요 여러분 하나의 쉐일이랑 똑같은 게 나와요.
64 플레인에다 가로 세로 있는 걸로 그거 더하니까 정확히 뭔지 알겠어요 여러분 정확히 뭔지 모르면 질문하세요.
정확히 알아야 돼요. 여러분 알겠죠 뭐 대충 아는 거 없어야 돼.
알겠죠? 뭐 하고 있는지 알겠어요 그래서 계속 가면은 최대 플링을 만약 가졌으면 단차 블록 할 때 여기서 보여주는 최대 플링이 있으면 사이즈가 줄어들잖아요.
그러면 무조건 세드 플링은 맥스 클링 하면 그건 원래 줄어들 수밖에 없잖아요.
그쵸 아니 원래 이제 커버 트루디 같은 건 패딩이 가능한데 맥스 플링은 무조건 줄어들잖아요.

참석자 1 26:57
맥스 플링 하려 그러면은 여기 스트라이즈는 투를 해서 나중에 이제 여기 똑같은 플레인을 만들어주면서 스트라이드는 2로 해서 또 크기에 맞춰줄 필요가 있다는 거예요.
스트라이드는 2로 하면 바로 솔도하잖아요. 케이가 두 번 펑펑 뛰니까 1 곱하기 1을 해도 슬라이드 2로 하면 반으로 줄어들겠죠 그 얘기예요.
그래서 맥스플링을 만약에 이제 2로 하면 스트라이즈를 2로 해줘야 된다는 얘기예요.
그 크기 맞추는 거예요. 여러분 크기 맞추는 거 크기 맞추는 거 이거 잘 못하면 아예 돌아가지도 않아요.
알겠어요 여러분 레지도 썼는데 알겠죠? 그래요.
그래서 이거 볼까 한번 이거 다 했나 그렇지 최대 플링층을 가진 게 지금 93이었고 그래서 지금 간단하게 만든 예를 여기에 교과서에 이제 345쪽에 나오는데 이 함수를 아예 만들었죠.

참석자 1 28:08
이스 필터즈 플링은 풀링이 있느냐 없냐에 따라서 플링이 있으면은 플링이 있게 하면은 이제 스트라이즈는 투로 하고 아니면은 아니면은 그냥 그리고 아니면 이제 필터 크기만 다른 필터 크기 다를 경우 있죠.
필터 크기가 다를 경우만 하면 되잖아요. 그쵸 필터 크기가 원래 레즈 듀얼 나온 거에 쉐비랑 쉐이비 마이너스 1이라는 게 여러분 제일 제일 작은 쪽에서 크기죠.
가로 세로 쪽에 세로 크기죠 그쵸 그러니까 가로 크기가 세로 크기가 이거는 가로 세로 중에 세로 가로지 가로 크기지 사실은 아니구나.
가로 미안해요. 거 있어 아웃풋 리다 가로 세로도 아니고 실수 내가 지금 완전 준비 여기 딴 거 보고 나가 미안해요.
이거 뭐겠어요? 여러분 이거 필터 아웃풋 채널의 크기 아웃풋 채널의 크기 그렇죠 결국 인풋 채널이 될 거 이거 마이너스 1이어러분 이게 원래 뎁스가 이거 크기가 레지 리얼 여기 나오는 게 어디 있어?
여기 이거잖아요. 이거 들어오는 모양이 이제 이런 것들이잖아요.

참석자 1 29:12
이렇게 이렇게 나와서 여기 이제 이제 여기 원래 입력으로 들어오는 거잖아요.
입력으로 들어오는 거 그래서 이게 쉐입이 쉐입이 몇 자리냐면 여기가 이제 뭐예요?
샘플 개수고 그쵸 샘플 디멘전이고 여기가 뭐예요?
배치라고 그러나 샘플 디멘전 그쵸 여기가 뭐예요?
화이트 뭐죠? 이거 위로 쓰고 여기가 채널이잖아.
그렇죠 마이너스 1은 이거죠. 채널 채널이 그러니까 필터 크기 저기 플랜 개수 그렇죠 플랜 개수로 봐야지 내가 지금 아까 그랬어요.
알겠죠? 여러분 코드 그냥 이렇게 무지성하게 이거 필터의 개수야 그쵸 저쪽에 아웃풋 채널의 개수 그쵸 레지 듀얼이 이제 인풋이 될 테니까 여기 들어오는 게 이제 다음에 인풋으로 들어가야 되니까 얘의 이제 셰이 채널의 개수가 안 맞아.
그러면은 이제 지금 원래 필터의 개수만큼 다시 리쉐이핑 해줘야 되는 거예요.
리쉐이핑 하기 위해서 이제 컴브 2D 이로 해서 해주는 거지 알겠죠?
맥스 플링이었으면은 스트라이드 2를 넣어서 해주는 거예요.

참석자 1 30:19
그렇죠 똑같은 토그 코드인데 리스플링이 있었으면 리스플링이 있었으면 이제 여기 넣어줬으니까 내가 알겠죠 됐죠 그래서 나 이거 이거 함수를 이렇게 만들어 놓고 내지도의 블록은 여러분 어떻게 생겼는지 알겠죠?
여러분 그러니까 그거를 여러 번 했어요. 짝짝짝 그쵸 짝짝짝 했어요.
그쵸 필터 크기를 32 6 4 128 이렇게 다양하게 했어요.
그쵸 다양하게 헤드 레지드 블록을 넣을 수 있는 거지 아까 다 해놨으니까 그쵸 중간중간 다 LGD 블록이 섞여 있는 거죠.
그리고 이것도 처음 보는 건데 글로벌 에버리지 플링이라는 거 여러분 제가 강의 자료에 있었어요.
그쵸 모르지 강의 자료에 기 교과서에 잘 안 나오는데 글로벌 레버리지 플링은

참석자 1 31:09
글로벌 브랜트닝은 제 강의 자료 솔루션 여기에 여기 안 한 것도 몇 가지 있는데 글로벌 브리플리 요거 요거거든요.
여러분 글로벌 플린 방금 나온 게 이거예요. 글로벌 머리티 플리 보여요.
여러분 여기 제 강의 자료 있어요. 그래서 어쨌든 교과 다른 데 없는데 그냥 이거 뺏겨 왔는데 여기 보면은 이렇게 원래 플랜 자체에서 제일 평균만 내가지고 숫자 하나 딱 뽑아내는 거예요.
플랜 자체가 숫자 하나가 돼버려 이해돼요. 여러분 이러면 뭐가 되느냐 이게 뭐가 좋냐 가로 세로가 없어지잖아요.
그쵸 그렇지 여러분 가로 세로가 없어지잖아. 그렇죠 그럼 이게 결국은 이렇게 이게 벡터만 되잖아요.
그쵸? 플랜트 시키는 거면 정보를 잃어버리지 않고 플랜트 시킬 수 있는 거죠.
그쵸 이걸 다 플래트 시키는 건 좀 별로니까 여기서 통역 단에서 딱 하고 통력 감에서 딱 해서 뭔가 추상화시킨 다음에 추상화시키면 바로 플래팅이 되잖아.

참석자 1 32:17
그렇죠 바로 이렇게 벡터면 쭉 나올 거 아니야 플레임별로 플레임별로 숫자 하나 채워놓으니까 이해돼요.
여러분 됐지 사실 이렇게 하나 리듀스 미인하나 똑같은데 평균만 내는 거랑 똑같은데 근데 그거를 이거 그냥 뭐 약간 전문적으로 보이게 하려고 글로벌 에버리지 플링이라고 부르고 있는 거예요.
평균 내서 그냥 딱 넣고 평균을 딱 넣고 그래서 뭔지 모르면 질문하세요.
여러분 그래가지고 이렇게 그러니까 이제 바로 댄스를 연결할 수 있는 거야.
이걸 화가 났으니까 바로 댄스를 연결하는 거 이해되죠 여러분 벡터가 됐으니까 이걸 하고 통과시키고 나면 벡터가 튀어나오잖아요.
클레인의 가로 세로가 다 사라져 숫자 하나 플레이 한 숫자 하나만 되고 디멘전이는 잘 날아가거든요.
그래가지고 벡터기 때문에 바로 연결할 수 있어야겠죠.
그리고 지금 시그모이드로 이제 일하는 거는 아마 이제 확률만 내려고 하는 거지 됐어요.
다 했지 그래요.

참석자 1 33:21
그리고 이거 서머리를 하면은 길어 보면 지금 345쪽부터 쭉 나오는데 맨 처음에 이제 인플레이어가 이랬나 보지 302 30 2 3이었나 보지 그래서 강아지 고양이인가 보다.
이래가지고 여기 지금 원래 이 리스케일링 하는 게 원래 이제 있었구먼 255로 하는 게 코드를 여러분 비교해 보세요.
345쪽에 위에 있는 코드 거기까지 어디 코드가 어떻게 해당하는지 보세요.
여기 여기가 인풋 얘가 첫 번째 줄 얘가 두 번째 줄이죠.
그쵸 이해돼 내가 뭘 하는지 알겠죠 다시 여기 코드가 이렇게 시작해줘.
요게 첫 번째 줄 두 번째 줄이에요. 그치 얘가 첫 번째 줄에 해당하는 거 이거 두 번째 줄에 해당하는 거에서 나온 거예요.
구조가 그쵸 그다음에 함수가 0이 된 거는 당연히 상관없잖아요.
그쵸 함수가 다음에 뭐가 있어요? 레지듀얼 블록이 몇 개 있어요 하나 둘 세 개 있죠 그쵸 하나 둘 세 개 있는 게 있고 글로벌 에버지 플링 있잖아요.

참석자 1 34:23
그쵸 그쵸 3개가 있겠지 3개가 있는데 하나가 생긴 모양이 생긴 모양이 이거잖아요.
컴브 2D 컴브 2D 그다음에 맥스 플링은 다 했나 안 했나 내가 어떻게 했나 아까 플링은 다 트루로 했네.
그쵸 트루 트루 포스네 그렇죠 트루 트루 포스로 했죠 그렇죠 투 트루 프로스트레스 그래서 투루 두 번은 이제 이 맥스 플링이 있고 그다음 맥스플링이 있고 그다음에 컴브 투디가 있고 나머지 그리고 마지막에 이제 에드 하는 걸로 되어 있죠.
그래서 이거 보면은 생긴 게 컴 투디 컴브투디하고 맥스 풀링하고 컴퓨터디하고 애드하고 요렇게가 하나겠다.
그쵸 그쵸 여러분 이게 레지디오 블록 하나야 하나 그쵸 이해되죠 여러분 이해되죠 그다음에 이렇게 또 해서 애들까지 하는 거 이게 또 두 번째 그다음에 이렇게 해서 하는 게 세 번째.
그쵸 세 가지 했고 그다음에 이제 글로벌 머지링 했고 그쵸 어떻게 되는지 알겠죠 여러분 함수를 하면 되게 편하다.
그렇죠 모듈을 쓰려면 함수를 만들어야 돼.

참석자 1 35:30
그렇죠 그리고 사실 이거를 그래서 이제 파이토치가 잘 돼 있는 게 이거를 이제 모듈로 정의하고 모듈로 또 부르고 이런 식으로 해놨거든요.
모델 모델 일종의 모델이니까 그래요. 그리고 뭘 더 봐야 되나 재밌는 걸 보여줄게.
여기 보면은 레주디얼이 연결되는 게 레주디얼이 연결되는 것을 보면 레주디얼이 여기 있잖아요.
레주디얼 레주디얼이 원래 들어오는 거잖아요. 입력으로 그쵸 걔가 여기 이 맥스 풀링 다음 또는 이제 컴퓨트리 두 개 두 개 있는 컴퓨트리 두 개 있는 다음에 세 번째 게 레지디어랑도 같잖아요.
그쵸 그쵸 그쵸 여러분 레주디얼 봤잖아 그쵸 레주디얼이 앞에 거지 그래야 잔 차죠.
그래서 여기 보면은 이게 한 첫 번째 두 번째 말고 세 번째 거 얘 있죠 얘 얘 얘의 입력이 리스케일링으로 돼 있죠 그쵸 리스케일링 어디서 나왔냐 요 앞에 있는 거 얘잖아 얘 이해되죠 여러분 얘 얘가 연결되는 거죠.

참석자 1 36:31
원래는 전부 다 구경해 보면 여러분 패턴을 보면 앞에 거 얘가 일로 오고 얘가 일로 오고 얘가 일로 오고 이렇게 바로 앞에 있는 게 주차 넘어오잖아.
원래는 시퀀셜하게 쭉 원래 그랬잖아. 다 그랬잖아.
그쵸 6 6 바로 앞에 있던 거지. 그쵸 근데 얘는 지금 이 스플링이 연결된 게 아니라 그 아웃이 앞에 있는 게 연결되잖아요.
그쵸? 이해돼요. 여러분 얘가 아니야 얘가 저 앞에 있는 거예요.
그렇죠 알겠죠 알고 해야 된다는 거지. 애드는 두 개를 하는 거니까 두 개 하는데 애들은 얘를 두 개 한다는 거 얘랑 얘랑 한다는 걸 적혀놓은 거죠.
그렇죠 그래요. 그리고

참석자 1 37:15
그다음에 뭐야 마찬가지로 여기도 여기도 두 번째 것도 컴브 2D 컴브 2D 한 다음에 얘 얘 얘가 얘랑 연결되는 거지 얘랑 연계되는 거겠지.
그쵸? 이해되죠 여러분 얘랑 똑같은 걸 받는 거죠.
얘랑 똑같은 걸 받는 거지 이해되죠 여러분 얘랑 똑같은 걸 받는 거예요.
그렇죠 블락 시작하는 거고 얘랑 똑같은 걸 받는 거지.
똑같잖아 그렇죠 그렇죠 그렇죠 뭔 얘기인지 알겠죠?
보세요. 이 정도면 됐지 않았을까 마찬가지로 뒤에는 또 그렇겠지.
그렇죠 알겠죠 여러분 이거를 정확히 알고 있어야 돼요.
여러분 어떻게 되는지 그래요. 그다음에 그림으로 그리면 더 좋을지도 모르겠는데 이 책들 나오는 사실은 이거 말고 왜 여러분 마델 점 서머리 말고 플라 마델이라는 것도 있잖아요.
여러분 플라 플라 하는 거 있잖아요. 그걸로 보면 더 잘 보이겠지.
연결 선 연결되는 게 보이니까 플렛 있잖아요. 플라 아플라 있는 거 기억하고 하셔요.
여러분 시험에 나와요. 필기에 어디 있죠?

참석자 1 38:19
호품목에서 그림 그리는 거 하라고 그러면 여러분 왜냐면은 저런 논문 쓰는데 이렇게 갖고 오면 사실 이것도 사실은 여러분 별로긴 해요.
사실 그림 그리는 게 낫잖아요. 그렇죠 그림으로 좀 그려보지 하면은 어떻게 하는지 모르는데 왜 모르긴 여기 다 있는데 있었어요.
그렇죠 10장에 있었어. 알겠죠? 그래요. 나도 못 외우는데 그거 찾아서 할 수 있어야 돼.
그에 알겠죠. 그림 그리는 게 더 낫겠지 이해되죠 여러분 그다음에 중요한 내용이 나온다.
배치 정규화가 나오는데 이거 진작에 안 가르쳐줬어요.
이런 느낌이 들 거예요. 진짜 진작에 안 갈 수 있어 이런 느낌이 드는 것들이 좀 많이 있지.
그러니까 지금 많이 쓰이는 것 중에 백치 전비화는 거의 다 쓰거든 레지디랩 거의 다 쓰고 CNN 쓰면은 배치 정규화도 거의 필수적으로 쓰고 있어요.
CNN 쓰잖아 CNN 마당하고 쓰고 있어요. 그래서 원래 우리가 피처 스케일링 했었잖아.

참석자 1 39:12
옛날에 피처 스케일링 할 때 밋맥스로 하는 것도 있었고 그쵸 그리고 그냥 스탠다드 디비에이션이랑 미니라해서 하는 것도 있었어요.
집값하고 막 그럴 때 했었거든요. 기억나나 그거를 원래 이거잖아요.
노멀라이즈 하는 게 이렇게 평균이랑 스탠다드 그쵸 하는 거였죠 그쵸 얘를 맨 처음 입력에 대해서만 하는 게 아니라 이 컨볼레이션 하고 나면 거기서도 다시 또 뭔가 이상해지는데 또 입력이잖아요.
새로운 입력 거기서도 메모를 하는 게 좋겠다는 생각이 들잖아 그렇죠 약간 그래서 이게 이제 그걸 했어 누가 2015년에 2015년 전에는 아무도 안 하다가 2015년에 니가 된다는 가정에 가서 다 이제 쓰기 시작했는데 먼저 이제 arsi 주위로 올려놔가지고 원래 알파고가 2014년 2015년 그때쯤이었잖아요.
그때 벌써 알파고를 쓰기 시작했죠. 지난 그러니까 지금은 100% 쓴다는 거지 사람들이 그러니까 원래 입력에만 대해서만 쓰는 게 아니라 중간중간 커버레이션 블록마다도 쓴다고요.

참석자 1 40:07
그래서 보면은 여기 막 구체적이면 근데 이거 이거 왜 도보이는 확실히 아는 사람 이 사람이 약간 좀 되게 비판적으로 하는데 좋아해 거 되게 좋아해요.
어쨌든 근데 실험적으로 좋으면 좋은 거야. 알겠지 진짜로 그래서 보여주면 돼.
이거 처음에는 그렇게 사람들이 안 쓰다가 나중에 너무너무 좋아서 거의 뭐 100% 쓰고 있어 348쪽 가시면은 비밀로 한번 읽어보세요.
여러분 이거 실제 쓸 때 어떻게 하냐면은 컴브 2D 하고 이게 완전히 파이썬 API로 들어가겠지 파이썬이 아니고 펜스 플로어 API 리시마 다 있어요.
저 저쪽 파이프트도 있고 노말라이션 이걸 해주면은 여기 나온 출력 있잖아요.
여기 나온 출력 여기 나온 출력을 이제 노말라이션 해주는 거지 이해되죠?
여러분 그런데 여기 바이오스는 여기 이제 폴스로 하기 시작해요.
여기서부터 배치 마션 할 거면은 바이어스 쓰면 어차피 웨이트 가 더 들어가는 거잖아요.

참석자 1 41:04
필터별로 플레임 별로는 아니고 필터별로 하나씩 들어가잖아요.
필요가 없는 게 어차피 얘가 평균은 분산 만들어서 뺄 거니까 평균은 빼고 하잖아요.
무조건 평균을 0으로 평균이 아니라 실제 노브라이징 하면 평균이 0이 되잖아요.
그쵸 표준 편차 1로 만들잖아요. 그쵸 꼭 그래서 바이러스가 필요가 없어요.
굳이 굳이 해봤다. 그거 해봤자 다 빼버릴 거니까 그래서 바이러스 안 써요.
알겠죠 그래요. 그리고 이제 또 중요한 점은 근데 또 재미있는 게 있어요.
여기 보면은 그다음 밑에 줄에 어디 있어 중요한 점은 이렇게 적혀 있죠.
그쵸 중요한 점이라고 큰 브레스 탑 위에 그쵸 그리고 95가 또 있죠.
그쵸 근데 그게 뭐냐면은 액티베이션 멜루를 여기 넣었는데 이러지 말고 여기 액티베이션을 하지 말고 액티베이션을 이렇게 따로 축으로 넣자 보여요.
여러분 액티베이션을 안 썼다가 여기 액티베이션이 없죠.
배치 로마저 한 다음에 액티베이션 하는 게 좋지 않냐 이거예요.

참석자 1 42:16
액티베이 왜냐면은 액티베이션을 해버리면은 lu 같은 거 써버리면은 이제 연골 장학까지 다 없어져 버리잖아요.
근데 원래 진정한 평균은 0골 장값 다 있는 거잖아.
이해돼요. 여러분 lv가 뭔지 알죠? 여러분 연구로 당한 거 다 날려버리는 건데 이제 평균 분지서 분산되는데 그게 없으면 곤란하잖아.
좀 그래서 액티베이션은 이렇게 나중에 컴퓨터에서 액티베이션 안 했다가 추노머리 한 다음에 액티베이션 하는 게 좋다는 얘기지 알겠어요 여러분 여기 교과서에 있는 거죠 95예요.
이거 알겠어요 여러분 뭐 하는지 알겠죠 이렇게 하는 게 정석이고 심지어 이제 재미있는 거는 파이터치는 아예 액티베이션을 무조건 이런 식으로 하게 해놨거든요.
거기는 액티베이션은 여기다가 못 넣어 그냥 원래 계층에다가 그래서 거기는 오히려 더 잘 보이지 파이토치가 나중에 나와가지고 뭔가 좋은 게 좀 알겠죠.
그러니까 또 여기는 돌아갈 수는 없으니까 또 그냥 이 여기도 할 수 있는 거야.
다 전부 다 알겠죠.

참석자 1 43:13
그리고 이게 또 약간 재미있는 거는 교과서에도 그려놓고 뭐 그렇지만 꼭 모범사례가 매우 중요한 거 아니라고 별로 안 그런 경우도 있더라 이러면서 막 그러는데 보통 이렇게 상식적으로 이렇게 하는 게 직관적이잖아요.
그쵸 평균 내고 분산 내는데 값 0 밑에 거 날려버리기 좋지는 않잖아.
사실 그렇죠 날려버려도 괜찮더라 이렇게 또 얘기하는 거예요.
이 사람들도 알겠죠 실제로 해봐야지 안다 이거죠.
별로 안 중요하다 이렇게 주장하고 있어야 알겠지.
교과서에서는 또 여러분들 두 시간 되면 두 개 두 개 다 해보는 것도 괜찮겠지 알겠죠 여러분 그러니까 이게 항상 교과서에 나오는 걸 믿지 말고 이것저것 해보고 여러분 잘 되는 거를 본인이 쌓으라 이거지 실제 여러분의 데이터는 어떨지 모르니까 알겠죠.
그래요. 그다음에 배치 정기화 할 때 또 여기 349쪽에 위에 있는데 파인 튜닝 하는 거죠.

참석자 1 44:04
파인 튜닝 파인 튜닝 할 때 배치 노발레제이션을 동결하는 것이 이게 좋다는 건데 이게 배치 노바이제이션이 여러분

참석자 1 44:24
베트로마일레이션이 트레이너블 속성을 폴스로 설정한다는 건데 배트로마레이션이 무슨 트레인이 되는 거냐 이게 트레이닝 된다는 게 벤치마이제이션도 여러분 이렇게 평균하고 분산이 있으니까 그게 트레이닝 변수인 거예요.
그래도 그래가지고 트레인 변수로 들어가 있어요.
지원 정도 배치노바리이 뭔지 별거 아니고 뭐겠어요?
평균이나 분산을 계산하는 건데 걔도 이제 학습을 하는 거야.
걔도 이렇게 통과하면 평균화 분산이 얼마가 된다?
트레인 변수의 평균하고 분산이 들어가게 돼요. 그거는 여기 교과서에서 얘기 안 했잖아요.
다른 교과에서는 그게 한 수식이 어마어마하게 나오면서 한 3 페이지 정도 나와요.
보통 교과에서는 이렇게 퉁 치고 넘어가는데 안 보여줘도 되겠지.
여러분 평균하고 분산이 학습의 대상이라고 이해돼요.
근데 그거를 평균화 분사를 학습시키지 않는 게 좋다 이 얘기예요.
여기서 하는 거는 근데 뭐 별로 신경 안 쓰고 또 해. 근데 나 이것도 나는 신경 안 쓰고 오늘 했어요.

참석자 1 45:28
여러분 실제로 여러분 제가 이것까지 하라고 지는 숙제 안 내놨는데 됐어요.
여러분 무슨 얘기인지 일단은 배치 이전의 평균화가 분산이 트레이너 밸류라는 거예요.
여러분 자세히 설명해 줄까 안 해도 되게 자세히 설명해 주고 싶었고요.
여기 보면은 원래 배출 바이전에서 하는 일이 배출 마신 시적 코드 여러분 경 별거 아니고요.
이거 평균하고 분산 있죠. 얘를 빼고 이 빼고 더하고 하잖아요.
그쵸 여기 빼고 여기 나누고 하잖아요. 근데 이거를 나중에 이거 얘네들이 계속 이제 앞에 원래 데이터 데이터 나온 거 갖고 하나 할 텐데 인퍼런스 할 때도 필요할 거 아니에요?
그쵸? 인퍼런스 인퍼런스 할 때도 그 테스팅 할 때도 그럼 이거를 샘플 하나만 들어왔는데 어떻게 하냐고 나중에 샘플 하나만 들어올 수도 있잖아요.
지금 들어온 샘플에 대해서 하면 곤란하잖아요. 그러니까 트렌드 데이터 지금 한 거에서 이걸 이거랑 얘를 값을 만들어내야 돼.
나중에 계속 고정을 바꿔놓고 써야 된다고 얘를 이해돼요.

참석자 1 46:36
이게 지금 이 값이 얼마였는지를 이것도 업데이트해 나가 사실은 매치 마션에서 이것도 업데이트해 나가는데 사실은 여러분 RMS 프라도 내가 안 가르쳤지 뭔지도 여러분.
근데 옛날에 이것도 아주 운영 체제 이거 떼버렸구나.
여러분 익스포네이션 에버레징이라는 거 저는 봤나?
항상 뭔가 원래 값이 있으면 지금 새로운 값이 예측 값이 있잖아요.
예측 값이랑 원래 값이랑 1 2랑 1 마이너스 알파를 곱해가지고 서로 이제 더해가지고 평균 내는 거야.
원래 값이 원래 이제 예측 값이 있어요. 그리고 원래 지금 계측값 값이 있어.
여기 각각에다가 알파 하나 1 마이너스 알파를 곱해서 더해서 평균 내는 거예요.
이거 반복하면은 이제 사실 원래 이제 각각에 대해서 익스포넨셜하게 이제 값이 디케이딩 되기 때문에 평균은 이런 식으로 내거든요.
그래서 얘도 이 평균이랑 분산도 그렇게 학습을 시켜요.
익스퍼네셜 레버리징으로 매치 노마이제이션이 그냥 쓰는 게 아니고 약간 그런 세크닉이 들어 있어요.

참석자 1 47:39
이게 왜 필요하냐면 일단 당연히 상식적으로 생각할 게 이게 나중에 인퍼런스 할 때도 이 값이 있어야 될 거 아니야.
이 평균이나 부산이 근데 지금 들어온 데이터에 대해서 하면 곤란하다는 거지 트레인 했을 때 학습해야 되는 값이라는 거죠.
트레인 했을 때 학습해야 되는 값이에요. 평균하고 분산이 지금 들어온 이 특정 입력에 대해서 할 수 있는 게 아니라고 모든 샘플에 대해서 이제 평균화 문서 내야 되기 때문에 여러분 옛날에 주 했네.
옛날에도 내가 옛날 가르쳐줬잖아요. 그럼 중간고사 때도 냈는데 많이 틀었지만 오픈북에서 옛날에 몇 주 몇 시야 지금 49분에 몇 종인지 찾으려면 귀찮아서 스탠다드 노바라시했던 거 기억나나 노무라 했던 거 5장에 있던 해줘야겠다.
5장인가 어디 있나요? 한번 얘기해 보죠. 기억이 나게 하는 게 좋을 것 같아요.
이게 어떤 게 아니라 어디 있어? 이거 쉬는 시간 찾아서 보여줄게요.
여러분 50분에 쉬는 시간을 확보해야 돼 그쵸?

참석자 1 48:36
이제 끝나고 시작할 때 원래 보상이 어디 있는지 보여주고 팀 짜고 그다음에 안 까먹었어요.


clovanote.naver.com

딥러닝 day19_1
2025.05.21 수 오전 11:00 ・ 50분 54초
심승환


참석자 1 00:00
분명히 저번 시간에 많이 틀렸는데 애들 171 감사합니다.
172쪽 172쪽 고마워요. 됐다. 주택 가격 맞네 4.3점 나왔구먼 표기 문제 주택 가격 지우구나 여기다 171쪽 볼래요 171쪽 이게 어디냐면은 배치 노물리제이션 하는 것 때문에 그래요.
배치 배치 노미라제이션 뭐지 보스톤 보스톤 안 적혀 있어 무슨 턴이구나 여기 있다.
그래서 교과서 171쪽에 이거 데이터 넘겨 8 24 보면은 미인이랑 스탠다드 이리이션을 구한 트렌 데이터 구했잖아요.
그쵸 그리고 테스트 트랜 데이터는 당연히 이렇게 민 빼고 스탠넷이 나오는 거 맞는데 되게 정교하잖아요.
그런 레데이션인데 테스트 데이터를 할 때도 테스트 데이터 민 테스트 데이터 스탠다드 디베이션이 아니고 그쵸 트레임 데이터의 미니한 스탠다드 디베이션을 해야 되잖아요.
그쵸 그쵸 그래서 이거를 이게 이거 미니얼 스탠다드 디베이션이 이게 파라미터의 일종이에요.

참석자 1 01:21
그러니까 이게 이거를 나중에 이게 이거 테스트 데이터 있잖아.
이거 이거 나중에 새로운 게 데이터가 나중에 이거 한 번도 훈련을 안 사용한 데이터잖아.
여기는 테스트 데이터들은 나중에 테스트하고 인퍼런스 할 때 실제 나중에 이거를 팔아 먹었어.
그때 이렇게 정교화해서 해야 되잖아요. 이게 파라미터라는 거 이해가 돼요.
여러분 이게 남아 있어야 되잖아. 같이 가야 돼. 파라미터야 이게 그리고 지금 배치 라이즈는 중간중간 계속하잖아요.
그 걔를 뭔가 저장해 놔리잖아. 저장을 해놔야지.
나중에 쓸 거 아니야 그래서 지금 트레이 하면서 걔를 계속 원래 이제 초기 값을 만들어내면서 다음에 새로운 값이 나올 때도 계속 뭔가 이제 아까 얘기했던 익스포네이션 레버리지라는 걸 해요.
익스포넨셜 에버리징은 다른 데서 배웠겠지. 아이들한테 그거 강의 의해서 아마 여기서 할 거 아닌 것 같아.
학교 과정 같은 데서 안 했나 몰랐어.

참석자 1 02:12
그는 익스트 디스크레시 에버러지라는 거 있어요.
여러분 되게 쉬워 그거 강하는 거야. 저는 아티 아이들 중에서 맨날 여기 교과서에 맨날 뭘 쓰냐면 아담 안 쓰고 아라비 스트라 있잖아요.
아르메스 프라도 걔도 맨날 익스폰에서 에버레징 하는 놈이에요.
아니 스폰에서 에버레징 모 보니까 너무 너무 무식해 보이면 안 되는데 큰일 났네 분의 자별지도 해줘야지 강의 자료에 그냥 추가 시험에 낸다기보다 익분에 잘 배일지 모르는 건 좀 그래

참석자 1 02:48
그리고 이거 그냥 이거 구글로 주면 사람들이 안 쓴대 사실은 여기 얘가 챗gpt에트 물어가지고 나오는 거 딱 덮어놓으면 더 좋긴 해.
그렇죠 이렇게 해가지고 검색해서 주는 거보다. 사실은 여기 보면 이게 머신러닝으로 나오네 봐봐요.
여기 보면 봐봐 원래 다음 단사를 지급하는데 베타 1 마이너스 베타 적혀 있잖아요.
그리고 지금 바로 지금 예측한 값이랑 원래 이제 우리가 예측 원래 초기 값으로 했던 거 있잖아요.
그걸 이렇게 웨이트를 1 마이너스 베타랑 베타 주잖아요.
두 개가 더하면 인덱게이 하는 거잖아. 확률적으로 이걸 계속 반복하면 이게 이게 vt가 계속 쌓일 거 아니야 그쵸 다음에 여기 다음에 vt 플러스 1이 되면은 요기 요 식에 들어가잖아.
다시 또 이게 제곱 제곱 되겠지 계속 그쵸 베타가 제곱 제곱 제곱 될 거 아니야 익스포레셜하게 계속 옛날 값이 dk가 돼요.

참석자 1 03:47
이게 무조건 다 일부러 작으니까 일부러 자은 곱하면 작아지잖아.
사실 시그모이도 여러분 그런 거 할 때도 전부 다 일부러 자음 곱해가지고 사실은 그렇죠 작아지고 그러는 거 있잖아요.
일부러 자은 값을 계속 곱해서 어쨌든 방금 예측했던 거가 무시되지는 않는데 점점점 희미해지겠죠.
그쵸 어쨌든 그래서 옛날 값에 대한 영향력은 줄어들고 그쵸 최근에 왔던 거의 영향력은 계속 좀 어느 정도 유지하고 맨 처음에 예측했던 값을 또 어느 정도 유지하게 확률적으로 그렇게 하는 거예요.
그래서 여러분 이런 거지 예를 들어서 여러분이 원래 평점 평균이 여러분 여러분 여러분이 이제 100점 만점의 점수를 받았어 얼마를 시험을 맨날 봐 그럼 다음에 몇 점 받았지?
할 때 원래 이제 내가 여러분이 몇 점짜리다 이 사람은 50점 맞는 사람이다 이렇게 했어요.
다음 시험에 일단 50점 맞을 걸로 기대하는 거야.

참석자 1 04:35
그런데 다음에 다 100점 맞았어. 100점 맞아도 바로 100.5로 이 사람을 보기 싫은 거야.
적당히 이제 만약에 저 배터리 0.5로 하잖아요.
그럼 0.5만큼 웨이트를 줘가지고 0.5 곱하기 100점 0.5 곱하기 50점 해가지고 75점 정도로 보는 거지.
다음에 또 100점을 받았어. 그러면 또 그 100점이 또 들어가서 또 100 100 곱하기 0.5에다가 옛날 예측했던 값 있잖아요.
100점 옛날 원래 옛날에 벤처했던 거에 여기 맨날 지금 0.5 곱하기 해서 50 더해지면서 계속 더해지는 거라고 그래서 이게 옛날 값 너무 맨 처음에 선입견이 사실 선입견이잖아 이거 사실 그렇죠 선입견이 별로 안 중요하면은 그냥 옛날 거 바로 방금 것만 보는 거잖아.
이게 이게 되면 극단적으로 선입견이 너무 중요하면 이게 베타가 1이 돼 0이 되겠지.
그럼 옛날 값은 보지도 계속 50점으로 바라보는 거고 그쵸 선입견과 실제 이제 달성한 것에 밸런싱을 맞추는 거예요.

참석자 1 05:28
알겠죠 실제로 이게 선입견이 더 중요할 수도 있어요.
사실은 결국은 옛날에 50이었다는 건 뭔가 이유가 있었지 결국은 그렇게 될 거야 이해돼요.
여러분 그래서 이게 적당히 조절해야 돼. 메타 값도 알겠어요.
여러분 여러분 그래서 학부 성적이 중요해. 학부 성적이 평생 따라다녀 학부 성적은 평생 따라다닙니다.
저도 옛날에 어떤 교수님이 카이스트 교수님이 되고 싶었는데 지금 다른 대학교 계시는데 어느 대학 교수님 얘기 안 해서 거기서 학교 평점 평균이 낮다는 이유로 아예 떨어졌거든요.
그렇게 추천을 많이 해도 그냥 한평이 변장에 놔둔 사람은 잠수를 탈 가능성이 있다.
이게 성실성을 보여주는 거니까 아무리 현재 지금 엄청난 업적을 달성해도 꼬리표가 붙는 거야.
이게 베타 값이 큰 거야. 카이스트나 베타 값이 작은 거야.
카이스트는 카이스트 대학은 근데 다른 대학교는 안 그렇지 보통 지금이 중요하지 그래서 몰라요.
어쨌든 그냥 농담처럼 잠깐 기억에 남으라고 얘기하는 것뿐이야.

참석자 1 06:24
알겠죠 여러분 그런 게 중요한 게 아니라 타이틀 좀 변했을 때 모르지 알겠죠 여러분 그래요.
그래서 이게 익스퍼레이션 에버리징이고 제가 이거를 그냥 갖다가 이게 왜 머신러닝이 중요하냐 아까 배치 노말레이션

참석자 1 06:46
스탠다드 디베이션 학습 시 사용 알겠죠? 네 실제로 이게 지금 한 거 또 다음에 갖고 이렇게 하면 한다고 했을 때 처음에 이제 해놓고 계속 하시면 진행하는 거예요.
그다음에 또 RMS 프라임에서도

참석자 1 07:05
그리고 이 아스 프라이 결국은 나중에 다 연결돼서 아담에도 다 들어가거든요.
이런 게 아담에도 비슷하게 들어가요. 아담에도 다 들어가

참석자 1 07:16
거의 거의 거의 대부분을 사용하고 있어요. 여러분 사실 이건 거의 거의 대부분 가장 기본적으로 많이 사용하는 거야.
알겠죠? 값을 예측할 때 수건형이 새로 돼 있어 가지고 할 때 됐지 그다음에

참석자 1 07:40
뭐 하는 거냐 그리고 와서 지금 이 일로 바로 넘어가지 말고 팀 프로젝트에서 팀을 빨리 짜야 돼.
그렇죠. 지금 여기 숙제가 오늘 진도 다 나가서 여기서 하라고 하는 거 다 해서 할 수 있다는 거지.
그래서 여기서 까먹지 말고 새로 생성해서 학생 자율 선택으로 하면 화면 하고 내가 쉬는 시간에 하라고 그랬어야 되는데 이거를 종료일 21 이게 뭐야 언제까지 종료일이 0시라고 이런 말도 안 되지 그쵸?
지금 몇 시야? 지금 11시 11시인데 11시인데 지금 잠깐 한 15분까지 대충 15분까지 여러분이 저 선택으로 팀 설정 인원수 팀 인원이 여러분 일단 기본적으로 4명씩 하면 8팀이 되네.
여러 팀이 되는구먼. 되게 아주 아름다운 숫자가 나오네.
그렇죠 다 그래요. 이렇게 해서 저장을 해놓으면 4명씩 한번 여기서 휴대폰으로 들어가서 지금 해보고 LA 제일 빠를 것 같아.

참석자 1 08:54
그렇게 하고 그다음에 내가 안 된 사람들은 쭉 묻고 이러면 끝나는 거잖아요.
그쵸 그래요. 그렇게 해버리고 얼른 진행을 하려고 해요.
지금 지금 그냥 잠깐 합시다. 여러분 나중에 따라 하라면 정신없으니까 여기 안 온 사람들끼리 모이겠지.
그러면 안 온 사람 한 명밖에 없구나. 그래 그래요.

참석자 2 09:19
그리고 보시면 안 돼요. 내 거 프

참석자 1 09:24
약간 떠들어도 괜찮아요. 여러분 얼른 빨리 사위라 부셔

참석자 2 09:30
캠핑 갑시다. 같이 알려지. 우리 8팀 들어가자.
맨 밑에 들어가면 있어 팀원이 팀장이 팀 프로젝트 형 팀원들 팀원 되세요 같이 하실래요?
좋죠 좋죠 근데 지금 누르면 맨 밑에 거 누르면은 이렇게 들어가시는 거 같아.
이 팀 선택을 하면 돼 4 4로 할게요. 근데 이미 있는데 이 밑에 하면 될 것 같다면 8팀 들어가면 되고 8팀 있어 1팀 없다.

참석자 2 10:39
다른 데 혹시 한번 알아볼 수 있어 다쳤어? 어

참석자 2 10:52
그럼 우리 3팀 들어갈까? 아니야 아니야 사업팀 구 팀장 혹시 혹시 아는 사람 있어?

참석자 2 11:13
혹시 팀 다 짜졌어?

참석자 1 11:16
제가

참석자 2 11:17
그러면 그냥 그냥 거슬리.

참석자 1 11:25
그러면은 1팀은 끝났고 2팀도 끝나고 이거 내가 어떻게 하는 거지 이제 자유 수정을 했으면 나는 더 이상 못하는 건가 나는 더 이상 못하나 팀 설정 여기 들어 있을 텐데 그래서 어떻게 되는 거지 이중 수강생 여기까 여기 이렇게 하면 된다.
이제 그래서 1팀 2팀 3팀 이렇게 됐는데 여기 정현우 정현우 정현우 정현우는 이렇게 누르면 보이나 뭐야 이거 보이는구나 한 두 명씩 있네.
이해준 이동권 짠 고민성 창이 팔지 정현우 여기서 골라봐요.
자기가 두 명씩 있는데 내가 여기 팀 8팀이 왜 3명인데 4명이 다시 해야 돼 이 부분 다시 어떻게 해야 되는 거지 이거 이렇게 하고 여기 지금 어디 갔냐?
미지정 수강생이 5명인데 다시 정현우부터 미안해요.
정현우 정현우 잘 골라봐요. 정현우가 최유정, 이수수요 이해준 이동건 고윤성 강창이 어디 들어갈래요?
정현을 만나 상관없어요. 그러면은 그냥 이거 여기다 넣을게요.
어떻게 하는 거지 뭐

참석자 2 12:56
응 상관없어.

참석자 1 12:57
이지정 수강생에 정현우를 2명씩 넣어야 되는구나.
아까 누구야 전부 하나 둘 셋이 근데 내 정식 어떻게 되지?
모르겠다. 정현은 일단은 이 팀에 넣고 팀 이동 팀 이동하고 그다음에 이준영 이준영은 어디 가고 싶어요 상관없나요?
네 그러면 또 아까 방금 6팀에 들어갔으니까 5팀 5팀 숫자를 밸런스를 맞춰야지.
여기 3팀이 없으니까 3팀에 또 넣고 어떻게 하겠어요?
이준영을 이준영을 3팀에 넣고 2 팀 이동 3팀

참석자 1 13:50
그다음에 또 밸런스가 맞아 지금 한 저 보면은 이종 수강생 이병진 이병진 어디 아무 데나 상관없이 또 그러면은 뭐 3팀에 넣을게요 하는 게 아니구나.

참석자 1 14:17
3팀 이동하고 그다음에 5팀 6팀 여기 나왔는데 이구한 영준아 어디 가고 싶어요 상관없어요.
그러면 그냥 지금 넣을게요. 다 했다. 6팀만 좀 미안하네.
6팀 6팀 사람 적네. 그지 대정에 상관없죠. 갑시다.
그러면 이러면 이렇게 해서 콘트레트가 있고

참석자 1 14:50
됐어요. 그러면은 이렇게 있고 마지막 진도 나가던 거 마저 나가면

참석자 1 15:06
또 이거 지금 나가려고 하는 게 4장에 9장에 데스 와이즈 세퍼로크롤린이라는 건데 이것도 뭔가 굉장히 9.3.4절이죠.
349.

참석자 1 15:27
이게 교과서에 그림이 이렇게 돼 있거든요. 이 저자가 만든 거예요.
이게 진짜 제일 잘 되는 것 중에 하나라서 지금 그러니까 근데 중요한 거는 또 약간 미리 알아둘 거는 350쪽에 적혀 있는 거 보면은 350쪽 351쪽 351쪽에 맨 위에 적혀 있는 게 NVIDIA 반복적으로 요청했지만 적혀 있죠.
여러분 그렇죠 NVIDIA 반복적으로 요청했지만 351쪽 이거 이거 여기 NVIDIA 보트로 요청했지만 이런 게 적혀 있잖아요.
주황색 박스 안에 그쵸 351쪽에 352쪽에 그런 말 적혀 있죠.
그쵸 이게 이게 소프트웨어적으로는 훨씬 우수한데 하드웨어 소프트를 지금 워낙에 CNN 대해서 많이 해놨기 때문에 하드웨어에서 얘에 대한 가속기는 잘 안 만들어놨어요.
그래가지고 사실은 잘 안 빠르다는 얘기를 하고 있는 거예요.

참석자 1 16:20
이 저자가 약간 속상하다 이거지 내 게 되게 좋은데 왜 서포트 안 해주냐 그 얘기예요.
알겠죠 그래서 실제로 여러분 소프트웨어적으로 되게 훌륭해도 하드웨어 소프트 안 해주면 잘 안 되는 게 많이 있어요.
운영 체제에서 하는 게 운영 체제에서 제가 가르키면서 이거는 결국 운영 체제 때문에 못 들어가는 페이지 화면 서 예를 들어 페이지 할 때 페이스 테이블 베이스 리스트 이런 거 없으면 되게 곤란하거든요.
mmu 같은 거 가상형 관리하는데 mmu 같은 거 없으면 되게 곤란하거든요.
그리고 페이지한테 또 tl이라고 해서 트랜스레이션 루버 사이드 버퍼라고 그래가지고 어쨌든 그런 것들 있잖아요.
그런 것들이 하드웨어적으로 소프트가 안 되면 되게 느려요.
사실 아무리 아이디어가 좋아도 그래서 사실 이게 지금 제가 제가 가리키는 게 굉장히 이제 시뮬레이션 상으로 성능이 되게 좋은 숫자 익터레이션 카운트 되게 작은 거예요.
콤플레스트 알죠? 여러분 레벨 5 알잖아요.

참석자 1 17:08
오 함수 개 오더러브 컴플레스티 알죠? 여러분 몰라 컴퓨터 우주 시간에 하는 거 이게 로그 짜리 로그 함수냐 로그냐 로그인이냐 n 제곱이냐 이런 거 있잖아요.
여러분 익스포렌셜 이 레스팅이냐 이런 거 오더 오브 컴플레스트랩이라고 그러잖아요.
그러면 오함수 그래요. 모르면 안 돼요. 여러분 알겠죠 그래서 그거는 작가요.
얘가 지금 가리키는 내용이 근데 실제로는 가속기가 별로 서포트를 안 해주기 때문에 별로 안 빠를 수도 있는 거야.
알겠죠 그리고 여러분 GPU를 쓰는 거랑 이런 CPU를 쓰는 건 흥분 차이가 어마어마해요.
MPU를 쓰는 거랑 그런 하드웨어로 뭔가 서포트가 되기 시작해 무조건 빨라 그런 게 있어요.
여러분 알겠죠? 하드웨어 쪽에서 되게 반도체 쪽이 중요한 거죠.
그러니까 알겠죠 그래요.

참석자 1 17:57
그래서 이 제목은 이제 여기 이게 350쪽에 하드웨어 소프트웨어 알고리즘의 공지나 코어 별루션 여러분이 뭔가 딥러닝을 해도 하드웨어 쪽을 간과하면 안 된다는 거지 그 하드웨어를 뭘 더 잘 쓰면 잘 될까 이런 생각도 생각할 필요가 있는 거지.
그쵸 그리고 삼성전자나 뭐 이제 이런 데서 열심히 이제 맨날 반도체 이런 거 열심히 하는 거지.
자동차 쪽도 마찬가지인 게 하드웨어로 뭔가를 해야지 빨라지는 거지.
온디바이스 이야기 그래요. 그렇지만 워낙 훌륭한 아이디어라서 그래도 성능이 더 좋긴 해요.
그래도 많이 써요. 그래도 이게 그림이 이렇게 있는데 이게 원래 입력이 들어온 거를 이게 지금 제목이 뭐냐면은 깊이별 분약성 보이잖아요.
영어로 영어로 먼저 알아야겠지.

참석자 1 18:46
이게 349쪽에 이것이 두 번째 줄에 이것이 주제 3절 4절에 이것이 기피 분리학성 시나 구글이 돼 있지 기필 분리학성 그쵸 영어로 뎁스 와이즈 샘플러블 컨볼루션으로 돼 있죠 그렇죠 뎁스 와이즈 뎁스 와이즈 그렇죠 세퍼러블 컨볼루션이 적혀 있죠.

참석자 1 19:09
그래서 이게 뭐냐 뭐 이제까지 컨볼루션은 아까 원 곱하기 1 커널 짜리 쓸 때도 그랬고 개수별로 어쨌든 한 커널에 필터를 쫙 쓰는 거잖아요.
그쵸 한 숫자를 더 어쨌든 한 갑 시켜어나게 해놨죠.
그래서 무조건 스 인풋 채널의 필터를 적용하고 나면 뎁스 사라져.
그쵸 필터 하나당 무조건 플레이 하나만 튀어나오잖아요.
그렇죠 그랬지 그랬는데 그러면 약간 좀 약간 정보가 뎁스를 뭔가 정보를 안 본다는 게 좀 이상하다고 그랬잖아요.
제가 그렇죠 하나 숫자 만들어버리는 게 숫자 하나로 만들어놓는 게 괜찮나 약간 이런 생각이 든다고 그랬잖아요.
제가 그렇게 안 한 거예요. 이게 그렇게 안 한 거 그래서 이게 이걸 보면은 채널이 채널이 여기 여기 여기 여기 들어오는 게 입력 채널의 크기죠.
입력 채널만큼 여기서 이제 4개짜리인 거지 여기 이쪽에 있는 채널이 이게 여러분 이게 항상 이게 지금 이렇게 간단한 건 여기 뭔가 뭔가 또 여기 결과가 있는 거잖아요.

참석자 1 20:19
그쵸 여기 아웃풋 채널의 크기가 여기 인풋 채널이 되겠지 그쵸 뎁스라고 하기도 하고 왔다 갔다 해서 헷갈리죠.
뎁스 채널 크기 똑같은 말 그렇죠 같은 말이야 그쵸 여기 아웃풋 채널 크기가 만약에 64야 그러면은 여기 인풋 채널 64개가 들어가는 거야.
그렇죠 여기 필터가 64개였어요. 앞에가 마지막에 그러면 64개가 들어가는 거예요.
그죠? 그래서 여기 지금 채널 분리한다는 건 64개 여기 이런 걸 64개 만든다는 거야.
이 그림은 지금 4개짜리를 썼다는 거죠. 필터가 4개가 있었다는 거죠.
앞에 필터 4 있었던 거죠. 그러니까 4개가 분리가 된 거예요.
알겠어요. 여러분 그리고 원래 이제 상급 배송 확성급 해가지고 옛날에는 채널별로 해서 이거 무조건 값이 하나가 튀어나왔잖아요.
숫자 하나로 만들어버렸지 플레이 하나로 만들어 버렸죠.
근데 여기서는 그 플레이들을 이게 합침이 아니야 이게 뭐야 연결 이게 교과서에 제대로 있네.

참석자 1 21:18
합침이 아니라 교과서는 내가 있지 연결돼 있지 컴퓨터 네이트 그쵸 합치는 거랑 연결하는 거랑 달라.
그쵸 합치는 거 여러분 우리 방금 자체 연결에서는 더 했잖아.
잔차액을 더 했죠. 진짜 크키트레이트는 그대로 갖다 붙이는 거예요.
마치 필터 하나가 있는 것처럼 이게 이거 이거 얘네들은 이거 같은 필터여도 이렇게 4개가 튀어나왔죠.
한 필터인데 이렇게 한 필터에 대해서 이렇게 삼각하 설성 여러 개 했잖아요.
이거는 채널별로 따로따로 해서 그거를 그냥 그대로 플레이는 안 사라지게 하는 거야.
하여튼 간에 플레인이 옛날 사라졌지 다 더해가지고 다 더해서 바이러스 하나 딱 고쳐버렸잖아요.
이거 안 사라지게 하고 연결시킨다고 옛날에 합치는 거지.
진짜로 한 플랜이 나왔죠. 그 한 필터가 데스트별로 있었잖아.
이거는 근데 지금 그리고 원래 다 다른 거였는데 이거를 그렇게 하지 말고 그냥 연결시키는 거야.

참석자 1 22:16
더하지 않고 여기가 연결이 중요하다고 연결이 이게 중요해 이게 이게 합치는 게 아니라 이거 이거 지금 슬라이드 잘못됐어요.
슬라이드 왜 잘못했냐 이거 합치는 게 아니라 에드 하는 게 아니라 컴퓨터 레이트가 중요해 알겠어요 여러분 그래서 그다음에 그렇게 한 다음에 점별 합성 연구파 합성을 해 연결한 다음에 다시 원급하기가 거기 이제 그 플레임별로 다시 언급하고 합성어 하면은 또 원래대로 이제 베스트가 또 원하는 필터 개수만큼 적용하면 또 나올 거 아니에요 그렇죠 그러니까 플레임별로 뭔가 또 웨이트 따로 주려고 하는 거야.
전에는 그런 게 없었잖아요. 전에는 플레임을 다 더했잖아.
그냥 이건 플레임별로 뭔가 또 학습을 시키는 거지 일종의 그런 거였어요.
그게 새 브라블 컴브투디고 이거를 잘 이해 안 될까 봐 제가 다른 데서 이게 아닌가 모르겠어요.

참석자 1 23:18
혹시나 해서 제가 그래 강의 자료 있다가 제가 뺐던 거였는데 제가 여기 강의 자료에 CNN 있잖아요.
CNN 요거 올린 거 알죠? 여러분 제가 새로 추가로 올렸거든요.
요거 스와 세포를 그물로 새로 올렸어요. 됐어요.
있어 있으니까 여기 보세요. 여러분 여기 갖고 왔어요.
기록에서 그래서 보면은 이게 여기 지금 플레인이 아웃풋 채널 인풋 채널이 되는 거죠.
밖의 아웃풋 전에 아웃풋 채널이 인풋 채널이 돼가지고 플레인이 몇 개예요?
여러분 하나 둘 셋 넷 5개니까 5개구만 5개죠 그쵸?
5개 저기 5개가 쭉 나오는 거예요. 분리 해. 플레인의 개수가 채널의 개수가 똑같다.
플레인 채널 개스 다 같은 말이야. 플레인의 개수, 뎁스 채널의 개수 똑같은 말 알겠죠?
플레인이 채널이에요. 알겠어요 여러분 이게 이제 플레처럼 보이니까 제가 플레인이라고 부른 거예요.
알겠죠? 여러분 전문가 보니까 이것도 내가 가르쳐줬는데 잘 접수가 안 됐네.
그쵸 미안해요. 다시 잘해봅시다.

참석자 1 24:21
잘 질문했어. 좀 여러분 또 물어봐 봐. 알겠죠? 이상한 거 있으면 제가 플레인이랑 채널이랑 같은 거 몰랐어.
알 플레인 지 플레인 내가 여러 번 얘기한 것 같아. 비 플레인 여러 번 얘기해도 여러분 처음 들으면 당연히 접수가 안 되지.
미안해요. 됐어요. 이게 있어요. 여기 지금 채널이 5개 그죠?
뎁스가 5 플레이 5개 됐어요. 이거를 쫙 펼쳤어.
그쵸 그리고 엔 곱하기 엔콤플레이션을 했어요. 그쵸 옛날엔 더 했지 더 했는데 더 하는 게 아니라 이거를 그냥 이렇게 연결시킨다고 그리고 원 곱하기 0 콤버레이션을 해.
여기서 원 곱하기 0 컨볼루이션은 사실은 여기 지금 개 3개 있었죠 필터가 3개라는 거예요.
그럼 3개가 튀어나오죠? 그쵸? 이해돼요 여러분 크기는 똑같이 되지 원 곱하기 오버레이션이니까 그렇죠 근데 요 요 필터 하나 있죠?
필터 하나 필터 하나는 요 데스만큼 또 사실 있는 거예요.
웨이트가 그렇죠?

참석자 1 25:26
여러분 이건 정상적인 컨볼루션이거든 여기도 정상적인 컨볼루션이에요.
알겠어요 여러분 이해돼요. 필터 하나 여 여 여기 세 개 적혀 있잖아.
요 세 개의 각각 하나는 사실은 이렇게 여기 5개잖아.
5개만큼 있는 거라고 웨이트가 만약에 바이러스까지 있으면 플러스 바이러스까지 해서 6개씩 있는 거지.
이 필터 하나에 사실은 알아요. 여러분 헷갈리지 나도 헷갈려요.
이게 참 정상이 이게 헷갈리는 정상이라고 다시 강조하지만 원 곱하기 형 컨볼루션은 데스트 크기만큼 뻥튀기 해서 또 한 곱하는 거라고 얘가 요 하나 생긴 거 있죠?
아까 보여줬잖아. 그 5개 숫자라고 숫자 5개 6개라고 해서 숫자 6개가 사실은 숫자 6개로 이루어져 있어.
하나씩 하나씩 곱해. 탁 탁탁탁탁 전부 다 그렇게 해가지고 이거 하나 만들고 더 하는 거니까 더 하자.
여기 여기 더 한다니까 언급한 플랜 다 더해 더 해야 돼.
여기는 그냥 더 해버린다. 여기 더 한다고

참석자 2 26:32
전에 뎁스들을 다

참석자 1 26:34
다 하나 숫자를 만들어버리는 거예요. 이 필터 하나가 이 뎁스를 쫙 모아가지고 하나로 탁 만들어

참석자 1 26:43
잠깐만 보여줄게요.

참석자 1 26:52
여기에 필터가 여기 이 표시할 표현하고 있는 게 필터가 3개라는 거예요.
하나 둘 셋이지 그쵸 이 필터 하나가 둘 셋 넷 다섯 개로 이루어져 있어요.
더하기 바이어스 그쵸 그래서 여기 다 곱해. 그쵸 하나씩 하나씩 그쵸.
그래서 숫자 비교부터 하나 숫자 하나 더 해서 이거 딱 하나 만들고 그 짓을 저기 n 곱하기 해야 돼.
여기 n 곱하게 아니라 이게 뭔지 모르겠지만 이거 hw라고 그러면 h 곱하기 w 통 하는 거지.
요 필터 하나가 요거 하나 만들어내기 위해서는 이런 짓을 한다고 이 플레이이 쫙 머지가 되는 거야.
이거 이거 더 해야 된대. 근데 여기 옛날에도 그냥 컨볼루션이었으면 다 이렇게 딱 해서 한 숫자 하나로 탁 튀어나오게 했지.
근데 그렇게 안 하고 딱 따로따로 해서 연결을 시켰지.
이렇게 그게 다르다는 거야. 됐어요.

참석자 1 27:49
연결을 해가지고 지금 다시 한 번 더 이렇게 한다는 게 다르다고 이게 뎁스 와이즈 컨볼루션이야 알겠죠 어쨌든 이게 이게 온갖 게 있지만 레지듀얼이랑 뎁스 와이즈는 좀 뭔가 인사이트가 있어요.
베티노 와이즈도 그렇고 그쵸 그렇지 않아요. 여러분 굉장히 배울 만하지 않아요.
진짜로 이거 나온 것도 2017년인가 그래요. 은근히 여러분 이쪽은 되게 따끈따끈해.
그쵸 여기는 진짜 한 여러분 챗gpt 달라지는 거 보셔.
진짜 이렇게 이미 이렇게 진짜 반지의 제왕을 지브리로 막 만들어줬는데 진짜 너무 너무 잘해.
진짜 되게 잘하잖아요. 그렇죠. 저는 엄청나게 잘해.
작년에 못했는데 올해는 하자. 그렇죠. 그런 게 많다고 이게 계속 공부해야 되는 게 있어요.
근데 근본적인 것도 있고 근본적으로 살아남을 것들에 대해서 지금 공부하는 거야.
알겠죠 그걸 모르면 다른 거 이해하기 힘든 것들 교과서에 나와 여기 있는 것들은 그래 그런데 어쨌든 이거 계속해야지.

참석자 1 28:45
일단 그래서 이게 여기 보면 349쪽에 아래쪽에 공간상의 위치가 높은 상관계를 가지면 채널과는 옛날에 옛날에 이게 뭐냐면 공간상의 위치가 높은 상관계를 가지지만 채널과의 매우 독립적인 과정을 채널 간이 채널이 매우 독립적일 수 있잖아.
사실은 채널이 걔는 거 다 합쳐버렸는데 채널별로 뭔가 정보가 따로따로 있을 수도 있잖아요.
사실은 근데 그게 정보가 날아갔었거든. 옛날에는 특정 채널이 되게 중요할 수가 있는 거야.
특정 채널이 얘가 보면은 옛날에 컨볼루션은 이게 전부 다 합쳐져 가지고 그냥 턱 칸이 탁 하나 튀어나오니까 옛날 건물이잖아요.
요 하나하나하나의 중요성에 대해서 이렇게 하나하나의 중요성 있잖아요.
필표 통과했을 때 플랜 하나하나에 대해서 나오는 거 있잖아요.
이거 계획의 중요성이 사라졌잖아요. 필터 이게 플랜 그 채널 하나하나가 뭔가 굉장히 독립적으로 뭔가 중요한 정보가 있으면 걔를 더 살리고 싶은 거야.

참석자 1 29:46
지금 걔를 더 강화시키고 옛날에 무조건 다 더해버려서 이제 없애버리는 거.
그래서 얘는 이제 옛날 CNN은 RGB가 예를 들어서 있다 그러면 r이나 g나 b나 비슷비슷할 거야.
약간 이런 가정하고 있고 얘는 알이 되게 특별할 수도 있어.
아래만 모든 정보가 다 나와 있어 있어. 이 개가 그 앞에 앞에 사이드 5개 나왔는데 특별히 얘가 뭔가 더 굉장히 중요한 놈일 수도 있고 그러면 데스트 이거 한번 통과하면 어쨌든 뭔가 더 정보가 사과할 거 아니야 그쵸 그래서 이렇게 따로따로 학습시키려고 하는 거예요.
이게 오히려 그런데 중요한 건 옛날보다 웨이트 수가 많이 줄어들어요.
옛날에는 여기 여기 뻥 튀게 한 것만큼 필요했는데 이거 다 나눠서 그냥 한 번에 쫙 연결시켜버렸기 때문에 웨이트 수가 엄청 줄어들거든요.
웨이트 수가 볼게요. 한 번 여러분 이렇게 보면 담배 350쪽 350쪽 350쪽에 이게 실제로 이거를 가지고 한 게 필자의 논문 여기 보면 보이죠.

참석자 1 30:48
여러분 섹션 익셉션 익섹션이 여러분 뭔가 익섹션 외초 예외 같은 느낌을 준 거지 그쵸 그래서 뎁스 y 세퍼롤 컨볼루션으로 한 건데 그래서 이 논문에 보면 예를 들어서 지금 주황색 박스에 있는 건데 브런스 박스 있는데 얘가 도대체 바이러스가 지금 없는 상태로 지금 여기 나와 있는데 35평이상 윈도우 예를 들어서 입력체로 64개 슬로체로 64개를 이제 생각해 봐요.
여러분 바이러스가 없는 경우에 얘기하고 있어요.
여기 전부 다 여기 얘에서는 바이러스 없어요. 그러면은 원래 가능한 파라미터 개수는 원래 훈련 훈련해야 되는 거.
프라미터는 3 곱하기 3에다가 64에다가 다시 64가 되죠.
그쵸 여기 64는 1채널 크기고 여기 나주 64는 출력 채널이야.
그쵸 얘는 어떻게 되냐면은 그냥 3 곱하기 3 24일 대만 하잖아.
그렇죠 24일 이거는 입력 채널에서 한 거야. 그쵸.
시험에 내니까 여러분 필기를 해도 높은 부분은 시험에 낼게요.

참석자 1 31:52
그리고 이게 요거는 여러분 여기 이거 정체는 뭐냐 이거 지금 이거는 순전히 그 필터 개수죠.
그쵸 요 행성 판 24 3으로 24는 여기서는 뭐였냐면은 성급하기 상 여기서 4지 4 그쵸 얘네들 프라미터 얘기하는 거야.
성급하기 4 4였다고 여기서는 여기 4였지 지금 입력이 64니까 94지 그쵸 이해돼요 여러분 됐어요.
여기 합치는 게 아니라 컴퓨터 레이트 하잖아요. 이게 몇 개만큼 있어요?
여러분 여기 여기 여기 4개가 있는 거잖아요. 여기는 그렇죠 4개가 여기 플랜이 있는 거잖아요.
그리고 그래서 여기 지금 여기 공급하게 화스고 하니까 64개만큼 있는 거잖아.
그쵸 플레인이 앞에 플레이는 논 곱하기 1인데 출력이 64개 나오면 그만큼 피터 개수가 있는 거니까 64 또 곱해야 되는 거지.
그렇죠 이거랑 이거랑 같은 숫자고 그렇죠 이해돼요.
여러분 여기가 4였으면 사야 된다. 앞에서는 알겠어요.
여러분 지금 64개를 출력을 64개 뽑고 싶으니까 64가 된 거예요.

참석자 1 33:03
그쵸 알겠죠? 이해를 정확히 하시라고요. 알겠죠?
연습시키는 거예요. 여러분 알겠죠? 실수하는 거예요.
알겠죠? 시험을 낸다고 안 낼 수도 있지만 알겠죠?
내면 맞추라는 거죠. 높은 북이 돼요. 알겠죠? 높은 북이니까 이 책 보면 찾아보면 되잖아요.
그쵸 그리고 이제 제가 시효를 낸다고 그랬으면은 좀 표시해 놓으면 좋겠죠 그렇죠 아까 저처럼 못 찾지 말고 아까 제가 못 찾았잖아요.
그럼 잘못했지 그렇죠 시험 본다고 그러면 제가 준비했어야지.
그렇죠 알겠죠 헷갈리는 거는 그래요. 그다음에 이거를 이제 9.3.5에서 적용을 했는데 이거를 스틱 세션은 좀 약간 또 이제 거대하니까 이거를 모두 적용해서 이게 사실 제목이 제목이 모두 적용돼 있는데 이게 원래 영어로는 푸딩이 월트 게라고 피딩 논문에 항상 이런 게 있어요.
제안 한참 한 다음에 푸딩이 올투게더 이런 걸 말을 많이 써요.

참석자 1 34:02
올투게더 푸딩이 크게 오르스르니 뭐든지 전부 다 한꺼번에 다 만드는 건데 그래서 이제 제목은 원래 면 미니 익셉션 만드는 건데 교과서에 여기 정확한 여기 뭔가 워딩이 나오는데 반복되는 침 블록 이 모듈 같은 거 있지.
그리고 이제 여러 개 합성 고층과 최대 프린층으로 되면서 지금 3352쪽 하고 있어요.
여러분 그리고 특성 맵의 공간 방향 크기가 줄어듦에 따라 층이 피트 개수 공간 방향으로 이제 가로세로 바라는 거야.
가로세로. 그렇죠 이게 줄어들면서 필터 개수는 증가하는 게 여러분 아까 전에 VGG 16 9 그림 있죠 v 16 이 너무 앞에 있네.
이 그림 약간 머릿속에 넣어놓을 필요가 있어요. 이렇게 생겨 먹었어 보통 알겠죠 이게 뭐냐면은 중간 방향이 가로 세로가 줄어드니까 포터 개수는 늘어나는 거 여기 보면은 5128 256 512 이렇게 늘어나고 있잖아요.

참석자 1 35:08
그쵸 늘어나는 거 보이죠 여러분 그렇죠 128이었다가 256에서 512로 그렇죠 필터 개수 늘어나고 있죠 그쵸 공간 개수 이 가로 세로는 줄어드는데 이 내가 뚱뚱해지고 있잖아 채널 개수 플레임 개수가 많아지고 있어요.
그렇죠 예 됐어요. 여기 맨 처음에는 얼마였는데 이거 3이었지 RGB 3이었잖아 알겠죠 여러분 그렇지 됐죠 8이었다가 나중에 obcb까지 증가했어.
그쵸 알겠죠 그렇게 된다는 얘기하고 있는 거예요.
정확히 무슨 얘기인지 알겠죠 여러분 그렇게 보통 한다는 게 너무 길다 그래요

참석자 1 35:51
이거 어디 갔어? 그래요 그다음에 깊고 좁은 학격차가 넓고 얇은 것보다 낫다는 거 이거 여러분 딥러닝이죠.
딥러닝 딥러닝 얘기하고 있죠 그렇죠 딥러닝이지 딥러닝이 층이 많은 거지 그래요 좁은 거는 여러분 뭐예요?
이게 지금 추상화시킨 거잖아 추상화 그래서 널 멀리서 보는 거지 맥스 플링 하면 이제 멀리서 보는 거잖아요.
여러분 그렇죠 그렇죠 나무로 보는 게 아니라 숲을 보는 거잖아.
그렇죠 그래요. 근데 잔체 연결을 추가하는 게 좋죠.
그렇죠 배치 전기화도 하는 거 방금 배운 거 그렇죠 알겠죠 그다음에 컴브투디를 세퍼로블 컴브투디가 자기가 제일 제안한 거고 좋다고 했죠.
본인이 제안한 게 그렇죠 그렇게 돼 있어요. 도움이 될 수 있어요.
그래도 항상 도움이 적어놨어요.

참석자 1 36:40
그래서 어쨌든 이게 보면은 이거 지금 데이터 어그멘테이션 먼저 하고 데이터 인식하고 우리 스케일링도 하고 맨 처음에 컴퓨터디 해놓고 그래서 여기 맨 처음에 분리 합성 보험의 특정 채널 대체소 독립적이라는 과정을 RGB는 안 그렇거든요.
RGB는 LGB에 굉장히 상관계가 있어요. 서로 보통 빨간색만 보든지 호스만 보든지 되게 가 훈련이 많기 때문에 얘네들은 그냥 같이 하기 위해서 따로 매치를 해버리고요.
맨 처음에는 이해돼요. 여러분 뒤에는 뎁스 와이즈 플래퍼롤 코미티디가 이게 덱스 와이즈거든요.
세프라고 하면 이렇게 그래서 이거를 하게 하는 건데 이게 보면 6%로 32 64 128 점점 크게 하는 거 보이죠.
여러분 이거를 사이즈로 해가지고 어디다 넣으려고 그러냐면은 여기 이 사이즈 컴퓨터 컨볼루션의 퓨터 개수죠.
여러분 히터 개수를 좀 더 늘리려고 하는 거예요. 이해돼요.

참석자 1 37:36
여러분 여러분 지금 352쪽 하고 있는 거 알고 있나 알겠죠?
그렇죠 됐어요. 레지디얼이 x가 여기 전체에 들어왔을 때 여기 들어오는 거잖아요.
마지막에는 이게 루프 돌고 나면 맨 마지막에도 x겠지 이 루프 맨 마지막에 x 있죠 에드 고기 또 다시 또 들어가게 맨 마지막 게 다시 들어가게 하는 거예요.
그렇죠 이렇게 돌면서 이렇게 돌면서 색을 쌓는 거지 처음에 사이즈를 30으로 쌓았다가 604로 쌓았다가 이해되죠?
여러분 차차차차 이렇게 많이 해요. 알겠죠 이거를 또 함수를 만들어 가지고 이렇게 할 수도 있겠지만 함수를 만들어서 이렇게 보면서 할 수도 있잖아요.
알겠죠? 여러분 그래요. 그다음에 여기 바이러스도 보통 여기 바이러스 안 쓰나요?
그다음에 베치노멀레이션 하고 멜로하고 보여. 여러분 여기 보면은 여기 액티베이션 없지 없고 배치 마이 하고 l로 하고 그렇죠 또 여기 발 수 없고 액티베이션 없고 여기는 배치 마 더 이상 안 하는구나 다치는 말로 이제 여기서만 한 번 했네.

참석자 1 38:42
그리고 세포 로브 커브 투디 또 한 번 한 다음에 넥스플링 투디하고 그다음에 커브 2D 하고 이런 식으로 했어요.
레지디어를 다시 레지디어를 한 번 더 한 번 레지디어를 앞에 있던 레지디어 앞에 있던 것처럼 앞에 있던 거 그 앞에 있던 거를 가지고 한번 컨볼루션 또 충가시켜줘서 사이즈를 맞춰준 건데 이게 스트라이즈를 2로 했죠 그쵸 이게 항상 두 배로 지금 만들어지고 있어서 그래요.
알겠죠? 두 배로 늘어나고 있거든 필터 개수가 계속 뜨면 두배로 두 배로 돼야 되니까 사이즈로 일로 해놨어요.
더 늘어나라고 그것도 다 맞춰서 해줘야 돼. 그렇죠 이런 거 못 맞추면 이제 나중에 막 좀 조작하려고 그러면 되게 힘들겠지.
그렇죠 CCPT한테 물어보면 잘해줄 수 있는데 p는 이제 또 골치 아프겠다.
그렇죠 그런 거지 그래요.

참석자 1 39:34
어쨌든 단순한 작업은 요즘에 거의 HTPT가 다 해결할 것 같고 거의 이제 코딩 단순한 컴퓨팅 사업에서 배우는 것들은 거의 다 될 것 같고 중간고사 기말고사 1학년 쪽에 하려는 건 이제 인력이 안 필요한 것 같아요.
진짜로 3 4학년은 언제까지 안 필요할까? 근데 계속 틀어나오니까 이제 아마 학습하는 것 때문에 아마 상당히 자동화가 될 거고 복잡하면 복잡할수록 이제 좀 그럴 거예요.
그래요. 그다음에 마지막에 다 한 다음에 글로벌 에버세플링하고 이거 왜 하는지 알죠?
여러분 이거 전부 다 벡터만 댄스를 연결하려고 하는 거야.
그쵸? 댄스 그래픽 이거 쓰는 거야 그렇죠 이해되죠.
여러분 그냥 플래팅 하는 것보다 이게 훨씬 나은 거지.
드라아웃도 여기서 여기다가 써줬네. 그래요. 그다음에 댄스를 하고 이렇게 해줬어요.

참석자 1 40:29
이런 식으로 하는 거를 제가 여기 보면은 원래 이제 이걸 안 쓰는 거에 비해서 저렇게 베스트 라이즈 안 쓰는 거에 비해서 많이 줄어든다는 걸 얘기해 주고 그래서 실제로 그래 유명한 거 많아요.
이게 되게 훨씬 더 훈련이 잘 되는 게 많이 보여요. 이거는 지금 강아지 고양이 하는 거 계속하는데 드디어 90%를 넘는 게 나오는 건가 83.5에서 많이 좋아졌다.
이런 게 나와 있어 교과서에 193쪽에

참석자 1 41:08
그래요. 그래서 다 했다. 성공적으로 숙제를 여러분이 할 수 있지 않을까 숙제를 한번 다시 적용을 하면 숙제에 숙제에 대해서 설명해 주면 숙제를 합시다.
2명이서 4명인데 지금 3~4명이 됐으니까 됐고 각 팀당 4 가지 이상 3명의 팀은 좀 힘들겠네.
그렇죠 미안해요. 근데 내가 거기는 적당히 나눠서 해보세요.
여러분 영입하려고 노력을 별로 안 했으니까 그냥 가는 거지.
그래서 이거 우리 팀이 어느 팀이지 하여튼 나 기억 안 나는데 전혀 사심 없이 했어요.
여러분 진짜 그다음에 각 팀당 두 개의 데이터 세스트라고 그랬잖아요.
그쵸 보면은 각 데이터셋을 동일 DNN 모델로 트레이형 이밸레이션 하는데 나눠서 하면 편하잖아.
그쵸 이런 게 다 혼자서 해도 일이 여러 가지를 해봐야 비교가 되잖아요.
여러분 여러 가지 해봐야 비교가 되는데 혼자 하면 괴롭잖아요.
그래서 그냥 이렇게 하는 거야. 알겠죠?

참석자 1 42:13
그 코렉도 여러분 되게 다양하게 요즘 비싸게 굴고 그렇죠 그럼 여러분 이거 이것도 있는 거예요.
여러분 여기서는 우리 오픈 게시판 여기 올려 있는데 학과 딥러닝 서버가 있어요.
여러분 이걸 쓰면 이거 쓰면 제한이 없어요. 엄청나게 늘어났거든요.
근데 여러분 또 다 들어오면 또 골치 아프겠지 코리 하다가 관심 있는 사람 들어오세요.
알겠죠? 너 우리 사실 서버가 별로 많지가 않아. 그래서 혼자 쓰면 아주 아주 어쨌든 적당히 쓰세요.
열어봐야겠죠. 가끔 프랩이 잘 안 되면 팔로 이게 여러분 유전면 비밀번호 기억하고 하세요.
알겠죠? 이거 너무 이거 많이 하면 저희 못 쓰게 돼요.
알겠죠? 사실 여기 관련 안 된 사람이 있어요. 알려줄까 한 원장 말고도 여기 이해진 듣지 말고요.
이렇게 어떻게 들지 알아 알아야지 잘 알기 바래요.
킬파레스 쿠모라티스에 대한 데브옵스의 뭔가 대가가 되길 바래요.

참석자 1 43:21
하여튼 그래서 지금 여러 가지를 해보기를 하면 콜랩을 여러 개 동시에 띄워가면 좋은데 그게 안 된다고 프레이 지금 한 개밖에 안 되는 경향이 있어요.
이제 지금 돈이 없나 봐 그래가지고 구글 검색도 잘 안 되잖아.
구글이 좀 약간 힘들겠다. 그렇죠 검색을 해야 돈을 벌 텐데 그다음에 개인별 한 가지 이상이고 이게 다 이해되죠 그럼 팀원에서 공유하라는 거지 그리고 팀원이 공유를 안 해줬어 그럼 안 했다고 쓰면 돼 알겠죠 그러니까 난 안 해서 못했다고 쓰면 된다고 리포트에다가 근데 그거에 대해서 전혀 나쁜 놈이라고 생각하지 않는다고 나는 팀원에 의해서 내가 피해 가는 거 바라지 않는다고 알겠죠.
너무 이거 숙제인데 뭐 그렇죠 이도 하고 그다음에 개인별 리포트를 제출하는데 팀 구성원이 누구다 항상 쓰고 알겠죠 헷갈리니까 여러분 팀원의 소스 코드를 보면 여기 되게 은근히 잘 적어놨어요.
알겠죠 하세요. 이렇게 논의는 각자 다 써요.

참석자 1 44:24
알겠죠 그다음에 이제 텐스 플로우 데이터셋이 여러 개가 있는데 너무 어려운 거 하면 힘들길래 제가 이렇게 조금 간단한 것만 갖다 놨어요.
알겠죠? 여기 대표적인 거로 예를 들어서 락 페이퍼 시즈즈가 제일 쉬운 거 3개밖에 안 하잖아 가볼까 한번 가면 가위바위보 가위바위 이게 이상하면 이게 이게 보장이라고 된 거야.
이거를 가위를 이렇게 이상한 가위를 이게 지금 가위냐 고자기냐 이거는 페이퍼 보자이고 얘는 가위래 뭐 그런 거죠 좀 그래 그러니까 원래 이거는 그러니까 뭐냐면은 이게 항상 트레이닝 로스트는 항상 줄어들 수밖에 없는데 트레이닝 로스트가 무조건 좋다고 좋은 게 아니에요.
물소리 더 작아진다고 그 밸리데이지 로스가 좀 어느 정도 안 줄어들기 시작하면 과대 적합 된 거고 릴리 노스가 무조건 커지는 게 아니라 안 줄어들어도 여러분 더 이상 할 필요가 없는 것도 있어요.

참석자 1 45:31
여러분 알겠죠 그래서 여러분 지금 제가 하라고 한 게 되게 많은데 보면 데이터 고미테이션도 해보고 말기도 하고 이거 이런 거 있죠 여러 가지 많잖아요.
그쵸 그런 거 다 이제 이것저것 디섹션도 여기 다른 디섹션 해도 되고 다른 거 해도 되고 알겠죠 여러분 그런 걸 해보라는 거고 이거 내용이 다 들어가야지 제가 점수를 다 주고 안 들어가면 점수를 까는 거예요.
알겠죠 심플하게 빼먹으면 까는 거지 뭐 알겠죠 다 이해되죠.
여러분 제가 숙제하는 걸 항상 그렇게 하잖아요. 여러분 이걸로 이밸리이션 하는 게 아니라 그냥 성실하게 다 했냐 말라 보잖아요.
점수 매기는데 그렇죠 거의 온오프지 그쵸 빼먹으면 감수를 까야지.
그쵸 어떻게 그쵸 알겠죠 본인이 사실은 이걸 어떻게 쓰길 바라냐면은 뭔가 인사이트를 얻길 바라는 거지 알겠죠 그래요.
그다음에 모델은 좀 세이브 해놔야지 나중에 또 다시 세분할 수가 있겠지.

참석자 1 46:30
그다음에 또 뭘 보여주려고 그러면은 강의 자료에 나서 이 많은데 3분 안에 사실 강의할 시간 없을 것 같은데 그래도 내가 아까 예를 들어서 지금 컴버넷을 좀 빨리 배우지 않으면 사실 대부분 이미지 처리에서 문제가 되죠.
그쵸 이미지 처리하는 됐을 때만 공부하고 있고 하는 게 이상하잖아요.
그쵸 컴버넷을 안 쓰고 또 뭔가 해보는 것도 좀 별로잖아요.
그러니까 프로젝트를 하는데 컨버레이션으로 해봐야지 그냥 데스 댄으로 숫자하는 건 좀 그렇잖아요.
제가 그래서 여기까지 온 거예요. 이 숫자로 이렇게 늦어져 드는 게 이해되죠 그걸 좀 알아야지 하니까.
그런데 교과서에 나오는 게 상당히 좋지만 지금 사실 제가 트랜스포 러닝 열심히 얘기했잖아요.
트랜스퍼 러닝 트랜스퍼 러닝에서 이제 피치 스트레이션이나 파인트닝 하잖아요.
그런데 이 세상에는 로라나 날리디 스트레이션이 거의 필수적이 됐어요.

참석자 1 47:26
난리디 스트레이션도 여러분 사실은 되게 훌륭하고 이게 나온 지가 다 얼마 안 5년 정도밖에 안 된 것들이 전부 다 채층을 5년 동안 이제 교과서에 안 들어갔어요.
근데 거의 옥소실도 무궁무진하게 많고 약간 모델이 크면 이거 안 쓰면 이상한 거예요.
안 쓰. 그리고 난리 디스테이션도 이것도 은근히 막 이거 원래 논문이 노벨 물리학상 받은 노벨 물리학상 바드식 제프리인팅 교수님이 쓰신 건데 이거 또 저자 3명밖에 없고 옛날에 썼어요.
벌써 근데 이것도 되게 훌륭하거든요. 이게 뭐냐면은 여기서 여기서도 인사이트가 말한 게 이것도 인사이트 만큼 이거죠.
그러면 만약에 여러분 LLM을 한다 LLL을 만약에 파인 튜닝 하겠다 그럼 거의 힘들어.
사실은 예를 들면 너무 파라미터가 많아서 로랑은 원래 파라미터 그대로 둔 상태에서 차이점만 이제 훈련시키게 하는 놈이에요.
나중에 머드 시키면은 전혀 임플런스가 늦어지지가 않아 머즈 시키는 것도 별로 오래 걸리지가 않아요.

참석자 1 48:26
머드 시키는데 아무리 빨라도 늦어도 한 10분이면 된다더라고 아무리 늦어도 머지 시키는 거잖아요.
더 더하기 위해서 넣는 거니까 어쨌든 원래 원래 건 그대로 두고 차이점만 시켜가지고 나중에 어차피 수학이니까 차이 프라이터라서 시키는 거예요.
차이점 그래서 나중에 다 그거만 웨이트 조그만 시키고 나중에 머지 시키면은 훨씬 잘 되는 게 유명한데 여기 근데 간단한 거면은 그냥 원래 파이팅에도 잘 되고 여기서도 이제 여기는 어텐션 쪽만 어텐션 아직 안 배웠는데 그쪽만 지시했어 어 미리 가르쳐주는 게 좋을 것 같아서 좋은 논문도 쓰고 있고 그런데 알겠죠.
드론 논문에서 내려라 하는 사람들 파인 튜닝은 그냥 무턱대고 하지 마시고 이거 쓰시고 수업시간 중에 교과서에서는 못하지만 그다음 날리디 스트레이션은 좋은 인사이트가 있는데 뭐냐면은 일도 남았지만 알려주는 가면 디스플레이션이 이거는 중요한 게 뭐냐면 원래 우리가 항상 원한 인코딩으로 정답 값을 알려주잖아요.

참석자 1 49:27
로스 값은 어떻게 되는 거예요? 포스 센트로피를 하는 거잖아요.
포스 센트로피는 이거랑 이거 얘랑 이렇게 다 곱해가지고 나와서 더하는 거기 때문에 얘가 몇에 몇 승이냐는 걸 보는 거거든요.
자연 상수 이 있죠. 이게 몇 승이냐가 크로스 엔트로피예요.
마이너스 룸의 로그잖아요. 룸의 로그 값이 이게 몇 승이냐 자연 상수에 이게 몇 승이냐는 뜻이거든.
그래서 이게 그래프가 이렇게 생겨가지고 이게 만약에 0에 가까우면 만약에 1인데 0.9면 이거 되게 이쪽이니까 숫자가 되게 작을 거예요.
지금 만약에 이게 0이 0.1이어 봐 그러면 코센트 효과가 무지 커져요.
이해돼요. 여러분 자유가 예측한 게 여기가 이게 0.9면은 프로센트로피는 어쨌든 여기가 여기가 1에 가까우면 0이 되는 거야.
오류가 그리고 여러분 우리가 메네쓰레기 크로스 엔트로피로 쓰잖아요.
몰라요. 그랬어 그래 나중에 이거 한 번 더 해줘야겠다.

참석자 1 50:19
어쨌든 이게 무조건 이렇게 만드는 게 좋은 게 아니라 무조건 이렇게 만드는 게 좋은 게 아니라 이렇게 만드는 게 낫다는 얘기야.
로스트가 장한 게 좋은 게 아니야. 케이스에는 케이스는 이렇게 되버려.
칸을 이렇게 작게 만들면 알겠어요. 여러분 50분이 지났구나.
여러분 여기까지 하고 다음 시간에 이어서 할게요.
알겠죠? 숙제하시고 여기서 많은 걸 얻어왔으면 좋겠어서 지금 어텐션도 빨리 들어가야 되거든.
사실은 어텐션이 되게 중요하잖아요. 사실 여러분 어텐션 그래요.
들어갑시다.


clovanote.naver.com