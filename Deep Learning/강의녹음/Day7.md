딥러닝 day7_1
2025.03.26 Wed AM 10:02 ・ 47Minutes 12seconds
심승환


Attendees 1 00:03
손실 함수가 중요하다는 얘기지 손실 함수였고 그렇죠 결국은 그냥 원래 이제 트레이닝 안 할 때는 그냥 트레이닝 한 다음에는 파라미터에 그냥 이렇게 결과가 나오는 거지만 트레이닝을 할 때는 손실 환산으로 항상 계산을 해서 손실 가수 계산해서 그거 가지고 가중치를 업데이트하는 건데 그렇죠 가중치 손실 함수가 이제 보통 뭘 쓰는지를 알려줬지 여기 여기 지금 리뉴 리그레션에서는 얘를 썼죠 그쵸 그쵸 여러분 지금 여러분 좀 약간 헷갈릴 것 같은데 지금 여기 보면 이렇게 생긴 게 이게 지금 요 sl 함수 요게 모든 머신러닝에서 다 그런 거예요.
머신러닝에서 딥러닝 이건 머신러닝이 딥러닝하고 머신러닝의 차이점이 뭐예요?
여러분 여러분 AI랑 그냥 그냥 AI랑 머신러닝의 차이점이 알아야 되고 그렇죠 그냥 AI 머신러닝 머신러닝은 학습하는 파라미터가 있는 거예요.
그쵸 이거는 지금 딥러닝을 얘기하는 거예요. 머신러닝을 얘기하는 거예요.
요 그림이 이거 딥러닝을 얘기하는 거예요.

Attendees 1 01:15
이게 이게 머신러닝을 얘기하는 거예요. 그냥 그치 여기 그냥 그러니까 꼭 딥뉴럴 네트워크 아니라도 아직 잘 모르지만 그렇죠.
근데 머신러닝에서 통용되는 건 다 딥러닝에서 통용이 된다는 거지.
그게 중요하죠. 그러니까 이게 여기 에셀 함수가 여기 니까 리뉴얼 리그레이션일 수도 있고 딥뉴 네트워크일 수도 있다고 그렇죠 어쨌든 다 중요한 거는 학습을 한다는 건 전부 다 점수를 매겨야 돼.
하여간에 점수가 다시 심실 함수죠. 사실은 그렇죠 이해됐죠 스코어 같은 거지 포스트 펑크리 함수가 따로 있는 거지.
그쵸 요 함수랑 다르게 그쵸 그래서 그 손실 하수 나온 거 결과를 가지고 가중치를 데이트하는 가중치를 업데이트하는데 손실 함수가 최소가 되게 손실 하수 값이 그렇죠 그리고 이제 보통 컨트리어스 밸류 프리딕션 할 때 가장 많이 쓰는 손실 함수는 보통 열심히 설명했죠.
이거는 또 퍼포먼스 내가 여기 베이직에 갔다 놔가지고 또 헷갈리네.

Attendees 1 02:13
여기 베이직에서 설명해 줬지 그쵸 베이직에서 내가 퍼포먼스를 어떻게 하느냐 일단 퍼포먼스 이트 여기가 다 낫지 그쵸 그렇죠 여러분 rmsa나 MA 있습니다.
그렇죠 그리고 심지어 이제 알아 나도 그냥 mse 쓸 수도 있어.
그렇죠 루트 루트 안 씌울 수도 있다는 거죠 그쵸 그래요 여기 지금 요 슬라이드에서 나온 요 리뉴얼 리레이션에서 얘는 루트를 안 씌웠어 그냥 MSD를 했어요.
그렇죠 이해되죠 그리고 그리고 실제로 이제 학습의 핵심인 가중치 업데이트 손실 합수 결과값 가지고 가중치를 어떻게 업데이트하느냐 그거는 결국 가중치가 얼마큼 변하면 손실 함수가 얼마큼 값이 얼마큼 변하냐 이런 거를 중요시하는 거잖아요.
그쵸 그러니까 가중치를 변화시켜야 되는데 손실이 최소화 되게끔 변화시켜야 되니까 그래서 변화율 사실은 이게 변화율이죠.

Attendees 1 03:05
그쵸 그 특정 파라미터에 대한 변화율 그쵸 변화율이 다른 말로 그레디언트 그렇죠 그렇죠 여러분 그리고 이제 결과적으로 이제 그레디언 디슨트라고 그러는 게 사 경사라고 그러기도 하고 그렇죠 변화율이 작아지는 쪽으로 그렇죠 변화율이 작아지면 보통 이제 손실이 작아지니까 요 기울기가 여러분 작아지는 거 보이죠.
산 내려가도 기울기가 크다가 작아지는 급한 쪽으로 내려가서 이렇게 작게 만드는 거 있잖아요.
제일 낮은 곳으로 가야 되는 거니까 지금 산에 같으면 이제 높이가 높은 게 현실이 큰 거지 알겠어요 여러분 내가 그 얘기한 적이 없었구나 산이 높으면 손실이 큰 거였고 그랬었어요.
거기서 상장이 한 번도 얘기한 적이 없었는데 괜찮았나 알아들었겠지 괜찮아요.
여러분 몰랐어 몰랐으면 질문 좀 하세요. 여러분 이게 함수가 이제 값이 높은 게 산 높이랑 똑같은 거였어요.
낮은 데로 내려오는 게 손실이 작게 한다는 뜻이었어요.

Attendees 1 04:07
그래요 어쨌든 근데 그냥 곱하는 게 아니 변화를 그냥 곱하는 게 아니라 항상 러닝 레이트라는 걸 곱한다고 안 그러면 이제 이렇게 발생할 수도 있고 막 이래가지고 그래서 여기 약간 지금 여기서 벌써 이제 가중치 업데이트하는 거 다 트레이닝인데 사실은 그렇죠 그때 용어가 막 많이 나오죠 그리디승천 용으로 나오고 러닝 레이천 용어로 나오고 그쵸 같은 말로 또 사람들이 용어를 다양하게 써서 스텝 사이즈라고 부르기도 하고 그렇죠 정신이 없지 약간 그렇죠 다 외워야 돼 그렇죠 안 외우면 이제 못 알아들으니까 뒤에 좀 외워야 돼요.
그렇죠 제가 아까 이렇게 뜸 들리고 있잖아요. 자꾸 반복하면서 뒤에서 하나도 못 알아들을까 봐 그렇죠 막 나갈 가능성이 있어요.

Attendees 1 04:45
여기는 지금 멀티플 리니어리그레이션 이렇게 하는 거는 파라미터가 한 개가 아니라 여러 개일 수도 있고 그쵸 파라미터가 아니라 파라미터가 아니라 피처 값 그쵸 요거 요거 요거를 굳이 열심히 설명한 것도 지금 세 번째 설명하는데 너무 헷갈릴 테니까 이게 피처가 여러 개인 것도 있고 그쵸 여기 x 제로는 근데 항상 있는 일로 보는 거고 근데 일관성을 위해서 x라고 붙인 것뿐이에요.
메일이라고 하면 왜 내 편한데 왜 이렇게 굳이 하느냐 왜냐하면 우리가 이제 뭔가 벡터 같은 걸 만들거나 프로그래밍 하려고 그러면은 이게 인덱스 제로의 값이라고 생각하는 게 좋잖아 그렇죠 그래서 이게 일관성 있게 이렇게 표현하는 거지 이해돼요.
여러분 사실 항상 1이라도 여기는 그렇죠 x 제로가 그래서 s j 분의 이렇게 한다고 알겠죠 이렇게 표현하고 특성이 j가 이제 0부터 n까지 있는데 실제로 n 개인데 픽처는 0은 뭐를 의미하는 바이어스를 위해서 산수인 이를 위해 만들어 놓은 거라고 계속 강조하고 있죠.

Attendees 1 05:43
그리고 이 위에 또 어퍼 스크립트로 항상 슈퍼 스크립트든 어퍼 스크립트 이거 서브 스크립트 그쵸 이해되죠?
어 영어 용어 요거 어플 스크립트도 있잖아 항상 왜 있냐 이거 데이터 한 개 갖고 하는 게 아니니까 우리가 트레이닝 데이터 그쵸 그리고 용어도 벌써 트레이닝 데이터 밸리데이션 데이터 또 춤추는 데이터라고 얘기했죠 여러분 그렇죠 그런 것도 있었죠 그래서 그렇죠 그래요.
그리고 또 약간 기억할 거는 결국 행렬 연산이에요.

Attendees 1 06:14
그렇죠 행렬 연산 사람이 이렇게 하는 게 그렇죠 여러분 이게 결괏값이 이거는 여러분 지금 순전히 여기 이거는 예측할 때의 얘기지 그쵸 프리딕션 할 때 그러니까 트레이닝 안 하고 인퍼런스 할 때 말이 너무 뭐라 이야 여러분 지금 제가 용어를 전에 이 여기서 얘기할 때 베이스 얘기할 때 베이지 트이베스 아이 여기 여기 제가 슬라이드 수가 있는 여기 트레이닝 테스팅 뭐 이렇게 했는데 그렇죠 제가 이거를 또 약간 지금 또 하나 더 쓸걸 그랬어.
프리덱션이라는 말을 많이 써요. 용어 대신에 프리디션 예측한다는 말을 하잖아 예측하는 거잖아 결국 여기서는 항상 예측하는 거니까 인퍼런스라는 말 대신 프리디션이라는 말도 많이 써요.
또 빼먹었네 프리디션도 할 거 알겠어요? 여러분

Attendees 1 07:12
지금 요렇게 요거 요거 구하는 방법은 이게 라지 x라고 하는 행렬이면 피처드 스타이고 플랜드 파라미터 있죠 걔만 곱하면 추락 데이터가 나옵니다.
사실은 행렬로 리뉴얼 리그레션에서는 근데 이제 딥러닝 같은 데서 어떻게 한다 거기다가 액티베이션 보급 다 쓰고 또 또 보급하고 또 급하고 또 급하고 이 짓을 한다는 거죠.
그렇죠 또 급하고 그렇죠 그리고 여러분 행렬 연산하면은 곱한 다음에 싹 더해버리는 거 알지 곱하고 다 곱한 다음에 열린 방영시처럼 하잖아.
그렇죠 연립 방식이 아니라 그런 게 그러니까 이제 입력 값 하나하나에 입력값 여러 개에 대해서 어쨌든 출력 값이 여러 개 나오려고 그러면 연립 방식으로 그러면 연료 방식이라는 것도 뭔지 알겠지 이게 y 값이 하나가 나올 수도 있고 두 개 여러 개 나올 수도 있는데 여러 개 나올 경우에는 그렇다는 거죠.
그게 헷갈리겠다 넘어갑시다. 일단 더 많이 너무 선명하면 또 이제 어른 오후 피팅이 제가 이상해질 수가 있어서 그래요.

Attendees 1 08:14
잘못 알면 더 위험해 언러닝이 되게 힘들거든 한 번 잘못 배워놓으면 계속 가는 게 있어가지고 여기 그리고 그랜드 디스트 제가 러닝 레이트는 확실히 알았어요.
대충 알았죠 그쵸 그래요. 그리고 지금 이제 배치 데이터에 대한 개념 사실 이거 배치 데이터에 대한 개념이 이거 사실 아까 로스 펑션 계산할 때 다 계산할 때 데이터를 어떻게 쓸 거냐 기본적으로 몽땅 다 쓰는 거 있는 거 다 쓰는 게 이거였고 극단적으로 하나만 쓰는 게 이거고 그쵸 d로 그렇죠 적당히 나눠 쓰는 게 미니 지고 그렇죠 사실 요즘에 다 그냥 이거 미니 배치 이거 다 거의 다 이거 쓰기 때문에 얘로 그냥 이거 다 커버하기 때문에 미니 배치라는 거 이름 귀찮으니까 베스지d라고 불러버린다.
그쵸 그러고 있어요. 그래요.

Attendees 1 09:06
그리고 이제 배치 그리던 디스트 이거 중요한 그거라기보다는 사실은 이제 로스 펑션 결국은 조심할 게 이게 보면 원래 함수는 이렇게 생겨 먹었지만 원래 함수는 뭐야 원래 원래 함수 아까 여기서 보여준 거 요거가 이제 예측할 때 이렇게 생겨 먹었죠 예측할 때 여러분 그쵸 근데 그레이디언시치 계산할 때는 예측 함수에 대해서 하는 게 아니고 어디다가 한다고 예측 함수를 나온 결과를 다시 로스 펑션 통과시킨 거 갖고 하지.
그래서 여기로 다시 로스 펑션을 통과시키지 루스 펑션이 지금 여기는 지금 mse를 넣어준 거지 로스 펑션이 중요한 거예요.
여기도 되게 결과적으로 그러니까 예측값만 갖고 하는 게 아니라 예측 가지고 다시 루스 펑션 값을 가지고 다시 그레이드에 전달해야 된다고 알겠죠.
이해되지 요거 그쵸? 그래요. 여기 수식 복잡해 보이지만 사실은 그냥 푼 거예요.
다 풀면 이렇게 돼요. 그래서 결국 이것도 행렬 현상이에요.

Attendees 1 10:08
이거 이렇게 생긴 거 생년 연산 이거 사실 예측 값이에요.
이거는 이거 이거 이거 저거 제거하려고 지금 이렇게 트랜스포즈 시켜가지고 한번 해볼게.
선형 대서에 나오는 거 트랜스포드 시켜서 곱하면 완전 제곱 시키거든요.
그래요. 그래서 또 다른 사람이 설명 잘해놓은 거 갖고 온 거고 피처 스케일링 얘기했는데 피처 스케일링도 이러면 또 이건 또 뭔 얘기냐 이거 x 값들 있잖아요.
어떻게 보면 분포가 다양하면 그쵸? 프라미터도 되게 다양하니까 학습이 잘 안 돼서 분포가 비슷하게 만들어서 한다는 거 그쵸?
이거는 지금 거의 대세가 돼가지고 뉴럴 네트워크에서는 배치 노멀라이션이라는 그런 칭까지 쓰고 있어요.
액티베이션 층 이런 것처럼 배치 노멀레이션이라는 걸 중간중간 항상 시켜버리는 거지 학습 어차피 파라미터 학습 업데이트하려고 그러는데 방해가 되는 요소들을 없애려고 파라미터들을 다 이렇게 좀 정비화시켜버리는 거야.

Attendees 1 11:03
배치 노멀레이션이 무슨 얘기냐면은 결국은 이 프랑스 전기한다기보다 이거 못 알아들을 것 같다.
근데 미안해 나중에 보여줄게 일단은 여러분 이게 이게 뭐 하는 건지는 알겠잖아요.
여러분 대체 뉴럴 네트워크 되면 어떻게 되는지는 나중에 또 깨달아 봅시다.
여러분 배치 노멀레이션은 그때 가서 설명해 드리지 뭐지 괜히 나 들어나 봤다 그냥 그렇죠 일단은 그러니까 여러분 왜 이렇게 자꾸 떠오르냐면 제가 나중에 배추 마이즈를 안 쓰면 안 좋아 써야 돼요.
그냥 무조건 이렇게 여러분이 뭔가 커스터마이즈에서 남이 갖고 와도 배치노 마이즈 넣으면 잘 돼요.
보통 뭔가 수렴이 잘 안 되다가 그걸 끼워넣으면 되게 잘 되거든.
그래서 미리 알고 그걸 왜 끼워넣어야 되는지를 알아야 될 거 아니야 근데 그쵸?
얼리 스타핑이라는 개념 있죠? 여러분 그쵸? 요거 트레이닝 언제까지 할 거냐에 대한 문제야 그쵸?
이거는 이제 이거 밸리데이션 데이터를 가지고 하는 거예요.
그쵸?

Attendees 1 11:56
얼리 스타킹은 그렇죠 테스트 트레이닝 셋에 이게 팀 트레닝 셋이랑 밸리데이션 셋을 나눠가지고 하는 거지 그쵸 알겠죠 여러분 그래서 여기 미리미리 저장해 놓고 이거 쓰는 거지 나중에 그렇죠 그리고 여기 이렇게 여기 이걸 지나간 거 이렇게 여기 보면은 지금 에폭이 한 이게 에폭이 여러분 뜻이 뭐냐면 한 에폭에서 한 데이터 한 트레이닝 데이터 다 쓰는 거예요.
미니 배출로 여러 번 해가지고 보통 그렇게 보통 그렇게 매체로 정해졌으니까 원래 이제 한 스텝이 한 한 스텝이 어떻게 느끼냐면 여러분이 그레이드를 업데이트하는 게 한 스텝이라고 생각하셨잖아요.
스텝 사이즈가 러닝 레이트가 이제 스텝 사이즈라고 부르는 것처럼 이해가 돼요.
여러분 한 스텝을 보통 한 단계 한 스텝 이런 게 이제 발자국 넘긴다는 게 사실 그레디언트를 업데이트 그레디언트래 그레디언트를 가지고 파라미터 업데이트하는 거지.

Attendees 1 12:50
그래서 러닝 레이트 스펙 사이라고 그런다고 사실 정확하게는 러닝 레이트를 가지고 원래 변화의 그레이저 줄 곱한 값이 사실 스펙 사이즈일 텐데 사람들이 막 그냥 막 불러가지고 요가 그걸 부를 때도 있고 저걸 부를 때도 있고 막 그래요.
그렇죠 알아들으라고 그랬죠 제가 그렇죠 시험에 내기 좀 그렇다고 그랬죠 그쵸 그래도 알아들어야지 뭐 그냥 그걸 의미하나 보다 이러면서 그래서 워낙에 이거는 지금 막 발전하고 있는 거라서 어쨌든 지금 이 에폭이라는 거의 의미는 뭐냐면은 데이터 하나 다 써가지고 미니 배치 다 쓴 거 이런 걸 의미해요.
보통 그래 사실 브랜드 트 업데이트 한번 여러 번 할 수도 있어요.
알겠죠 데이터 트레이닝 데이터 다 끝났을 때만 트레이닝 데이터 한 번 다 쓰고 한 폭 지나가고 보통 그렇게 해요.
알겠죠? 이해되나 미니 배치를 만약에 원래 전체 데이터를 미니 배치 만들었더니 5개로 만들었으면 다섯 번 업데이트한 상태가 하나의 폭인 거지

Attendees 1 13:46
한 스텝이 5개씩 그러니까 전체 데이터를 미니 배치로 5개를 나눴다고 만약에 말로 던지면 여기 이걸로 보여주자고 미리 얘기해도 괜찮을 것 같아요.
나중에 가면 여기 여기 다섯 여기 5개 맞나 5개 5개가 좋아서 사람 스크라 5개 돼가지고 여러분 보면은 이거는 원래 데이터는 이렇잖아요.
그래서 요거 요기 미니 배치를 하면 이렇게 만들었잖아요.
그러면 이렇게 한 번 한 거를 한 스텝이라고 부른다고 미니 배치 써가지고 포인트 업데이트 할 거 아니야 그렇죠 근데 이거 다 합치면 전분 스텝 적용할 거 아니에요?
그쵸? 그쵸? 이걸 한 에폭이라 부른다고 그래서 이해돼요.
한 데이터 다 썼으니까 몇 개 포 할래 그러면 이거 몇 번 몇 번 이 짓을 할래 이런 거지 그쵸?
미니 배치 사이즈만큼 더 자주 업데이트하겠지 시간은 비슷해 시간이 이거 하는 거로 하면 이거를 이제 한 이거를 이제 한 번 업데이트하는 그게 한 스텝이 한 에폭이 되는 거지 사실은 그렇죠 그래 여기 알겠죠.

Attendees 1 14:53
결과적으로 미니 베치 쓰면 빨라진다고 보통 생각하지만 사실은 f 숫자로 똑같이 해버리면 이게 비슷하게 걸리게 되겠지.
데이터 다 쓰는 거로 열어놓으니까 이게 내 루프가 두 번씩 되니까 여기서 한번 죽여도 되니까 그렇죠 그것도 왜 그런지도 이제 이렇게 자세히 알아야지 이해가 될 거 아니에요 이해되나 뭔 얘기인지 알겠어요 여기 좋아요.

Attendees 1 15:20
300번을 데이터 데이터 다 한 번 데이터를 훑은 거지 300번에 동일한 데이터를 그렇죠 트레이닝 데이터가 오직 트레이닝 데이터만 없고 근데 한 에터 안에 미니 배치 사이즈가 작아서 스텝이 여러 개였으면 업데이트도 그만큼 더 곱하기 미니 배치 개수만큼 한 거지 이해돼요 됐죠 지금 하세요 명확하게 알겠죠 그래서 어쨌든 지금 뭐 하고 있냐 미니 배치가 지금 대세니까 제가 미니 배치로 하는 거고요.
알겠죠 미니 배치가 대세라기보다 미니 배치가 사실 여러분 제일 뭔가 항상 뭔가 새로운 알고리즘이 나오면 결국은 아주 극단적인 거 두 가지가 있다가 그냥 다 커버하는 게 나오는 거지 여러분 컴퓨터 구조 다 배웠죠 안 배운 사람 없죠 거기서 캐시가 지금 안 쓰는 데 없잖아요.
캐시가 디트렌트 캐시가 있고 프로디어 소시트 캐시가 있는데 극단적이잖아요.
프리스트 소스 너무 비싸 듀테이트는 너무 싼데 너무 미스가 많이 나 중간에 절충한 게 셋 어스 스웨이트 잖아요.

Attendees 1 16:22
투웨이 코웨이 에이 데이 이런 식으로 지금 제일 많이 쓰는 게 4웨이 2a거든요.
써는 소시 대비 거의 다 써요. 절충 원웨이 하고 제도도 되고 그리고 원래 플리스 슈티브는 너무 비싸니까 잘 안 하고 이렇게 내사 사주만 갖고 그러니까 내 말은 얘가 하고 싶은 건 여러분이 뭔가 항상 공부할 때마다 뭔가 세상에 이런 게 있으면 세상에 지금 만약에 전도가 사람들이 이렇게 이렇게 BGD로 살고 있어 그러면 이 생각도 하고 요거 제안을 하면 되지 아니 근데 그렇게 해야 돼요.
항상 그렇게 다 돼 있어요. 이미 여러분 최소한 박사 받은 사람들은 다 생각들이 이렇게 하고 있어요.
진짜로 박사 받는 게 맨날 훈련하는 거야 이렇게 그래서 뭐가 남았나 뭐가 세상을 다 아주 추상적으로 이해해서 한 가지를 나서 5만 가지 이해하려고 하는 거거든 이해돼요.

Attendees 1 17:07
여러분 근데 이제 그냥 학부만 나오면 이것만 달랑 쓰고 이거 이거만 달랑 쓰고 학부만 나온 게 아니라 이제 무슨 얘기냐면은 학부만 나와서 그렇게 해도 다행이지 그렇게라도 하면 그러니까 자꾸 여러분이 근데 어떻게 해야 되냐면 잘하려고 그러면 사실 여러분 어제 지난 주말에 슬프게도 거기 잠시만 대 삼성전자에 부회장님이 돌아가셔가지고 결혼식 알아요.
여러분 결혼식 너무 끔찍한 일인데 결혼식 딸님 결혼식에서 와인 잔에다가 양주를 따라 마시다가 돌아가셨대.
누가 따라줘가지고 술 때문에 돌아가신 거예요. 한 번도 여러분 보시면 안 돼요.
술이 위험해진다. 양주 먹으면 죽어요.

Attendees 1 17:47
여러분 없죠 너무 딸 거시게 죽어버리면 돌아가셨으니까 얼마나 끔찍해 근데 이 얘기를 왜 하냐면 갑자기 이제 그분 그래서 저렇게 저도 이거 누구 누구시지 하고 찾아봤더니 그분이 되게 높은 자리에 올라가신 건데 인하대학교더라고 서울대학교들은 다 밑에 있어 아니니까 내 말은 여러분 학명에게 중요한 게 아니라 어떻게 생각하느냐가 중요하다고 그래서 이런 거 있잖아요.
이런 거 이런 걸 맨날 그냥 덜덜덜덜 외우고 있는 게 아니라 이렇게 생각하고 이러면 훨씬 더 일을 잘할 수 있다 이 얘기죠.
엉뚱하게 이야기가 됐지만 나는 그게 하도 충격적이라 가지고 술 먹다가 죽을 수도 있고 너무 좋은 날 그게 그렇죠 너무 슬프잖아요.
그렇죠 다른 일 다 얼마나 스트퍼요 그렇죠 어쨌든 충격적이라서 그냥 얘기했고 어쨌든 중요한 거는 이거 사고 방식이 이렇게 생각하는 거야.

Attendees 1 18:29
박사 이런 거 중요한 게 아니라 이렇게 생각을 하는 게 중요하다는 얘기하는 거야.
공부할 때 좀 정리 좀 하라고 정리하면서 공부하라고 알겠죠.
그냥 막 달달 외우지 말고 알겠죠. 그리고 이게 도대체 무슨 의미인지 생각하고

Attendees 2 18:43
그 미니 배치가 그럼 여러 개 데이터가 있을 때 미니 배치 사이즈가 5라고 하면 각 데이터가 5씩 잘라지는 거 맞나요?

Attendees 1 18:50
그러니까 여러 개 당연히 항상 데이터는 하나 트레이닝 데이터를 얘기합시다.
우리 트레이닝 데이터 트레이닝 데이터에 쌓인 트레이닝 데이터 셋이 사실은 그렇죠 체인 데이터 셋 사이즈가 예를 들어서 보통 얼마냐 아까 저기 앱 리스트 같은 거에는 전체 이제 7만 개를 주는 게 예를 들어서 7만 개가 있잖아요.
7만 개 중에 보통 한 4천 개 3천 개 이렇게 쪼갰다고 쳐요.
예를 들어서 4천 개를 트레이닝으로 하고 3천 개를 테스트로 쓰기로 했어요.
그러면 또 4천 개 가지고 또 3천 개는 트레이닝 진짜 트루 트레이닝을 하고 천 개는 밸리데이션 쓰기로 했어요.
그럼 그럼 3천 개라고 치는 거지 이제 예를 들어서 3천 개 3천 개 중에서 미니 배치를 한다 그러면은 미니 배출한다.

Attendees 1 19:34
그러면은 이제 한 번에 업데이트할 때 3천 개 다 하는 게 이 예고 한 개 갖고만 업데이트 계속하는 게 요 네고 얘는 뭐냐 적당히 보통 디폴트 값이 텐스 플로우랑 파이터치는 전부 다 32 데이 데이터 그냥 32 30이고 7천 30 아까 3천 개라 그랬지 3천을 32로 나누면은 대충 100번 하겠구먼 그쵸 100번 안 되겠지 이해되죠?
배포 스트 업데이트하는 거예요. 그리고 그 데이터를 다 합치면 다음 에폭이라도 이해돼요.

Attendees 2 20:10
그럼 애플이 마마지

Attendees 1 20:11
그럴 수 있죠 사실은 한이퍼에도 그냥 잘 되기도 해.
한 내폭 내에서도 보통 그렇죠 한 넷폭만 해도 그냥 뭐 쓸 만해 아니 쓸 만한 게 나올 수 있다고요 여러분 그래서 그래서 보통 이렇게 미니 배치를 하니까 한 대만 해도 많이 넣는 거지 이렇게 이렇게 해서 점점 줄어들잖아요.
그쵸 근데 이게 러닝 레이트 작게 하면 너무 크게 하면은 이제 아주 운이 좋으면 한 번에 쫙 줄어들 수도 있고 진짜로 확 근데 러닝 레이터가 너무 작으면 되게 조금 조금 줄어들겠죠.
그렇다고요 여러분 알겠죠 그리고 여기 지금 뭐 하고 계시냐면 이거 지금 여기서 오피팅이 사실 이 교과서에 다 있는데 미리 그냥 설명을 다 해버려도 괜찮겠다.
그냥 근데 그냥 여기 보면은 이게 복습을 여러분 하면 여러분 세 번 네 번 보면 뭔가 기억이 잘 남는다잖아요.

Attendees 1 21:07
그 사이에 잠을 자고 세 번 네번 볼 때마다 잠이 한 번씩 들어가서 그랬더니 진짜로 되게 중요한 것 같아 나도 공부할 때 잠이 최고야 중요한 거 그래야지 장기 기억이 되거든요.
여러분 안 까먹어 그래서 내가 지금 반복 한 세네 번 해 주려고 그래야 되는데 이제 교과서 또 입고 나도 하고 그렇죠 지금 이러고 있잖아요.
이해되죠. 말이 빨라서 많이 반복할 수 있어 그래요.
그래서 여기 보면 이렇게 줄어드는데 실제로 우리 보통 로스 값 나오는 건 이거잖아요.
그렇죠 이렇게 이렇게 이거 한 번도 보지 못한 데이터 갖고 계속 이런 거 테스트하고 있는 거 제품마다 그래서 트레이닝 할 때는 계속 줄어들어 점점점 트레이닝을 하면 할수록 줄어들 수밖에 없어요.

Attendees 1 21:45
보는 거에 대해서 정답을 계속 주입시키니까 그렇지만 실제로는 잘하는 게 아닐 수 있는 거지 너무 여기에 오피팅 돼가지고 이제 그래서 이거 미리 요구했다가 여기 있을 때 저장 이게 실제로 여러분이 이제 캐시플로우나 파이토치 쓰면 저장을 중간중간에 놓게 하고요.
나중에 이거 이걸로 업데이트해버리죠. 그냥 나는 모르겠으니까 이 정도까지 해봐.
그리고 아니면 또 이제 얼리 스타핑 파라미터 줘가지고 몇 번째 점점 더 이후 밸리데이션 루스가 계속 안 좋아지면 실제로 이게 이렇게 되지 않고 제가 전에도 얘기했지만 이 생긴 모양이 여러분 나중에 알파 버튼 누르다 보면은 다 이렇게 생겼거든 이렇게 이러거든 셋째 이렇게 이렇게 경향 추위를 보이니까 한 번 증가했다 감소했다고 해서 알 수가 없다고요.
이해돼요. 여러분 실제로 이렇게 예쁘게 하지 절대로 없어요.
절대로 없어 이런 거 없어 세상에 알겠죠.

Attendees 1 22:41
여러분 막 이러고 있지 그래서 이게 이거를 이제 올라갔다 내려갔다 하는데 나빠지는 게 몇 번 반복되면 이제 그만두고 이렇게 할 수 있는 거죠.
계속 할 수는 없으니까 이거를 한 500이 아니라 거의 계속 할 때까지 해 봐 이러고 보통 근데 이거 이 이거 얼리 스타킹 파라미터 줘가지고 몇 번까지 나빠지면은 참을 길래 이런 거 있잖아요.
줘가지고 한 10번까지는 한번 해봐 이런 식으로 그렇죠.
근데 이게 보면 계속 나빠지는 것 같다가 사실 더 좋아지기도 하거든.
운이 약간 이게 파란트가 많으면은 알겠어요. 여러분 그래요.
그리고 이제 여기서부터가 사실 오 피팅인 거지 이해돼요.
여러분 모 피팅이 이 뜻이 뭐라고요? 여러분 과학의 적합 여기가 뭐라고 언더 피팅이지 언더 피팅 학습이 잘 안 된 거 학습이 안 된 게 언더 피팅이고요.
오버 피팅은 학습이 공부한 것만 잘하는 거야.

Attendees 1 23:35
실제 문제 잘 못 풀고 알겠죠 베스트 피칭이 필요한 거죠.
그쵸 오브 피팅이랑 언더 피칭 중간이 베스트 피칭 거죠.
그쵸 공부를 그러니까 이상한 거에 대해 적합함을 시키면 안 되는 거겠죠 그래요.
그래가지고 정말 실전에 강한 게 중요한 거거든. 실전에 강한 게 그렇죠.

Attendees 1 24:01
이게 이게 되게 위험한 게 정말 희한한 특성을 발견하기 시작하거든.
베이스 뭐냐면 여러분들 공부하다가 어떤 교수님은 이상하게 ox 퀴즈 내는데 ox에 우연히도 그 교수는 항상 ox 퀴즈에서 o가 있는 경우에는 맞는 거에는 뭔가 5 문제에 대해서는 문장이 길고 x 문제에 대해서는 문장이 짧은 거야.
그걸 파악하기 시작했어. 그렇게 해서 거의 다 만점 맞기 시작했어요.
근데 그거는 이제 오 피팅이지 실제로 잘 못할 거 아니야 이해돼요.
여러분 거의 만점에 맞기 시작해. 이제 사실은 그게 자기가 잘하는 게 아니잖아.
그런 일이 생기기 시작한다는 거예요. 이해돼요.

Attendees 1 24:39
여러분 오버 피팅이 일어난다는 게 진정으로 잘하는 게 아니라 이상한 특성을 파악하기 시작해가지고 그래서 그리고 이제 그렇게까지 하는 거는 사실은 이상한 특성을 더 뭔가 보기 때문에 너무 파라미터가 많은 게 좋지도 않아서 파라미터 적당히 이게 여기 뒤에 나오는 요거 바이어스 베이런스 스트랜스 오프에서 모델이 복잡하다는 게 이제 파라미터 개수가 되게 많다.
그런 거랑 상관이 있어서 라텍스 파라텍스를 너무 많이 쓰는 거지 특성 파악을 중요한 특성을 가지고 해야 되는데 작은 특성 별로 무시 중요한 특성이지 실제로 심전의 중요한 특성을 봐야 되는데 테스트 데이터 관련해서 이야 트레이닝에 쓰는 데이터에 대한 특성만 파악해가지고 파라미터가 복잡해지거든 나눠지거든.
그래서 지금 이제 여기 여기까지 이렇게 다 했지 대충 대충 다 하고 제가 뭐 하려고 그랬냐면은 여기 해소 플로 플레이하는데 지난 시간에 하다가 마쳤잖아요.

Attendees 1 25:33
근데 그렇게 내가 보기에는 그냥 책을 막 열심히 읽는 것보다 이렇게 좀 한 다음 책을 팍 훑으며 지나가면서 다시 또 공부해라 이렇게 해서 지금 제가 여러분한테 세네 번 자고 난 다음에 그런 거 시키려고 하는지 알겠죠.
여러분 또 보면 책은 보면 또 내가 설명한 거는 지나가지만 책은 또 여러분 볼 수 있잖아요.
그래서 어쨌든 여기에서 이제 설명을 다시 하려고 그래요.
여기 이거 가지고 한번 이거 이게 너무 잘 만들었거든.
너무 나한테 감탄스럽거든. 되게 이게 이렇게 이렇게 하면 되지.
여러분도 한번 숙제를 내줘야지. ox 퀴즈 ox로 그러니까 냈다 안 냈다 정도로 할 테니까 한번 이거에 대해서 거의 제가 항상 낼까 말까 하다가 실점 말리 귀찮아가지고 근데 거의 냈다 냈다로 할 테니까 과제를 제가 그럼 열심히 듣잖아요.
이제 배점이 5점으로 해놨네. 5점 5점에서 넘어가면 10점은 5점 5점으로 합시다.
일단은 어쨌든 뭔지 알겠어요. 여러분 얼마나 그렇다고 안 내는 건 좀 괘씸하지.

Attendees 1 26:32
근데 5점이면 이제 감수 할 것 같다 없어요. 그래도 어쨌든 여러분 댄스 플로우 플레이그라운드를 연습했는데 여러 학점 조정 인사이트를 기록해서 제출하는 거예요.
심지어 화면 캡처는 화면 캡처 화면 캡처 5점짜리 넣어 두면 10점을 해야겠다.
캡처 두 장 이상 여러 가지 알겠죠. 그리고 여러분 미안하지만 인터넷에 너무 커뮤케 할 게 많거든.
그래서 여기 브라우저 전체를 캡처하고 URL 창에 주소 앞에 본인 학번 이름 찾기 이해돼요.
여기다 이렇게 하는 거예요. 여기다 이렇게 해서 1 2 3, 4 5 홍길동 아이 해 줄까 이렇게 여기 홍길동 이렇게 해서 캡처하라고 이렇게 해놓고 캡처하면 여러분 밖에서 또 못 가고 이거 이거 합성하면 더 귀찮거든 그냥 하면서 하겠죠.
티가 나거든요. 사실 아니 할 수 있어. 사실 요즘 AI는 근데 그냥 하셔 알겠죠 내가 왜 이러냐면 여러분 사실 내가 좀 괘씸하더라고 작년에 보니까 그런 선배들이 있어서 이해는 돼 정말 효율적으로 한 거지.

Attendees 1 27:48
근데 뭐 그렇게까지 사람 속여야 되나 싶어가지고 저거 분명히 본인이 안 한 것 같은데 그래서 그래서 점수는 그래도 사업 개체까지 하는 30일인가 있으니까 10점으로 올릴게요.
저장 날짜가 여기 여기에 이렇게 돼 있구나 이게 지금 여러 가지 그냥 어떤 인사이트를 얘기하는 거니까 이것저것 해보는 거잖아요.
여러분 대단한 거 아니잖아요. 그쵸 그냥 써보는 거야.
그냥 그리고 이게 뭐 잘했다 못했다로 점수 매기지 않겠다는 거예요.
여러분 그냥 했다 안 했다로 이해되죠. 여러분 갖고 놀라고 가끔은 증거를 만들고 이게 여러분한테도 어떤 식으로 하면 생각하냐면 내가 노트를 남겼다 생각하고 하시라고요.
알겠죠 내가 노트를 남겼다 이해돼요. 여러분 뭐 뭔 얘기인지 그러니까 내가 나중에 볼 만한 뭔가 공부해서 봤으면 뭔가 노트 남기기 좋잖아요.
그렇죠 그런 차원에서 이거 언제 일주일 밖에 2주만 하면 될 것 같아요.
다음 주 목요일 여기까지 할게요.

Attendees 1 28:47
그냥 토요일 전까지 그래서 이거 보면은 여기 제가 지난 시간에 사실 설명을 했는데 여기 데이터가 있고 아웃풋이 있고 그렇죠 그쵸?
그리고 여기 에폭이라는 게 있네. 그렇죠 에폭이 뭔지 대충 설명을 다 했다는 거예요.
러닝 레이트도 알았다. 이제 그쵸 액티베이션 펑션도 뭔지 대충은 알아 그렇죠 그다음에 레글러라이제이션 이거 설명 안 했어 아직도 그렇죠 아직도 안 했어 슬라이드를 제가 만들어 올까 말까 고민했는데 왜 안 했냐면은 이게 레글라이제이션이 여기 보면은 에런 램프 있잖아요.
여러분 교과서를 갖고 다니다 볼게 보람 있게 제가 한번 보여드릴게요.
여러분 지금 여기서 설명해 드릴게요. 이 교과서를 내가 왜 알아듣잖아?
교과서 한번 볼래요? 여러분 교과서 다 있지 없으면 옆에 사람 같이 봐요.

Attendees 1 29:39
여러분 교과서 교과서에 에러프가 어디 나오냐면은 순서 이게 원래 에러네이트라는 거는 여러분 또 뉴럴 네트워크랑 상관없이 그냥 머신러닝에 파라미터가 있는 놈이면 다 쓸 수 있는 거예요.
에러레이트는 어디 있냐? 차례 교과서 차례 교과서 지금 지금 목차 보고 있어요.
지금 목차 목차에 어디 나오냐면은 목차에서 여기 보면은 5장에 목차에 보면 5장에 머신러닝이 기본 요소라고 적혀 있잖아요.
그쵸 일부러 이 사람도 명확히 해 주려고 목차 보세요.
여러분 목차 목차 알겠죠? 머신러닝 기본 요소 있잖아요.
거기에 5장에 일반화가 머신러닝 목표라고 적혀 있잖아요.
그쵸 여러분 그쵸 일반화 이게 뭐냐면 오더 피팅 하고 싶지 않다는 거야.
그쵸 언더 피팅은 공부도 안 한 거고 그쵸 일반화라는 거는 그냥 실제로 자강하게 하겠다는 거지.
그쵸 그다음에 2장에 평가 나오죠. 평가 평가제가 지금 퍼포먼스 매트릭 같은 거잖아요.
그렇죠 어떻게 평가할 거냐? 그쵸 잘 돼 있어요.

Attendees 1 30:43
3장 훈련 성능 향상시키기 그쵸 로스 거 어떻게 하면 줄일 거냐 이런 건데 그래서 그다음에 4장 또 일반화 성능 향상하기 이렇게 나오는데 여기 일반화 성능 향상하기 일반화 성능 향상하기 나와 있잖아요.
5.4절 제목이 5.4조 제목이 일반화 성능 향상하는 거잖아요.
그 오브 피팅 안 시키려고 바아다 하는 거지 오브 피팅 안 시키려고 거기에서 맨 마지막에 5.4.4에 5.4.4에도 조기 종료라는 게 있죠.
조기 종료가 지금 제가 여기서 뭐 했을 것 같아요. 여러분 슬라이드에서 얼리 스타 얼리 스타핑 훌륭해요.
얼리 스타디오 우리나라 말로도 알아요. 조기 종료 영어로는 얼리 스타핑이 지금 5.44에 나왔잖아요.
그쵸 이해되죠 여러분 그래요. 그다음에 5.4.4가 모델 규제학이라고 돼 있잖아요.
규제가 영어로 레귤러 라이제이션이에요.

Attendees 1 31:32
여러분 여기 이제 레글로 라이브 이제 얘기 알겠어요 여러분 레글라이제이션이 뭔가 정규화시킨다는 느낌이 나는 레귤러 하면은 뭔가 보통 그렇죠 몸축성이라서 레글라이제이션 하면 이게 정규화하는 건 노멀레제이션이고 이거는 규제라고 번역을 해야 돼.
알겠어요 여러분 그러니까 약간 레귤러 라이제이션 하면 뭔가 정교한 느낌이 좀 들잖아.
약간 레귤러 하면 정규적인 이런 이 있잖아요. 근데 사실 그게 아니고 그렇게 번역하면 절대로 안 되고 레그레이션 정규화라고 번역하면 안 되고 뭐라고 규제 알겠죠 여러분 규제라는 게 맞아요.
규제가 뭐냐면은 규제가 뭐예요? 여러분 뭔가 좀 제약을 두는 거잖아 못하게 하는 거지 이거는 그래서 모델 규제학이라고 적혀 있잖아요.
모델 규제하기 207쪽에 가볼까요? 여러분 207쪽에서 이게 맥락이 뭐라고요?
여러분 일반화 생긴 형상이에요.

Attendees 1 32:22
그렇죠 알겠죠 일반화 성능 형석할라 그러면 여러분 지금 원래 제 슬라이드에서는 일반화 OPT 안 시키려고 그러면 어떻게 해야 된다고 그랬어요 아까 얼리 스타킹도 있었고 사실은 오브 피팅 안 시키는 거 얼리 스타킹도 분명히 오브 피팅 안 시키려는 거잖아요.
왜냐하면 거기서 멈추게 그쵸 더 못하게 하는 거니까 이해돼요.
여러분 트레인 데이터에만 너무 잘하려고 하는 거죠.
그 원리 스터핑도 마찬가지지. 그쵸. 그리고 또 이 모델 이게 이게 지금 또 뭐도 있었냐면은 제가 슬라이드에 다시 한 번 약간 맥락을 보여 요거 요기 모델 컴플레스 너무 높이 안 하려고 내가 노력하는 게 있죠.
모델 컴퍼레스틱 모델 컴플레스틱 이게 너무 여기가 여기서부터 오피팅이잖아요.
진짜 얘가 커지잖아. 오히려 더 여기는 언더 피팅이고 여기는 오피팅이라고 그랬죠.
너무 프라미트가 없어도 문제인데 공부 안 하는 게 없어.
프라미트가 없는 게 공부 안 한 게 없는 거지.

Attendees 1 33:11
파라미터 키팅을 너무 세게 하면 여기 또 그쵸 그렇다고 그랬죠.
파라미터가 너무 많거나 너무 너무 오밀조밀하게 많이 되는 게 문제예요.
그쵸. 그래서 여기 보면은 5.4.4자를 모두 규제하기에 이게 보면 207쪽인데 207쪽에 지금 지나가 보면은 이게 아니 근데 이게 그리고 여기 진한 글씨가 어디 나왔냐면 211쪽에 211쪽 보이죠.
여러분 교과서 211쪽 211쪽에 가중치 비대 추가라고 나와 있죠.
이렇게 이렇게 강의해도 별 문제없어요. 여러분 슬라이드 없이 슬라이드 보여줘야 돼.
어떻게 해요? 슬라이드가 사실 있어 이기는 찾기가 귀찮아서 지금 이렇게 말로 떠들고 있는데 빨리 진행하는 괜찮지 객관성 다 없는 사람 있어요.
지금 옆에 볼 사람 없으면 옮겨서라도 봐요. 여러분 왜냐하면 무슨 말인지 못 알아듣잖아.
됐어요. 다 있어. 여러분 지금 보고 있어요. 211조 봐요.

Attendees 1 34:07
211쪽에 가중치 기제 추가라고 있죠. 여러분 211쪽에 시까만 글씨로 가중치 기재 추가라고 적혀 있잖아.
그쵸. 그다음에 이게 근데 이게 좀 교과서가 그렇게 이거 좀 약간 제목이 별로 안 달렸어요.
그다음에 213쪽에 시꺼먼 글씨도 드라마 추가 적혀 있죠.
213쪽에 보여요. 여러분 그 두 가지가 뭔가 대표적인 방법인가 보네.
그쵸? 그런가 봐 사실은 방금 제가 얘기한 거 있잖아요.
여기 모델 컴플렉스 높이지 않는 거 그게 사실 앞에 설명으로 그냥 막 여기 까만 글씨도 없이 나와 있어요.
보면은 208쪽에 208쪽에 208쪽 208쪽 있잖아요.
이게 지금 뭐 하고 있냐면 우리가 미안한데 무슨 책 교과서 성경처럼 보고 있지만 성경 같은 맞아요.
성경 맞아요. 이 책 이 책 되게 잘 됐어요. 진짜 훌륭해.
지금 270쪽에서 모내기 조항이 들어왔잖아요.
이 8쪽에서 여러분 나중에 까먹어도 이거 보고 가서 공부하라고 지금 이러는 거야.
알겠죠?

Attendees 1 35:11
이 8쪽에 세 번째 단락이 너무 작은 모델은 과제적합되지 않는다는 것은 이미 배스가 바꾸겠죠.
너무 작은 모델은 가려 적합이 안 돼. 프라미터 별로 없다는 얘기야.
작은 모델이라는 게 무슨 뜻이에요? 프라미터 별로 없는 거야 알겠어요 그거 절대로 가려 적합이 안 돼.
머리가 머리가 별로 기억하는 게 없는 거예요. 여러분 알겠어요.
퀄리티가 별로 적다는 거는 기억하는 게 별로 없는 거야.
파라미터 특정 상황에다가 이렇게 곱해서 일을 하는 거잖아.
항상 웨이트라는 게 작다는 거는 그 웨이트가 별로 없다는 얘기지.
작은 모델 이게 첫 번째가 그러니까 모델의 파라미터를 개수를 별로 안 쓰는 거라는 걸 알려주고 있지.
모델 컴플렉스가 낮은 거 요거 요거죠. 요거 요거 요거 가전 접합 안 되는 거 이게 여기서부터 가전 접합이니까 베리우스가 큰 거죠.
여러분 알겠어요 여러분 분산이 큰 거라고 분산 분산이 큰 거야.

Attendees 1 36:03
진짜로 그래서 첫 번째는 그거였고 두 번째가 211쪽에 가중치 규제라는 거고 그쵸 세 번째가 드라바웃인 거예요.
그러니까 213쪽에 이렇게 돼 있어 교과서가 잘 돼 있어.
사실은 되게 정리가 잘 돼 있다고 보통 규제라고 그러면 레그라제이션 하면 에러네트밖에 안 나와요.
교과서에 다른 교과서는 근데 이 교과서는 되게 잘 돼 있잖아.
사실 정말로 규제에 대한 걸 확실히 보여주잖아요.
그 난 되게 아름답다고 생각해요. 이 교과서 300 맞다니까요 나도 이렇게 정리 못했는데 이 사람 정리 되게 잘했잖아요.
뭔 얘기인지 알겠어요? 여러분 레그라이제이션이라는 게 뭔가 뭔가 모피팅 안 하려고 봐라.
일반화 아까 머신러닝이 제일 중요한 게 뭐라고요?
일반화 그러면 실전에 강한 거예요. 일반화라고 다시 강하게 하지만 실전에 강한 거 실제 못 보던 문제를 봐도 잘 푸는 거야.
알겠죠? 여러분 수능 잘 보기 알겠죠?

Attendees 1 36:58
평가 원금제고 모의고사고 뭐고 다 필요 없어 기출 문제고 진짜로 잘 풀어야 될 거 아니야 그렇게 하기 위해서는 일단 파라미터가 너무 많아도 곤란하고 그렇죠 그렇죠 그리고 또 두 번째가 지금 가중치 규제라는 거 가중치 규제 가중치 규제가 뭐냐 이게 바로 에러넬트예요.
여기 나오는 거 여기 에러넬트 적혀 있잖아요. 이게 어쨌든 이게 이게 그런 거고 이게 가중치 규제라는 거고 그다음에 213에 드라바웃이라는 게 뭔지 잘 모르지만 드라바웃이 여러분 이 느낌이 와요 드바웃 드라바웃이 뭐예요?
여러분 떨어뜨려서 없애버리는 거죠. 드라바웃을 제거시키는 거예요.
드라아웃이 뭐냐면 그 파라 터가 너무 학습을 이상한 걸 많이 하면 별로 안 좋으니까 너무 트레이 데이터에 간에 적합되는 게 싫어가지고 학습시킬 때 몇 개 데이터를 몇 개의 프로젝트는 안 쓰게 하는 거예요.
나중에 할 거예요.

Attendees 1 38:00
여러분 중요한 뭐냐면은 너무 이제 미리 네트워크 프라트가 많거든 파라미터 몇 개는 아예 학습에 참여 안 시키는 거예요.
너는 놀아 이래 그냥 40년 정도 하는 근데 지금 유행하는 LNM에서 믹스트로브 엑스포치니 이런 거 다 인기 기법이에요.
사실은 드라바웃을 발전시킨 거야. 믹스트로 에스처럼 스포츠라는 거 나중에 보여줄게요.
여러분 시간이 되려나 이렇게 해가지고 진도가 그래도 되겠지 알아먹어요.
여러분 그래요 하 그래서 시간 되면 해줄게요. 그래요.
다시 합시다. 그래서 레글라이젠이 여기 적혀 있는 거는 여기 교과서에 나오는 에러 l 2 교과서에 나오는 212쪽에 가중치 규제 추가 요 요 부분이에요.
알겠어요 여러분 에로 대추가 뭔지 먼저 대충 설명해 주겠다고 알겠죠 에런 l2는 오칼 면도날 이러면서 어쨌든 면도날은 여러분 잘라버리는 거잖아요.

Attendees 1 38:56
호카이라는 사람이 제안한 이론인데 교과서 211조에 적혀 있는 거 별로 재미가 없는 거 알았는데 불필요한 건 웬만하면 없애버리자고 이상한 거 자꾸 신경 쓰지 말고 몇 년 만나 다 없애버리자는 뜻이에요.
여러분 잘라버려라 쓸데없는 거는 그런 뜻이에요.
그래서 그래서 여기 보면은 ep12쪽에 또 근데 결국은 애초 픽은 아직 설명도 안 했는데 어쨌든 중요한 거는 에런 규제 2 규제 이렇게 써 있죠 여러분 가운데 교과서에 211쪽에 그쵸 써 있잖아 에런 규제 2 규제 보여요.
여러분 보고 있어요. 에런 교재는 가중치의 절대값에 비례하는 비용을 추가한데 비용이라는 게 여러분 뭐예요?
영어로 포스트 어디다 추가할 것 같아요. 로스트 코스트 펑션이지 뭐 코스트 펑션에 코스트 이걸 더 넣는다고 그러니까 여러분 로스 펑션 코스트 펑션 같은 말이라고 그랬잖아요.
여러분 그래요 에런 에러는 절대값에 비례하는 거를 추가하고 코스트에다가 l2는 뭐래요?

Attendees 1 39:55
가중치 재고에 비례하는 거 추가하는데 가중치니까 다 나오잖아요.
여러분 리뉴얼 리그렉션이면 어떻게 돼요? 리뉴얼 리그렉션에서 만약에 피처가 1개면 세타 드럼 세타 1 바이러스 값이랑 웨이트 값 하나잖아요.
곡을 그쵸 그거를 에런 규제에서는 둘 다 그냥 절대값에서 추가하고 로스에다가 원래 로스 이렇게 제곱 폈잖아요.
거기다 에러 네스트 추가한다고 거기 세타 제로 세타 원을 그냥 절대값으로 넣는다고 추가로 그리고 l2는 뭐예요?
이 제곱 세타 제로 제곱 더하기 세타 1 제곱 알겠어요 여러분 됐어요.
이 책 말고 다른 책에서는 바이어스는 절대로 규제에 넣으면 안 된다고 나오는 데도 있어요.
여러분 어떤 책은 나도 그런 줄 알고 살았는데 아니더라고 요즘에 또 바이러스까지도 이 바이러스로 너무 크기 싫어가지고 이 로스에다가 이런 거 넣기 시작하잖아요.
이런 가치를 가중치를 넣잖아요. 지금 가중치를 로스에 일부러 넣잖아요.
이게 느낌이 와요. 여러분 가중치를 어떻게 만들려고 하는 거예요?

Attendees 1 40:51
작게 만들려고 하는 거야 가중치를 가중치를 최대한 작게 만들려고 기울기도 최대한 작게 만들고 바이러스를 최대한 작게 만들려고 이해돼요.
여러분 웨이트를 최대한 줄이려고 뭔가 특정 특성에 대해서 너무 반응하지 않게 만들려고 하는 거예요.
뭔가 아까 얘기했듯이 내가 문장이 특별히 길고 이런 거 이런 거 너무 이걸로 학습이 너무 잘 돼서 막 그걸로 만능이야 그러면 다른 거 다 못 배우잖아 이해돼요.
그러면 그중에 약간 민감도를 좀 줄이는 느낌으로 했거든요.
그래 그렇게 해도 괜찮을 것 같아 특정 특성에 대한 민감도를 줄일 수 있겠지.
걔가 너무 강력해가지고 그걸로 다 풀려. 예를 들어서 그 트레이닝 데이터에서 이상한 특성이 있는데 그걸로 모든 걸 정답을 맞출 수가 있잖아요.
아까 얘기했듯이 교수님 문제를 어떻게 근데 우리가 이상해가지고 키자에 나가면 다 말 절대 안 통할 거잖아요.
그쵸 그거에만 완전히 막 가중치가 확 커질 수 있잖아요.
다 맞출 수 있잖아요.

Attendees 1 41:43
그런 걸 막으려고 하는 거라고 생각할 수도 있지. 내 마음대로 이해하는 건 내가 맞는 것 같으니까 맞잖아.
아니 여러분들 마음대로 자기만의 생각을 해보세요.
알겠죠 나는 그렇게 이해하니까 좋더라고 알겠죠.
여러분 그래서 어쨌든 가중치를 최대한 작게 만들려고 하는 건데 가중치를 골고루 알겠죠.
여러분 그런데 이거는 여기 나와 있지만 이게 중요한 게 얘가 이걸 왜 이렇게 슬라이드까지 빼버렸냐 214쪽 가보실래요?
여러분 드라바우추가 바로 위에 있는 단락 있죠? 213쪽에 그거 뭐라고 적혀 있어요?
여러분 일반적으로 작은 딥러닝 모델에 사용된다.
대규모에서는 프로미터가 많으니까 이게 별로 소용이 없어요.
그래서 사실은 안 써 벤티라이이션 써도 필요 없어요.
필요 없다고 그래요. 그래서 여기 딥러닝할 때 별로 안 필요하다고 알겠죠 그래요.

Attendees 1 42:38
만약에 거기 저쪽 약간 혈압이 약간 굉장히 샬로우 모델이나 굉장히 조그마한 모델에서는 이게 좀 효과를 보는데 조금 모델이 좀 깊어지고 이러면 별로 필요가 없어요.
그리고 그리고 대치 노말레이션 같은 거 해도 필요 없고 아까 얘기했던 대신 내가 노멀레이션 하는 거 있잖아요.
피처 스케일링 그걸 층마다 다 하는 게 있거든요. 그거는 필요가 없죠.
그래서 별로 안 필요하다고 아니 앞으로 여러분 이해되죠 그래요 그래서 별로 안 가르쳤다고 알겠죠.
나한테 알겠습니다. 여기서 이게 뭔지는 알았죠 여러분 안 하겠다고 이거는 여러분 갖고 놀아 보세요.
얘기했죠. 근데 이건 여러분 여기서 충분히 의미가 있을 거예요.
왜냐하면 여기는 지금 데이터가 되게 작고 모델도 별로 안 크게 만드는 거라서 마음대로 하세요.
여러분 여러분 장난은 여러분 마음대로 내가 거기에 대해서 규제를 하지 않겠어요?
규제하지 않겠어 나는 여러분 마음대로 하세요. 알겠죠?
그래요.

Attendees 1 43:34
레귤라이제이션도 레이트가 있어요. 여러분 그 가중치 내가 이것도 레귤레이제이션도 그냥 그대로 로스에 넣으면 너무 크잖아요.
그러니까 이렇게 곱해요. 보통 이해돼요. 여기 10은 너무 심하지 이렇게 할 수도 있다는 거잖아.
여기 지금 대표 값 준 거야. 이해돼요 여러분 원래 로스 값에다가 이거 이만큼 곱해가지고 아까 뭘 곱한다는지 알겠죠 여러분 절대 값이 나와 걔 제곱 값이나 이런 거를 이해돼요.
여러분 그걸 로스에다 추가시키겠다는 거지 알겠죠 상대적으로 중요성이 덜하기 때문에 열심히 안 하겠다.
알고는 있어라. 그래도 알겠죠. 여러분 알고는 있어야 된다고 알고 있으라는 건 시험에 나온다는 얘기예요.
여러분 알겠어요 시험에 나오지만 내가 그렇게 강조를 안 하기 모르면 곤란하잖아 또 알겠죠 그래요.
그래서 이거 결국은 이거 들어가기 전에 지난 시간에 여기서 끝나네.
또 여기서 끝나네. 여러분 이거 한 시간 또 있잖아.
그렇죠 우리 우리가 계속 해봅시다.

Attendees 1 44:26
그래서 그래서 이게 지금 지난 시간에 이거 어쨌든 여기 나오는 걸 설명 안 하는 게 되게 찝찝해가지고 지금 a는 또 여러분 어차피 알긴 알아야 되니까 이거는 머신러닝 과목에서 하는 거라는 거예요.
사실 계속 지금 강조하지만 근데 머신러닝이 딥러닝이 또 굉장히 중요한 부분이기 때문에 그래서 먼저 할 수밖에 없는 거지.
그래서 머신러닝이 안 되면 딥러닝이 안 되는 거지.
그래서 여기 보면은 여기 요거 요거는 도대체 뭘 보여주고 있냐면은 피처즈가 적혀 있잖아요.
피처가 지금 여기 무조건 2개로 고정시켜놨어. 1개 갖고는 안 돼.
2개로 무조건 했어요. 알겠죠? 여기에 아웃풋은 또 한 개만 나오게 돼 있어요.
이해돼요. 여러분 아웃풋도 여러 개 나올 수 있어요.

Attendees 1 45:07
여러분 y 값이 y가 여러 개 나올 수 있다고 y만 아웃풋 하고 근데 여기는 하나만 했다고 알겠어요 여러분 그리고 여기 입력 들어오는 게 x1 보통 하나만 보통 얘기했는데 이리 이랬잖아요.
두 개 들어오는 걸로 했고 알겠어요 계속 똑같은 거 지금 또 반복하고 있어.
내가 왜 그러냐 기억이 남으라고 알겠죠 그럼 여러분도 복습을 하고 복습을 할 때 근데 위험한 게 잘못 배우고 복습을 하면 또 이상하게 업러닝이 안 돼서 내가 지금 이렇게 하고 있는 거 알겠죠 그다음에 이게 이게 이게 도대체 어떻게 표현하고 있느냐가 중요하지.
또 이제 여기 그림에서는 여러분한테 뭔가 많은 정보를 주기 위해서 x1 x2는 여러분 이게 스칼라 값들이에요.
스칼라 값 스칼라 값이에요. 스칼라가 뭔지 알죠?
여러분 스칼라는 벡터란 뭔지 알지 다 칼라는 값이 하나 벡터는 값이 여러 개 어레이 그쵸 그쵸 매트릭스 행렬은 가로 세로 있는 거 그쵸 텐서는 가로 안 해도 되겠지 알고 있죠 여러분 그 정도까지 지금 알았어.

Attendees 1 46:06
알겠어요 그러면 x1은 스칼라 값 하나 값 하나 근데 여기서 아웃풋을 어떻게 보여주고 있냐면은 여기 나오는 네모들은 전부 다 어떻게 보여주고 있냐면은 이 x1 x2가 x1 이게 요 요 그래프 상은 그래프를 보여주냐면은 여기가 x1 가로축이 x1 세로가 x2예요.
그리고 결괏값 있잖아요. 결괏값 결괏값의 분포를 보여주고 있는 거예요.
예측해야 되니까 또는 이 원래 원래 데이터의 값 x 여기서는 이 데이터 지금 내가 눌러놨잖아요.
데이터를 막 고를 수 있는 데이터가 여기 여러 가지가 있잖아요.
그쵸 이게 이게 제일 지저분한 문제이기도 하다. 그렇죠 복잡하죠.
이거는 좀 심플하게 어떻게 돼 있어요? 여러분 이것도 이게 제일 심플하네.
그쵸 이 선을 기준으로 해서 이쪽 이쪽으로 나눠지잖아요.
그쵸 근데 이게 여러분 이걸 선 이거 이걸 언더 패치 이거 제일 잘하는 건 이거 여기 짱 거을 안 된다.
맞아 이거 50분이네 10분 쉬었다 할게요. 여러분.

딥러닝 day7_2
2025.03.26 Wed AM 11:00 ・ 50Minutes 51seconds
심승환


Attendees 1 00:00
지금 x1 x2 x1을 할 수도 있구나. 하나만 할 수 있는 하나도 안 할 수도 있네.
되게 그래 그랬구나 입력이 하나도 없어 x1만 할 수도 있고 x 원만 할 수도 있었네.
이런 완전히 나는 이거 다 이거 눌러져 있는 거 다 선택할 수 있어.

Attendees 1 00:26
지금 여러분 이게 다시 다시 여러분 다시 얘기할게요.
예 이거 요거 요거 요 그래프 있죠 여러분 여러분 요거 요거는 아니 이걸로 해야지 이걸로 이게 다시 얘기하면은 이거는 지금 여기가 x1 여기 x2 이때 색깔이 이제 제트 축 같은 걸로 그렇죠 여기 다 다 파란색이 1 주황색이 마이너스인지 여러분 기억해야 돼.
이거 안 그럴 헷갈려서 못 알아봐 알겠죠 파란색이 플러스 주황색이 마이너스야 나는 이상하게 주황색 플러스니까 좀 헷갈리는데 이 흡수가 좀 파란색 같잖아.
안 그래요 여러분 나는 그런데 플러스가 암수 같고 안 그래요.
근데 이 사람들은 그래요. 여기는 거꾸로 돼 있어요.
알겠죠 파란색이 플러스예요. 알겠죠 그래서 요거 x1일 때 이렇게 x1일 때 바로 y1이 x1이라고 그러면 이제 이렇게 되겠지.
여기 여기 그래프가 지금 0부터 여기가 0이고 여기가 0이잖아요.
여기가 여기가 0이잖아요. 그러니까 x1은 이쪽이 다 양수잖아요.

Attendees 1 01:33
그러니까 여기 파란색이 되는 거지 여기가 움직일 거 이해돼요.
여러분 x1이 x2는 거꾸로 이렇게 이쪽이 위에가 양수 이거 아래가 문수잖아 여기 봐봐요.
알고 있어요. 여러분 이거 잘 안 보이죠 여러분 이거 여러분도 띄워봐 좀 이거 봐봐요.
여기 잘 보이잖아 마이너스 1 이 보여요. 여러분 플러스 1 알겠죠?
여러분 여기가 원점인 거 알겠어요 그래가지고 이렇게 된 거야.
그거 알고 있어야지 이게 뭔지 알아보지 알겠어요.
이것도 못 알아보면 이제 무슨 인가 시킬 수 있으니까 알겠어요.
여러분 일단 됐죠. 그다음에 근데 이거 일단 우리 데이터 전부가 다 데이터 전체가 다 x1 2 두 가지 특성 갖고 만들어놨기 때문에 얘가 이거 하나만 누르는 건 좀 불가능하긴 하지.
근데 이게 되는 애가 얘는 조금 여기만 약간 애매모호한 영역이 있고 다 된 거지.

Attendees 1 02:30
여기 x1 갖고만 하면 만약에 x1 갖고만 해도 얘는 문제가 풀릴 수 있잖아.
좀 0보다 오른쪽에 있으면 더 다 여기 x1 갖고만 해도 대충 됐잖아요.
여러분 되지 않나 그렇죠 그렇죠 여러분 뭔 말인지 알아요 여러분 이 문제는 이 데이터 샘플은 x 마 피처가 하나만 있어도 풀 수 있다고 분류가 가능하잖아.
이해돼요. 여러분 안 돼요. 돼요. 내 말이 뭔지 알아요 몰라요.
여러분 이게 지금 분류 문제로 제가 해놨는데 프라우드 분류 문제도 있고 리그레션도 있죠.
리그레션은 뭔가 또 이제 데이터 값을 이제 여기서는 이제 이 값을 예측하고 이러는 건데 소수점 예측하고 이러는 건데 분류는 그냥 1 아니면 0인 거라서 바이너리 플리케이션으로 이렇게 이렇게 해놨어요.
지금 내가 근데 여러분 얘기하면 이 문제는 문제 여러 가지가 있잖아요.
이렇게 생긴 문제가 있고 이렇게 생긴 문제가 있고 이렇게 생긴 문제가 있고 이렇게 생긴 문제가 있잖아요.
근데 이 문제는 x1 피처만 넣어도 풀 수 있잖아.

Attendees 1 03:24
x2는 몰라도 그쵸 또는 또 x2만 넣어도 돼 x2 넣어도 될 것 같아 이렇게 이렇게 하면 될 것 같아.
그렇죠 그렇죠 그렇죠 뭔 얘기인지 알겠죠 여러분 그리고 그래서 만약에 여러분이 그렇게 쉽게 잘 아는 리니이 그렉션으로 하면은 이게 히든 레이어 리니 리스는 히든 레이어가 몇 개 개예요 여러분 여기 히든 레이어가 있는 거야 없는 거야 시드 리어라고 하기 이렇게 출력하고 입력하고 사이 이게 출력이잖아요.
출력은 한 개잖아 지금 그쵸 출력하고 입력 사이에 레이어가 있다고 볼 수 있잖아요.
하나 있는 거지 알겠죠 여러분 리뉴얼 이그렉션은 우리 액티베이션이 없는 거지 그걸 알라고 알겠어요.
여러분 내 말이 이해돼요. 아직 뭔지 잘 모르지만 여기 여기 있는 게 전부 다 뉴런이라고 적혀 있는 거 있죠 여기 다 뭐 하는 거예요?
다 곱하는 거랬잖아요. 이니어리 그레션은 이게 뉴런도 한 개야 그쵸?

Attendees 1 04:26
됐어요. 이게 리뉴얼 이게 율은 여기 없어야 돼 히 레이어가 하나 이게 리뉴얼 리그렉션이라고 여러분 알아먹었어요.
여러분 그리고 액티베이션 펑션이 없어야 돼. 액티베이션 펑션이 리니엄은 없는 거랑 똑같지 뭐 그대로 가는 거니까 액티베이션이 리니엄 펑션이라는 거는 없는 거나 마찬가지야 이해돼요.
여러분 비선형성이 아니라 선명성이잖아 선호 얘기잖아요.
진짜로 알겠어요 여러분 됐어요. 그래요. 그래서 지금 여러분 다 내 말 이거 이것도 시험에 내야지 알겠어요.
여러분 이거 이거 이거 이거 완전히 선호액이라는 거 알아먹었어요.
여러분 그래서 여기 웨이트 값이 그럼 몇 개인가 웨이트 값은 여기 이 선에 담겨 있다 그랬지 그쵸 그리고 여기 얘도 지금 여기 여기 하나 점 있잖아 점 점 이게 뭐라고요?
여러분 바이어스 그래서 웨이트가 2개인 거예요.
알겠어요 여러분 파라미터 세타 앞에 맨날 나오는 거 이 세타 제로 세타 들어가시면 0.10 세타 1이 0.34로 우연히 세팅이 된 거예요.

Attendees 1 05:33
초기 값이 그래요. 그래서 이거는 이거는 그래프가 이렇게 나온다니까.
근데 이거 문제는 이게 여러분 이게 프라블럼이 클래시피케이션이잖아요.
근데 이미리 미리 해놨는데 이 사람들이 테스트 로스가 있잖아요.
여기 가격이 가파하이 값이 다 있는데 이거 약간 문제는 뭐냐면 리니이을 하기가 이게 루스 펑션이 사실은 제가 이 바이너리 클리피케이션에서 퍼포먼스 액팅이 뭔지 여러분한테 대충 보여줬죠.
멀시 러닝 시간에 하라고 그러면서 지나갔잖아요.
모임 저지 기억이 나요 여러분 공부하라고 그랬잖아요.
여러분 보고 알아서 공부하라고 그런 게 뭔지 내가 알려줄게 다시 뭐 있어요 여러분한테 공부하라고 하나 열어놨지 안 열어놨나 뒤에 퍼포먼스 여러분들 공부하려고 그랬던 게

Attendees 1 06:31
퍼포먼스 매트릭스 폴 컨트랜스 밸리피테이션 이거 열심히 설명했잖아요.
제가 RMSE랑 맨날 MA 그쵸 근데 퍼포스 트리스쿨 바이오리 프레시 파이어 이거 지금 당연히 바이어리 프레시 파이어아 1안이면 마이너스 1로 하잖아요.
지금 포란색 아니면 그쵸 주황색 하고 있잖아요. 그쵸 여기에는 정체 이게 퍼포먼스 매트릭이 뭐라고 타이머 에러 타이트 에러 에큐로시 프리시즌 막 이런 놈들이잖아요.
여러분 어 그쵸 리콜 스페이스 피스트 그렇죠 센스티비티 이런 것들이잖아요 그쵸 그렇죠 그 에크시 몇 프로지 맞아 이런 거 그쵸 이게 이건데 이거가 퍼포먼스 매트릭인데 로스 펑션을 예를 못 써요.
여러분 문제가 이 미문이 안 돼가지고 미분이 이게 값이 완전 극단적이잖아요.
얘네들이 그래서 생태 모양 스텝 펑션같이 생겨가지고 이게 미분이 안 돼가지고 그래서 결국은 크로스 엔트로피라는 걸 써요.

Attendees 1 07:26
크로스 엔트로피는 여러분 배운 적이 없지 나한테 그렇죠 그래가지고 제가 슬라이딩을 올려놨는데 내가 공개 안 했나 근데 미리 얘기해 놓는 게 좋을 것 같은데 나중에 또 오해할까 봐 너무 자꾸 막 히디 스토리가 많아서 미안해요.
여러분 자꾸 뭔가 좀 알 것 같은데 자꾸 새로운 자꾸 던져 그렇죠 근데 그럴 이유가 있는 거지.
아니 어디 갔지 자기 자리에 어디 있어요? 아지 스튜브레이션 가 내가 공개한 지 오래됐죠.
여러분 그렇죠 요거 요거 그쵸 프로스 센트로트라는 게 있어요.
여러분 그래요. 그래서 여러분 해피할 줄 알았는데 좀 약간 이런 미안한데 이게 바이너리 클리피케이션 문제는 다 사실 원래 우리가 알고 있는 RMS ms 있죠 mse 인스퀘어 여러 그걸로 해도 돼요.
태그 되긴 되는데 민스퀘어도 사실은 1하고 0 값이 이제 1이나 마이너스 1 나오는 거 이거 1이나 0 나오는 거 있잖아요.
그 데이터 그거 갖고 윈스케어를 해도 되긴 되지 학습이 되겠잖아요.
그렇죠 여러분 돼요.

Attendees 1 08:29
그런데 잘 안 돼. 덜 잘 되는 거예요. 덜 잘 된다는 것도 책으로 이만큼 있어요.
여러분 논문도 있고 재미있어. 근데 뭐 당연히 덜 잘 되거든요.
그래서 그래서 아까 또 그냥 원래 우리가 퍼포먼스 매트리 쓰는 거는 미군이 안 되고 그리고 사람들이 잘 발견한 게 이게 제일 잘 돼가지고 이걸 쓰고 있어요.
라이스 티뷰에서는 그 슬라이드는 언제 또 하냐 그쵸 계속 진도를 못 나가잖아요.
그쵸 제공을 하는데 여러분이 너무 너무 이제 힘들어 하는 것 같으면 내가 보여줄게요.
알겠죠 일단은 그래도 보여주긴 해야겠다. 그래요.
너무 길어지는데 난 또 한 번 하면 또 라디스티그레이션 있잖아.
이거 이거 디스트리미션이 막 이렇게 이거 내가 나도 헷갈려서 정리해 놓은 거예요.
여러분 베르는 군포 아니지 베르누이랑 바이노미알이랑 카테고리가 되면 이게 정리 트라이어리 싱글 트라이어링 앤디 페티 트라이어 바이너리 아웃컴 인데 이런 거 있잖아요.
지금 우리가 풀고 있는 건 뭐예요? 바이너리 아웃컴이거든.

Attendees 1 09:31
근데 어쨌든 트라이얼은 앤 디펜트 계속 여러 번 할 거고 바이노미얼인데 그런 디스트리션인데 여기에서 이제 결국은 이제 이제 실제 여기 이거 할 때 프라미터로 주어지는 문제에서 샘플 샘플이 뭐냐 동전 하나 뒤집는 거랑 그 동전을 여러 번 뒤집는 거 그런 거예요.
동전 뒤집었을 때 아피나에서 뒤로 나는 이런 확률이 있잖아요.
그런 거지. 그래서 나는 이거를 강의하면 제대로 하면 너무 또 오래 걸리잖아요.
사실은 머신러닝 4번 머신러닝에서 하죠. 이거 다 이거 확률과 통계 내가 확률과 통계 이렇게 강요하고 있으면 안 되지 라지은 머신러닝에서 근데 이건 알으라고 이런 모양이 전에도 보여줬던 것 같은데 마짓은 뭐냐 생긴 모양이 프라볼리티를 012 프라볼리티 함수가 프라블리티가 0하고 이 사이잖아요.
원래 무조건 이거 여러분 강의 자료 줬어. 내가 명함 이사이 줬는데 전체 실수로 레킹 되는 놈이에요.
약간 탄데트 비슷하게 생기기도 했다.

Attendees 1 10:39
그렇죠 근데 이런 함수가 왜 필요하냐면은 오히려 사실 거꾸로 라지스틱이라는 게 이것도 역함수거든요.
라지스틱이 라지의 역함수예요. 라지은 확률 라제이스의 역함수를 하면은 확률로 바뀌잖아요.
그래서 어떻게 생각하냐면은 실제로 존재하는 뭔가 누리가 학습을 하고 하면 결국에 실수가 나올 거 아니에요 원가 원가 실수가 그거를 다시 역함수를 얘를 역함수시키면 어떻게 되겠어요?
확률로 나올 거 아니에요? 우리 확률이 궁금하잖아.
바이너리 플리시케이션은 전부 다 예스 노잖아요.
1하고 0하고 이 사이의 확률로 생각할 수 있잖아요.
그래서 라젯이라는 건 뭐냐 확률을 만들 수 있는 실수인 거예요.
느낌이 와요. 여러분 라짓을 확률로 만들 수 있는 실수예요.
실수 어떻게 만드냐면 이거 실제로 1 마이너스 p 분의 피로 해졌잖아요.
원래 확률이 있을 거 아니에요 확률의 확률의 반대 확률에다가 월 확률 비율을 나누면 이거 승산이라고 그러는 영어로는 안 하고 이기을 확률 확률 이런 거 보는 거죠.

Attendees 1 11:41
이길 수 있는 뭔가 비율 이래서 승산 승산이 있냐 없냐 할 때 이길 수 있는 확률이랑 질 수 있는 확률 서로 비율을 나눠서 얘기할 수 있거든요.
그렇잖아요. 여러분 그리고 그거를 이제 그렇게 얘기하면은 성수가 비율이 저희가 표현하기가 힘드니까 로그로 치워버리는 거죠.
자연로로 그게 이제 라지인데 이러면 이제 역함수하면은 피가 나와버려요.
라지 라지 함수가 나오지 그래가지고 라지 스틱이 라지은 어쨌든 여러분이 외워야 되는데 라지 스틱 실수야 실수 알겠죠.
이미의 범위가 라짓은 이미 확률을 넣었을 때 이의 실수가 나오는 거고 좋은 게 확률이 만약에 0.5보다 크면 숫자가 점점점점 커지죠.
양수지 보여 이 함수 생긴 모양이 그러다가 1에 가까워지면 엄청 커지잖아요.
그쵸 그리고 확률이 0.5보다 작아 그럼 어떻게 돼요?
점점점 음수가 되다가 무조건 마지막에 엄청나게 큰 작은 음수가 되죠.
작다는 거 값이 작다고 마이너스 반에 가까워지고 확률이 1에 가까우면은 얘는 어떻게 돼요?

Attendees 1 12:48
무한대에 가까워지고 그렇죠 시장 엄청 커진다고 확률이 1에 가까우면 좋잖아 이거 숫자가 크게 나오잖아 그렇죠 확률이 0.5를 기준으로 확률이 0.5 원래 0.5가 넘냐냐 냐 안 넘냐가 중요하잖아요.
사실은 0.5 넘어버리면은 양수 나오고 0.5 안 넘으면 음수 나오다가 그렇죠 1에 가까워지면 막 거의 무한대로 치솟고 0에 가까워지면 마이너스로 치솟으니까 약간 직관적으로도 괜찮잖아요.
숫자로 실제로 했을 때 숫자로 나온 거를 뭔가 딥러닝해서 막 학습시켜가지고 숫자가 크게 나오면은 그거는 확률을 1로 만들고 숫자가 작게 나오면은 이해되죠.
0 미만이면 이거 반반 확률이다 이런 식으로 한다고요.
그래서 실제로 라짓을 끄집어내려고 그래 사실은 바이너리 프레시피케이션을 이거 되게 좀 라짓이라는 라짓이라는 말을 많이 써요.

Attendees 1 13:36
그래서 사람들 짓 라짓이 뭐냐 평균을 만들 수 있는 실수 값인 건데 개념은 플러스 무한대는 1이야 마이너스 반드는 확률 이런거 이해되죠?
여러분 마이너스 반드는 영상 0의 확률 그런 식으로 느낌이 와요.
느낌 느낌이 달라요. 여러분들 느낌이 달라요. 이거 좋네 좋다 라지 하면 좋다.
라짓이라는 게 있으면 확률 만들어야겠구먼. 숫자가 크면 이거는 1위에 이게 모르겠죠 그래요.
그리고 라지 스틱은 거꾸로 라지스의 역함수인데 그래서 임의의 실수를 0하고 1로 만들어놓은 거예요.
0하고 1 사이를 알겠죠 거꾸로이기 때문에 아까 얘기한 거랑 똑같이 0이면 0.5로 하고 숫자가 0이면은 뭐 그냥 0.1률이지 이해돼요.
0보다 크면 점점 더 1에 가까워지는 걸 본다는 거지 0보다 작으면 마이너스 0의 확률로 바꿔지는 걸로 본다고 알겠어요 여러분 이게 라지 함수예요.
라지스틱 함수예요.

Attendees 1 14:35
라지스틱 라지이랑 라지스틱이랑 역함수라는 거 여러분 넣어요.
이거는 시험에 낼 거예요. 외워 외워요. 시험에 내야지 시험에 중간고사에 나와요.
여러분 라지하고 라지스틱이 역함수예요. 외워 너무 중요해 알겠어요.
여러분 라지 함수랑 라지스틱 함수랑 역함수라고 알겠어요.
그리고 그래서 라지은 확률이 넣으면 실수가 나오고 라지스틱은 실수를 넣으면 확률이 나와요.
알겠어요 실수가 뭔지 알죠? 여러분 리얼 넘버 아 진짜 존재하는 기자 그다음에 외우라고 그랬죠.
여러분 그리고 또 뭐 왜냐하면 막 나오거든 지금 영어가 라지스틱이 맨날 나와요.
여러분 아제스틱이 다른 말이 뭐라 뭐라고요? 여러분 시드 모인 s자 모양이라고 해요.
유명한 s 모양이라고 알겠어요 여러분 이 미분이 너무 잘 돼 싱그모이도 너무 이게 있죠 여러분 로브 치이기 때문에 나중에 식이 이렇게 정리가 돼요.

Attendees 1 15:28
예쁘게 6 플러스 2 알파 3 이하 마이너스 알파 분의 여러분 이가 이 자연 상수 이가 얼마나 좋은지 알죠?
여러분 오일러 상수 이거 맨날 적분하면 1 되고 막 그렇죠 미분할도 그냥 계속 남아 있고 되게 좋잖아요.
이거 맨날 프린트랜스폼 같은 거 이거 갖고 다 하잖아요.
아니 라프라스 만든 이거 갖고 다 해버렸지 그쵸 배웠지 우리 너무 좋잖아 이거 이게 되게 좋아요.
아주 아주 깔끔해져 수식이 깔끔한 거 왜 하느냐 계산 별로 안 하려고 알겠죠 여러분 그리고 라이클 리우드라는 용어도 되게 많이 쓰거든요.
여러분 라이클 리우드도 이제 저도 헷갈려가지고 정리해 놓은 거예요.
근데 옛날에 했다가 사실 지금 논문 같은 데서 막 섞어 나오는데 너무 정신없어가지고 제가 여기 적어놨거든요.
제가 엔트로피다 적어놨어요. 제가 크로스 엔트로피도 결국은 중요한 거는 크로스 엔트로피를 할 거예요.
크로스 엔트로피를 우리가 결국 손실함수를 포스센트로 그을 거야.
그래요.

Attendees 1 16:21
맨 마지막에 로스 로스 펑션을 크로스 엔트로피 갖고 한다는 거지 크로스 엔트로피가 작으면 작을수록 좋다는 거지 사실은 0에 가까워질수록 근데 크로스의 트래픽 실체가 뭔지 열심히 설명해 놓은 거예요.
이거는 머신러닝 내용이잖아요. 이거 아까 우리 AI 공합 AI 공합 개론이 있어 AI 정보 이론으로 생긴 거 거기 음대론으로 생겼잖아요.
그쵸 거기서 하는 거야 거기서 엔트로피 열심히 해요.
2학년 과목이 여러분 벌써 지나가는 작년에 생겼잖아 맞죠?
들었나 전대 갔다 왔을 때요. 그리고 어쨌든 다음 주 수요일 수업 월요일 수업하죠.
일단 월요일 수업에 다 있잖아 어떡하지 실시간으로 하고 거의 여러분 다음 주 수요일에 거의 예비군이라는 소문을 들었어요.
그래요. 그래서 한번 고민해 볼게요.

Attendees 1 17:20
여러분 그래서 계속 하면은 어떻게 되냐 이게 정보량 이런 거는 다 AI 정보량에서 배우시고 정부니까 그리고 크로스 엔트로피가 이렇게 나오는데 되게 사실 설명하면 재미있긴 한데 오늘 또 이거 그냥 이거는 다른 과목에서 하는 걸로 생각하고 중요한 거는 원래 케인 아우저스가 진짜 진짜 중요한 거거든요.
쿠벨 라이브러 발산이라고 그래서 프로스트 앤트로피랑 원래 엔트로피랑 이거를 뺀 값이 진짜 우리가 주고 싶은 값인데 예측 확률이랑 원래 확률이랑 정보량을 빼서 똑같이 만들려고 하는 거거든요.
확률은 똑같이 만들고 싶은데 그냥 또 이것도 비슷하게 확률 계산하려고 그러면 힘드니까 또 이제 이렇게 엔트로피라는 걸로 변환시켜가지고 하는데 그렇게 대충 생각하시고 자세히 보세요.
여러분 나중에 그래서 걔를 프로센 트로피가 결국은 피의 다이러스랑 같다.
언제냐면은 원화 입금이니까 원화 입금 등이 뭔지 있지 이거는 나중에 해줄게요.

Attendees 1 18:23
여러분 그 이거는 그래서 지금 그만하고 중요한 건 여기서 얘기하는 게 여기 지금 우리가 또 약간 이 펜스 플로우 플레이 그라운드만 하려고 그래도 벌써 여기서 로스 펑션이 되게 이렇게 중요하다고 그랬는데 로스 펑션이 크로스 엔트랙션 여러분이 모르는 함수가 나왔어요.
알겠죠 그랬어. 근데 그거 산수로 하면 돼요. 산수에 사실은 그래요.
그래서 걔가 이제 확 크기 실제로 잘못 맞추면 커지고 맞추면 작아져요.
알겠죠? 0에 가깝게 되고 그래요. 그래서 어쨌든 이게 지금 이거를 이제 그래서 여기 또 나오는 게 트레이닝 레이시 오브 트레이닝 투 테스트가 적혀 있잖아요.
그쵸? 여기는 밸리데이션 같은 건 없어요. 여기는 지금 이 이 테스트 플로우 플라이 트레이닝 중에 전체 중에서 데이터 중에 트레이닝 얼마큼 테스트를 얼마큼 쓸지에 대한 거예요.
여기 지금 데이터가 찍혀 있잖아요.

Attendees 1 19:16
여기 지금 몇 개인지 잘 모르겠지만 몇 개인가 나 밑에 적혀 있는데 어쨌든 대충 100개는 되나 100개라고 칩시다.
여러분 알겠죠? 100개라고 치면은 한 50%씩 50개씩 50개씩 쓴다라는 거 이해되죠?
여러분 됐어요. 무슨 얘기인지 알겠어요 여러분 데이터가 이제 x1 xt 2의 쌍이야 알겠어요.
노이즈는 실제로 이제 좀 약간 데이터가 실제 값이 틀리는 경우를 말하는 거예요.
노이즈가 있다는 건 왜냐하면 노이즈가 이제 왜 중요하냐면은 실제 트레이닝 데이터는 대부분 다 그렇거든 여러분 내가 너무 보여주고 싶어가지고 나중에 보여주는 것보다 지금 보여주면 재미있을 것 같아서 교과서에 5장 있잖아요.
20분이지만 가능할 것 같아요. 교과서 5장에 일반하고 멋있는 곡도 나오는데 이게 너무 지금 미리 보여주는 게 나을 것 같아요.
왜냐하면 저는 저번 시간에도 약간 오프팅 묻더라고요.
여러분이 교과서가 최고로 좋아요.

Attendees 1 20:10
어제 인터넷에 나오는 것보다 그래가지고 교과서에 지금 이거 다 교과서가 이게 지금 181쪽 있죠.
181쪽 너무 아름다워 교과서 그림 181쪽에 있는 그림 있잖아요.
여기는 제가 분산 이런 거 안 하고 그냥 이게 훈련을 반복하면 이렇게 과소 적합이다가 가도 적합 되는 거 보이잖아요.
교과서 181쪽에 좋잖아 그쵸? 이렇게 되죠. 그래프가 여기는 아까 바이어스 에러 에러가 모디컬 폴리시티가 아니고 x축이 달라요.
여러분 x축에 다른 면 보여요. 여러분 생긴 게 이렇게 생겼지 그래서 그래서 여기 중요한 게 이쪽은 계속 트레이닝 데이터 출연 소실 있죠.
훈련 소실에 계속 잡아주면 될 거 이게 중요한 거야.
이쪽은 손실이 무조건 작아져 훈련의 손실은 훈련 데이터에 기출 문제 다 무조건 잘 풀어 계속 오래 하면 알겠어요.
여러분 그렇지만 실제 문제 잘 못 풀기 시작한다는 거지 알겠죠 영어로도 알아야 된다는 거죠.
언더 피티 OPT 베스트 티팅 알겠죠 베스트예요.
베스트 알겠죠 체적이 그래요.

Attendees 1 21:20
그리고 아까 또 얘기하고 싶은 게 여기 교과서에 요 그림 여기 180이 아까 노이즈가 있었잖아.
노이즈 텔레이가 보여주고 있었는데 노이즈 여기 1802쪽에 있어요.
여러분 앱 리스트에 실제 이런 게 적혀 있어요. 여러분 뭐 숫자 글씨를 이렇게 썼나 그쵸?
도대체 뭘 썼는지 모르겠죠 여러분 재밌지 않아요 이거 되게 이게 무슨 글이오 도대체 그쵸 여러분 이제 경악스럽잖아.
여러분 재밌지 않아요 여러분 되게 정성스럽잖아.
교과서가 그렇지 않아요 이거 딴 데 안 나와요. 좋잖아.
근데 이런 데이터가 항상 섞여 있다는 거지 세상에는 알겠죠 이거 갖고 공부를 해 맞추라고 계속해 봐요.
그럼 어떻게 되겠어? 이상해지겠지 사람이 아니 걔가 이상해진다고 이걸 이렇게 다 맞추라고 그러면 이상해진다고 이해되죠.
여러분 이런 거 못 맞춰야 될 거 아니야 문제가 이상한 건 열심히 하면 공부하면 안 된다는 거지.
정승재 그 사람 말을 주더라고.

Attendees 1 22:12
그 사람 아니 봤더니 동영상 재미있는 거 애들이 보여서 왔는데 애들이 지금 고3이 대 그 사람이 막 심판을 문제에다가 막 던지고 난리를 피던데 속이 시원하더라고요.
이상하면 좀 공부하면 안 되지 그쵸 아니 그렇지 이걸 왜 맞추라고 그래 안 되잖아요.
그렇죠 제껴줘야지. 그쵸. 그리고 여기 또 이제 재미있는 건 이거죠.
이거 답이 틀린 거 다 밑에 이게 4거든요. 사실은 근데 레이블이 부모 돼 있어요.
4예요. 4 4 쓴 건데 그 얘는 리스먼드 7로 돼 있고 얘는 이거 진짜 이거 경악스럽지 이거 이거 여러분 7이잖아 확실히 레이블을 4번 해놨어요.
누가 근데 이런 걸 일부러 안 고쳤어요.

Attendees 1 22:51
에니스트에서 왜냐하면 진짜 이렇게 했으니까 옛날에 그래서 여러분 뭔지 알겠어 무조건 100% 맞출 때까지 하는 게 좋은 게 아니야 이거를 여러분 때 사라고 계속 공부시켜봐 이걸 구잖아요.
이걸 3이라고 공부시켜 봐 이거 숫자 3이거든요.
근데 5라고 돼 있거든요. 이거 5예요. 3이에요.
여러분 혹시 3승 같지 않아요? 이거 근데 이걸 5라고 가르치고 이걸 3이나 가르쳐 봐 이상해질 거 아니야 이거 맞추는 게 중요해요.
안 맞추는 게 중요해요. 안 맞추는 게 중요해요. 이런 게 노이즈예요.
여러분 알겠어요. 그래서 오히려 너무 학습 아로 시키면 안 되는 거죠.
이상한 애들이 그래서 여러분 진짜 그래요. 그만해야지 왜요?
그래서 노이즈는 일단 알아놓으시니까요. 배치 사이즈가 여기도 있잖아요.
여러분 10개 10개씩 할 거냐 3년에 한 번에 몇 개씩 할 거냐 미니 배치라는 말 굳이 안 썼지 배치 한 번에 후면 업데이트할 때 쓰는 걸 말하는 거야.

Attendees 1 23:42
알겠죠 그리고 어쨌든 로스 펑션을 크로스 엔트로피라는 걸 쓰는데 보여주는 지표는 또 액트로시 써요.
액트로시 우리 다 알잖아. MSD는 바로 그 성능 지표랑 로스 펑션이 동일한데 바이너리 프레스피케이션은 성능 지표와 로스 펑션이 다르다고 이게 느낌이 와요.
여러분 홍수표 항상 에큐르시인데 이해돼 레큐루시가 뭔지 알죠?
여러분 전체 데이터 중에서 맞춘 거 개수 그러니까 이게 여기 요거는 여러분 1이라고 그래야 되고 얘는 마이너스 1이라고 해야 되잖아요.
파란색 1이고 그게 사실 두 개 분류하는 거잖아. 그러니까 확률로 만약에 나온다면 1 0 해야 되는 거잖아요.
10 확률로 하면 그런데 그러니까 너는 0.5만 넘어도 되지.
사실은 그렇죠 시그모이드 산수 마지막에 있었으면 그런데 실제 정말 실제 퍼포먼스 매트릭스는 매트릭은 전체 내가 준 트레이닝 데이터 또는 테스트 데이터 중에 테스트 데이터에 프로 몇 프로를 맞췄냐 그걸로 한다고 알아요.

Attendees 1 24:44
여러분 1인 건 얘를 1이라고 그러고 얘를 마이너스 이라고 한 거 개수 비율이라고 자못 분리한 거를 빼고 알겠어요.
여러분 그렇게 한다고 크로스 엔트로피 값으로 하지 않고 뭔지 잘 모르지만 여러분이 알겠어요.
여러분 됐어요. 그게 그사이 그게 뭔지도 모르는데 내가 잘못했어요.
얘 너무 그래도 이거 하고 싶은데 어떡하지 그건 중요하지 않고 그러면은 크로스 세트를 잊어버려.
어쨌든 거기 좋은 함수가 있어요. 훈련시켜줘라 알겠죠 MSD MSD 썼다고 생각해도 돼.
사실은 이 정도 문제는 mse로 해도 다 훈련이 돼요.
사실은 mse 미스퀘어러로 해도 된다니까요. 진짜 훈련 잘 돼요.
여러분 이 정도 문제는 이게 문제가 심플하잖아. 사실은 여러분 알겠어요.
여러분 이거 리니어 리그렉션이잖아요. 그쵸?

Attendees 1 25:32
리그렉션 리그렉션도 풀 수 있는 문제가 없는 문제인가 이거는 풀 수 있지 그쵸 그래서 한번 해볼까요?
여러분 이거 누르잖아요. 제가 누르면 뭐가 되겠어요?
에폭이 지나가면서 그래프가 나와요. 봐봐요. 짠 너무 빨리 지나가겠어 폭이 서울 48까지밖에 안 했잖아요.
그쵸 러닝 레이티브 되게 컸는데 봐봐. 여기서부터 바로 쫙 줄어들어서 이거를 다 다시 잡을게요.
여러분 다시 너무 빠 패킷을 제한할 수 없나 이렇게 뭔 얘기인지 되게 잘 맞추잖아요.
여러분 이게 이게 지금 결과값이 이게 이쪽은 다 이게 선이 선형이 나왔어요.
그냥 선형이야 이 문제는 그래서 이렇게 그냥 사실 바이어스는 0으로 하고 바이어스가 바이어스도 0이고 걔도 0인 거지.
곱하는 것도 0인 거야. 그쵸? 그래서 그냥 곱하는 게 여기 곱하는 게 그거네.
그냥 그냥 x 값 그대로 간 거죠. 그냥 라지수로 간 거죠.
라지 수 양수 나오면은 1 라짓 함수로 그쵸 라짓으로 응수 나오면은 그렇죠 원래 값 그대로 가져가면 되잖아요.

Attendees 1 26:43
x 값 x값 잘 되는 학습이 잘 되는 거 보이죠. 여러분 잘 되는 거 보여요.
여러분 감이 안 오는구먼 여러분 이거 이걸로 여러분 만약 이 문제 풀어보라고 그럴게요.
여러분 이 문제 풀어볼게요. 여러분 일단 되겠냐고 이게 될 수가 없잖아요.
여러분 이거 도저히 못 풀죠. 이 색깔이 엉망진창이고 로스 값이 트레이닝에서는 0.499까지 내려갔는데 0.5502로 안 되죠.
트레이닝 데이터 제가 아까 전에 여기서 오 피팅이 될 수가 없어.
모델이 너무 간단해서 이거는 모델이 너무 파운트가 작으니까 정보 x2 두지도 않았잖아 알려주지도 않았어 이거는 어떻게 돼요?
여러분 공부를 할 수 있는 데려다 주지도 않았어 그렇죠 이거 맞출 수가 없어요.
여러분 그쵸? 이해돼요. 이건 네버 베스트 핏이고 항상 언더피티파이 될 수 없는 거예요.
이해돼요. 여러분 이해되나고 이런 걸 해보라는 거예요.
여러분 이거는 지금 리니어 리그렉션 갖고 떠들었잖아요.

Attendees 1 27:35
제가 그렇죠 이제 이제 리뉴얼 리그렉션도 사실은 너 얘 줘야지 사실 이런 거 리니게이션도 얘는 줘야 돼요.
리얼레이션에서도 얘 이거 줘야지 이러면 이제 하나 곱하는 게 더 늘었잖아.
그러면 리뉴얼 리그렉션은 이 문제는 여전히 잘 풀겠지.
또 여기 여기 테스트 로스랑 트레이 로스 나오는 거 이해되죠?
여러분 이게 이게 가로 x축이 뭐겠어요? 여러분 4 y축이 보스가 두 개 선이 두 개 나오지 이해돼요.
여러분 해볼게요. 짠 되게 확 줄어들죠. 이것도 그렇죠 천이 이렇게 나오죠.
그래도 정상적으로 약간 좀 이게 더 맞잖아. 사실은 아까 그거보다는 이해돼요.
여러분 x2가 들어갔으니까 여러분 이해돼요. 내가 뭐 하는지 알겠어요 여러분 선영혁이라고 지금 제가 이렇게 열심히 하고 있는데 선영혁이 아니라 그러면 이제 어떻게 되느냐 소정 얘기가 아니라 그러면 액티베이션 펑션이 여기 넬루 이런 거 쓰기 시작하잖아요.
여러분 이러면 이제 여기 벌써 여기서부터 뉴럴 네트웍이라고 해요.

Attendees 1 28:36
이런 거를 퍼셉트론이라고 불러요. 알았어요. 여러분 퍼셉트론이라고 부른다고 이런 거였어.
액티브 비선형성이 들어갔잖아요. 멜로는 비선형이죠.
쫙 잘리잖아. 이쪽에서 여러분 내 말 알아들어요.
제발 제발 알아들었으면 좋겠는데 내가 재밌는데 사실 이거 굉장히 알아요.
여러분 엘루는 기상형이야. 여기서부터 지금 벌써 뉴럴레척이라는 얘기야.
벌써. 근데 딥러닝이 아니야 왜 아니에요 피트 내가 하나니까 알겠어요 여러분 그래가지고 이렇게 하면 이것도 금방 잘될 수밖에 없지.
왜냐하면 이거 렐로 써서 렐로 써서 나빠질 게 없어요.
사실은 나빠질 게 없어. 그다음에 이런 문제도 풀 수 있나 이거 이것도 대답 궁금하다.
그 멜로 썼으니까 그쵸. 짠 안 된다 안 된다. 하나 갖고 도저히 안 된다.
그렇죠 그렇죠 하나 갖고 도저히 안 돼요. 왜냐하면 이게 이 이게 선이 안 되잖아.
이거 이거 뉴런 하나 있으면 파라미터가 지금 이거밖에 없잖아요.

Attendees 1 29:38
지금 파라미터가 곱하는 게 딱딱 요거 하고 로 해봤자 무슨 소용이 있어 이게 그냥 세출이 되는 것도 없는데 그쵸.
그래서 최소한 레이어 하나 더 줘보자. 그쵸. 근데 여기 지금 디폴트로 2개 줬어요.
그쵸? 2개는 더 줘 볼까 보통은 근데 앞단에 뉴런을 많이 넣는 게 더 좋아요.
그냥 지금 뭐 그냥 내가 장난으로 이렇게 하고 있는 거야 나 잘 모르겠어 알겠어 일단 해봤어요.
이렇게 하고 되나 이러니까 다시 리제트 할게요. 짠하니까 아까 처음부터 안 하고 다시 바꿔서 했으니까 좀 약간 얘는 얘는 두 배 잘하기 시작해.
여기랑 여기는 잘 분리하는데 여기는 못 맞추고 있어.
계층이 두 개 되니까 계층 하나 더 돌려볼까 그럼 다시 이제 이렇게 하고 아직도 잘 못한다.
대충 하나 이것도 이게 근데 이게 뭐 하는 짓인지 알겠어요 여러분 파라테이션 느리고 있잖아.

Attendees 1 30:36
지금 안 되니까 된다. 이해돼요. 여러분 그런 거 그래서 이런 걸 느껴보라는 거예요.
여러분 알겠죠? 다른 방법도 장난 쳐보시고 재밌잖아.
약간 약간 감이 생기잖아요. 여러분 이거를 맨날 코딩해서 하면 더 안 좋은 게 코딩을 하면은 이게 더 시각적으로 잘 안 보여서 오늘 느낌이 안 온다는 거죠.
그래서 여기도 보면 값이 이게 여기다 이걸 곱하는구나 이런 게 보이잖아요.
여러분 그쵸 당나라 좀 쳐보라고 그리고 그거를 본인이 느낀 점을 나를 보여주기 위한 게 아니라 본인을 위해서 남기라는 거예요.
알겠죠? 됐어요. 여러분 그리고 지문도 좀 하시고 지문 질문을 강제로 또 하라고 하고 별로기 같아 저기 어떤 교수님은 숙제로 매주 질문을 하라고 그런대요.
퓨얼할 것 같아. 진짜 그래요. 그래서 근데 이게 이게 여기 보면 여러분 되게 잘하죠.
그쵸? 기가 막히죠. 여러분 근데 여기서부터 교과서를 보여주고 싶어요.

Attendees 1 31:33
좀 보여줄까 이게 이게 뭐냐면은 여기 노이즈를 넣었을 때 있잖아요.
노이즈가 넣으면 이제 데이터가 이상하게 끼는 거잖아요.
그러면 이제 테스트 데이트만 잘하고 트레이닝 데이터에서만 잘하고 테스트를 잘 못할 수가 있겠지 그런 것도 장난칠 수가 있는 거죠.
그쵸 근데 지가 이 이론적인 강의를 더 하면은 교과서에 교과서로 그냥 교과서가 너무 잘 돼 있으니까 교과서로 그냥 빨리 하는 게 좋을 것 같아.
교과서 5.1절에 지금 이거 제가 보여줬잖아요.
데이터가 노이즈라는 거의 개념에 대해서 그다음에 바로 지금 182쪽 아래에 이 그림이 있어요.
182쪽이 바로 밑에 보이죠. 여러분 이게 뭐냐면은 여기 실제로 이 데이터가 여러분 보면 이거 딱 보면 이거 이거 이쪽 왼쪽에 이렇게 있는 게 회색이고 오른쪽에 있는 게 빨간색인 거 보이는데 여기 회색들 여기 중간중간 섞여 있잖아요.
이거 얘는 좀 이상한 데이터라고 보이죠. 사실은 이상한 데이터가 맞을 거예요.
진짜로 이해돼요.

Attendees 1 32:30
여러분 원래 데이터 이렇게 챙겨 먹었어. 근데 이런 거는 보통 이런 거라는 거지.
이거 군인 대 3이라고 그러고 7인데 4라고 그러고 이해돼요.
그런 데이터일 가능성이 높아요. 사실은 얘는 빨간색인데 잘못 분리된 거야.
이해돼요. 여러분 내 말이 이해돼요. 여러분 이거 그러니까 여기 특성에 따라서 이렇게 분류한 건데 딱 보면 비슷한 것끼리 모여 있잖아요.
지금 이 정보는 여기 있는 거 그리고 여기 있는 거는 빨간색 여기 있는 건 다 회색이어야 되는데 잘못 라벨된 거라고 그래 그럼 이런 거는 이렇게 이렇게 학습시키고 끝내야 될 거 아니야 그래서 이게 최적 접합이 이거고요.
사실은 이상치 아웃라이어라고 불러요. 아웃라이어 아웃라이어 이상하게 놓여 있는 거 라이어가 놓여 있는 거 아웃라이어라는 거 주문 용어예요.
잘 써요. 나중에 글 나오는 거 이 장치 영어로도 알아요.
쇼에 낼 수 있어요. 아웃라이어 제발 적어봐요.

Attendees 1 33:23
어디다가 아웃라이어 알겠어요 여러분 영어로 다 알아야 돼서 이게 우리나라 교과서를 그래도 공부하는 게 빠르더라고 이해되죠.
여러분 아울라에요. 첫째 적어줄까요? 아웃 라이어 알겠죠?
아웃 라이 이 사체 그래서 아웃라이어인데 이거를 여러분 계속 관련을 시키면 어떻게 되냐 이렇게 된다고 이렇게 하겠죠.
여러분 이게 바람직한 거냐고 아니라는 거예요. 알겠어요 여러분 이상한 데이터에 대해서 이거를 이렇게 생긴 거는 사라고 공부를 해 봐.
알겠죠 안 된다는 거지 그렇죠 더 나쁘다는 거지 정말로 잘 못한다는 거고 그래요.
그래서 여기 여기 여기 또 이제 또 보면 브라시라는 게 섞여 있잖아요.
브라시라는 게 이렇게 이런 데이터 이거 183쪽이 그렇죠 그런 거는 이렇게 끊어버려야지 이렇게 이렇게 끊어버려야지 이렇게 하면 안 된다는 거야.

Attendees 1 34:18
알겠어요 여러분 그래서 여러분 지금 제가 몇 번 떠드는데 최근 트렌드에 마디 디스플레이션이라는 게 되게 유명해요.
여러분 지금 LLM 있잖아요. LLL LLM을 요즘에 온디바이스 AI를 많이 해야 되거든요.
온디바이스를 온디바 짧게 만들어야 되는지 여러분 여러분 노트북에서 돌리고 싶잖아.
딥시크 같은 거 돌아가게 만드는 방법이 많아요. 근데 이제 여러분 서울 코인페이스라는 거 있잖아요.
거기에 온갖 모델들이 올라와 있는데 딥시크도 작게 만들어서 만든 게 많이 있는데 거기서 쓰는 방법이 뭐냐면은 아래 디스플레이션이라는 건데 거기서 핵심적인 것 중에 하나가 이렇게 있잖아요.
이렇게 돼 있는 원래 이렇게 학습된 거 있잖아요. 이런 거 이런 거를 이렇게 만들어서 하는 게 많아요.
이 이런 프라미터가 줄어들거든 파라미터를 줄이는 게 중요하잖아.

Attendees 1 35:09
프라미터가 1조이었는데 파라미터를 천개 돼도 될 수도 있다는 거지.
이렇게 만들어버리면 실제로 더 잘 푸는 거야. 오히려 그러니까 애매한 거를 억지로 공부해 놓는 게 좋은 게 아닐 수도 있잖아요.
여러분 플로드가 제가 지난 시간에 되게 욕했잖아요.
그래서 플로드 플로드가 애매한 거 이상하게 좀 이상해졌어요.
좀 오히려 제가 문제 잘 못 고 그래요. 됐지 대충 일 나중에 코디까지 가는 걸로 하고 그래요.
그래가지고 여기 숙제 숙제도 나온 거 알겠죠? 여러분 됐죠 그러면은 이제 한 일단락이 끝났고 지금 라지스티그레션을 더 하느냐 마느냐 약간 이런 문제가 있는데 할까 아니면 그냥 진도를 드디어 교과서로 나가 볼까요?
여러분 그럽시다. 드디어 진도를 이제 대충은 된 것 같아요.
여러분 교과서로 드디어 진도를 나갑시다. 여러분 그래서 지금 교과서가 제가 지난번에 일장을 조금 나가다 말았었죠.
그쵸 근데 금방 나갈 수밖에 없어요. 이제는 2장

Attendees 1 36:20
딥러닝이란 무엇인가 제가 거의 그래서 제가 이거 교과서 이거 교과서에 이 촉수대로 그냥 가지 뭐 거의 교과서가 다 잘 쓰여 있으니까 한번 보라고 했어요.
여기 보세요. 그래서 1.2절 요거 요거 요거 요거 여러분 다 알아야 되지.
그쵸 그쵸 여러분 이 부분 다 알고 그쵸 이 관계 확실히 아시고 거의 그림 위주로 하면 되지.
사실 그 지는 여러분 읽어보면 되고 다 읽어볼 필요 없고 너무 말이 많아.
이게 교과서 31쪽에 나오는 거는 머신러닝이 이제 규칙을 잘해야 된다고 해주는데 그냥 답만 나오는 게 아니라 사실은 난 이거 동의 아니라고 그랬죠.
여러분 별로 아닌 것 같아. 그리고 이제 중요한 건 파라미터가 있는 거야 알겠죠?
학습하는 파라미터 그래요. 그다음에 여기 내가 특별히 중요한 건 없어서 여기 다 했으니까 이거 교과서에서 문제 있지.
어쨌든 지금 35쪽 가면은 딥러닝이 무엇인가 나오잖아요.
여러분 그쵸 딥러닝 딥러닝 집이 무엇인가 그쵸 딥 뭔지 알죠?

Attendees 1 37:30
여러분 저한테 배웠죠 그쵸 딥이 뭔지 피딩 레이어가 2개 이상 그쵸 아까 그 딥러닝 뉴럴트크의 강력함에 해서 약간 경험했던 펜스 플로우 그라운드에서 그렇죠 안 되던 게 되잖아.
그쵸 비사용성에 의해서 그렇죠 손으로 절대로 없지 않는 걸로 해.
그래서 이게 그다음에 페이지에서 이제 이렇게 여기 나와 있죠.
여기 지금 36쪽에 그림 보면은 숫자 분류를 위한 심층 신경망 이 숫자 분류가 여러분 이게 여러분 저한테 배웠으니까 이제 알지 저 원본 입력 저게 뭐예요?
여러분 m 리스트 데이터 분명 그거네. 그렇죠 나중에 최종 출력이 저렇게 0부터 9까지로 0부터 9까지의 뭔가 출력 값이 하나가 아니라 10개가 나오는 거예요.
여러분 얘는 지금 보니까 아까 제가 이 텐스 플로우 그라운 플레이 그라운드로 보여준 거는 출력값이 몇 개였어요 한 개였죠 x1 x2가 들어왔을 때 얘가 1이냐 마이너스이냐 그거였잖아요.
그냥 그쵸 사실은 0하고 1 사이로 나왔을 때 확률로 그렇게 보여준 거죠.

Attendees 1 38:38
여기가 그런데 이거는 여기는 사가 사냐 아니냐 이런 거 구려하는 게 아니라 0이냐 1이냐 2냐 이런 거 하는 거잖아요.
그래서 이거를 0일 확률 1일 확률 2일 확률 이렇게 다 하는 걸로 돼 있어요.
보통 이렇게 많이 해요. 이해돼요. 여러분 이해되지 그래서 이거 전부 다 더하면 얼마나 된다고요?
확률 01 확률 1이 확률 더하면 다 여기 기본적으로 4가 1이고 나머지 0나을 줬겠지 그게 원래 원래 입력 데이터로 그렇게 보여 가요.
입력 데이터로 이게 이제 이게 지금 여러분 벡터죠 그러고 있는 이거는 이거 벡터야 그쵸 값이 여러 개가 아니 이거 이해되죠 여러분 이거 정체는 뭔가 원보 입력 이거는 이거는 여러분 이미지잖아요.
이미지는 픽셀들의 집합이잖아요. 그래서 이거는 이게 28이고 이거 28이거든요.
28개 28 이렇게 숫자가 있어요. 28 곱하기 28개의 인테저들이에요.
한 인테리어는 0부터 255 사이의 숫자고 뭐 알겠죠?
여러분 그래요.

Attendees 1 39:43
그래서 사실은 28 곱하기 28이 들어가는 거지 이게 208 숫자가 여기 시커먼 게 해서 0이고 하얀 게 255예요.
중간에 회색 값은 120 얼마 되지 이렇겠지 이해돼요 여러분 지금 그 비슷하게 쫙 주는 거야.
대 값을 액션부터 x 얼마까지 228 곱하기 28 마이너스 1까지 있겠지 여러분 계산하기 전 알겠죠.
이해되죠. 이게 다 들어가서 곱해. 층이 4개나 있었네.
그쵸? 그래가지고 마지막에 확률이 나온다는 거지 알겠죠 그렇게 해서 시킨다는 거예요.
그리고 이게 여기 층 1 표현 교과서에 이제 층 일 표현 층이 표현 막 이렇게 나오는데 이거는 지금 사실은 뭔 얘기냐면은 아까 이게 그림이 안 보였구나.
중간에 이렇게 4개로 나와서 다시 들어가고 그러고 있잖아요.
여러분 이 얘기는 여러분 사실은 이거는 사실 이제 이거는 나중에 CNN이라는 거랑 상관이 있는 건데 CNN이라는 거 CNN 나중에 뭔지 모르지만 이걸 하지 말고 이거 먼저 다 내가 여기서 설명을 좀 제대로 안 했네.

Attendees 1 40:57
아까 텐서 플로우 그라운드 있잖아요. 여기서 여기서 이거랑 상관이 없는 걸로 이것도 상관이 없는데 이거 제가 설명 안 한 게 있어가지고 여기 입력이 2개 들어왔잖아요.
여기 2개 여기 입력 여기서 출력이 지금 3개로 했잖아.
이런 3개 하면 결국 출력이 몇 개로 변해요? 3개로 변했잖아 3개에 대한 3개 입력으로 바꾼 거예요.
그렇죠 여기도 3개 입력으로 바꾸고 여기 더하면 여기는 더하면 이제 4개가 갑자기 중간에 중간 표현이 생기는 거라고요.
여러분 그래서 이것도 어떻게 생각하면 되냐면은 중간 표현을 지금 네 가지를 만들었다고 원래 이미지가 하나 들어왔는데 중간 표현을 네 가지 만드는 거예요.
이해돼요. 원래 이미지 이건데 이걸로 추상화시키려면 입력 이미지를 만들고 또 또 이거 또 여기 우연히 4개 또 만들었어요.
또 4개 만들었어. 그래서 결과를 판단한다는 거예요.
추상화 축 추가시키고 이해했어요. 여러분 특성을 잡아서 이게 이렇게 만드는 게 전부 다 뭔가 곱해서 만드는 거야.

Attendees 1 41:51
나중에 잘 모르는 여러분이 모르는 다른 뉴럴 네트워크가 나와도 전부 다 뭔가 곱해 곱하고 더하고 뒤집고 볶아서 액티베이션 통과시켜서 중간 표현을 만들어요.
중간 표현이라는 게 뭐냐 뭔가 추상화시킨 다른 표현인 거예요.
여러분이 사실 내가 여러분 바라보고 있고 막 그러는데 여러분 흑백으로 만들어도 전혀 상관이 없어.
그렇죠 여러분은 흑백으로 만들어도 내가 여러분 알아보잖아 표현을 받는 것 자체가 정보가 줄었어요.
늘었어요. 줄었지 줄어드는 게 오히려 여러분들이 파악하기에 더 좋아.
중간 표현으로 여러분에서 내가 모든 걸 다 디테일하게 파악하고 정신이 하나도 없죠.
중요한 것만 파악하고 있는 거예요. 그쵸 중요한 거를 다양한 관점에서 파악할 수 있잖아요.
여러분은 내가 다음에 기억하기 위해서 이 친구가 안경을 썼니 안 썼니 막 이런 거 있잖아요.
그런 게 기억에 남잖아. 인상으로 저거 옛날에는 어떤 게 있었냐면 맨날 여기 앉는 친구가 있어.

Attendees 1 42:37
거기 다른 친구 안 났는데 걔가 개 줄 알아 자리를 다 파악을 해버린 거지 이해돼요.
여러분 기 특성을 그러니까 뭔가 다 대충 기억하는 건데 사실 그렇죠 여기 중요한 특성을 보는 거잖아요.
그쵸 근데 여기 있는 애가 질문 한 줄 알았는데 걔가 저기 앉아있으면 계속 얘가 질문을 한 줄 알아 이해되죠 여러분 그런 위험 운동도 있지만 어쨌든 학습이 그렇게 된 거지 나도 그래서 여기 보면 얘도 층이 여기도 보면은 얘는 이렇게 이 선만 잡아들고 있고 이런 특성을 파악한 거야.
이쪽 선으로 된 것만 파악했고 얘는 뭔가 이렇게 위로 뚝뚝 올라간 거 파악했고 이런 식으로 파악을 한 거예요.
대충 중요한 특성이 뭐야? 그게 그렇게 중요하게 특성 파악한 게 다 뭐냐 여러분 정체가 나중에 실제 이 4 말고 온갖 숫자가 너무 높은 숫자가 들어왔을 때도 잘 파악할 만한 특성을 잡는 거죠.
이해돼요.

Attendees 1 43:19
여러분 대충 이해가 되지 그죠 그렇게 계속 사실 그리고 여러분 이게 좀 멀리서 바라보거나 아니면 특정 부분만 바라보면 또 잘 되는 것도 있지 사실은 잘하는 건 살 수 있지 좀 뭔가 이런 특성이 있잖아요.
이렇게 생긴 그쵸 뭔가 달라 6이랑 이런 거랑 그쵸 날카로운 부분이 좀 많이 있어.
4 6 같은 건 동글동글한 것도 많고 그 팔은 동글동글한 게 아래 위로 두 개 있고 이런 거 있잖아요.
여러분 그렇죠 근데 이거 사실 이런 거는 비지도 스터 해도 될 정도로 잘 돼.
숫자 같은 거는 안 가르쳐줘도 분류를 해보라도 여기까지 분류할 수 있다고요 잘 그때 또 그래야 된대.
무용 문제는 그래서 점점점 이제 슈퍼바이즈도 셀프 슈퍼바이즈가 되게 잘 되는 거지 그래요.
그래서 가면은 세이프 슈퍼바이즈라기보다는 얘기하려면 되게 복잡한데 공조하고 그다음에 37쪽에 제가 지난번에 얘기 다시 강조한 다시 복습이야 다시 또 하는 거예요.
여러분 또 또 복습하고 있어.

Attendees 1 44:16
다시 가도 되지만 이게 지금 신경망으로 보면 이제 층이 있는데 그쵸 층 층이 입력을 받아서 데이터를 변화하는 게 여기 층의 정체가 뭐냐면 아까 그 텐서 플로우 그라운드 생각하라고 플레이 그라운드 여기 뉴런들이 있는 거야.
그쵸 뉴런이 여러분 뉴런의 가중치 값이 있는 거잖아요.
그렇죠 여러분 가중치가 따로 빼놨잖아. 이 사람들이 여기는 그림을 아까 텍스 플로우 그라운드라는 가로 세로가 바뀌었고 이해돼요.
여러분 그리고 가중치가 여기 이 아래 표현되는데 여기 밖으로 빼놨어.
그쵸 여기 사실 근데 여러분 이거 어떻게 볼 수 있냐면은 여기 있으셔야 돼.
여기 진짜 여러분 이 그림이 너무 좋아요. 테스 클로브라는 거는 정말 너무 잘 만들었어.

Attendees 1 45:03
이게 가중치가 어디 있는 거야 이게 도대체 여기 3개 유런이 있으면 3개의 뉴런이 있는 건가 이게 가중치가 3개인가 여기 4개고 3개인가 가정치가 여기 3개고 4개고 3개인가 여기 입력이 x와 x 2개잖아요 맞지 입력 피처는 여기도 입력 부처가 다음에 3개가 되잖아요.
여기 요 층이 4개 유런 있는 데에서의 입력 비처는 3개 맞잖아요.
여기 얘 입장에서는 입력 피처가 3개 4개인 거지 이해돼요.
여러분 다 정확히 이해해요. 그런데 그러면은 웨이트 가중치 값 있죠?
가치 값 가중치 값이 여기 요 예층 있죠? 3개인가 몇 개예요?
6개 몇 개 6개예요 6개예요. 여러분 가중치에 바이어스도 넣으세요.
여러분 항상 알겠어요. 여러분 왜냐하면 가중치라는 게 웨이트고 파라미터인데 세타 제로부터 세타 2까지 있는 거예요.

Attendees 1 46:02
여기는 세타 원 세타 2가 이게 세타 1 세타 2 세타 1 세타 2 세타 1 세타 2가 리뉴레이션 세 번 하는 거죠.
그쵸 그리고 여기 또 액티베이션 통과하는 거 그쵸 이해돼요.
여러분 여기를 만약에 지금도 재밌는데 여기 렐루가 아니라 아까 아까 제가 이렇게 열심히 만들었잖아요.
했더니 되게 잘 됐잖아. 여기 리뉴얼 하면 절대로 안 돼.
절대로 학습이 안 돼. 이거 둘이 그렇게 한번 계보 장난 쳐보든지 알겠어요.
여러분 안 보여줘도 되겠지 안 돼. 학습 리뉴얼 비선형이 들어가야지만 학습이 된다고 잘라주는 게 있어야 된다고 얘네들 렐러 트니트라 시그모에서 전부 다 잘라주는 데가 있어요.
그게 중요하다고 알겠어요. 여러분 값을 없애버리는 게 있잖아요.
얘기를 얘는 그냥 통과시키는데 안 된다고 알지 다시 내가 지금 이거 강조할 게 너무 많아서 여기 지금 메이트가 그래서 몇 개야 여기 이 계층은 요기 요기 요 기층 하나예요.
기층 하나 매트가 몇 개예요?

Attendees 1 47:03
파라미터가 몇 개냐고 요 계층은 이렇게 소심해 틀려도 괜찮지 여러분 안 창피해요.
여기는 시험 잘 보는 게 좋은 거지 몇 개 6개인 것 같습니다.
6개 아니라고 미안해. 바이어스도 가족이라고 치자고 같이 바이어스도 다시 할게요.
미안해요. 여러분 고마워요. 바이어스 있죠? 가중치도 바이러스로 칩시다.
아니 거꾸로 바이러스도 가중치로 칩시다. 그래서 원래 리뉴얼 리그리스에서 가중치가 몇 개라고요?
여러분 만약에 입력이 하나면 니 리그레스 가중치가 몇 개예요?
2개 입력이 2개면 가중치가 몇 개 3개 오케이 얘는 지금 가중치가 몇 개 들어는 가중치가 몇 개 3개 오케이 그런 게 몇 개 3개 9개 다 성공했어요.
여러분 이렇게 해놓으면 안 까먹을 것 같아. 내가 아무리 그냥 이거 이걸로 각인시키는 게 좋은 것 같아요.
이거를 괜히 막 코딩으로 하고 이러면 정신이 하나도 없고 이걸로 기억해 봐요.
여러분 이 순간 좋지 않아요 이거 이걸 기억해요.

Attendees 1 48:13
가중치가 몇 개라고 입력 특성에다가 그 바이러스까지 해서 하나 항상 더해져 그다음에 그 개수는 이런 개수만큼 무조건 곱하기 해버려요.
왜냐하면 무식하게 해버렸거든. 저기 여기 여기는 덜 주고 더 줘 이런 거 안 하고 그래서 여기 지금 9개가 있고요.
이거 이거 하다 끝내야겠다. 그냥 여기는 여기는 여기 지금 중간 계층 있죠 이 두 번째 계층 여러분 두 번째 계층이 이게 이게 퍼스트 레이어죠.
그쵸? 여기 두 번째 계층 제가 이름 붙였어요. 그냥 두 번째 계층이라고 했어요.
이 두 번째 계층은 지금 이것도 히든 레이어 맞아요.
여러분 다 히든 레이어 입력 계층이나 출력 계층 빼고 다 히든 레이어예요.
여러분 입력 계층이에요. 알겠어요. 여러분 개도 액티베이션 아니 뭐야 첼로 네트워트 히들 레이어가 있는 거예요.
그쵸? 그래요. 다시 여기 여기는 4개의 유런이 있는데 이 이론은 지금 파라미터가 몇 개인가 가중치가 몇 개 4개 4개 잘했어요.
4개 맞지?

Attendees 1 49:14
왜냐하면 여기 3개가 들어오니까 항상 하나 더 해서 그렇죠 바이어스펙에서도 알겠죠 그래서 전부 몇 개예요 파라미터 2개층이 166개 16개 4 곱하기 4 그렇죠 얘는 4 곱하기 4가 아니다.
4개가 있는데 5개잖아 15개야 진짜로 그래요 여기는 그런 식이에요.
알겠죠? 여기는 지금 웨이트가 있나 출력 나오는데 출력 나오는데 웨이트가 뉴런이 아니기 때문에 웨이트는 없어요.
여기 지금 사실 뭐 하는 함수가 있어요? 여러분 아직 예측할 때 근데 어쨌든 여기는 나중에 마지막에 아웃풋 나올 때 뭘 쓸지에 대해서 이제 결정해 줘야 되는데 어쨌든 여기도 지금 웨이트가 있는 것 같은데 웨이트가 있네.
여기도 여기도 계층이 있구나. 여기도 끝까지 계층이 있네.
미안해요. 여러분 있었어 여기도 여기도 그냥 3개만 있었어요.
여기는 아웃풋 할 때는 웨이트가 있잖아 웨이트가 그쵸 있네 나도 지금 헷갈렸는데 여기 있네 있네 이거 곱해가지고 가는 거지.
마지막에 그 9개 층에도 됐어요.

Attendees 1 50:18
여러분 바이어스는 얘는 없구만 그래가지고 여기까지만 하면 될 것 같아요.
오늘은 여기까지 하고 확실히 리얼수 돼야 여기 이 그림 이해할 때도 교과서 지금 잠깐만 한번 얘기해.
이거 지금 보여주다가 들어갔잖아요. 요 층 있잖아요.
이 층이 뉴런이 몇 개 있고 가중치라는 거는 다 어떻게 됐는지 그 그림으로 이해하라고 이제 알았으니까 알겠죠 여러분 그럽시다.
좀 여기부터 할게요. 여러분 다음 시간에.


clovanote.naver.com