머신러닝 day9
2025.04.03 목 오후 2:03 ・ 48분 18초
심승환


참석자 1 00:00
누구지 그이나 글씨가 보이는 이거 3 네이버 그죠?
3 리얼 스테이로 주로 했을 때 이렇게 나오고 이게 아마 커널 리액션을 하면 이렇게 매끄럽게 영역이 한 제가 볼 때는 사면이 이게 제일 나을 것 같아요.
이렇게 할 수 있다는 거죠. 그래서 결론은 뭐냐 하면 한번 읽어보세요.
이거 내가 써놓을 때 로컬 러닝이 되게 플렉스브하고 그치 효과적이기 하지만 단점이 좀 있다는 얘기죠.
단점이 단점이 이렇게 크게 보면 세 가지가 있어 이게 무슨 말인가 봐.
근데 이제 얘가 익스텐스 받아요. 계산량이 많으니까 왜냐하면 스트랜 데이터 그 자체 가지고 내가 우리가 알고리즘을 만들어야 되기 때문에 다 또 틀린 데이터를 다 기억하고 있어야 돼.
저장하고 있어야 돼요. 음모가 들어가고 거리를 다 계산해야 돼요.
그렇지 트랙 데이터가 많아지면 이거 만만치 않게 계산하면 아까 그랬지

참석자 1 01:10
그래서 일일이 다 계산을 해야 된다. 이거지 이제 거리를 계산하는 기 때문에 데이터가 적으면 괜찮은데 수행 데이터가 많아지면 얘가 만만치 않게 계산이 됩니다.

참석자 1 01:25
그 데이터가 이제 이게 이게 두 번째가 이해가 되나 이게 뭐냐하면 이 데이터가 차원이 커집니다.
예를 들어서 이거를 내가 앱 리스트 데이터 가지고 이걸 하겠다.
만약에 로컬 앱이 한다 그러면은 784차원이 되는 거거든 그지 그럼 10억 4천에는 데이터 사이에 거리가 무지하게 길어져요.
왜냐하면 루트에 784개를 제곱해서 그렇지. 빼서 우리 두 이미지 간에 그지 784차원이라 그러니까의 데이터를 서로 빼서 제곱해서 우투수 만약에 우리 내가 l2 디스턴스 쓴다면 그렇게 하려면 상대방으로 계수 차이가 얼마 안 되는데도 784개의 제곱을 이렇게 되면은 이게 제가 멀어진다고요.
이게 그래서 계산량도 엄청나게 많고 그죠? 이게 가까운지 뭔지 계산 이게 만만치가 않아요.
그래서 이게 뭐냐면 그런 걸 뭐라고 그러면 코스트 업 디벤저널트라고 그래 차원의 저주 차원이 많아지면 차원이 이런 알고리즘에서는 너무 많은 비산량이 들어가게 돼서 구경 거의 불가능해요.
이거는 200보다는 손해가 너무 크죠.

참석자 1 02:44
이거 안 쓴다 이거지 데이터가 저차원에요. 저차원의 적은 데이터 가지고 갈 때는 로컬 러닝은 괜찮은데 데이터가 많거나 차원이 너무 크거나 차원이 너무 크면 당연히 데이터가 많아야 돼요.
그렇지 않으면 뭐가 생겨요 이것은 뜻이 과잉 접속하는 점이 사야겠지 그렇지 차원이 크다는 데이터도 많아지는 거 아니야 그런 데이터에 대해서는

참석자 1 03:20
너무 쓸데없는 컴퓨팅 파워를 너무 많이 잡아놓기 때문에 못 쓴다 이거지 쓰는 게 의미가 없다.

참석자 1 03:40
그리고 이제 데이터가 그렇죠 데이터가 노이즈하고 나오고 노이즈가 많이 타 있는 데이터 가지고 있다.
그러면은 거리 함수가 별로 이렇게 소용이 없습니다.
로이스 노이즈에 되게 민감하죠 그죠 로컬러 그렇다는 거죠.
작은 몇 개 안 되는 데이터라고 할 때는 그냥 아주 상식적인 개념이라서 의미가 있을 수 있는데 이제 아까 같이 차원이 많고 크거나 데이터가 많고 많고 이렇게 되면은 그리고 트레이 데이터가 되게 노이즈 한 데이터다 그러면 또 이용하게 노이즈 되게 민감하죠.
그렇죠 거리가 커집니다. 그래서 많이 쓰지는 않는다.
그 아이 너무 너무 심플하잖아 그지 초등학생 수준의 뭐라 그럴까 알고리즘이라고 생각할 수 있잖아.
이제 제일 가까운 놈한테 성적으로 한다라는 거 또 아주 간단한 데는 또 아주 생각보다 괜찮은 퍼포먼스를 내기는 해요.
이 정도로 해서 그냥 로컬 러닝은 그냥 맛보기를 하는 거야.

참석자 1 05:03
이거는 현재 로컬은 머신러닝에서는 가장 저수준 중에 하나라고 보면 되거든요.
이런 것도 있다 이렇게 알아두시면 될 것 같아요. 그럼 여기까지 질문 있으세요?
로컬 러닝에 대해서 하시면은 이제 그다음 강의 7번 7번 강의 자료 할까요?
7번

참석자 1 05:41
PCA PCA 들어봤죠 프린스플 컴퍼넌트 어너스 이게 머신 아니 뭐야 공업사 2에서 혹시 기억나나 작년에 작년에 이해가 안 가서 어떻게 올랐는지 모르겠네.
작년에 들은 사람들은 어떤지 모르는데 거기도 뭐 공업적 투에 있거든요.
이게 이게 7장인가 한 장 있는지 기억이 안 나네요.
혹시 혹시 이거 하면서 좀 어렵다고 생각하면은 한번 찾아보세요.
하여튼 모단을 다 찾아보세요. 거기에 피지컬 컴포넌트 어그니시스가 나오거든요.
알겐 베리 알겐 베타 모 지요. 그와 관련이 있어요.
근데 프레스퍼 컴플리트 어레스 얘가 정보량을 줄일 거야 압축할 때 굉장히 효율적인 알고리즘이에요.
이게

참석자 1 06:40
어차피 우리는 머신러닝이라는 거 하는 거지 차원을 줄여야 돼.
예를 들어 우리가 엠리스트 데이터 조그마한 이미지인데도 784차원에서 그죠 데이터가 784차원에서는 우리는 그걸 할 수가 없어.
그래서 이 데이터를 차원을 엄청 줄여야 되는 거예요.
차원을 심지어 제가 얘기했죠. 784차원을 2차원 3차원을 데이터를 받고 문서화하는데 나의 효율적인 방법이 PCA다.

참석자 1 07:19
그래서 여러분 뭐야 알겐 별로 알겐 베타의 개념으로 했잖아요.
그렇죠 보이 하나로 그럼 거기서 아이겐 밸류의 의미가 뭐고 아겐 베타의 의미가 뭐고 그리고 이제 티스컬러 미닝을 여기서 한번 이해해 볼게요.

참석자 1 07:39
그러면 PCA는 이제 대신 파플라 아까도 얘기했죠.
디벤전을 이었죠. 차원을 줄이는 거죠. 차원을 줄인다는 건 우리가 압축한다는 얘기야.
차원을 줄이는 데 있어서 매우 파피라 알고리즘이 PCA다.
그리고 이제 그래서 PCA 목적이 이제 알고리즘 관련된 내용이 쭉 여기에 있어요.
제일 먼저가 뭐냐면은 디렉션하는 메타를 찾는다.
왜 데이터가 맥스먼 베이런스를 갖는 디렉션 데를 찾는데 무슨 얘기냐면은 데이터가 이렇게 있어 이게 이게 제가 리버스 할 때 했었죠 그렇죠 데이터가 이렇게 주어졌어요.
그러면 여기서 이게 뭐죠? 그러면 아까 제가 설명했던 데이터가 맥시멈 밸런스를 갖는 디렉션 벡터가 뭘까?
이 데이터에서 맥스먼 베어리어스라는 디렉션 벡터가 뭐야 어느 방향에 있어요?
히천이 오는 어디 있어요? 여기서 디렉션 벡터가 뭐 있어요?
맥시멈 밸런스라는 디렉션 벡터가 뭐 있어?

참석자 1 08:55
직 직선 어느 방향이 어느 방향 상향이 그게 무상향이 이거 예 왜 이 바프 2벡타 맥시멈 바이러스 이게 이게 데이터의 맥시멈 바이러스죠 미니멈 바이러스는 어떻게 될까 이 방향 이해가 되나요?
그러면 결론적으로 보면 이 이 벡터 이 벡터가 뭐와 관련되냐 알겐 밸류가 큰 알갱 벡터 이 방향이 이터야.
내가 이 데이터에서 내가 코밸런스는 저기 지금 이렉터의 코밸런스 매트릭스를 구해서 이 데이터에 대해서 그래서 결론적으로 얘기하면 얘 아이겐 밸류가 큰 아이겐 밸류가 이 방향이고 아이겐 베개가 작은 알이겐 밸류가 작은 그거는 얘의 이번 수직 수직인 거 그래서 우리가 뭐라고 그랬어요?
코미네스 매트릭스는 뭐야? 시메트릭 매트릭스고 시메트릭 매트릭스의 아기 벨트는 어떻게 된다 그랬죠 서로 올 송으로 하자고 그랬지 올수으로 하자고 그랬어 기억나요?

참석자 1 10:21
미리 좀 알고 그래서 PCA는 뭐냐면 리액션 벡터를 찾는 거와 동시에 어디 데이터가 맥스먼 베이런스를 갖는 그런 방향 메타를 찾는 게 PCA다.
여산 3세.

참석자 1 10:47
그래서 물론 이제 얘는 데이터가 리니어한 서브 스페이스에 있어야 돼요.
이런 얘기가 얘가 가지 넌 리니어 스페이스에 있으면 의미가 없어요.
넌 리니어 스페이스가 뭘까 이거

참석자 1 11:28
이런 데이터예요.

참석자 1 11:46
이런 데이터가 이런 데이터가 런 리니한 데이터 셋이야.
3차원 공간에 있는데 데이터가 이렇게 데이터가 우리가 변화된 이 면이 이런 면을 말하는 거니까 이 점에서 쉽게 얘기해서 이 점에서 이 점까지 거리는 요거리죠.
그렇지 이 점에서 이점까지는 뭐야 내가 유클드 공간의 데이터라면 거의 다 이거겠죠.
그죠 근데 여기서 이 공간에서는 이게 이게 유의미하다는 건 뭐야?
평면에 풍성한 현상이죠. 그렇죠 얘는 이걸 우리가 얻은 거지 거래가 이런 개념으로 돼 있는 거리에서는 PC를 정리하면 안 돼요.
이 데이터가 그러니까 이점과 음영 사이의 거리가 직선으로 돼 있는 거예요.
예를 들어서 우리 지구상에 있는 공 공에 있는 데이터 저 데이터가 어디 있다고 그러면 공은 지구 구면 상에 있다고 그러면 어떻게 돼요?

참석자 1 12:45
근데 여기서 여기서 아르헨티나까지 걸어가면 거리가 어떻게 되나 원래는 이게 유클리드 공간이라면 여기서 아르헨티나까지 땅을 파가지고 그렇지 아르헨티나 여기 우리 지구에서 우리 우리나라에서 보면 대척점에 있거든요.
그냥 지구에 구멍을 딱 뚫어가지고 한 거리 그게 유크리 공간에서의 거리야 그게 엔투 디스턴스지 그지.
근데 실제로는 실제 구매 면세는 어떻게 돼요? 이걸 1월달에 가야 되지 그러니까 구매 면세는 이쿠리 공간이 아니고 데이터가 같은 3차원 공간에 있어도 이 거리 개념이 달라진다.
이런 류의 데이터에 대해서는 우리 아까 얘기했던 pcad를 쓸 수 없는데 데이터가 유코드 공간에 전다 이용과 요점의 거리가 이렇게 정리돼 있죠.
이런 공간에서는 우리가 PC를 쓸 수 있다. 이건 그런 것만을 리니어 스페이스 리니어 스페이스 또는 리니어 석스페이스라고 부릅니다.
참고로 무슨 말인가 헷갈릴 수 있을 것 같아서

참석자 1 13:58
데이터가 이제 리니어 스페이스 리니어 스페이스에 있을 때 쓸 수 있는 원 리니어 스페이스에 데이터가 있으면은 못 써보니까

참석자 1 14:15
데이터가 이제 데이터가 이렇게 우리가 이렇게 많이 써먹죠.
n VP 핵넓이 데이터가 뭐야? p 차원 데이터가 ND가 있을 때 우리 행렬로 쓰잖아요.
그렇죠 여기인데 여기 r을 이제 리니어 스페이스라는 얘기지 리얼 스페이스라는 얘기고 NBP 차원의 공간에 데이터라고 가졌을 때 그건 행렬이라고 돼 있었어 그지 이거 인바 PA 의미는 아니고 이게 데이터의 개수고 p는 데이터의 차원 그럴 때 우리가 우리가 뭘 찾냐면 d 차원의 그지 b 차원의 리니어 s스페이스 되면은 아까 같이 내가 예를 들어서 이런 거지 아까 이미지라고 했어 이미지는 784차원이잖아 그지 784차원의 데이터를 내가 3차원으로 누리고 싶다.
그러면 여기서 보면은 784에 해당되는 수가 p가 되는 거죠.

참석자 1 15:21
데이터의 차원이지 원래 네이터의 차원이고 내가 그거를 2차원 3차원이면 d 차원 3위 d가 해버리는 거 784차원 데이터를 줘서 내가 3차원 간에 리뉴얼 서클 리뉴얼 서스페이스예요.
15 8 4차원에 대한 리뉴얼 서브 스페이스가 그러니까 서스페이스는 뭐예요?
디멘전이 줄어드는 차원이죠. 들어오는 데이터 그걸 리뉴얼 서브 스페이스라고 그 서브 스페이스인데 그게 리뉴얼 리뉴얼 하니까 거리가 리뉴얼 l2 디스턴스 쪽에서 보는 거니까 이렇게 생각하면 돼

참석자 1 15:59
이해가 되나요? 그러니까 왜냐하면 내가 데이터를 784차원을 내가 갑자기 차하면 너무 계산 내용도 많고 그렇지 메모리도 많이 들어가고 이렇게 되니까 이거를 차원을 확 줄이는 거지.
그리고 내가 만약에 784차원의 데이터를 2차원이나 3차원으로 주실 때는 내가 실제로 전부 찍어볼 수 있잖아.
그렇지 1 2 3 4로 찍다고 이렇게 있으면은 이 데이터가 여기 몰려 있는지 그것도 우리가 알 수 있는 거죠.
눈으로 볼 수 있는 거지 데이터 비주얼라이제이션이라 데이터를 내가 눈으로 볼 수 있죠.
그렇게 되면 그렇지 784채널이 되면 데이터가 어떻게 분포돼 있는지 얘가 고칠 수가 없잖아요.
그래서 그래야 볼 수가 있지 그렇다는 거지. 그래서 내가 여기서 PCA는 뭐냐 하면 그니까 7억 8 4차원 서부터 3차원의 그 공간을 축소하는 차원에 확 축소해놓은 공간은 없는데 위험하다 이런 얘기 그게 말이 무슨 뜻인지 아시겠죠?
내용은 모르지만 하여튼 그런 거다.

참석자 1 17:09
그래서 이제 그런 공간을 찾으려면 어떻게 해야 되느냐 x v 여기서 x 하다가 v 를 미니마이즈 하는 이게 x v의 베어리언스를 미니마이즈 하는 조이를 찾은 거다.
물론 이제 v는 뭐냐 하면 p 차원 p 바이 d 매틱스야 b 바이 d 매틱스인데 일단 일단 이렇게 알았어 그거가 p 차원에서 2차원으로 지는 행렬 행렬을 생각하면 돼 그거 있고 x x 그러면 x v가 그러면 어떻게 될까요?
차는 어떻게 될까? x는 뭐라고 그랬어요? x는 n바이p죠 n바이p에다가 v가 p 바이니까 뭐야 n바이 b 행렬 n바이 DNA 에어랜스를 맥스마이즈 하 그런 부위를 찾아내는 게 pcaa 근데 그 부위는 뭐와 관련이 있냐 마겐 벡터와 관련이 있다.

참석자 1 18:16
그래서 이제 v가 이제 여기서 d가 숫자가 좀 있으면 이게 뭐야 이해가 잘 어려우니까 d를 편 이상 1로 보자 이거 d를 1로 봐 d d를 이게 보자.
그럼 v는 뭐야 그럼 p 바이 1 2 바 p 바이 1 미소에 대해서 그러면 XV는 뭐였어요?
XV는 n 바이 1 이렇게 가야 되는 거죠. 그렇죠 엔바이 행렬 그지 개의 베어리언스는 맥시마이즈 하는 부위 어떤 부위를 써는 분들은 XV의 베어리언스가 최저화 돼 있다 이거지 아까 뭐라고 그랬어요?
그게 알겐 벡터라고 그랬죠. 알겐 밸리가 큰 알겐 벡터요.
굉장히 결론이 그렇게 된다는 거죠. 그러니까 왜 그런지를 이제 우리가 따져 보겠죠.
그래서 이 부위는 기본적으로 우리가 단위 벡터를 쓰죠.
그죠 in 벡터 그죠 왜냐하면 방향 벡터니까 크기는 의미가 없어요.
그죠? 방향 그 방향이 뭐냐 데이터가 분포되어 있는 최대 베어레스 베어레스 방향 메어레스 최대 액이 되는 그 방향이 무엇이냐를 알려는

참석자 1 19:41
대충 PC의 개념을 아주 진짜 요약하면 이렇게 정리가 될 수 있을 것 같아.
그러면 이제 이걸 한번 여러분들이 읽어보세요. 이게 쭉 이해가 되는지 여기서 부위는 뭐야 여기 소문자 부위는 그러니까 뭐죠?
p 바이 1 벡터야 그러니까 x v 양은 n 바이 1 수익률 또는 베타라고 볼 수 있죠.
그러면 x v는 맥스 마이저 v의 절대값인 크기는 1이거든요.
오마라이즈가 있으니까 1위 했을 때 베어스 XV의 베어리스를 맥스마이즈 하는 맥스마이즈 맥스마이즈 하는 값이 얼마냐 그 값이 얼마냐를 구하면 수식 내가 쭉 적었어요.
하여튼 여기서 우리가 민 센트 데이터 데이터 x가 민세트 데이터는 얘기는 무슨 얘기냐면 원래 데이터가 이렇게 돼 있어요.
원래 그렇지 여기 이렇게 되면은 이 데이터는 평균이 0이 아니겠죠.
그지 거의 다 양수잖아 그렇지 데이터가 그지 x도 양수고 y도 양수니까 이게 평균은 0이 안 돼.

참석자 1 20:53
그럼 여기서 뭘 빼야 여기 평균이 뭐겠어 관심도 나올 거 아니야 그렇지 이 평균 2점이 좌표 이점 좌표가 평균 정도 될 거 아니에요 그럼 이 점을 다 뺀 거야.
그럼 데이터가 어떻게 0을 중심으로 쭉 분포돼 있죠 그죠?
그럼 이게 다 더하면 어떻게 됐어요? 다 더하면 원점이 되겠지 이런 데이터를 이제 이런 데이터를 우리가 인센터드 데이터다 이렇게 얘기 이해가 되나요?

참석자 1 21:23
용어를 용어를 이해할 때 이 세트 많이 쓰이는 말이니까 그 그거 기억해 두시고 그랬을 때 이 수식을 쭉 한번 읽어보세요.
일단 제가 한 5분 시간 줄 테니까 한번 쭉 읽어봐 이게 이해가 되나 봐 여러분들이 여러분 지금까지 배웠던 민 맥스 개념 하고 비대 값 베어리스트 다 알잖아요.
그죠? 그거를 이제 이해가 되는 거

참석자 1 22:56
여러분 잠깐 생각을 해봐야 돼요. 그냥 설명만 듣고 얘기하는 거 다 배웠네요.
다 배웠어요. 여기 이거를 제가 저기 이해야지 다 설명했던 거 지금까지 한 내용은 다 포함됐는데 이게 이게 일치냐 여러분 그래서 이게 어디가 막히냐 생각을 해보세요.

참석자 1 23:50
이게 매끄럽게 이해가 안 된다 그러면 내가 많이 부족하다 이렇게 생각하고 본인이 정소를 하셔야 돼요.

참석자 1 24:12
맥시멈 베어런스 같은 벡터를 찾는 과정이

참석자 1 24:56
다 이해가 됐어요.

참석자 1 25:04
다 이해하신 분

참석자 1 25:14
아무도 없죠.

참석자 1 25:19
아무도 없으면 좀 곤란한데

참석자 1 25:31
한번 볼까요? 그러면 요게 대열에서 이게 대런스라는 건 아시겠죠?
여기서 이거는 알겠죠 그죠? 근데 우리가 데이터가 빈센트 돼 있으니까 XV가 제로죠.
기대값 XV의 기댓값이 그렇지 맞아 몰라 반응이 있어야 넘어갈 거 아니야 XB의 길이 값 제로라는 게 이해가 안 가시는 분 혹시 있으세요?
없지 여기 여기 제가 여기 인세트 돼 있으니까 그래서 얘예요.
얘가 그렇죠 이 문제는 이 문제하고 고이야 그렇죠 그럴 때 기대값 이거는 사실 이 꼴이 아니라 요걸 밑을 이걸 구해야 돼.
근사 값이지 그지 정확하게 하면 어떻게 되나요? x v 정합 구분은 데이터 n 분의 원래 정확하게 하면 n분의 1의 XI 분의 그죠 얘를 맥스하이 해 주는 사고 같은 거죠.
따지자면

참석자 1 27:05
됐나요?

참석자 1 27:12
그래서 보면은 그러면 얘를 몇 사냐 그러면 얘는 뭐냐 이거죠.
그 이지 그죠 요거 요거는 이렇게 쓸 수 있죠. 그죠 얘는 XR 여기 그러면 여러분들 이 이렇게 말로 하면 또 혹시 또 힘이 가 하는 거 지금 만난 내가 쓴 게 스가 뭐냐면은 XL 1에 1 그지 x의 2의 1 x의 이게 p라고 그랬죠 p의 x의 1의 2 x 2의 2 x의 p의 두 번째 데이터 이렇게 된 거예요.
쭉 해가지고 x의 1의 이게 n이지 xm 2m n xm p의 그래서 얘가 n바 p 행렬이에요.
그죠 근데 아까 그 v는 뭐냐면 여기에 v가 v1 v2 VP 그죠 여기 저 그러면 얘가 얘가 vv 그지 요게 각각 로가 요게 요게 요게 요게 x 1 여기 x 2 그지 x의 m인가 저 이 로케이션 하면 XI 할 때 XI지 이 XI는 뭐냐면 이 로야 아 번째 로 아이 번째 로지 이게 이게 XI지 아 번째 로가 xri와 v 간의 내적이죠.
내적 그지 그렇죠 그거를 다 제곱미터 더한 거야. 내접 값을 다 더한 거야.

참석자 1 29:29
내접 값을 XY와 v를 내접하면은 이렇게 내력하면 값이 나오겠죠.
그렇죠 스칼라 요거는 이거 스칼라 스칼라 다 대부분이 더한 거는 뭐하고 똑같아 그러면 그거는 이렇게 쓸 수 있죠.
그죠 내야죠. 그렇죠 이렇게 하면 x라고 스가 그렇죠 이렇게 이렇게 이렇게 여기도 사실 이건 스칼라니까 트랜스포트 안 해도 되지만 일단 트랜스포트 썼어요.
왜냐면 그 다음을 위해서 그래서 그러면 그러면 이렇게 되잖아 필 부모가 되는 그지 그러면 얘는 얘는 이거 이거 똑같은 거 그렇죠 그거는 얘하고 똑같은 돈이에요.
맞나 여기서 이거 이해가 되나요?

참석자 1 30:23
무슨 얘기하면 이거 하면은 어떻게 되냐면 하면 하면 대체 어떻게 되냐면은 이게 x1하고 내적 v야 그죠 요거 요거 요거는 스칼라야 한 줄 이거는 x 2하고 v 쭉 해가지고 x n의 v다 이거지 그렇지 그러면 그러면 여기 다 저급하고 그걸 그러면 그건 뭐야 시그마 xi1 v의 매직 컨버를 제곱해서 다 더한 거는 뭐 하게 돼 있어 얘가 얘가 뭐였죠?
이 수직의 x 이게 이게 이게 그러면 사실 얘라는 건 뭐예요?
x v하고 같죠 이게 x v잖아 x하고 v하고 행렬 곱하면 이게 나오잖아요 n바이와 그지 그럼 얘는 얘는 뭐겠어 x 하고 x 아 x v에 트랜스포즈 x v와 같겠지 그죠 이게 XV니까 이걸 각각 다 제곱해서 다 이걸 각각의 즙을 더한 거는 XV와 XV를 제곱한 거잖아.
그렇죠 그런 걸 행렬을 쓰면 뭐야 그래서 쓰면은 v 트랜스 포즈 x 트랜스 포즈 x VR 사이즈를 보면 여기 뭐야 얘는 뭐지 v가 p 바이 어디 p 바이 y 얘가 x가 뭐였죠?

참석자 1 32:08
n바 p는 p 홈 p 바가 있는데 1 바위 p죠 얘가 사이즈면 1 바이 p 얘가 p 바이 n n 바이 p p 바이 1 그죠 하니까 스칼라죠 이게 그래서 얘는 1 바이 1이잖아.
그렇죠 그래서 얘를 얘를 이렇게 쓴 거를 이렇게 쓸 수 있다 이거지 어 먹죠

참석자 1 32:54
그러면 다시 쓰면 뭐 한 거냐 그러면 여기에 매각 그러면 만약에 tx가 뭐죠?
xt x는 뭐냐 그러면 x 트랜스포즈 x는 어떻게 돼요?
사이즈가 이거는 p 바 p 행렬이죠. 그렇지 그럼 엑스 트랜스포드 x가 뭐냐 이거지 엑스트랜스 포즈 x가 우리가 배웠죠.
이게 무슨 행렬이냐면 코베이런스 매트릭스예요.
코베이너스 매트릭스가 아니라 코베이런스 매트릭스의 에스트 메시는 지 그건 이해가 되나요?

참석자 1 33:40
x가 이거니까 그지 그러면 뭐야 얘는 얘는 어떻게 쓸 수 있냐면 이렇게 되는 거지 여기 미니 얘는 뭐야 이렇게 미가 제로야 그렇지 다 미니 제로죠 이 제로야 밍이 제로니까 어떻게 하면 얘는 얘는 어떻게 되냐면은 첫 번째 얘는 x1의 제곱 그지 얘는 여기는 시그마 엑스 1 2 시그마 엑스 1 엑스 알겠지 p 이렇게 되죠 이게 이해가 되나요?
그다음에는 여기는 시그마 x2 x 1 시그마 x2 x2 xq의 적용 시그마 xq xq 쭉 해가지고 시그마 x p x 1 그지 쭉 해가지고 시그마 XP의 제곱 이렇잖아요.

참석자 1 34:48
그렇지 정확하게 여기서 이렇게 되니까 뭐야 이게 p 개니까 p 분의 1 이렇게 해주면 네 각각 그렇죠 이렇게 양 p 분의 1은 다 p 분의 1이 다 들어가는 거죠.
그렇죠 가만 있어 봐.

참석자 1 35:10
쉽게 얘기하면 여기 p 분의 개수 이 수고

참석자 1 35:25
이렇게 해주면 그럼 얘는 뭐예요? 이게 선의 배열랜스죠.
이거 x1의 베어런스인 제가 x1의 밸런스를 엑스포메이션 하는 거잖아.
그죠 이게 베어 베어런스 그지 이건 x1과 x2 간의 커밸런스 그래 이게 결과적으로 보면 얘가 코베이런스가 되는데 이 코베이런스 매트릭스를 근사화시킨 거지 근사화시킨 거야.
이게 이해가 되나요? 이게 이해가 돼야 돼.

참석자 1 36:05
이해와 계 이게 이해가 돼요. 다른 사람들 다 이해가 되나요?
근데 처음에 저희가 x를 오소보널 매트릭스로 잡고 갔으면은 x 트랜스포스 x는 그냥 아니 여기 지금 아직 그거 안 나왔어요.
트랜스포에서 그 매트릭스가 뭐냐 브라 그래 v가 뭘 얘기하는 거지 앞에 페이지에서 엑스는 오소보을 매트릭스라고 잡고 지금 풀이를 진행한다는 아니 왜 x가 오소 말이지 엑스는 데이터에 데이터 이 데이터 트레이 데이터 전 데이터가 왜 오스 오라 하지 그냥 슬라이드 앞에가 그렇게 써져 있어서 잘 말씀드릴 올스막이는 저 부위가 오스망을 하더라는 부위 막 부위가 올스방이 그거 전혀 관계가 없어요.

참석자 1 37:05
이게 이해가 돼 이게 이게 핵심이 이거예요. 그러니까 이게 이게 이해가 안 가면 커베런스같이도 알고 있는 데이터도 알고 있는데 그럼 그 데이터가 또 커베런스를 어떻게 구하느냐 그러면 그럼 그게 다 죽은 지식이야.
맨날 코베런스 코베런스 하면서 코베런스가 뭔지도 모르고 모르는 거야.
데이터가 있을 때 코베런스 네티스 구해 바라는 못 구한다.
그건 말이 안 되지

참석자 1 37:51
저거를 얘가 이제 인지 구별 했으니까 결과적으로 보면 여기서 피브레를 한다면 피브레를 해준다면 결과적으로 뭐예요?
트랜스포즈 x가 얘가 코밸런스 매트릭스라는 거예요.
코밸런스 스 그럼 다시 쓰면 저걸 c라고 그러면 v 트랜스 포즈 CTV 이게 이게 이해가 돼야 돼.
그래야 내가 실제 데이터와 수학적인 이론적인 내용이 연결이 딱 되는 거지.
이게 이해가 안 되면은 수학은 수학대로 공부하고 데이터는 데이터대로 있어.
그럼 내가 이 데이터에 대해서 내가 수학적인 알고리즘을 적용할 수가 없는 거야.

참석자 1 38:43
질문 있어요.

참석자 1 38:48
그러면 결과적으로 뭐냐는 자 그럼 얘가 최댓값을 구해서 어떻게 구해야 되느냐 그러면 CV는 CV는 뭐야 우리가 쉽게 얘기해서 CV는 얘 얘 여기 또 여기서 알고 CV는 우리가 뭐 얘가 우리가 알고 있는 그러면 CV는 CV는 쉽게 CV는 람다 v라고 할 수 있죠.
그렇죠 알게 될터. 그럼 얘가 맥스만 가지면 어떻게 돼요?
얘가 얘가 최대값이라 그러면 뭐지 v가 v가 흡수의 알겐 메타 에인데 어떤 아이디어가 있냐면은 알겐 밸류가 최대 값이 되는 알겐 벡터를 잡아주면 얘 얘는 어떻게 되냐면 라보다 맥스가 되는 거죠.
아 메가 최소화 되는 아겐 벡터를 잡아주면 얘가 최저값이 되는

참석자 1 39:54
이해가 되나요?

참석자 1 39:58
그래서 그림 그리고 얘기하면은 이 그림에 데이터가 이 방향이 이 방향의 아이겐 벡터가 맥스가 되는 값이야.
그러면 그러니까 데이터에 얘 배열 수급할 때 이쪽 방향으로 배열 수급이 제일 맥스 마이스 무슨 얘기냐면 내가 데이터가 여기 있잖아.
그럼 내가 여기다가 선을 쭉 걸어가지고 알 벡터 알겐 벡터가 이제 알갱 벡터가 2단계니까 쭉 선을 줘.
그럼 데이터를 다 거기다 매핑을 시켜 그거의 밸런스가 얘의 밸런스가 제일 크겠지 그게 아니면 다 작아져 그지 데이터를 만약에 이 방향으로 이 방향으로 해서 아이 밸런스를 구하면은 작아진다.
이거지 제일 나갈 때가 언제냐 요 방향일 때가 이 방향이 이게 이번 데이터가 무슨 얘기냐면

참석자 1 40:55
이 베어리 쓰라는 결과적으로 뭐냐면 여기서 이게 데이터가 아까 여기 찍혔잖아.
그렇지 이제 이렇게 있는데 내가 알겐 벡터를 내가 이게 알겐 벡타면은 이때 배열할 때 이때 아이드 밸류가 제일 클 때 그 데이터를 보면 어떻게 하냐면 데이터를 쭉 이렇게 연결해서 손을 가장 손을 쓸 때 이 이 방향의 베어런스라는 얘기는 이 점이 있으면 점을 이 점 다 여기 수집을 다 매핑 시킨 거야.
이게 그지 모든 점 이렇게 매핑이 되겠죠. 리니얼 유리슨처럼 다 매핑이 돼.
그럼 이 데이터의 밸런스를 구하는 거야. 근데 만약에 내가 방향을 벡터가 이 방향으로 한다 그러면 선을 어떻게 돼 있어요?
이 방향으로 간다고 그러면은 선을 이렇게 해가지고 그렇지 이 점을 다 이렇게 매핑하는 거지 이렇게 수직으로 수직으로 뚝 떨어뜨리는 거야 할 수 있겠죠.
프로레스를 시키는 거야.

참석자 1 41:53
프로스 이렇게 이렇게 해가지고 대화 구하면 어떻게 될까 좀 줄어들겠지 줄어들 거야.
그러면 최소화는 뭐겠어 아이겐 벡터를 v를 내가 이 방향으로 벡터를 잡으면 돼요.
그럼 이 선을 이렇게 쭉 연결했을 때 모든 점을 여기 가면 어떻게 점이 다 여기에서 다 몰리겠지 그렇죠 그럼 이 데이터의 베어런스는 되게 작겠지 무슨 얘기인지 알겠어요 데이터가 데이터가 내가 이 선에다가 컬렉션 시키면 어떻게 해?
이 데이터는 이 점은 쭉 2점 2점으로 매핑할 거 아니야 그렇죠 2점으로 우리가 프로스는 시킨 다 프로시는 게 뭔지 알죠 거울 갖다 놓고 다 여기 다 매핑시켜 이 점은 2점으로 매핑될 거고 이 점은 쭈르륵 하면 이 점이 매핑 돼.
그러면 데이터는 어떻게 되겠어요? 이 데이터가 여기서 왔다 갔다 하겠죠.
그죠? 여기서 왔다 갔다.

참석자 1 42:46
근데 베어런스가 얼마 되게 작겠지 그러니까 배어량이 제일 작을 때는 뭐냐 하면 아이겐 밸류가 이게 다 남다녀가 니 모양이 아이겐 밸류가 최소 값을 갖는 제일 작은 아이겐 밸리를 갖는 아이겐 벡터 방향으로 플로싱 시키면 그 베어런스가 제일 작아진다.
이 제일 크게 하려면 알겐 벡터가 크게 알겐 벡터 큰 방향으로 내가 플렉스 시키면은 제일 크게 된다라는 얘기예요.
이게 이해가 돼야 돼요.

참석자 1 43:24
이게 우리가 알겐 별로 알겐텍터를 사용하는 의미 의미예요.
의미 이게 이게 이해가 안 가면 알게 메일로 알기 몇이 내가 왜 구하지도 못하고 실험에 나오니까 구하는 일밖에 안 되어.
이거는 발언 지식은 있지만 공학에서는 아무런 시자 쓰기 있다.

참석자 1 43:50
그럼 여기까지 질문 그러면 이 수식에서 그렇죠 결과적으로 얘기가 나오면 그렇지 그러니까 xt x v가 결과적으로 라메나 v하고 똑같은 거야.
그럼 그렇죠 그러면 여기는 람다니까 람다는 상승이니까 바깥에 뽑아 면 v 트랜스포즈 v는 1이죠.
크게 1이라고 그랬어요. 그럼 레이어는 람다 레스가 최대 값이에요.
얘의 최저값 미는 뭐겠어? 라모나 밀레이즈 아이겐 밸리가 최저

참석자 1 44:26
그러면 얘가 이해가 되는

참석자 1 44:34
다 배웠어. 다 배웠는데 얘가 이해가 안 간다 그러면은 더군다 구멍이 나 있는 거예요.

참석자 1 44:54
보시고 그러면 10분 쉬었다가

참석자 1 45:01
생각해 봐. 내가 뭘 이해를 못하는지 10분 후에 아주 지근지.

참석자 2 46:10
나는 이렇게 생각해. 우리 저울 0.31 이잖아.
정상이 켜져 있으니까 저울도 그냥 아무것도 안 올라가 마이너스 마이너스 3

참석자 2 46:55
최소 최대가 여기 다 떨어져 있는 데 수직 방향으로 수직 방향으로 가던 것처럼 보이는 그 여기 안에 보구나.
거기서 줄어들게 되면

참석자 2 47:46
내가 내가 이해한 거랑 비슷한 거. 근데 이게 맞나?
진짜 모르겠어. 정확하게 판단을 못했어. 맞아라고.


clovanote.naver.com