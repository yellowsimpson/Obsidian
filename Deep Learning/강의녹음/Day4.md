딥러닝 day4
2025.03.17 월 오전 10:01 ・ 49분 2초
심승환


참석자 1 00:00
그니까 기본인 거라서 막 붙이고 있어요. 그냥 기본인 거라서 어쨌든 슬라이드 지난 시간에 했던 거 이어서 계속할게요.
일단은 지난 시간에 하던 거 속수가 약간 다를 수 있어도 내용은 다 있어요.
여러분한테 그래서 지금 거의 다 했어요. 사실은 강화 학습까지 얘기 다 했고 인간 내 구조 얘기했고 여기 딥러닝 딥뉴럴 네트워크가 유닛 계층이 2개 이상 있는 거라는 거 얘기했죠 했나 안 했어 이게 문제야 이게 제 AI 공학 이론에서는 거기서 이제 또 난림으로 해야 되잖아요.
그래서 깊이 안 들어가고 막 하다가 겹쳐요. 내용이 그래서 거기서 한 거랑 여기서 한 거 하면 헷갈려가지고 슬라이더를 겹쳐서 문제네.
여러분들 여기까지 봤나 보다. 내가 이거

참석자 1 00:56
이 대유 여기 좀 하다가 말았나 사람들이 재밌다고 그러면서 그때 맞아 여기 책도 나갔었구나 그쵸 그래가지고 그래서 어쨌든 인간 내 구조 그 인간의 신경망을 얘기했었고 지난 시간에 그렇죠 그리고 이제 이게 이거를 사람이 어쨌든 이런 식으로 은근히 심플한데 우리 사람이 동작 무슨 생명체가 동작하는 방식이 이제 뉴런들이 연결된 건데 뉴런들이 차곡차곡 쌓여서 뭔가 된다는 거지 하는 일이 보면은 신호를 전달을 하는 건데 이게 전달을 많이 할 수도 있고 안 할 수도 있고 이런 거라는 게 되게 재미있죠.
이게 전달을 그대로 하는 게 아니라 똑같은 식으로 하는 게 아니라 비선형적으로 한다는 게 핵심이에요.
여러분 비선형 선형이 아니라는 거 제가 선형이니 뭐니 선형이 뭔지는 좀 떠들었었죠 약간 그쵸 선 모양이다.
그렇죠 비선형은 선 모양이 아닌 거다면서 좀 뒤로 가서 막 이 사람 키도 얘기했었죠.

참석자 1 01:55
그렇죠 했었지 그쵸 그래요 여러분 혹시 제가 실수로 내가 안 얘기는 했다고 주장하는 만큼 제동을 거세요 여러분 사람이 말을 하는 게 이제 비슷한 게 두 개 있어 이게 약간 좀 문제긴 한데 AI 공학 개론을 이제 어쨌든 1학년 과목이 새로 생겼는데 그게 원래 제가 하면 별로 좋지 않은데 또 이제 아니 근데 딥러닝 가르치는 사람이 하면 좋을 것 같아서 그냥 제가 했고요.
문제는 이게 너무 겹친다. 앞부분이 뒤에 가면 이제 달라질 텐데 헷갈려요.
좀 미안해요. 여러분 그래서 제가 어쨌든 안 한 거 근데 이게 또 AI 고약계에서 하는 내용이 무지 중요한 내용이잖아요.
왜냐하면 너무너무 기본이라서 근데 그걸 안 하고 휙 넘어가 버리면 곤란하니까 제동을 걸지 말고 이제 물어볼게요.
저도 어쨌든 여기 전달되는 거 내가 내용 했었고 여기 그래서 이게 이렇게 된다는 것도 했나 아직 지나가지도 여기 시작도 안 했구나 이거 합시다.
그냥 요거 뉴런 하나 뉴런 하나 그렇죠 뉴런이 생긴 모양도 다 달라요.

참석자 1 02:54
사실 여러분 여기 공교롭게도 저희 아이 어쨌든 제 큰아이가 이제 이쪽을 이런 거 공부하고 있는데 시험 공부한다고 막 보여주던데 정말 유런이 다양하게 생겼어요.
온갖 유런이 다양하게 생겼더라고요. 그래서 나는 좋은 공부 안 했으면 기분 좋긴 해요.
너무 할 게 많더라고요. 이어 상해 어쨌든 이 뉴런이 이렇게 생긴 건 뇌 세포가 이렇게 생겼고 뇌 세포 말고도 여러분들 신경 세포는 많잖아요.
감각 세포들도 그다음에 운동 세포들도 있고 하여튼 간에 뭔가 다 느끼는 거는 전부 다 뉴런들이고 다양한 모양이 있어요.
여러분 근데 어쨌든 지금 우리는 인공신경망 이거는 기본적으로 뇌의 뉴런을 모방한다고 생각하고 이렇게 생겼어요.
그래서 여기 보면 여기 여기 몇 개 연결됐대요.

참석자 1 03:38
100조에서 천 조개가 연결되고 개수 자체는 천억 개인데 벡티이션도 이게 어마어마하죠.
그렇죠 연결이 되어 있는 거예요. 근데 그중에 오는 것은 어떤 데서는 신호가 오기도 하고 어떤 데 안 오기도 하고 막 이럴 거 아니야 얘도 지금 얘 연결된 건 다 줄 거 아니에요?
연결된 거는 그렇죠 여기서 연결된 것만 지금 신호를 받겠지 뭔가 오는 그런 식으로 되는 거예요.
그리고 이제 얘는 어쨌든 똑같은 신호를 내보낸다는 게 중요하고 받는 거는 다 들어오는 게 아니라는 게 중요하지.
그렇죠 여러분 느낌이 와요. 여러분 그냥 그게 이게 중요한 게 뭐냐면은 여기에 실제로 얘는 똑같은 신호를 어쨌든 주긴 주는데 여기에서 이제 받을 때 이 학습하는 방식이 이 신호를 그대로 보낼 것이냐 억제 시키냐 이런 게 있다고 그랬잖아요.

참석자 1 04:27
그쵸 그래서 정보를 저장하는 것이 뭐냐 아까 여기도 여기 여기 보면은 이게 여기가 학습과 기억의 원천이라고 그랬는데 이 연결이 되냐 안 되냐의 문제라기보다도 되는데 어느 강도로 되느냐가 문제인 거예요.
여러분 그게 학습의 원천인 거예요. 전에 제가 지난 시간에 기억나죠.
여기서 무슨 조현병 얘기도 하고 adhn 얘기도 하고 했었어.
그렇죠 신호가 무조건 다 가버리면 곤란해요. 여러분 그렇죠 여러분 화가 난다고 화를 내고 있으면 안 돼요.
여기서 갑자기 그렇죠 내가 배가고프다고 배고프다고 떠들고 있으면 곤란하죠.
억제하는 거죠. 그쵸? 억제가 되게 중요하다고 얘기했죠.
증폭되는 것도 감정이 막 증폭돼서 갑자기 여기서 뭔가 어제 본 영화가 생각나서 울고 있으면 곤란하잖아요.
여러분 그쵸. 아니 그러니까 그런 억제를 하는 것도 필요하고 또 집중할 때 집중되게 해야 돼요.
그쵸 이게 지금 연결이 천 조개씩 연결돼 있는데 이 신호를 내가 학습을 통해서 조절을 한다는 거지.

참석자 1 05:21
그래서 여기 지금 기억의 원천이 시네스라는 건데 이 시냅스가 연결의 어떤 값을 증폭시키냐 마냐 하는 걸 사실 여러분 제일 간단하게 표현하는 게 곱하기죠.
곱하기 0을 곱하면 어떻게 돼요? 여러분 없어져 그쵸 많은 가을 곱하면 곱할수록 숫자가 커져요.
그렇죠 그리고 그거를 이제 다 들어오는 신호들을 합쳐서 이제 이쪽으로 보내잖아요.
그쵸 보내는데 그거를 신 신호 들어온 신호를 전부 다 합쳐가지고 그대로 보내면 또 신호가 어떤 거 데이프 어떤 거 잡고 막 이렇게 노말라이즈가 안 되잖아요.
그쵸 그래서 그거를 뭔가 또 노멀라이즈 해놔야 되는데 노멀라이즈를 하든지 아니면 또 다 받았는데 이거를 또 보내지 말아야겠다.
이 정도 값이면 보내야겠다 이렇게도 할 수 있잖아요.
여기 신호들이 막 왔는데 너무 막 미미하게 오면 이거 보내지 말자 이거는 보내자 이런 식으로 뭔가 또 이게 바운더리가 있어야 되지.

참석자 1 06:11
여기서도 그래서 중요한 게 여기서 들어오는 신호들은 전부 다 뭔가 여기 온갖 천조개가 들어오는 것들에 대해서 각각에 대해서 웨이트가 다 있고 웨이트 가중치라는 느낌 알잖아요.
여러분 가중치라는 얘기 제가 되게 많이 하잖아요.
그쵸. 근데 가중치를 곱해서 더하는 거 있죠? 그건 선형 시스템이야 그쵸.
선이잖아. 그냥 다이지 커피를 좋아하는 건 선영이에요.
무조건 아무리 천소리가 들어와도 선영이에요. 그쵸.
근데 선형 시스템이라는 정의는 여러분이 신호로 시스템을 언제 듣지 그렇네 서형 시스템이 뭔지 아네.
여러분이 다 서형 시스템 정의가 뭐냐 선형이 사실은 선형이 된다는 게 사실은 원래 모든 입력이 들어오면 전부 다 그 입력을 나눠서 들어오고 다 들어오고 똑같이 결과가 변하지 않는다는 거지.
그쵸. 이게 원래 원래 선형 함수의 특성이지. 그쵸 선형 감수를 생각 선형 함수를 생각해도 되고 어쨌든 실험 시스템 중요해요.

참석자 1 07:03
여러분 이거는 여기도 실험 시스템이 똑같이 들어오기 때문에 어쨌든 선영배 심사 배웠잖아 아닌가 공업수학에서 신 사람 배웠을걸 사에서 공업수학 앞에 안 들리는 것도 좀 곤란해.
그러면 어쨌든 그래서 얘기하고 싶은 거는 여기 가중치가 곱해져가지고 다 들어온 걸까 선형인데 그다음에 다시 여기 중요한 게 여기서 나갈 때 비선형성이 들어간다는 건데 아까 전에 키도 그냥 키도 계속 전형 키 보였지 계속 가는 게 아니라 여기서 여기서부터 비선형이야.
그렇죠 여기 선형에다가 그렇죠 그런 것처럼 여기도 이제 이런에서 신호가 들어오다가 여기서부터 이제 지가 이제 값을 어느 정도 바운더리를 만들든지 아니면은 너무 크면은 이제 센트레이트 시키고 어쩔 때는 너무 작으면 아예 보내지도 않고 이런 일을 한다는 거예요.
이런 걸 잘 못한다.

참석자 1 07:48
신경 전달이 잘 안 된다 이런 게 그러면 이제 온갖 병에 시달리는 거예요.
여러분 환청에 시달리고 난리가 아니에요. 진짜 이게 실제인지 가짜인지 구분도 안 되고 난리가 나는 거예요.
여러분 그래요. 예측이 돼. 왜 그런 병이 생기는지 약을 왜 세균되는지도 약간 예측이 되고 어쨌든 이런 거 잘 되게 하는 거겠지.
여튼 그래서 이게 이거를 따라서 이게 내가 대단한 일을 하니까 대단한 일을 하는 걸 따라서 뭔가 시스템을 만들면 좋지 않을까 생각이 들잖아요.
당연히 여러분 그렇죠 우리 인공지능 만들고 싶은 거니까 사람이 비슷하게 만들면 좋을 거 아니에요 그래가지고 이거를 따라서 이런 거 하나 따라서 만들어서 이걸 여러 개 연결하면 비슷해질 거 아니에요 여러 층으로 그쵸.
그래서 얘를 여기 지금 이게 뉴러드 요거에 해당하는 게 여기 있고요.
이거 매핑 굳이 안 보여도 이렇게나 보이잖아. 잘 그쵸.
들어오는 신호 그러니까 여기 다양한 데서 들어오잖아.

참석자 1 08:37
다양한 데서 어떤 데는 여기 손에서 들어오고 어떤 데는 팔에서 들어오고 막 다른 데 보시다 들어오고 그런 것들이 입력 값들이에요.
이렇게 쭉 그리고 그래서 이렇게 들어오는 데다 그냥 들어오는 게 아니라 여기에다가 약간 시냅스 연결된 부분 있잖아요.
연결된 부분 여기에다 연계 연결 스윙이 약간 다르지만 이렇게 생긴 게 약간 다르게 생겼다.
그림이 다르죠. 여기 연결된 게 얘는 이거 하나가 이렇게 여기 이렇게 생겼는데 여기는 이렇게 그려졌는데 약간씩 달라요.
사실은 진짜로 다르게 생겼더라고 그래 됐어요. 그래서 어쨌든 여기에다가 이 시냅스 연결 여기 기억의 원천이 여기 사실은 이렇게 동글동글한 것들이 막 옮겨다니더라고요.
화학 물질들이 탈모 이런 것들이 어쨌든 걔네 그래서 약으로 이제 그런 것도 그런 데 있는데 그래 나도 너무 많이 나 제대로 아는 것도 아니고 어쨌든 그래서 여기에 그 웨이트를 곱해야 된다는 거지 알겠죠.

참석자 1 09:31
여러분 이게 이게 이게 파라미터야 이게 이거를 전달 여기를 여기 이거 중요하니까 이거 크게 하느냐 이건 별로 안 중요하니까 약하게 0으로 하느냐 이런 게 중요하다고 이해돼요.
여러분 됐죠. 그다음에 이걸 다 더해야지 보편성 공분 다 더 해야 될 거 아니야 당연히 여기까지 완전 선형이야.
그렇죠. 이게 여기까지가 여기 지금 여기까지 가 이만큼까지 가 완전히 리뉴얼 리그레션이 똑같아요.
선형 회기랑 똑같은 거예요. 선형 회기 원래 많이 쓰던 선형 회기가 이거예요.
그럼 입력 값에 들어오는 거에다가 얼마를 곱해서 출력을 만들어야 되냐 이런 거 거기다 바이스 더 할 수 있는 거 이해됐죠?
여러분들 이게 설명 일이에요. 됐죠 그런데 중요한 거는 뉴러는 선호 행위로 끝나는 게 아니라 여기서 다시 또 이제 신호를 보내느냐 마느냐를 얼마큼 보낼 거냐 여기 여기서 비선형성이 들어갔다는 거지 이렇게 함수를 통과시킨다고 이해돼요.

참석자 1 10:33
여러분 그래서 이 선형 시스템에다가 비선형성이 들어가는 게 이제 함수가 통과된다는 건데 이 함수가 우리가 어떻게 부르냐면 이걸 액티베이션 펑션이라고 불러요.
왜냐하면 액티베이션의 뜻이 여러분 뭔가 더 좋게 더 크게 하든지 뭔가 아니면 버려도 되고 이런 느낌이 들어서 또는 이제 변형시킨다고 해서 트랜스포어라 프로모션이라는 말도 쓰기도 하고 이해되죠.
여러분 그래요. 전달 전달이라는 뜻도 있고 활성화시키는 뜻도 있고 해가지고 트랜스퍼 펑션 액티베이션 션 같은 말인데 지금은 거의 액티베이션이라는 말만 써요.
왜냐하면 API가 애플리케이션 프로그램 인터페이스 쪽에서 이제 텐서 플로우 파이터이 전부 다 액티베이션 용어를 쓰기 때문에 트랜스포펑션이라는 말을 안 쓰고 액티베이션 펑션이라 불러요.
어쨌든 얘가 이제

참석자 1 11:21
얘가 빨간색 깔 들어야겠네. 이 쇼를 아까 전에 액티베이션 펑션이 전에 내가 지난 시간에 이거 보여주긴 했던 것 같은데 아니에요 안 보겠어 자기 합시다.
그래서 그러냐면 액티베이션 펑션이 대표적인 게 옛날에 많이 쓰던 게 식용 오일이었어요.
시그모이드가 시그모이드 방수가 있거든요. s 모양이라고 시그모이드라고 불렀어요.
보여줄까 미리 미리 보여주는 게 낫겠다. 시구모이 담수 되기 쉬우니까 여기 보면은 여기 여러분 이 슬라이드 있죠 시모이드 담수가 여기 초록색인데 여기 여기 보면 약간 s자 모양으로 생겼어요.
여러분 이쪽에서 보면 s 같이 보이죠. 저기 이쪽에서 이쪽에서 보면 여러분 자 좀 보여요.
s 같잖아 s 같아 이 뚜껑을 봐도 s처럼 보이지 약간 좀 그냥 평평하지만 s처럼 여기서 s 모양이라는 뜻이에요.
시구 모이가 s 모양 이런 뜻이에요. 여러분 s 모양 이게 이게 뭐가 중요하냐면은 여기 보면은 한 마이너스 4랑 플러스 4부터는 전부 다 그냥 클릭하죠.

참석자 1 12:24
그러니까 모든 값이 들어오면 전부 다 0하고 1 사이로만 다 맞추고 전부 다 그 넘어가지 않게 이거보다 너 마이너스 4보다 작으면 그냥 0으로 플러스 4보다 크면은 1로 그 사이는 0하고 a 사이 값으로 해줘서 하고 그 사 뭔가 여기서 마이너스 4하고 4 사이로 들어오는 값만 이제 좀 의미 있는 값으로 전파시키고 그 이상하게 들어올 값을 전부 다 그냥 버리던지 아니면 완전히 일로 보내든지 약간 이런 게 뭔가 의미가 있잖아요.
뭔가 0하고 1 사이로 뭔가 로버라이즈 시킨 게 있으니까 이걸 많이 썼어요.
옛날에 근데 이제 이걸 많이 쓰고 있다가 이거 안 쓰면 좋다는 걸 나중에 깨달았지 사람 어쨌든 식모이드라는 게 여러분 이해되죠?
여러분 이거를 보통 여기 다 상식적으로 여기 그냥 막 내가 여러분한테 말했던 모든 이렇게 검토하는 건 좀 곤란할 것 같고 어떤 거는 0으로 그려도 돼 어떤 거는 최소한 3대 1로 보내야지 다 보낼 수는 없다는 느낌이 들잖아요.

참석자 1 13:15
여러분 그러니까 시그모이드 함수가 그렇게 생겼으니까 그거 썼다고요 자연 모든 존재하는 리얼 넘버를 실수를 0하고 1 사이로 만들어주잖아 또 비례해서 그렇죠 괜찮잖아요.
그래서 그런 함수를 많이 썼고 그래서 중요한 건 이제 여기 리니어 리니그레션 하는 거랑 액티베이션 통선 통과시켜서 나온 값이 여기 연결된 거 그냥 똑같은 값으로 들어가는 거지 이해돼요.
여러분 그래서 이게 지금 계층 하나라고 볼 수 있고 또 계층이 또 쌓이고 또 쌓이고 또 쌓이고 이런 식으로 그런 식으로 한다는 거죠.
그래요.

참석자 1 13:56
그리고 여기에 혹시나 섞어주면 여기 식이 적혀 있는데 여기 이제 이게 여기 인풋을 인풋이라는 아가 인풋에서 나왔겠지 용어 자체가 인풋 II 이게 이제 아이 번째 이제 뉴런이라고 생각하면은 결국은 출력하면 YI가 나올 것이고 그쵸 그리고 거기 각각의 웨이트는 이제 1부터 i에 대한 웨이트로 j n개가 있으면 x 아니까 여기 입력이 n 게 들어오면은 이렇게 생길 거 아니에요 그쵸 wn까지 그쵸 답 i에 대해서 그쵸 이해되죠?
여러분 아는 함수예요. 여기서는 아이가 여기 아이 번째 아이 뉴런이라는 뜻이지 그래서 아 URL에서 NG 인풋이 들어오면은 NG 웨이트가 있는 거지 이해되죠 여러분 그래서 이거 진짜 여기 요 입력 자체는 결국은 이렇게 서메이션 해가지고 x j j가 이제 얼마부터 얼마 1부터 a까지 해가지고 그렇죠 1부터 n까지잖아요.
그쵸 그래서 다 곱해서 더한 거지. 그렇죠 이게 이게 뭐라고요?

참석자 1 14:59
여러분 정체가 미니 리그레이션이라고 그리고 j를 이제 만약에 0부터 한다.
여기 j가 0이 없잖아요. x 제로가 지금 없죠. x 제로가 여기 줄이면 x 7 이렇게 안 돼 이거 앞으로 보부터 x 제로가 여기 없어요.
그쵸 x 제로를 무조건 1이라는 걸 두고 이렇게 이렇게 하면은 여기 웨이트 또 준다고 생각하면 여기 r w 0 i 이렇게 하잖아요.
그러면 이제 이게 바이스지 측정 값으로 이렇게 이제 상수를 주는 거잖아요.
그쵸 이해되죠 여러분 이건 너무 이뉴럴은 중요해서 인뉴럴에서 들어오는 건 무조건 좀 값에 뭔가 증폭시켜야겠어 하면 여기다 바이러스 여기다 이렇게 주면 이라는 거에다가 그런 거죠.
지금 기본적으로 높아지는 거잖아요. 그쵸 이해돼요.

참석자 1 15:46
입력이 하나도 거의 안 들어와도 무조건 여기 여기 웨이트 값이 그냥 다 접하지는 밑에 주의를 기울이고 있는 거지 이해됐죠 여러분 그래서 이게 너무 심해지면 이제 잠도 못 자고 뭐 그런 거지 알겠죠 여러분 그래요.
뭔가 트라우마 때문에 이게 막 강해졌어요. 바이어스가 어쨌든 이해되죠.
여러분 그래서 이제 이게 이게 이런 리그렉션이라는 거야.
알겠어요 그다음에 이제 실제로 이 II를 이제 f를 통과시킨다고 비선형성으로 만들기 위해서 비선형 함수라고 이거는 비선형 함수를 이 액티베이션 펑션의 특징은 액티베이션 해야 되니까 분명히 원래 입력에 비례해서 넘겨야 되는 부분도 있어요.
있어야 돼. 그러니까 선형적인 부분도 반드시 있어야 돼요.
액티베이션 포션으로 적합한 놈은 항상 선향성이 있어야 돼.
얘도 그러니까 어떤 구간은 선형적이어야 돼. 선형적이라는 게 뭐냐면은 원래 들어온 입력 값에 비례해서 그대로 전파해야 된다는 얘기잖아요.

참석자 1 16:39
근데 이제 너무 이제 바운더리가 없으면 곤란하니까 뭔가 좀 무시하거나 아니면 시키거나 이런 부분이 누가 이렇게 떼추를 시키거나 떼추리티가 있으면은 여러분 억제한다는 뜻이에요.
여러분 이해되죠? 여러분 그래서 여기 맥티 펑션 구경을 먼저 시켜주면 여기 액티리소펜션이 옛날에 맨날 15일을 쓰다가 그전에는 오히려 스텝 썼다가 왜냐면은 스텝을 마이너스 하고 하고 1 사회로 보내고 싶다고 스텝 쓰고 그다음에 이거 너무 나중에 이제 배우겠지만 트레이닝 하려고 그러면은 변화율이 중요하거든요.

참석자 1 17:09
왜냐하면 오차가 변화 이게 이 웨이트가 얼마큼 변할 때 오차가 얼마큼 변하냐 하니까 변화율이 중요할 거 아니에요 변화율 변화율을 여러분 뭔가 계산해가지고 곱하려고 그러면은 계산해서 전파하려고 그러면은 항상 미분이 가능해야 되잖아 미분이 가능해야 계산 변화율이 계산해 지는데 미분이 안 되잖아 얘가 그쵸 여기 스텝 펑션 미분이 안 되잖아요.
이렇게 딱딱 올라가는 그래가지고 스텝 펑션을 안 쓰고 시금고를 쓰기 시작했어요.
시금고는 미분이 깨끗하게 잘 되거든요. 이 이게 이렇게 수무당으로 이분이 잘 돼 그리고 그다음에 이거 그래도 저기 시크모이드는 너무 이제 여기 0으로 만들어 버리니까 마이너스 1로 하는 것도 의미가 있는 것 같은데 좀 큰 걸로 하고 싶어서 탄센트 하이퍼블리 탄센트 있죠 파란색으로 생긴 거 그것도 마이너스 1하고 1 사이인데 거의 비슷하게 생겼잖아요.

참석자 1 17:54
그쵸 칩스텝이랑 브랜드 미분 가능한 놈이니까 하이퍼볼리티네트 쓰기 시작했고 그러다가 나중에 렐루가 최고구나 이런 걸 깨달았어요.
완전 선형이죠. 여기 여러분 렐루는 뭔지 모르겠지만 완전 선형이죠.
선이잖아 0보다 작은 거는 그냥 0으로 만들어버리고 0보다 큰 거는 선형으로 전파시키는 이게 최고로 잘 된다는 걸 깨달아서 사람들이 나중에 왜 그런지 볼게요.
여러분 그래서 이렇게 엘로라는 펑션을 쓰기 시작했어요.
어쨌든 여기서 내가 얘기하고 싶은 거는 이 액티베이션 펑션들은 전부 다 어쨌든 원래 입력 값에 비례하는 부분이 있어요.
그쵸 그러면서도 약간 비서용성이 가미가 돼 비선형 부분 인은 원래 값을 이제 우리가 0으로 만든다든지 마이너스로 만들든지 1로 만든다든지 이런 듯이 하고 있잖아요.
그쵸 목적을 알겠죠 여러분 그래서 이거 말고도 뭔가 액티비션 펑션이 있는데 다 비슷해요.

참석자 1 18:41
이렇게 뭔가 어떤 값은 그대로 보내고 어떤 값은 버리고 그대로 이해되죠 여러분 대프시키는 거 이런 거예요.
그래요. 그다음에 참고로 여기 오른쪽에 해놓은 거는 이제 디리버티브는 미분 값이죠.
미분 값 미분 함수예요. 이건 미분했을 때 하면서 생긴 모양인데 스텝 펑션은 이제 미분이 안 돼서 이렇게 이렇게 x로 딱 표시가 됐고 그다음에 뭐야 시그모이드는 아주 이쁘죠 이쁜데 이분이 삼수 자체가 되게 플릿하죠.
그쵸 변화율이 되게 좀 작아 그다음에 탄센트는 좀 크고 크게 생겼고 지금 괜찮죠 쓸만하고 그다음에 렐루는 여기는 변화율이 항상 1이던지 아니면은 0이잖아요.
이게 이게 무슨 뜻이냐면은 변화율이 1이라는 거는 원래 변화 웨이트 값 변환 그대로 로스 값이 변한다는 뜻이지 그래요.
나중에 루스 값이 뭔지 여러분 지난 시간에 교과서 보여줬나요?
한번 루스 펑션 보여줬죠. 다시 한 번 보여줄게요.
루스 펑션이라고 결국은 우리가 중요한 거는 로스 줄이는 거잖아요.

참석자 1 19:48
에러가 그런데 이제 에러에 대한 에러 변화율이 이 웨이트가 변하는 이게 변화율이이라는 거는 에러도 일변한다는 거지 그러니까 그런 식으로 이건 근데 더 확실하잖아요.
이게 완전히 변화율이 완전히 살아 있어. 그렇죠 그러니까 이게 뭔가 학습하기에 되게 좋은 거지 그래서 레로이드를 많이 쓰고 핸드토도 이렇게 확 줄어드는 데 그렇죠 왜냐면 여기서 이렇게 변화율이 없잖아.
그쵸 그러니까 이 입력이 어떻게 들어오는지 잘 모르잖아.
그렇죠 여기 변화율이 없다는 거 이렇게 완전히 없애버리는 거는 레눈을 전파시켜버리니까 오히려 더 학습이 잘 되는 경향이 학습이 잘 돼요.
나중에 깨달으면 좋겠어요. 여러분 어쨌든 이렇게 돼 있고 지금 여기서 이거 설명한 이유는 얘가 여기 이거 효율성을 느끼면은 오히려 저게 더 이해하기 쉬운 것 같아서 바로 설명했어요.
알겠죠 그래 저도 눈치 보면서 설명하고 있어요. 여러분 지금 알아듣나 보나 괜찮죠 일단 갈게요.

참석자 1 20:41
여러분 그다음에 앞에서 학습한다는 거는 지금 이건 별로 높은 것 같지는 않지만 어쨌든 학습한다면은 여기서 나온 거에서 결국은 에러 값 있죠.
에러 값을 계산해서 이걸 이걸 보여주고 싶었던 건데 이 슬라이드에서 에러 값을 나온 거 가지고 얘는 변환시켜야 된다.
그쵸 여기 여기 여기 여기 얘네들 웨이트 값을 조정해야 된다.
그쵸 에러 값 나온 거 가지고 그리고 여기서 다시 강조하면은 결국은 이제 인공신경망에서 다면 파라미터 w 값들이 이거 이거죠.
이거 요거 요거 얘를 여기 에러값 나와 가지고 얘를 변화시키는데 변화시키는 건 계속 얘기하지만 이제 요 델타 w 이렇게 적혀 있는 게 변화율이 이거를 이제 w를 원래 값이 있는데 얼마큼 조정해야 될지를 우리가 정해야 되잖아요.
이런 거죠. 여러분 항상 자전거 타고 이럴 때도 이렇게 이렇게 이쪽으로 힘을 더 주고 싶었는데 얼마큼 덜 줘야 되나 더 줘야 되나 이런 거잖아.
그렇죠 여러분 피아노를 치고 연필 글씨를 쓰고 이런 겁니다.

참석자 1 21:44
힘을 여기다가 얼마나 더 주고 덜 주고 약간 뭔가 변화를 줘야 되는 거잖아요.
변화율을 계산 변화율을 계산해서 w에다가 더 해야 된다고 빼던지 이해되죠?
여러분 그래서 그게 변화율이 지금 이렇게 적혀 있는데 원래 이제 x에 비례해서 변화율을 계산한다는 걸 보여주고 있는 거예요.
입력 값에 의해서 사실 나오는 거라서 이건 나중에 합시다.
여러분 이거 지금 이거 지금 안 할게요. 여러분 일단 일단 이런 게 있다는 것만 알고 계세요.
여러분 어쨌든 어쨌든 이름 자체는 계속 얘기하는 훈련을 시키는 건 이 방향으로 다시 가야 되는 거예요.
그렇죠 전파를 그래서 백워드 에러 프라포게이션이라는 용어를 쓰기도 하고 이 이건 설명 아니가 지금 하려면 길어지니까 지금 넘어갈게요.
그다음에 여기서 지금 이게 스토리가 어떻게 되고 있었냐면은 이게 우리가 이제 머신러닝에서 딥러닝 넘어왔고 그쵸 제가 항상 이점이 차이점이 중요하다고 그랬지.

참석자 1 22:44
머신러닝 이랑 딥러닝이랑 차이점은 이거 신체 신경망과 프로그램 그쵸.
프로그램 중에 신체 신경막이라고 그랬는데 여기 심층 신경망이 아닌 기계 학습 중에는 퍼센트론 이런 네트워크 있다고 그랬잖아요.
신경망인데도 심층이 아니면은 딥러닝이 아니라고 그쵸 그럼 심층이라는 거 정의해야 될 거 아니야?
그쵸 심층이 도대체 무슨 짓이냐 심플하게 그냥 딥과 난딥이 있을 거 아니에요.
그러니까 난 딥을 다른 말로 쉘로우라고 할 수 있어요.
여러분 원래 뒤에 반대말로 쉘로우예요. 진짜로 그냥 써요.
여러분 쉘로우 러닝이라는 말도 써요. 그래서 딥러닝 쉘로우 러닝이라는 말도 써요.
그래서 일부러 머신러닝이면서 딥러닝이 아닌 건 쉘로우 러닝이라고 불러요.
진짜 사람들이 알겠죠 여러분 그래요. 그래서 여기 이렇게 여기서 지금 딥뉴럴 네트워크는 여기 첫 나오는 용어인데 은닉 계층이 2개 이상이면은 딥러닝이라고 부르고 있어요.
2개 이상 2개 2개라는 거는 여기 보면 은닉 계층의 정체가 뭐냐?

참석자 1 23:56
원래 여기 보면 지금 처음으로 제일 간단한 게 이게 퍼센트로 뉴럴 네트웍이라는 형태인데 세트론이 여러분 별 거 아니야 맨 처음에 이제 있었던 신경망이에요.
맨 처음에 사람들이 생각해서 사람 따라 해 봐야지 하면서 만들었던 건데 이게 인플레이어가 있잖아요.
여러분 그렇죠 인플레이어가 뭐냐? 원래 입력 값이야 이거는 머신러닝에 뭐든지 다 있는 거예요.
그러니까 이거는 리플레이어는 x 1부터 x 까지 이거는 그냥 원래 있는 거니까 이거는 사실 뉴런이랑 상관이 없는 거예요.
이거 원래 이렇게 웨이트 같은 것도 없는 거고 이해돼요.
여러분 인플레이어는 그냥 있는 거라 이게 계층으로 사실은 계층의 개수를 치기 애매한 놈이라고 이해돼요.
뉴런이라고 같이 뭔가 심플하게 하기 위해서 동그라미 표시하고 이렇게 하지만 그냥 얘는 입력 값들이에요.
계층이라고 보기 애매하다고 알겠어요. 여러분 출력 값도 사실은 계층이라고 보기에 애매한 거예요.

참석자 1 24:47
왜냐하면 그냥 결과로 나온 거니까 뉴러 진정한 뉴러는 저 히들리 액티베이션 펑션이 중요하다고 그랬잖아요.
제가 액티베이션 펑션이 있는 그거는 레이어는 인풋 레이어하고 아웃풋 레이어 안에 있는 레이어 그걸 이제 숨겨졌다고 그러는 거예요.
인풋이랑 아웃풋은 보이는데 안에서 뭐 처리하는 놈은 안 보이니까 그걸 히들 레이어라고 부르고 은닉 우리나라 말로 은닉 계층이라고 부르고 걔가 이제 기본적으로 한 개에 있는 놈이 원래 처음에 필스파트트로 뉴럴 네트워크로 나왔어요.
뭔가 인지하는 신경망 이러면서 그때 스텝 펑션 썼었어요.
맨 처음에는 마이너스 1하고 이 사이로 나와야지 이러면서 결괏값이 그 어쨌든 그래도 낫닐레트가 들어간 거지 원래 리니 리그레션하고 다르잖아요.
액티베이션 형태로 들어갔으니까 그랬었어요. 근데 어쨌든 이런 거는 쉘레어고요.
이거를 이렇게 하나가 아니라 하나 더 여기 몇 개 있어요.
히든 레이어가 3개 있죠 2개만 있어도 디노 네트웍이에요.

참석자 1 25:41
그러니까 여기 3개까지나 있는 거죠. 3개나 있죠.
한 번 통과하고 또 통과하고 또 통과한다고 비선형성을 세 번이나 다하는 거지 여기 비선형성 한 번 이해돼요.
여러분 원래 나오는 결 다 서빙 원래 원래 리뉴얼 리그렉션을 아까 액티비션 형신 통과시키면 비선형이 생긴다 그랬잖아요.
여러분 선 모양 아닌 거 약간 생기잖아 이게 하나인 거가 신경망이 신경망인데 켈로 뉴럴 네트워크이고 페세트론 뉴럴 네트워크 이거 페세트론 뉴럴 네트워크이라고 불렀고 딥뉴럴 네트웍이라 하는 건 히들레어가 최소한 2개 이상이다.
그렇다고요 2개 이상이면 상당히 복잡하다는 거지 그리고 뭔가 되게 옛날에 안 되던 게 되게 많이 되기 시작하는 거죠.
옛날에는 왜 이 짓을 안 했냐 처음에는 일단 이거 하는데도 계산이 느려가지고 느려가지고 힘들었는데 이렇게 하는 거 점점점 힘들어서 복잡하거든요.

참석자 1 26:35
이게 사실 여러분 여기서 다시 미리 또 얘기해 놓으면은 학습의 대상이 되는 값 있죠 학습 웨이트 값이 어디 존재하는 거냐 문제 학습이 여기 여기 존재하나 1번 2번 3번 4번 5번은 이제 3 4 5는 전부 다 이제 이 뉴런들을 말하는 거예요.
학습의 대상이 되는 웨이트 값은 어디 존재하고 있어요?
우리가 201 2 3 4 5 제가 적었잖아요. 대충 대충에 다 적혀 있잖아요.
어디 있어요? 학습의 대상이 되는 웨이트 값이 도대체 어디 존재하는가 여러분 따라오고 있는지 안 따라오고 있는지 보려고 그러는 거예요.
여러분 보세요. 미안해요. 어떻게 돼요? 틀려도 상관없어요.
틀리면은 잠이 달아나고 좋지 여러분 2번인가요?
2번 아는 거 아니야 미리 나한테 배운 게 아니라 아는 거에 대해서 보고 2번 맞아요.
여러분 2번에 있어요. 알겠죠? 잘했어요. 2번에 있어요.

참석자 1 27:25
2번 이번에 여러분 값들이 들어가 저 선들 하나하나에 얼마 웨이트가 들어가 딴 데 아니야 그쵸 알겠어요 됐죠 여기도 여기도 지금 학습의 데이터가 학습 데이터가 어디 있냐 여기 여기 여기 여기 웨이트들이 다 있어요.
선 하나하나가 다 값이야 선 하나하나에 얼마 곱해야 되지 이게 문제라고요 알겠어요 됐어요.
선이 개수가 몇 개야 도대체 여러분 선 개수 보여요.
여러분 선 개수 여기 보면은 모든 유러끼리 다 연결됐죠.
그러니까 이게 지금 몇 개야 하나 둘 셋 넷 다 이거 여섯 줄이었네.
여기는 몇 개야 9개가 있네 여기 몇 개 있어요? 그러면 hs야 8 곱하기 9지 뭐 곱하기 요 8 곱하기 9 72개나 있네.
아까 89 72 99 81 72 맞죠? 헷갈려 간만에 공부다 했더니 72 맞지 7 이게 나 있네.
그렇죠 여기는 99 89 81이네.

참석자 1 28:27
그렇다고 누락 개수는 72 더하기 90 72 더하기 81 더하기 81이지 그렇죠 알겠죠 여러분 그렇다고 진짜로 그런 거예요.
그게 파라미터 기수가 이렇게 많다고 여러분 벌써 이런 별로 별 볼 일 없어 보이는데 그렇죠 알겠죠 여러분 그래요.
그래가지고 이런 게 여러분 거기 라지 랭귀지 모델 이런 데 테스큐치 이런 거는 막 빌리언 몇 백 그쵸?
조개 몇 쪽에 있다고 여기 사람도 사람이 지금 더 이게 유런의 개수가 천억 개가 중요한 거냐 이 산드론의 초 이게 붙은 게 중요한 거예요.
여러분 그래서 이게 중요한 거지 이게 이게 여러분 이렇게 여러분 여기도 감이 와요.
유론의 개수가 중요한 게 아니에요. 그쵸? 이해돼요.

참석자 1 29:17
여러분 유론의 개수가 중요한 게 아니라 시냅스의 개수가 중요하다고 사람 머리는 다 똑같이 어떤 사람 머리가 똑똑똑해 보이고 어떤 사람 멍청한데 사실 이 누런 얘기도 똑같은데 저 시냅스는 공부하면 생기고 공부하면 안 생긴다고요 여러분 여러분 꼭 공부로 생각하지 말고 운동도 마찬가지야.
자전거를 타고 수영을 하고 그다음에 피아노 치고 이러면 계속 뭔가 연결이 되고 안 하면 안 되고 뭐 이런 거라고 말이죠.
알겠어요 그렇다고 그래요. 달리기도 해야지 달리기도 안 하면 여러분 나중에 잘 못 달리는데 어쨌든 그런 거 있죠 여러분 그래요.
그래서 보면은 저도 요즘에 이제 나이 들었다고 운동을 막 강제로 하고 있는데 보면 불안한 데 올라가서 이렇게 뭔가 불균형 신체적으로 불균형 뭔가 좀 평평한 게 아니라 울퉁불퉁하거나 이렇게 막 불안한 데 있잖아요.
그런 데 올라가서 균형을 잡으면 안 쓰던 유런이 연결되기 시작한대요.

참석자 1 30:11
그럼 훨씬 더 오래 산다 하게 산다고 나중에 이제 막 다 망가져 가지는 게 아니라 다 살려야 되는 거지.
사실 시넥스를 머리뿐만 아니라 모든 게 맞지 신경이라는 게 기본적으로 더 중요한 건 연결되는 거라는 거지.
이 개수 자체가 중요한 게 아니라 치매도 이게 이게 다 죽는 거라고 이것도 다 죽는 게 크게 되고 이런 거 있어 그래요.
여러분 어쨌든 여러분 재미있으라고 한 얘기고 이런 거는 그래요.
그래서 계속 갈게요. 이거는 여러분 안 넣었다가 제가 선명액이 혹시 말로만 떠들어서 좀 그래가지고 넣어놨는데 여러분 이거 제가 새로 올린 슬라이드는 또 있어요.
근데 별로 뭐 이거 인터넷 찾으면 다 나오는 거고 제가 그냥 안 받아도 문제없을 것 같아요.
여러분 다시 안 받아도 그때 제가 이거 새로 슬라이드 올려버렸어 쓱 바꿔버렸어요.
알겠죠? 이거 넣었어요.

참석자 1 30:55
그냥 어쨌든 이게 뭐냐면은 여기서 그냥 뺏겨놨는데 남의 거 그림이 내가 그리고 싶었는데 여기 그려놨길래 하우징 프라이스 프리딕션 적혀 있죠 그쵸?
하우징 프라이스 집값 집값이 집값 집값인데 이게 월세만 생각해도 되고 여러분 보증금 생각해도 되고 전세만 생각해도 되고 이거 매매라고 생각해도 되는 게 사이즈 있죠?
사이즈 사이즈가 이제 핏으로 돼 있는데 우리나라는 평이죠 그쵸?
평 평에 따라서 가격이 이렇게 형성이 돼 있는 거예요.
x 값들이 진짜 값이야 이해돼요. 여러분 이거 이거만 보면 돼요.
여러분 슬라이드 내가 안 줘서 미안하지만 그냥 슬라이드 줬어요.
지금 옛날에 안 올려도 새로 올렸어요. 다시 또 강의 중계하다 보니까 하다 보니까 좀 올렸어요.
새로 그냥 알겠죠 이게 이렇게 값이 있는데 뭔가 이 추세가 보이잖아.

참석자 1 31:45
분명히 여러분 이게 실제로 추세가 평수가 낮으면은 가격이 낮을 것이고 평수가 높으면 가격이 높다는 게 보이잖아요.
우리가 알고 있잖아. 실제 데이터들 이렇게 생겨 먹었어요.
여러분 저 x 파란색 x 표시로 실제 데이터 한 건데 이런 게 있으면 여러분 우리가 선형 함수를 만든다고 이해되죠 여러분 그래서 지금 선형 이렇게 초록색 선을 우리가 찾고 싶은 거예요.
이게 선형 획이예요. 알겠어요 여러분 왕한테 보여주려고 그리고 또 하나 보여주고 싶었던 게 결국은 저렇게 만약에 여기 여기는 여기 여기 이렇게 입력 값이 이거 하나인 거잖아요.
쌀 평당 평수 입력 값이 입력 피처가 하나인 거라고 여기는 입력 피처가 몇 개예요?
여러분 6개 여기는 8개 이해됐죠? 여러분 이런 식이에요.
이해되죠 근데 한 개만 덜렁 들어오는 거라고 한 개만 덜렁 들어오는데 한 개만 들어올 때는 한 개만 들어오고 그러면 이제 여기에 실제 원래 거에다가 이게 이게 1차 함수잖아요.

참석자 1 32:56
바이어스에다가 원래 얼마큼 곱하냐 웨이트가 여기 이거 그쵸 이거랑 이것도 여러분 웨이트로 볼 수 있다 그랬죠.
왜냐하면 이게 다 못하는 거라고 이해돼요. 이거 여러분 그냥 별거 아니고 말로 다 떨어졌던 건데 한번 또 갖고 왔어요.
제가 그냥 그랬어요. 여러분 하지 말까 별로 안 좋은데 그냥 갑시다.
이거알아 나중에 할게요. 여러분 필요할 때 그냥 넘어갈게요.
괜히 지금 슬라이드도 없는데 하는 걸로 가 내가 갈게요.
나중에 할게요. 여러분 트레이닝 할 때 선우기가 어쨌든 뭔지 보여주려고 그랬어.
선형액이라는 건 어쨌든 선 모양 만드는 거다 알겠죠 그걸로만 하고 나중에 저거 필요하면 보여줄게요.
어쨌든 이게 제가 이거 첫 번째 시간에 막 정신없이 했는데 바쁘게 이거 선형 함수 아니라고 왜냐하면 원래 선형 함수는 이렇게 가야 되는데 여기서부터는 키 크면 안 되잖아요.
그렇죠 나이가 지금 얼마야 남자 한 남자 이게 이게 이 사람들은 이키가 별로 빨리 안 되네.

참석자 1 34:01
19살부터 미묘하게 또 틀어봐 근데 여기 수질이 안 맞잖아요.
이 보면은 언제까지 거의 15살까지 선형이다가 그다음부터는 약간 이렇게 좀 만만하게 되잖아요.
그쵸. 세상에 많은 것도 이렇다고 집값도 사실은 집값 예측도 이게 얘가 한 거면 사실은 이거 약간 이렇게 되잖아요.
그쵸 집값도 사실은 데이터가 많아서 사실은 약간 선형이 아니에요.
지금 이런 거를 잘하고 싶잖아요. 그렇죠 그러니까 이게 사람들이 선호하는 상태에 따라서 약간 어떤 데는 약간 작아도 높아지고 막 이런 거 그리고 너무 커도 또 안 올라가 사실은 평수가 너무 크면 또 사람들이 수요가 없거든요.
비슷해져요. 사실 진짜로 그래요. 여기 낮아봤자 또 좋아져 사실 이거 낮은 게 왜 좋으냐 이것도 수요가 많고 거지 진짜로 그래서 사실 이거 선형 아니야 사실 선형 아닌데 우리가 억지로 선형으로 만들어 놓은 거예요.

참석자 1 34:48
사실은 알겠어요. 그래서 서형이 아닌 거를 하려고 그러면은 저렇게 액티베이션 분 통과시키고 이런 거 여러 개 쌓아야 된다고 그래서 이거에 대한 설명을 지금부터 할게요.
여러분 나린이라는 전략은 비선형성에 대해서 비선형성 이거를 실제로 이제 어떤 함수가 어떤 데이터가 이렇게 생겨 먹었어요?
파란색 점처럼 생겨 먹었다고요. 이거 선영 앞으로 회개해 봐요.
엉망 기창이지 그쵸 여기서 맞다가 여기서 왔다가 이렇잖아 이렇게 이렇게 회기해 놓으면은 여기서 이쪽 데이터에서 맞고 이렇게 얘기해 놓으면 이쪽 데이터 s 맞잖아요.
이해돼요. 여러분 내가 뭔 말하는지 실제로 그렇게 하든지 아니면은 이렇게 진짜 여기 이렇게 이렇게 쪼개가지고 하면 좋잖아요.
이렇게 쪼개서 하는 방법이 없나 까 생각했는데 이게 바로 금융 네트워크가 이걸 정말 잘하는 거였어요.

참석자 1 35:35
왜 그런지 보여주면 이거 내가 유튜브에서 그냥 여러분 내가 이거 어떻게 설명하지 내가 귀찮지 찾아봤더니 유튜브에 있더라고요.
이거 갖다 놨어요. 유튜브 너무 좋은 것 같아요. 그래 설명 잘해놨더라고요.
그래서 이게 원래 이제 뉴런이 첫 번째 레이어에서 어떻게 되냐 첫 번째 뉴런이 있으면 이제 얘가 데이터가 이렇게 생겨 먹었다고 하고 여기 여기 여기는 서프레이시키고 여기는 이거 찾았어요.
이러면서 하고 이해돼요. 여러분 이게 여러분 슬라이드 있잖아요.
그쵸? 얘는 여기 선정성을 찾은 거야 이해돼요. 이쪽 한율러는 얘는 이선형성을 찾았다고요.

참석자 1 36:11
여기는 이렇게 증가하면서 요 값이 이하에서는 이렇게 증가하는 거야 이러면서 찾았다고 그러고 학습했다고 그리고 이 중간에서 또 여기도 이제 렐루라는 게 아까 지금 뭔가 0에서는 서프레시키고 원래 여기 렐루가 이렇게 생겨 먹으니까 이해가 되는 게 여러분이 원래 렐루가 어떤 값 이하에서는 값을 0으로 만들어 버리잖아요.
그래서 이게 가능한 거예요. 이해돼요. 그대로 전파시키고 0으로 만들어버리는 그대로 전파시키고 0으로 만들고 그러고 있잖아요.
렐루가 그런 거잖아요. 여러분 렐루가 정확히 뭐였냐면은 원래 입력값이 0보다 작으면 그냥 0으로 만들고 0보다 크면은 그대로 전파시키는 건데 이게 원래 뉴런 하나 통과시키면은 기울기도 얘가 웨이트에서 정한 대로 가고 그다음에 이제 어디서부터 할지는 이제 바이어스로 결정되는 거야.
프라피가 두 개잖아요.

참석자 1 37:01
여러분 기본적으로 그쵸 선형도 그래서 이게 원래 선형이 이렇게 생겨 먹었잖아.
이렇게 이해돼요. 여러분 이것도 이렇게 생겨 먹은 이런 선형 함수 나오잖아요.
거기다 옐로 통과시켜서 이렇게 만들어버렸다고 이렇게 이렇게 이렇게 알겠어요.
여러분 됐어요. 이해돼요. 질문하세요. 이상하면 여러분이 질문하는 거 다 되게 좋아요.
알겠죠? 쉬는 시간 질문해도 좋아요. 어쨌든 그런 다음에 다음에 서도 똑같이 얘가 이제 여러분 이게 다 합쳐져 들어가거든요.
모든 게 이게 어떻게 만들어놨냐면은 무조건 연결 다 시켜놨어요.
우리는 이 전설을 못했구나. 원래 사람 유러는 연결이 되기도 하고 안 되기도 하고 막 이러는데 아예 웨이트를 안 하면 근데 여기는 기본적으로 다 연결시켜놓고 필요 없는 건 영어로 만드는 식으로 해요.
이해돼요.

참석자 1 37:47
여러분 그래서 지금 연결이 다 돼서 이것들이 이렇게 온 게 여기 다음 유런이 지금 유런이 첫 번째 두 개 유런이 있었고 그러니까 여기 인풋들이 몇 개 들어왔는지 모르겠지만 어쨌든 인풋 들어 완성 됐고 두 번째 또 2개 유런을 해놨어요.
근데 얘가 여기에는 이 두 개 유런이 다 합쳐서 또 들어가는 거라고 이 두 개 합친다고 입력 들어온 거를 얘를 합쳐 더 해 알겠죠?
그래요. 더하고 또 이제 여기 이미 액큐레이션 통해서 통과시킨 건 또 여기 이제 더한 것들이 들어가서 여기 얘가 들어가서 다 두 개 다 들어가서 거기다 또 웨이트 곱해 얘네들 가까이 그쵸 여기는 웨이트 곱하기 있지 곱해가지고 두 개 합쳐가지고 또 렐론 통과시켜서 이렇게 여기를 또 두 개 합쳤는데 여기를 또 이제 얘가 서플레이 시킨 거죠.
가운데다.

참석자 1 38:41
두 개를 합쳐가지고 이 두 개를 합쳐서 여기를 서플레이 시키고 얘는 이만큼 만들고 이렇게 또 시켜서 이런 식으로 만들고 그리고 또 만들어서 이렇게 결국은 합쳐서 이런 모양을 만들어내는 이게 이런 이런 실제로 원래 진짜 이런 데이터 였던 거지 이런 데이터를 선형을 여러 개 연결시킬 수가 있어요.
선형 선형형이랑 비선형이 여러 개 연결시켜서 정말 원래 모양 원래 진짜 데이터에 가까운 모양 가져도 된다고 그리고 이게 되게 중요한 게 엘 레이어 앤 앤 뉴런이라고 그냥 해버리면 간단하게 그냥 n의 제곱승으로 그러니까 이게 레이어의 깊이 레이어의 거듭제곱별 거듭제곱만큼 계속 뭔가 선형선을 만들어낼 수가 있으니까 선형인 부분과 비선형선인 걸 섞어가지고 잘 돼서 그래서 이게 되게 중요한 게 이 이 계층이 깊으면 깊을수록 굉장히 복잡한 데이터를 따라갈 수 있는 거예요.
그렇다고 그래서 깊이를 많이 만드는 게 굉장히 집유 트이 저거 d이 중요하다는 거지.

참석자 1 39:44
첼로는 비서장이 아닌 건 이것만 듣다 마는 거고 쉘로 이거 하나로 쫙 맞는 거고 그리고 이쪽은 이제 이런 걸 또 하고 또 하고 이런다고 그래가지고 이렇게 데이터가 많아 원하는 그 모양을 만들어낼 수 있다는 거죠.
그래요. 그래요. 그래가지고 내가 이제 이거를 보여줬나 안 보여줬죠 이것도 이것도 이거 이게 강화 학습의 예로 제가 지난번에 아타리 게임 보여줬죠.
여러분한테 그래요. 비기계 학습의 예로 자율주행기 보여줬지 잠깐 안 보여줬나 비기게 하실 거예요.
그거 재미없을 것 같아. 진짜 이게 기계 학습 아닌 건데도 되게 자율주행 잘하는 게 있어요.
그래서 무슨 사람이 은근히 잘하는 것도 있다고요 항상 예외는 있다는 거지 단정 지을 수 없다는 거고 보여주긴 해야지 그래요.
이따가 다시 가야지

참석자 1 40:47
이게 언제 거냐 2018년이잖아요. 그러면 2018년인데도 여전히 이제 딥뉴럴 네트워크이나 아니면 머신러닝 안 하고도 근데 지금 보니까 별로 되게 전에는 이렇게 사람이 지금 항상 회의를 안 잡고 법 때문에 이제 어쨌든 잘 간다고 했는데 이 정도 가는 거야.
그죠 아니 지금 보니까 레벨 다 잘 가요. 사람 차가 근데 이게 지금 머신러닝 안 하고 살이 다 짰대요.
배분 잘 안 되는 거지 그래요. 그리고 또 뭐냐면은 그냥 보여준 거고 그다음에 이거 여기 이거 이게 지금 요 슬라이드인데 다음 슬라이드 이거는 이제 딥뮬 네트워크로 뭘 할 수 있는지 보여주려고 여기는 강학습이긴 하지만 아까 신경 마성 방금 봤잖아요.
약간 감을 주려고 갖다 놓은 건데 이 슬라이드를 실제로 이게 저기 동영상 링크를 가보면은 뭐야 이도 못해 가볼게요.
그러면 이렇게 어쨌든 지금 동영상이 있는 거 보이죠.
여러분 그래서 지금 뭐 하고 있는지 느낌이 오게 먼저 잠깐만 있어 볼게요.
지금 차들이 되게 많죠.

참석자 1 42:02
차들이 많은데 지금 계속 점점 앞으로 가고 싶어 하는데 초록색이 제일 앞에 있는 거고 노란색이 그다음에 있는 거고 빨간색은 쳐진 놈들이에요.
그래서 1등이 초록색 2등이 노란색 이런 식으로 하고 있어서 초록색이 점점점 잘 가고 있는 게 보이죠.
여러분 빨리 가는 게 중요한 게 아니라 멀리 가는 게 중요한 거예요.
여기는 리워드가 이해돼요. 여러분 그래서 자꾸자꾸 점점점 뭔가 좀 요즘 얘 좀 똑똑해졌는데 그쵸 그래서 이게 지금 계속 하고 있는데 이걸 학습시키기 위해서 어떤 짓을 하고 있냐면은 이게 지금 보면 여러분 x 표시 돼 있는 거 보이죠.
x 표시 돼 있는 게 잘 안 보이지 초록색은 잘 안 보이는데 잠깐 지 선 하면 여기 선이 여기 여기 각 차에 초록색 동그라미가 있죠.
이게 센서들이에요. 센서들이 벽에션 센서 같은 거라서 앞에 거리를 측정하는 거예요.

참석자 1 42:54
가게 앞에 벽이 있으면 이제 얘가 각각이 각 차들이 다 다 쏘고 있어가지고 x표시가 각 차들이 이제 정의 센스를 써서 요 앞에 뭔가 장애물이 있나 없나를 보고 있는 거예요.
그 x 표시 아무것도 없으면 갈 수 있는 거고 x 표시 뭐가 있으면 가면 안 되는 거잖아요.
그런 식으로 각 사들이 지금 쏘고 있는 거예요. 앞으로 그래요.
그래서 지금 제 이래서 입력 값이 그러면 뭐가 되냐면은 아까 이 한 차 한 차의 모습이에요.
한 차의 뉴럴 네트워크 모습이에요. 이해돼요. 여러분 한 차에 아까 센서 5개의 값들 그러니까 거리 값들 내가 이 측정한 값들이 들어와요.
x 1부터 5까지 이해되죠 여러분 그러고 그다음에 히드앤이 2개 있고 마지막에 출력이 2개 있잖아요.
출력 2개의 정체는 뭐냐면은 이 차가 움직이는데 파라미터가 2개면 충분한데 키를 돌리는 거 있죠 여기 보면 여기 턴 턴 키를 돌리는 거 마이너스 1부터 1까지 이렇게 왼쪽으로 이렇게 뒤로는 안 가거든.

참석자 1 43:51
그래서 이게 마이너스 1부터 1까지 있고 엔진은 뭐예요?
속도 0이면 정지하고 이제 1이면 나가는 거고 그죠 너무 빨리 나가는 거죠.
셀 하는 거죠. 그래서 고기 출력이 두 개 있는 거예요.
그럼 차가 가는 거지 이해돼요. 차는 철저하게 이걸로 가는 거지.
근데 이거를 학습을 잘 시켜서 지금 이렇게 지금 초록색 빨간색 왔다 갔다 하고 있잖아요.
감소시키고 증가시키고 이러고 있는 거예요. 그래서 제일 잘 된 놈 걸로 이걸로 가다가 이거 어쨌든 가다가 이게 더 잘하네 이게 더 잘하네 이러면서 바꿔버리는 거지 또 그 개 걸로 이해되죠 여러분 이게 그래서 강화 학습하고 있는 건데 어쨌든 제가 이거 다운로드 받아서 해봤더니 재현이 안 되더라고요.
저는 시트 값도 안 알려줘가지고 어쨌든 저는 저렇게까지 멀리 못 가더라고요.
하튼 그래서 여러분한테 시켜볼까 하다가 재미없어서 말았어요.
알겠죠 약간 재미있는 건 어쨌든 이걸 잘 보여주고 있잖아요.
재밌잖아요.

참석자 1 44:47
그래서 이럴 레트워크가 어떻게 어떻게 동작하는지 좀 보여주는 측면이 있다고 그래서 갖다 놨고 여기를 요약한 그림이고요.
나중에 이렇게 잘 놨다고 이런 거 마지막에 이렇게 여러분 도면 끝에서는 끝까지 밖으로 나가버려요.
나가서 막 미친듯이 돌아가고 있어요. 어쨌든 어쨌든 잘한다 이거지 그렇죠 그래요.
그리고 3분밖에 안 남았어. 그래서 여기까지 해서 어쨌든 대충 기본적인 거는 정말 대충적으로 했고요.
대충 제가 중요한 거 아주 중요한 거 위주로 했고요.
감을 줬고 그다음에 중요한 거는 이제 우리가 딥러닝을 해가지고 만들어내는 게 여기도 얘기했지만 이런 일들을 컨트리 빌리 프레션이면 지키 바이러스 트레시케이션 이런 거 하는 거잖아요.
그쵸 그리고 또 이제 사실 생성형 AI도 있고 걔네도 사실은 프리존에서 하는 거라서 이게 가장 기본적인 건데 얘네들 있잖아요.
근데 이거에서 이제 제일 기본적인 컨트니스 밸리 프리딕션이랑 바이너리 클래시 파이어 있잖아요.

참석자 1 45:51
이 두 가지 문제에 대해서 결국 우리가 로스트 값이 중요하고 뉴스 값이 성능 평가를 해야 될 거 아니에요 이게 우리가 항상 여러분 졸업 논문 쓸 때도 제가 맨날 발표할 때 여러분 그래서 여러분이 잘했다는 걸 보여주는 게 되게 중요하다고요.
딥시크 논문 여러분 나오는 것 또 알파고 논문도 맨 첫 번째 페이지에 뭐가 있냐면은 항상 그래프가 나와 옛날에는 이랬는데 나 이만큼 잘했지.
근데 거기 매트릭이 있어요. 매트릭 여기 그래서 이게 예를 들어서 DCP 같은 경우에는 무슨 수학 문제 얼마나 잘 푸는지 우리나라는 맨날 수능 필리을 주고 그런데 어쨌든 몇 문제 맞았는지 이런 거 하고 있는 거죠.
여러분 요즘에 변호사 시험도 막 보기하고 잘 해 그러고 있어요.
어쨌든 중요한 거는 성능 지표가 항상 중요하다고 공학에서는 그래서 제가 성능 지표를 먼저 아주 기본이 기본이라고 또 갖다 놓은 거예요.
여기다가 홍기표가 2분 나가면 될 것 같아요.

참석자 1 46:44
이리 나 커트리스 밸리 피터 컨트리스 밸리디 밸류를 예측하는 곳 있잖아요.
이게 미니 리그레션이 하는 거죠. 그쵸? 거기서의 퍼포먼스 매트릭이랑 우리나라 말로 성능 지표 여러분 영어로도 알고 우리나라 말로도 알아요.
퍼포먼스 매트릭 이거 저번에 안 넣었더라고요. 여러분 제 슬라이드에 안 넣었어요.
이것도 근데 안 넣어도 이거 너무 쉬워서 제가 빼뒀는데 아닌 것 같아서 사실 넣었어요.
알겠죠? 새 슬라이드는 있는데 여러분 굳이 적어도 적기가 될 것 같아요.
알겠죠? 그다음에 여기 다음에 퍼포먼스 매트리스 풀 바이너리 프레시 타이어 요거 이거 사실 제 영역이 아니라 머신러닝에서 열심히 배우는 영역이라서 대충 하고 넘어가는데 그래도 알려고 그냥 놔둔 거예요.
알겠죠? 스트랜스 밸리 페딕션에서는 어쨌든 보통 원래 이제 데이터들이 MD가 있으면 MD가 있으면 전체 점포에 대해서 뭔가 값이 얼마나 벌어졌는지 봐야 될 거 아니에요?
그쵸?

참석자 1 47:35
그래가지고 걔의 분산을 하든 아니면 절대 값을 하던 절대값 편차 편차 전에 얘기했었어.
지난 시에 사실은 이거 말로 때우려고 그러다가 이거 안 보고 그냥 보여주지 하고 제가 넣어놓은 건데 알겠죠?
여러분 이해되죠? 이거 두 가지 보통 RMS 루트민 스퀘어 에러 또는 미인 에솔루트 에러 적어놨죠 그쵸 용어 그쵸 그거를 보통 퍼포먼스를 2개 쓰겠지 그쵸 그래 실제로 5차 함수로 이걸 써요.
그리고 그냥 알겠어요. 나중에 오차 함수 내 오차 함수가 뭐야?
포스트 펑션 에러펑션 로스트 펑션 다 이걸로 쓴다고 알겠죠 리니어 리그렉션이나 아니면 그냥 어떤 콘텐츠 밸류 프리딕션에서는 다 이렇게 쓴다고요 이해되죠?
그다음에 여기 나오는 거는 바이너리 프리시피케이션 있죠.
거기서 성민이 표현은 사실 이런 걸 쓰고 있어요. 에큐리시 프리시점 니콜 스피스피스트 에퍼스트 이런 거 쓰고 있어요.

참석자 1 48:25
근데 시간이 안 돼서 여기까지 다음 시간에 얘기해서 할 텐데 이게 로스 펑션으로 또 못 쓰는 게 문제가 있어요.
왜냐하면 이분이 안 돼가지고 그런데 퍼포먼스 매트릭은 이거야.
그래서 그거를 강조하고 싶어서 내가 진짜 작년 수업만 해도 그걸 따로따로 연결시켜서 설명을 못 했던 것 같은데 어차피 미문이 안 되니까 이걸 안 쓰는 거야.
사실 이거 쓰는 게 바람직하지. 퍼포먼스 매트 자체로 로스 프로서 쓰는 게 좋은데 얘는 미분이 안 뜨니까 못 쓰고 있는 건데 그걸 넓게 사용할 필요가 있어서 다시 얘 살게요.
여봐요. 그래요.


clovanote.naver.com