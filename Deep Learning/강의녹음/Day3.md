딥러닝 day3_1
2025.03.12 수 오전 10:04 ・ 52분 38초
심승환


참석자 1 00:00
근데 여러분 일단 교과서가 다 준비가 됐나요? 일주일이 지났으니 됐지 그쵸?
아니에요. 교과서 이거 이게 교과서예요. 여러분 이거 없는 사람은 학과 사무실에서 빌리세요.
계속 계속 여전히 진짜 하는 사람이 많아서 빌릴 수 있더라고요.
그리고 PDF로만 있을 경우에 본인이 교과서를 갖고 있지 않으면 저작권 침해에 걸려요.
여러분 조심하세요. 저는 분명히 알렸어요. 여러분

참석자 1 00:36
PDF를 이제 본인이 책을 갖고 있는 상태에서 갖고 있으면 문제가 없고 책이 없는데 PDF만 갖고 있으면 저작권 침해가 돼요.
여러분 알겠죠 그다음에 그거는 어쩔 수 없이 제가 준비한 강의 자료를 보면서 하는 거를 하지 않고 책으로 그냥 바로 들어갈게요.
여러분 갔다 갔다 해도 상관없어요. 책을 봅시다.
책에 교과서를 보면 여러분 교과서 없는 사람은 옆에 살이라도 같이 다 옮겨가 알겠죠 열심히 할 때도 많아가지고 그래요.
그래서 보면 이제 지금 제가 보고 있는 목차를 볼게요.
목차 목차 목차가 머리 말 나온 다음에 바로 나와요.
목차 목차 밖에 없었어요. 여러분 목차에 보면은 1장 딥러닝이란 무엇인가 이렇게 적혀 있죠 그렇죠 딥러닝 2장의 제목이 뭐예요?
신경망의 수학적 구성 요소 이렇게 적혀 있죠 그렇죠 신경망이 이제 영어로 뭐냐 색깔 들어가나 그래 가까운 데 있어 가까워요.

참석자 1 01:39
신경망이 영어로 뭐냐 이런 거 그래요 상관없을 것 같아요.
안 들어도 이렇게 할게요. 여러분 영어로 뭔지 다 알아야 된다고 그랬죠.
그래서 신경망이 영어로 뭐예요? 그냥 정확하게 뉴얼 네트워크 그렇죠 이렇게 나와야 돼요.
뉴얼 네트워크 근데 사실 보통 신경만 이렇게 부르는 경우는 별로 없어요.
영어로 뭐라고 불러요? 보통 아티피셜 뉴얼 네트워크나 딥 뉴럴 네트워크 이렇게 부르죠.
그래서 줄여서 al는 dl는 이렇게 불러요. 그쵸 그쵸 여러분 그래서 거기에 나오는 거기에서 필요한 수학적 구성 요소래요 그렇죠 근데 사실 이거 요 2장 내용은 딥러닝이라 살아있는 내용이 아니라 사실 그냥 원래 그냥 머신러닝 내용이에요.
쓱 지나가죠. 왜냐면은 머신러닝 있잖아요. 우리 집 방문이 우리 과목에서 할 게 더 많아서 그다음에 이 3장이 케라스와 텐스 플로 소개라고 돼 있죠.
그쵸?

참석자 1 02:28
캐러스 센서 블로 여러분이 이제 딥러닝 이제 코딩을 해야 되는데 거기 쓰기 위해서 라이브러리가 있는 게 좋죠.
그렇죠 우리가 다 생으로 짤 수 없잖아요. 있는 고차원 라이브러리를 쓰는 건데 그중에 이제 제일 유명한 게 지금은 제일 많이 쓰는 게 이제 학생들은 제일 많이 쓰는 게 파이토치라는 거예요.
왜냐하면 파이토치가 오픈 소스가 엄청 많거든요.
근데 실제로 회사에서 많이 쓰는 건 텍스 플로우고 그리고 펜스 플로우가 원래 되게 API가 어려웠었어요.
좀 할 수 있는 걸 원래 옵션을 많이 주면 API가 어려워요.
가마솥으로 뭔가 답을 만들려고 그러면 되게 힘들잖아요.

참석자 1 03:02
정렬 수술로 만들면 쉽고 근데 이제 텍스 플로우 너무 가마솥처럼 만들어 놓은 거지 할 수 있는 거 너무 많은데 너무 힘들어요.
그렇죠 그래서 조금만 잘못하면 어떻게 돼요? 여러분 가게 다 타버리고 난리도 아니죠.
그쵸 진짜 맛있나 할 수도 있지만 펜스 플로우는 웬만하면 잘 안 돌아가는 거예요.
사람들이 사람들이 다 버리기 시작해서 파이처럼 이렇게 번성을 한 거죠.
그래서 케라스가 뭐냐면은 케라스가 원래 이제 여러분이 지금 파이토치랑 테스도 플로우만 알고 있지만 그전에는 필라노니 많았어요.
딥러닝 라이브리들이 근데 걔네들을 전부 다 표준 API로 만들어버린 약간 래퍼처럼 만든 그런 API도 있었거든요.
그거 만든 사람이 이 저자예요. 케라스라는 걸 만든 사람이 이 저자예요.
이제 저자 이 사람이 주로 만들었지. 그런데 이제 테스터 플로우가 망하기 시작하니까 망해가니까 이제 아무도 안 쓰잖아요.

참석자 1 03:50
어려워서 그래서 과감하게 텐서 플로우가 이제 버전이 원래 1점대 0점대 시작하다가 1점 대 가다가 2.0으로 바뀌면서 그게 언제냐면은 2009년 9월이거든요.
되게 옛날인데 지금 2019년 9월이다. 2019년 9월 2019년 맞았어.
2020년 2019년 2019년 9월이면 임금이 얼마 안 됐죠.
여러분 그때 통합 해서기 직전에 캐러스로 API를 바꿔버렸어요.
전격 전격 바꿨죠. 그래서 이제 이제 사용하기 싫으니까 제발 돌아가자고 하는데 잘 안 돌아가고 있지 사람들이 여전히 파이터치로 워낙 오피스가 많으니까 그런 상황이에요.
알겠죠 어쨌든 이 책은 그래서 캐라스를 캐라스 위주로 되어 있어요.
알겠죠? 텐서 플로우 API인데 얼리디서플로이고 어렵던 API가 아니라 쉬운 캐라스트 API에서 사실은 거의 파이토치랑 유사해요.
그렇지만 파이토치랑 유사하지만 파이토치가 워낙 또 이제 중요하고 파이토치에서 뭔가 철학이 약간 다른 게 있거든요.
그래서 제가 파이토치도 강의를 해 줄 거예요.

참석자 1 04:53
왜냐하면 실제로 8주치 갖고 갖다 쓸 게 되게 많으니까 그래서 비교해 주고 이런 것도 슬라이드 만들어 놨어요.
제가 나중에 보여줄게요. 여러분 그래요. 어쨌든 이런 게 돼야지 다 그런 강의를 할 수 있는데 그렇죠 알겠죠 그러면 그다음에 사장이 이제 드디어 디뉴럴 네트워크 시작을 하는 거죠.
그래서 근데 또 하면서 바로 분류화 핵이라고 적혀 있잖아요.
그렇죠 분류와 획이 분류하고 획인데 이제 원래 종류 뭔가 이제 AI로 할 수 있는 가장 기본적인 일 중에 하나가 이제 분류란 획이거든요.
행위는 값을 예측하는 거고 분류는 원래 정해진 클래스 중에서 뭐냐 종류 중에서 뭐냐 이렇게 맞추는 거거든요.
이거를 이제 딥뉴럴 네트워크로 푸는 내용인데 사실 이거 원래 이제 머신러닝에서도 이걸 하고 있지.
당연히 그냥 그냥 빈말 레터가 아닌 거에서도 그거 내용을 먼저 좀 아는 상태에서 하는 게 좋은데 제가 약간 리뷰에 두고 할게요.

참석자 1 05:52
알겠죠 그다음에 이제 5장 또 들어가면 바로 재미있는 게 5장 6장은 바로 머신러닝 들어가잖아요.
또 그쵸 사실은 머신러닝이랑 딥 뉴럴 네트워크이랑 딥러닝이랑 공통 부분이 되게 많겠죠.
러 못하면 망하는 거잖아요. 지니 라이트도 못하기 때문에 다시 5장 6장에서는 사실 머신러닝 내용이 나와요.
알겠죠 그래요. 어쨌든 전 리뷰는 어쨌든 여기서 또 할 수 있겠지만 워낙 중요한 내용이라서 하고 7장에서 이제 케라스 완전 정보라고 적혀 있는데 캐라스 워낙 쉬우니까 한 장에서 끝내버리겠다 이러면서 있는 거고요.
이때 제가 이제 파이토치도 비교해 줄 거예요. 알겠죠?
그래 그다음에 이제 8장부터 좀 이제 8장이 사실 디뉴럴 네트워크는 이게 DNN 지금 딥러닝이 딥러닝이 중요한 게 이제 공동 신경망이라는 게 나왔고 그 아키텍처가 되게 다양한 다양하게 생겼어요.
이제 원래 생긴 모양이 가장 기본적인 게 있고 그다음에 이제 약간 좀 모양 변형들이 이렇게 나오기 시작하거든요.

참석자 1 06:52
그 변형들 하나하나가 응용 분야별로 특성화가 약간 달라요.
잘 되는 게 어떤 분야에서 잘 될지를 이제 여러분이 이제 이걸 배우고 나면 알아야 되는 건데 첫 번째가 이제 팔자니 컴퓨터 비전이죠.
그쵸 그다음에 10장이 시계열이죠. 그쵸 그리고 11장이 텍스트 그다음에 12장이 생성 모델 이런 식으로 나와 있잖아요.
그쵸 이런 식으로 일단 컴퓨터 비전에서는 기본적으로 어떤 딥뉴럴 네트워크 마케터가 좋은가 이걸 배우는 거예요.

참석자 1 07:21
그래서 이제 여러분도 나중에 이미 들어본 사람 많을 텐데 여기는 컨볼루션 뉴얼 네트웍이라는 거 CNN이라는 거 컨볼루션 디뉴얼 네트워크이 사실은 그래서 이제 CNN이라는 거를 배울 거고 그리고 9장에서는 이제 CNN 여러 가지 워낙 유명한 변형들 이런 걸 배울 거고 갖다 쓰는 거 그리고 여기에 사실 여기도 있지 여기도 재미있는 게 또 8장 자체에 보면은 8장에서도 재미있는 게 여기 중요한 게 뭐가 있냐면은 8장에 8점 3절이 보이면 8자 3절 제목을 보면 뭐라고 돼 있어요?
여러분 사전 훈련된 모델 활용하기로 돼 있죠. 사전 훈련 사도 훈련이 이제 영어로는 뭐냐면 프리트레인이거든요.
프리트레인 여러분 그 유명한 챗gpt의 GPT의 플랜이 뭐겠어요?

참석자 1 08:09
이제 알아야 돼요 여러분 지 에너티 그렇죠 p 프리 체 사전 훈련 사전 훈련 사전 훈련 된 모델 하라고 하는데 트리 트레이드 모델을 활용하는 GPT 트리 트레이드 그렇죠 사전 훈련된 모델 누가 많이 이제 만들어 놓은 웨이트를 우리가 얻을 수 있으면 그걸 활용해서 뭔가 일을 하겠다는 거잖아요.
이게 요즘에는 거의 맨탈 헤딩하는 거 이상한 거죠.
오픈소스 소프트웨어 안 하려고 하는 거 이상한 거예요.
그쵸 여러분이 텐서플로우 안 쓰고 내가 다 짜겠다 미친 일이거든.
그죠? 여러분 파이토치를 해서 플로우 안 짜고 내가 다 짜겠다 알아도 여러분 있는 걸 갖다 써야 돼요.
그쵸 빨리 하려고 그러면 제대로 하려면 검증도 워낙 중요하기 때문에 여러분 뭔가 내가 다 짜는 건 안 돼요.
그렇죠 같이 일해야지. 그래서 이게 지금 모델도 마찬가지 이제 딥러닝 할 때 제일 중요한 것 중에 하나가 이미 존재하는 세상에 존재하는 필 트레인드 모델을 갖다 쓰는 게 되게 중요해요.

참석자 1 09:04
그거에 대해서 계속 공부하는 게 필요하고 갖다 쓸 수 있는 거 안 갖다 쓰는 건 나쁜 짓이야.
그렇죠 왜냐면 굉장히 자원 당분이 또 트레이드 하는 거잖아요.
사실 여기 그래서 8.2절 내용은 밑바닥부터 프롬 스크래치라고 돼 있어요.
영어로는 밑바닥부터가 프롬 스크래치라고 그래요.
바닥부터 하는 거 프로 스크레처에는 스크래치를 긁어 모으는 거잖아요.
그렇죠 이게 바로 8.2 바로 그냥 데이터 바로 올려놓는 거고 8.3절은 원래 훈련된 그가 모델이 프리트레이드 모델을 가지고 다시 활용해서 어떻게 할 것이냐 이런 내용이에요.
그쵸 사실 여기 보통 전문 용어가 안 나오는데 이 기터에도 나오고 하는데 이거 이거 미리 제가 여러분 여러 번 들으면 좋으니까 다른 데서 이게 이게 이름이 뭔지 알아요 여러분 이런 거 전문 용어로 보면은 우리가 지금 러닝 딥러닝 머신 러닝 계속 러닝 러닝 하고 있잖아요.

참석자 1 09:53
그리고 우리나라 말로는 기계 학습 그다음에 심층 학습 이런 식으로 학습 이렇게 듣잖아요.
그렇죠 이것도 뭔가 무슨 학습이라고 부르는 거 있는데 아세요?
여러분 무슨 모양이라고 부르는데 이제 여러분 이제 여러분이 예측하다 보면 이제 나중에 이상하게 용어가 잘못 구어질 수 있으니까 바로 답을 알려줄게요.
우리나라 말로 전이 학습이라고 하고 영어로 트랜스퍼 러닝이라고 해요.
이거 되게 중요한 용어 외워야 되는 거야. 트랜스퍼 러닝 해야겠다.
무조건 트랜스퍼 러닝 해야 돼요. 트랜스퍼 러닝이나 전이 학습 우리나라가 그러잖아요.
그런 걸 해야지 그냥 맨땅에 헤딩하는 거는 안 돼요.

참석자 1 10:28
요즘에는 알겠죠 오픈소스 쓰는 거랑 똑같은 거 오픈 소스 중에 오픈 웨이트가 있는 거지 뭔지 알겠어요 여러분 여러분 미키 세븐틴이라는 그런 영화가 망했던데 소설은 되게 재미있었는데 영화를 했더 영화를 너무 다르게 만드셨더라고 그래요.
나는 할 말이 많지만 여러분 영화 안 본 사람도 많아서 이렇게 그래서 거기 보면 그게 중요한 게 복제 인간을 만들 수 있는데 원래 머리를 다 이렇게 뭔가 딱 메모리 넣어놨다가 싹 다시 넣을 수 있거든요.
디스크 이만한 거 보여주거든요. 영화에서도 돌더리 같은 거 보여주는데 그런 게 웨이트예요.
여러분 머리를 거기 그 머리를 다시 안 씌우면은 옛날 기억을 못해 새로운 경험을 기억 못해 그 머리가 웨이트 같은 거라고 여러분 저기 트랜스포러닝은 그 머리를 가지고 와서 다시 새로운 걸 해야지 빈트 껍데기 소용없잖아요.
여러분 귀 껍데기로 선부터 학습시키려고 그러면 몇 년이 걸릴 거 아니에요?
그쵸? 알겠어 여러분 전 학생이 뭔지 알겠어요?

참석자 1 11:23
여러분 알겠지 그쵸 그리고 그 미키 세븐틴도 그렇게 왜 열심히 그렇게 맨날 백업시켜놓냐면은 이 경험을 쌓아놓은 거를 알아야 될 거 아니에요?
그쵸? 그래야지 새로운 미션도 하고 이러니까 어쨌든 사전 랜드 모델 활용하기 이게 트랜스포 러닝이다 전 학습이다 이런 거를 미리 알 거예요.

참석자 2 11:42
그 프리 트레이닝이랑 파인투닝도 따로 있는 걸로 알고

참석자 1 11:45
그래요. 그거 좋은 질문인데 두 개 차이 이겨버릴게요.
지금 나왔으니까 지금 이 친구는 파인 튜닝이라는 말을 들어본 거예요.
그쵸? 그럼 파인 튜닝이랑 프리 트렌드랑 무슨 상관이냐 전의 학습이랑 무슨 상관이냐 그렇죠 그런 건데 전이 학습을 할 때 사실 여러분이 지금 미리 들어놔도 별로 상관없을 것 같은데 뉴럴 네트워크를 쓰잖아요.
그럼 항상 항상 항상 어떻게 나냐면 이게 여러분이 미리 얘기해도 상관없으려나 항상 두 분으로 나눠져요.
보통 리럴 네트워크 자체가 맨 처음 입력받아가지고 약간 좀 추상화하는 부분도 있고 그리고 나중에 이제 실제로 출력을 내놓는 부분이 있어요.

참석자 1 12:26
출력을 내놓게 약간 뭔가 이 계층이 깊은 건데 이 일이 안 보여서 좀 미안하긴 한데 그래도 들은 가다가 있으면 나중에 더 잘 알아들으니까 그래가지고 어쨌든 전이 학습이 크게 두 가지가 있어요.
트랜스포 러닝에 하나는 그냥 원래 있는 모델을 그대로 제로 샷 러닝이라고 그래서 한 번 거의 이제 트레이닝 안 시키고 바로 원래 모델 그대로 갖다 쓰는 거예요.
원래 모델을 하나도 건드리지 않고 원래 머리를 건드리지 않고 그 사람이 뭐라고 얘기했으면은 그거 다음에 약간 이제 그걸 가지고 해석해가지고 이제 분류하는 거지 예를 들어서 전의 학습을 구체적인 예를 들면은 여러분 뭘로 할 수 있냐면은 원래 이미지 넷이라고 이제 10천 가지 모델이 천 가지 이미지를 구분할 수 있는 게 있어요.
예를 들어서 천 가지 이미지를 구분할 수 있는데 천 가지밖에 안 되니까 낙타는 구분 못하는 거지 이해돼요.

참석자 1 13:19
여러분 낙타랑 말 품종 같은 거 구분 못한다고 근데 우리가 말의 품종을 구분하고 이러려고 그러면은 약간 더 학습을 시켜야 될 거 아니에요 그렇죠 근데 전이 학습할 때 아까 제일 두 가지 방법 중에 한 가지는 원래 이제 원래 이미지 넷이 일단 구분을 해 그냥 이미지 그대로 놔두고 구분한 다음에 그다음에 얘가 예측한 거 보고 얘는 뭐에 가까울 것인지 아까 말 품종 중에서 말 품종이나 낙타인지 이런 게 구분하는 거를 하는 것만 이제 새로운 웨이트만 새로 이제 학습시켜가지고 원래 원래 웨이트는 하나도 건드리지 않고 그렇게 학습시키는 방법이 있고 그다음 나머지 하나는 원래 이제 학습시켰던 이미지 넷 있잖아요.
이미지 넷 같은 원래 뉴럴 네트워크를 약간 거기도 웨이트를 바꿔버리는 거지 그게 파일 튜닝이에요.

참석자 1 14:09
그러니까 원래 있는 원래 있는 머리 있죠 원래 있는 뉴럴 네트워크 이 메이터 받아온 거를 약 다 학습을 시켜버리면은 파일 튜닝이고 그걸 그대로 두지 않고 뒷부분만 이렇게 바꿔가지고 학습시키는 게 다른 또 그냥 제로s 러닝이고 제로샷이라 이라고 안 하고 그걸 뭐라고 그러냐 지금 내가 이름을 까먹었는데 그것도 어쨌든 그 방법도 있어요.
이름이 없어 없어 사실은 나도 정해야 될 것 같아요.
두 가지가 있어요. 파이트윙 하는 것과 파이트윙 안 하는 것과 그래요.
보통 전이 학습이 두 가지가 있어요. 파이트 하는 거 파이트윙 안 하는 거 파인 튜닝을 하는 거는 어쨌든 원래 모델 원래 웨이트 건드리는 거 파인 튜닝 안 하는 거는 원래 모델 안 건드리고 그냥 뒤에 부분만 그냥 건드리 붙여가지고 하는 거 이렇게.
근데 두 가지 다 유용하거든요. 아무래도 파일 튜닝이 더 좋겠지.
그쵸 그래서 사실 여러분은 파이 튜닝을 잘해야 되고 이 책에도 파일 튜닝이 나와요.

참석자 1 15:02
파이 튜닝이 우리나라 말로 뭐냐 미세 조정 미세 조정이죠.
미세 조정 미세 조정 학습 그쵸 그게 다 8.3절이 나오는데 우리 교과서에서는 알겠죠.
근데 이게 제가 재미있는 게 이 교과서 말고 이게 지금 1판이 1판이 한 2018년쯤 나왔다가 우리 이게 펜스 플로우가 너무 바뀌었잖아요.
아까 테라스로 2019년 9월에 근데 이게 지금 2021년 2년인가 그때쯤 나왔는데 이 판으로 완전히 75% 바꿨다고 재밌는 게 이 책에 맨 뒤에 보면은 아닌가 안 적혀 있네.
여기 있어 안 적혀 있구나 여기 있다 여기 있다 다 있다.
여기 밑에 뒷장에 여기 보면 여기 있잖아요. 일판 대비 75% 변경이라고 적혀 있죠.
이런 책이 다 있나 있죠 여러분 일판 대비 75%를 변경하는 책이 어디 있어요?
그쵸? 대단하죠. 여러분 그래요. 그리고 이 책 뒤에 또 뭐가 나왔냐면은 이 책 같은 출판사인데 여기는 지금 딥러닝 위드 파이썬이잖아요.

참석자 1 16:10
딥러닝 그도 자바스크립트라는 책이 나왔거든요.
그것도 구글 딥마인드 사람들이 만든 거예요. 이분 있잖아요.
이 저자 저자가 누구냐 프랑스와 숄레 적혀 있죠. 이분이 어느 나라 사람 같아요 여러분 프랑스 사람이에요.
프랑스 와 왔는데 프랑스 가는데 진짜로 이 사람이 지금 딥마인드에 있어요 딥마이드가 뭔지 모르세요?
여러분 혹시 구글에 구글 인수 합병한 사실 구글이에요.
그러니까 원래 알파고 하다가 구글한테 인수 합병되고 계속 구글에 있지 그냥 지금도 특히 구글 사람들이 구글 딥마인드 팀이라고 부르죠.
그래서 이제 회사가 아니라 팀이 됐으니까. 근데 그 사람이고 여기 구글 딥마인드 팀에서 캐라시도로 파이썬으로 하는 사람들도 있고 자바스크립트를 하는 사람들도 있는 거예요.
자바스크립트를 하는 사람들이 나중에 책을 또 썼거든요.
그 책이 제가 지금 저번에 아직 못 보여드렸는데 기톱으로 제가 공개한 것 중에 자바스크립트 있잖아요.
그쵸?

참석자 1 17:00
그렇죠 거기에 책이 나왔는데 그 책은 근데 이 책에 있고 일부 뺏기고 막 약간 그랬더라고요.
막 겹치는 게 있어요. 근데 전의 학습이 3장에 나와 되게 빨리 빨리 엄청나게 중요하다는 얘기지 한마디로 이 책은 일판 등기 변경을 어마어마한 75%가 했는데도 내용만 바꿨지 그렇게 순서는 안 바꿨거든요.
근데 요즘 나오는 책이 전이 학습이 되게 앞부분에 나온다고 뭔 얘기인지 알겠어요 여러분 전의 학습이 되게 중요하다 그러니까 진도를 앞에 빨리 나가야 돼.
그쵸 그래야 중요한 거 할 거 아니야 그쵸 근데 제가 자바스크립트 책으로 참여할 수는 없죠.
거기 또 뒤에 내용이 또 너무 이상해. 그냥 자바 스크립터로는 자바스크립트는 별로 어쨌든 주류가 아니잖아요.
파이썬이 주류잖아요. 그러니까 어쩔 수 없이 자연어 처리 이쪽은 거의 저자가 되게 많아요.
저자가 많은 책은 약간 의심이 되지 어떤 부분 되게 좋고 어떤 부분 되게 나빠요.
거기 앞부분은 되게 좋은데 뒷부분은 되게 이상해.

참석자 1 17:52
진짜로 그래서 교육 가설은 제가 그냥 참고만 앞부분은 너무 좋아요.
앞부분은 전야스 나오고 이런 부분은 너무 좋아요.
이 책보다 더 좋아요. 거기는 완전히 힘을 엄청 잔뜩 줬죠.
그러잖아요. 그래서 앞부분 보고 이렇게 좋은 책에다가 뒷부분 보고 이렇지 그러니까 그런 거죠.
저장 하도 많으니까 그런 거죠. 어쨌든 그런 상황이고 지금 8.3절이 전약기고 거기 파이프닝도 나오고 여기 보면은 여기 그러네.
그냥 피처 익스트렉션이라고 부르는구나 미안해요.
그 용어가 트랜스폼 러닝에 크게 두 가지가 있는데 여기 다 적혀 있네.
83.1절 83.2절 적혀 있잖아요. 보이죠 여러분 보여요.
여러분 지금 미리 알아둬요. 여러분 지금 저도 확실히 정리하니까 여러분 시험에 낼게요.
나중에 8.3절에 8.31절은 마지막에 물로 끝나요.
여러분 특성 추출로 끝나죠 그쵸? 8.32절은 미세 조정하게 끝나죠.

참석자 1 18:47
여러분 이해됐어요 봤죠? 지금 방금 봤죠 미세 전정막이가 영어로 뭐라고요?
파일 2니 그다음에 특수 추출이 피처 익스트랙션 피처 알죠?
여러분 피처 FEA 그쵸 그쵸 익스트랙션 특성 추출이잖아요.
그쵸 전이 학습이 크게 두 가지가 있는 거예요. 특성 추출 또는 파일 뷰닉 특수 추출은 그냥 거기서 걔를 건드리지 않는다고 웨이트를 다시 웨이트 원래 있는 오픈 오픈 웨이트 받은 거 있잖아요.
걔를 건드리지 않고 그 뒷부분만 내가 학습시켜가지고 이해돼요.
여러분 실제 출력 부분 쪽만 약간 감은 오죠 여러분 그렇죠 되게 조금만 학습시키는 거고 미세 조정은 원래 가져온 그 웨이트를 건드리는 거야.
알겠어요? 여러분 무슨 그다음에 프리트레인을 타고가 아니라 어서 갖고 오는 거 있어 프린트레인은 어디서 돼 있는 걸 내가 갖고 온다고 생각하면 돼요.

참석자 1 19:43
사실은 이제 로드를 하는 거지 어디 세이브 된 거를 누가 세이브 해간 거를 로드를 해놓은 거지 내가 해놓은 걸 수도 있지만 보통은 이제 이런 거는 이미 유명한 프라블럼들에 대해서 풀어놓은 것들이 많이 있어요.
무슨 경진대회 이런 게 있어요? 딥러닝 경진대회 이런 것들 유명한 문제들이 아주 일반적인 문제들이지 그거를 가지고 와서 내가 하는 거라고요.
그리고 이게 지금 제가 처음에 맨 처음에 시작할 때 허깅 베이스라는 거 제가 얘기했었잖아요.
허깅 베이스 허기 페이스가 트랜스포머 라이브러리를 잔뜩 모아놓고 온갖 거 하는 거라 그랬잖아요.
근데 그 허기 페이스의 트랜스포머는 전부 다 거의 언어 모델들이거든요.
비전 쪽도 있지만 거기에 온갖 모델들이 있는데 걔네들을 파악을 하고 갖다 쓰는 게 굉장히 중요하겠죠.
그러니까 그거는 이 수업에서 다 할 수는 없는데 여러분이 알아서 하시라는 거지 내가 자꾸 소개해 주겠다고 알겠죠.

참석자 1 20:31
나도 모르고 계속 올라올 텐데 뭐 거기 그렇죠 거기 그래서 허기 케이스를 쓴다.
이 얘기는 또 허기스 파이스를 쓴다는 거는 이제 여러 가지 측면이 있는데 그중에서 이제 오픈 소스 트랜스포머 라이브러리를 가져오겠다 이걸 하면 그다음에 중요한 게 오픈소스 모델 오픈 웨이트 모델을 가지고 온다는 거지.
웨이트 그래서 거기에 있는 온갖 라이브러리들 중에서 라이브러리 온갖 모델 중에서 머리가 있는 웨이트가 있는 모델 중에서 적당한 걸 가지고 와서 내가 피처 익스트렉션만 하든지 파인 튜닝만 하든지 이런 식으로 해가지고 여러분이 되게 대단한 일을 한 것처럼 보일 수 있는 거예요.
졸업 논문에도 그것만 해도 그냥 내 아버지 사실은 움직이신 거야.
사실은 회사에서도 그것마저도 대단하네 이렇게 된다고 하세요.
왜냐하면 회사에서도 사실은 별 사람이 다 있으니까 사실 그렇죠 되게 욱한 사람들이 태반이지 미안해요.
여러분 여러분이 생각보다 똑똑해요.

참석자 1 21:29
그냥 학생이구나 이래요. 나는 지금 기대하는 사람이 있어.
직원분이 오시나 그래요. 왜

참석자 3 21:37
용어로 전이라

참석자 1 21:38
트랜스퍼 트랜스퍼가 전달받아서 쓰는 거니까 저희가 여러분 전달해 주는 거잖아.
지식을 전달받아서 하는 느낌이 들지 않아요. 그 좋은 이유 한번 생각해 보면 굉장히 좋아요.
왜 전이냐고 그쵸 뭔가 전달받아서 하는 거잖아요.
누가 많이 잘해놓은 거를 가지고 내가 활용하는 거잖아요.
지금 제가 전이하고 있죠 여러분 그쵸 제가 전에 밖에 사무실

참석자 1 22:11
예 네 오고 있는 중인 거예요. 지금 그렇죠 습니다.
그걸 어떻게 알았지 그래요 어떻게 거기서 보이나 보지 아니요.
아까 그래 고마워요. 어쨌든 오고 계시대요.

참석자 2 22:32
그러면 그 뭐지 파인트닝이랑 피처 익스트레션 쓰는 경우를 어떻게 나누나요?

참석자 1 22:38
일단은 일단 시간 없으면 피처 익스트리션 먼저 해보고 그냥 제일 심플하게 그리고 좀 여유가 되면 파인투이 가는 거지 여유가 되면 트레셋이 보통 잘 되면 그냥 끝이잖아.
사실은 그게 좋잖아요. 파이프닝 어쨌든 학습을 하는 거 트레이닝 시키는 거는 지금 사실 좀 예를 들어서 피처 섹션이 얼마나 좋냐면은 지금 아까 허깅 페이스 인 페이스 허깅 페이스 모델들 갖고 왔잖아요.
그거에다가 그냥 내가 붙여가지고 하는 거니까 내가 조금만 뉴럴 네트워크 붙여가지고 웨이트 몇 개 없는 걸로 학습시켜도 되는 거잖아요.
이해돼요. 여러분 내가 뉴럴 네트워크 여러분 진짜 머신러닝 정도로 굉장히 사실 이 머신러닝에서는 거의 곱하기 더하기 끝이잖아요.
머신러닝 곱하기 더하기 끝이야. 근데 이쪽에서 이제 우리 뉴럴 네트워크 액티베이션 통해 통과하는 것 정도 뭔지 모르지만 아직 배울 거예요.
이제 궁금해하고 있어 봐요.

참석자 1 23:30
그런데 허기 페이스에서 이렇게 쪼아 붙여가지고 학습시키는 게 굉장히 쉬운 일인데 5차 변파를 거기까지 5차 나중에 이제 웨이트 파라미터 조정한다 그랬잖아요.
걔를 내가 만들어 놓고 조그만 뉴라토프 하면 되잖아요.
근데 파일 터닝 하기 시작하잖아요. 파인 튜닝이라는 거는 그 빌리언 이런 거를 내가 지금 학습시켜야 되는 거잖아요.
근데 거기서 어디까지 시킬지 내가 정해야 되잖아요.
그러니까 일단 그걸 일단은 이제 백프라프로케이션을 하면서 백프로케이션 미세 조정하려고 그러면 파라미터 일단 로딩을 했다가 오차 값을 전파를 해야 되거든요.
우리가 어느 정도 터졌는지를 다 웨이트에다가 곱하고 더하고 해야 되잖아요.
빼기 해야 된다고 이제는 근데 그러려고 그러면 계산량이 어마어마해지잖아요.
보통 컴퓨터는 잘 안 되기 시작하지 근데 피처 스트리션은 잘 되지 이해돼요.

참석자 1 24:19
그러니까 여러분이 이 노트북이 40 70이면은 파인 튜닝은 너무너무 잘 되는데 알파인 튜닝이 아니라 진짜 익셉션은 4090이 아니면 이제 파이 튜닝이 안 되는 거야.
파이 튜닝이 좀 컴퓨터가 되게 좋아야 된다고 메모리가 어마어마하게 필요한 거죠.
이해돼요. 그래가지고 그것 때문에 그렇게 너무너무 이제 원래 웨이트를 건드리는 게 너무 약간 파이틀링이 쉽지 않은 일이라고 솔직히 이제 엄청나게 웨이트가 큰 것들은 여러분 GPT 같은 거는 여러분 우리 학교 아무리 갖다 줘도 소스 갖다 줘도 웨이트 갖다 줘도 돌리지도 못해 그쵸 어마어마하게 매몰이 많이 들어가니까 그거는 이제 심지어 이제 돌리지도 못하는 거고 파인트닝 하려고 그러면은 더 필요하다고 메모리가 더 사실 이제 그냥 전에 제가 얘기했잖아요.
그냥 추구하는 게 있고 학습하는 게 있다고 그랬잖아요.
추론하는 건 이 방향인데 그렇죠 학습하는 건 이 방향이잖아요.
여러분 거꾸로 그 알아서 보세요.

참석자 1 25:16
알겠죠 그쵸 근데 그거 하기 위해서는 계산량이 더 필요하니까 그쵸 근데 파이 튜닝은 사실 그래서 그거를 나 상당량을 메모리에 올리고 해야 되기 때문에 안 될 수가 있다는 컴퓨터가 나쁘면 돈이 많이 드는 일이라고 알겠죠 그래요.
그리고 미리 한 마디 더 하면 사실 지금 유행하는 온디바이스 AI 쪽에서는 아니면 또 그쪽도 트랜스포머 쪽은 워낙에 웨이트가 양이 많으니까 그냥 파이 튜닝도 그냥 그렇게 원래 웨이트를 건드리는 식으로 안 하고 로라라고 그래서 로우 레이크 어레이라고 그래가지고 들어봤어요.
여러분 안 들어갔지 아무도 그래요 한지 2년 됐는데 그것도 그러니까 옆에다가 별도의 매트리스를 만들어 가지고 별도의 RA 만들어가지고 나중에 해줄게 해가 아니고 너무 지금은 거의 로라로 변하고 있어.
로라를 거의 다 쓰고 있으니까 로라라는 게 원래 파이트닝을 하더라도 원래 베이트를 건드리지 않고 내가 아까 전에 지금 보통 피스 섹션은 뒷부분에다가 왜 이럴레트 붙여서 하는 거 있잖아요.

참석자 1 26:19
학습을 근데 이제 학습을 어쨌든 웨이트 전파를 다 뭔가 중간중간 많은 수많은 웨이트들을 다 메모리 로딩하는 거 너무 힘드니까 옆에다가 별도의 어레이 만들어 가지고 걔만 학습시켜서 뭔가 협조 짬뽕한 결과가 학습된 결과가 나오게 됐어요.
혹시 리모컨 리모컨 문제인 것 같아 잠깐 오시면 안 돼요.
전화로만 얘기하시는 거예요. 고마워요. 패스트 리모컨 리모컨으로 그럼 어떻게 제가 여기는 왜 안 나와요?
근데 큰절이 선생님

참석자 3 26:53
지금 네 리모컨 가지고 컴퓨터 화면도 안 나오거든요.
지금 네 이거 사진 하니까 이거 화면 아예 자체가 안 틀어져 있는데 안 그 본체는 본체는 틀어야 이 좀 해봐야겠다.
진짜 네 이거 화면 30분 나왔는데

참석자 3 27:19
컴퓨터를 껐다 샀다.

참석자 1 27:20
잠깐만요. 자금액 안 껐죠 안 되잖아

참석자 1 27:31
그럼 절대로 안 오시네. 컴퓨터 모를 그래서 어쨌든 수업을 알차게 보내야 되니까.
그래서 여러분 질문 많이 하니까 좋네. 그래서 뭐 내가 뭐 얘기했지 로라라는 거 있다고 근데 지금 이렇게 얘기하면 참 오테니까 나중에 슬라이드 만들어서 보여줄게요.
여러분 네네. 네가 슬라이드 만들어 보여줄게요.
그거는 거의 지금 그게 그게 대세거든 놀라쓰는 근데 이제 교과서가 못 따라오지 이쪽은 워낙 빨리 발전하니까 어쩔 수 없죠.
그렇죠. 그래서 새로 나오는 교과서들을 보면 새로 나오는 책을 보면서 거기서 어떻게 발전하고 있는지를 볼 필요가 있죠.
제가 그래요. 그래서 여러분은 어쨌든 알겠죠. 무슨 얘기인지 지금은 전혀 학습이 되게 중요 중요해 현장 산들이 엄청나게 중요하다는 거야.
알겠죠 그리고 질문 잘해야 되는데 피처 익스트레스 나는 확실하게 이제 그 용어가 빨리 스트레스라고 빨리 진행해.
알겠죠 좀 늦어요. 그리고 계속 하면은 제가 다시 보여드릴까요?
오시면 안 됩니다.

참석자 1 28:26
그다음에 9장 9장은 지금 로딩 중에 있어요. 그다음에 하나는 네 지금은 떠요 떴어 어떻게 떴어요?
네 그리고 스는 그다음에 h 10장이 12월이고 11장이 텍스트로 돼 있죠 그쵸 텍스트 11장 11장이 텍스트인데 텍스트 하다가 11.4절에서 여기 11.4절에 통수가 없어가지고 7.4절에서 뭐가 나와요?
트랜스포머라고 나오죠. 여러분 그쵸 트랜스포머 지금 이게 허기 페이스가 트랜스포머 라이브라고 했잖아요.
그쵸 트랜스포머 그러면 그걸 배우는 거예요. 중요 재미있는 게 이 책은 트랜스포머를 짰어요.
진짜 코드를 다 줘요. 그러니까 트랜스포머가 어마어마하게 큰 건데 이 사람은 정말 훌륭한 인사이트로 정말 이 사람은 다 짰어.
트랜스포머를 그냥 우리가 소스를 보고 다 이해할 수 있게 해놨어요.
그래서 진짜 인사이트를 줘요. 봅시다. 여러분 그리고 나올 여기 트랜스포머 아키텍처 안에 보면은 17.4.1에 447쪽이라고 적혀 있는데 어쨌든 셀프 어텐션이라고 적혀 있잖아.

참석자 1 29:36
어텐션 어텐션 어텐션에 대해서 이해하는 게 되게 중요해요.
여러분 트랜스포머를 안 쓰더라도 어텐션 자체를 이해하는 게 더 필요하다고 그게 이제 많은 인사이트를 주기 때문에 알겠죠 봅시다.
나중에 그다음에 12장이 생성 모델이죠. 생성 모델 있죠?
생성 생성 모델이 이제 제너러티브 i잖아요. 그렇죠 아까 GPT의 첫 번째 키워드 제너러티브 그쵸 사실 생성 모델은 생성을 하는 것뿐이지 사실은 원래 다른 학습에서 뭔가 변형된 거라고 볼 수 있어요.
여러분 그러니까 이거는 이것도 이제 생성형 AI를 위한 아주 특별한 뭐가 있다고 생각할 수도 있지만 사실 특별한 뭐가 있긴 있는 것도 있지만 앞에 걸 다 기반으로 한 거라서 이걸로 바로 하는 건 바람직하지는 않아요.
알겠죠 이게 뒷부분에 있는 게 다 이유가 있는 거죠.
그쵸 그래 어쨌든 생활 AI는 최대한 여러분 많이 활용해야 되고 그런데 이제 원래 앞에 내용을 다 차곡차곡 쌓아가면서 머신러닝을 모르면 안 되는 것처럼 그렇게 공부를 합시다.

참석자 1 30:41
여러분 그래서 이제 어쩔 수 없이 교과서로 그냥 바로 들어가서 1장부터 볼까요?
1장 교과서 종류가 많으니까 1장 1장에 가면은 28쪽 그래 28쪽 28쪽 28쪽 보세요.
여러분 28쪽 28쪽에 그림이 바로 탁 나오죠 그쵸 맨 아예 그쵸 28쪽에 뭐라고 나와요?
여러분 인공지능 머신러닝 딥러닝 단계를 보여주고 있어요.
제 그림에도 있어요. 똑같이 저는 이거를 이런 교과서에 아무것도 안 나올 때 맨 처음에 그렸는데 그거를 어쨌든 알겠죠.
근데 이게 되게 중요한 그림이야 그쵸 머릿속에 넣어놓으세요 여러분 이걸 뭘 보여줘요?
모든 딥러닝은 머신 러닝이야 그쵸 모든 머신러닝은 인공지능이에요.
그쵸 여러분 그쵸 근데 아닌 것들이 존재한다는 게 중요하죠.
구분하는 게 그렇죠 그러면은 딥러닝은 가치 있는 거고 딥러닝이 아닌 머신러닝이나 AI는 다 가치가 없는 거냐 아니죠 다 기반으로 하는 거예요.

참석자 1 31:41
그 기능들이 원래 내용들 그리고 가벼운 거는 머신러닝으로 그냥 끝내도 되는 게 바람직하지 딥러닝 아닌 걸로 알겠죠 그래요.
그래서 그리고 원래 원리는 자체는 그대로 가는 거기 때문에 그리고 이거는 뉴럴 네트워크에 특화된 거고 이건 뉴럴 네트워크가 아닌 거야는 것도 구분할 수 있어야 되고 그래요.
계속 갑시다. 그래서 인공지능이 뭐냐 이렇게 얘기하는 건 제가 열심히 저번에도 얘기했으니까 넘어가고 31쪽 가볼래요.
31쪽 30조 31쪽 여기 이제 머신러닝 먼저 설명하죠.
당연히 그렇죠 머신러닝 그래서 여기 31쪽 맨 위에 그림 보면은 그림 1 2에 전통적인 거랑 머신러닝 구분해 놨어요.
그쵸 그 전통적인 거랑 머신러닝 근데 여기 지금 머신 러닝에서는 해답을 주는 걸로 적어놨죠.
그쵸 여러분 그렇잖아요. 근데 이거는 제가 제가 이 그림이 동의하지 않아요 난 이거 아니야 이거 이 사람이 틀렸다고 생각해요 여러분 테레을 주는 건 뭐예요?

참석자 1 32:33
여러분 원래 제가 지난 시간에 대충 했는데 원래 머신러닝을 원래 머신러닝을 학습을 크게 세 가지로 사람들이 구분하고 있잖아요.
지도 비지도 일단 구분하는 게 맞고 그쵸 슈퍼바이즈든 언슈퍼 바이즈든 그리고 그거 말고 이제 이인포스먼트 러닝 이렇게 한다고 그랬잖아요.
사실은 슈퍼바이즈 슈퍼바이즈로 끝나지만 이름 용어 자체가 그쵸 어쨌든 그렇게 할 수 있잖아요.
그렇죠 그래서 지금 슈퍼바이지드 러닝에서는 레이블이 필요하고 정답이 필요한데 다른 데는 그렇지 않아요.
지금 이 그림은 뭘 보여주고 있어요? 그래서 지금 이 머신러닝이라기보다는 슈퍼바이즈 듯 머신러닝을 보여주고 있는 거지 이해돼요.
여러분 그냥 머신러닝이 다 이렇지 않다고요. 그 이름이 틀렸다고 리젝트 만약에 전화로 논문이 왔으면 내가 리뷰 했으면 이렇게 하면 안 되지 그쵸 이해되죠 그래서 많은 정보들이 틀려요.
여러분 그렇기 때문에 여러분 채집 생성형 AI를 믿을 수가 없지 이해돼요.

참석자 1 33:30
여러분 머신러닝 이거 틀린 그림이야 진짜로 슈퍼 바이젠드 머신러닝은 이렇다고요?
알겠죠? 그래요. 어쨌든 dh도 학습 같은 거는 정답 값 안 주잖아요.
해답 값도 안 주고 이 강아지도 그래요. 여러분 사장 값이라는 거 없어요.
보상 감수만 주지 어떻게 할지 안 알려주고 니가 잘해 봐 정수가 느끼면 돼 이러는 거잖아요.
그러는 거죠. 여러분 그래요. 그래서 어쨌든 이 교과서를 다 맹시하지 마세요.
여러분 그리고 모든 교수님들 수업 마찬가지지만 저도 저만의 뭔가 나름의 시어리가 있잖아요.
여러분 그래서 그래서 이제 교과서는 내가 여러분 공부 잘하라고 하는 것뿐이지 교과서에서 틀린 거를 제가 이렇게 얘기해 뒀는데 시험에서 교과서에 이렇게 나왔는데요.
하면은 미리 얘기해야 돼. 그래서 나랑 디베이트 해가지고 오케이 했으면 상관없지만 내가 분명히 아니라고 그랬는데 그거를 교과서에 나왔다고 그렇게 시험 봤다고 봐달라고 하는 말도 안 되는 소리.

참석자 1 34:19
그렇죠 원래 이 사상사가 그런 거지 그렇죠 우리 앞에 그럼 왜 배우는데 그렇죠 어쨌든 알겠죠.
여러분 그래요. 그래서 아직도 안 되는군요. 그래요.
그다음에 계속하면은 32쪽으로 넘어가면은 데이터에서 표현 학습하기 이렇게 돼 있는데 그쵸 이게 리프레젠테이션 러닝이라고 그래가지고 그쵸 표현이 여러분 영어로 리프레젠테이션이에요.
리프레젠테이션 영어로 다 뭐라고 그러는지 되게 중요하고요.
그리고 여러분 생성 AI를 이용할 때도 그냥 우리나라 말로 물어보면 대답 이상하게 해요.
많이 왜냐하면 학습된 데이터가 적잖아요. 그쵸 영어가 훨씬 많으니까 그래서 영어로 웬만하면 다 알고 있어야 되고 중요한 건 이제 디플이나 아니면은 여러분 번역기 이런 거 돌리면은 걔네들이 맥락을 잘 모르니까 번역 이상하게 한다고 그래서 여러분이 정확한 용어를 알아야 돼요.
알겠죠. 다시 그래서 강조하지만 이거는 영어로 효율 학습하기는 리프리젠테이션 러닝이에요.
알겠죠 표현 학습이에요.

참석자 1 35:14
이게 중요한 부분인데 어쨌든 그래서 이게 원래 데이터는 전에도 얘기했지만 데이터는 사실 우리가 뭔가 의미를 부여해야 되고 그쵸 그래서 정보가 되고 그렇죠 마찬가지로 지금 딥러닝을 주로 쓰는 게 이제 뭔가 새로운 지식 창출 이런 건데 레이블을 새로 붙이는 것도 있고 그러기 위해서는 개의 뭔가 데이터를 뭔가 추상화시켜야 돼요.
지금 사실 여러분 지금 내가 말하는 것도 다 추상적인 거잖아.
그쵸 뭔가 이렇게 여러분 다 알아듣는 거잖아요. 그쵸 그래서 뭔가 좀 인공지능적인 걸 하려고 그러면 전부 다 표현을 약간 다 뭔가 추상화시키는 게 필요한 거예요.
그렇죠. 그래서 이게 데이터 로데이터는 엄청나게 많지만 그거를 리프리젠테이션 러닝한다는 거는 뭔가 중요한 특징들만 추출해가지고 뭔가 표현을 바꾼다는 거지 이해돼요.

참석자 1 35:57
여러분 그래서 딥러닝도 사실은 그렇게 해가지고 나중에 뭔가 일이 큰일을 하는 거고 여러분도 지금 제가 쏟아붓고 있는 어마어마한 이제 많은 것들에서 뭔가 표현을 내부적으로 바꿔가지고 정장하고 있는 거지 그래요.
그리고 일단 지금 33쪽에 그림을 보면은 이게 데이터가 일단 형태가 여러 가지가 있을 수 있는데 지금 이 데이터는 생긴 게 여러분 일단 여러분 일단 이거는 카티시안 이거 뭐 데카르트 자표겠죠 그쵸 좌표기잖아 그냥 2차원 좌표기잖아요.
여러분 2차원 좌표기 볼 줄은 다 알잖아. 근데 이거 유치원생은 몰라.
그쵸 뭔지 알잖아요. 여러분 이건 뭐예요? 사실은 x y x장 y라는 좌표 값이 있어요.
그쵸 그래서 지금 각 데이터는 동그라미인데 각 동그라미는 사실은 어떤 데이터로 표현되고 있냐면은 x 좌표가 y 좌표가 두 가지예요.
그쵸 그쵸 원래 이렇게 돼 있는 거죠.

참석자 1 36:49
그거는 원래 로 데이터고 그쵸 그다음에 지금 여기 정보가 하나 더 있어요.
여러분 뭐가 더 있어요? x y 값으로 끝났나 이게 지금 데이터가 뭐가 있어요?
또 뭐가 있어요? 색깔이 표시돼 있다. 그쵸 색깔 색깔은 사실 여기 2차원으로 표현되는 게 아니잖아요.
사실 이건 뭐예요? 그러니까 이게 3차원이야 그쵸 3 자표기가 있고 제트좌표기에서 1 아니면 0 이런 거지 사실은 빨간색을 1로 보고 흰색을 0으로 볼 수 있잖아요.
이해돼요. 여러분 이거 사실 3차원이야 그쵸 빨간 색깔 표시하는 순간 3차원이에요.
그쵸 이해됐어요.

참석자 1 37:21
여러분 됐죠 그래서 지금 여기는 예시 데이터에서 이거에서 여기 보통 이렇게 표현하는 거는 이 사람의 의도는 지금 원래 로 데이터 자체가 XY로만 주어지는데 그걸 가지고 제 값을 뽑아내는 거지 제값 이해돼요 제트 값이 그래서 빨간색이 흰색인지 그쵸 이 빨간색이 있으면 1 흰색이면 0 이렇게 뭔가 내놓으라는 거지 그쵸 근데 이게 그냥 x y 값만 봐도 사실 우리가 지금 쓱 보이죠.
대충 대충 대충 어떻게 보여요? 여러분 그럼 이렇게 이런 선 있죠 여러분 제가 거꾸로 다 이렇게 뭔 얘기지 이런 선으로 딱 끓으면은 이쪽은 빨간색 이쪽은 흰색인 거 보이잖아요.
그쵸 오른쪽 위는 빨간색 왼쪽 아래는 흰색인 거 보이죠.
그러면 이거 이렇게 학습시키면 되게 편할 거 아니에요?
바로 x 와이프만 주어지면 이게 빨간색인지 흰색인지 알 수 있잖아요.
그쵸 그거 근데 이거 이 데이터는 보니까 이게 선이 자르는 선이 선형이면 되겠다.

참석자 1 38:19
그쵸 여기 지금 그림 보면은 여기 보게 34페이지 아래쪽에 보면 2 4 보면은 원본 데이터가 맨 처음에 나와 있고 그렇죠 그다음에 좌표 변환한 거가 지금 있고 3번에 더 나은 표현이라고 돼 있잖아요.
그쵸 더 나은 표현은 어떻게 돼 있어요? 완전히 그냥 빨간색 흰색이 다 구분이 돼버렸고 도표 변환한 것도 빨간색 색 구분시키긴 했는데 그래 이렇게 도표가 약간 오르 튀어 있죠 그쵸 뭔 얘기하는지 알겠어요 여러분 이렇게 좌표를 변환시켜서 학습하는 방법이 또 머신러닝에 있겠죠.
근데 사실 이거는 여러분 리니어 리그레션이라고 해서 리니어 리그레션이라기보다 어쨌든 이게 보면은 선형적으로 뭔가 딱 구분이 되는 게 보이죠.
여러분 선 자체가 선원 하나만 있으면은 구분이 되잖아 그쵸 그래서 그런 되게 심플한 거고 이런 거를 이제 학습을 시켜서 자동으로 하는 게 좋겠다는 생각이 들잖아요.

참석자 1 39:18
그쵸 이거를 맨날 어떤 이미지 표 값이 나와도 이건 빨간색이겠지 흰색이지 한번 이거 보면 알 수 있잖아요.
그쵸 그렇죠 여러분 그렇죠 이거 보면 우리가 그래요.
근데 왜 그분은 왜 안 오신대요? 안 오신대요 몇 번이에요?
몇 시야 이지 4355 0 3 1, 3 5 5 4 1, 3 5 0 3 6 6기 화면이 안 돼 아니 이거 안 나오는 건 없는데 저것만 나올 수도 있어요.
이게 지금 이렇게 하나 나오거든 그게 나와요. 그러면 할 말이 없네 번 해주면 누가 알아 누 아무도 안 했네 이쁘네요.
그러면 그게 안 이걸로 구분 자력만이라고 돼 있네요.
이 나쁜 사람 그 말 하지 복제 됐어요 이러니까 안 올라갑니다.
됐네 됐어요. 이러니까 안 올라가시네요. 이해가 됐어요.
네 감사합니다. 됐네. 안 오면 안 오는 게 이유가 있겠지.
그 그분도 학습이 돼가지고 여기서 맨날 부르는 거는 이런 일이겠지 하고 안 오는 거예요.
진짜 되게 썰렁하네. 그래요.

참석자 1 40:35
그래서 내가 지금 뭐 떠들고 있었냐 그래서 뭘 하고 있었지 중요하다고 지금 자꾸 머릿속에서 신호를 줘가지고 이게 자꾸 중요하다고 지금 제가 그래서 33페이지 했어요.
그쵸 33페이지에서 데이터가 어쨌든 리프리젠테이션 러닝에서 표현에 대해서 그렇죠.
학습하는 거에 대해서 했는데 표현을 이제 바꾸는 거를 여기서는 이제 XY 축 이거 변하는 거 그런 얘기를 했는데 이거는 이제 머신러닝에서 주로 하는 짓이고 딥러닝 쪽에서 딥러닝이 별로 안 필요한 데이터잖아요.
사실 이거는 왜냐면 데이터도 선행적으로 구분이 되잖아요.
그쵸 그런 거였고요. 그래서 지금 4분 남았는데 우리 쉬는 시간까지 그래도 이거를 계속 보는 것보다 내 슬라이드가 나은 것 같아요.
슬라이드 다 키 분 제 슬라이드라고 보는 것 같아요.
슬라이드 본 다음에 읽어보면 더 이해가 잘 돼서 제 슬라이드로 할게요.
저도 이제 책이 100% 마음에 드는 건 아니라서 제가 이 책 쓰는 거 너무 이 책이 너무 고맙긴 한데 오늘 책이 나오지

참석자 1 41:53
어

참석자 1 41:59
그래서 여러분 이거 제가 했던 거 여기서 다시 여러분 이거 다 있잖아요.
그쵸 다 있어요. 여러분 교과서 말고 이걸로 하다가 다시 교과서 넘어올게요.
그리고 이 내용 보면은 저도 이렇게 열심히 막 국어과 갔다 왔고 하다 보니까 학습이 돼서 나중에 전화하시면 시간이 없더라고요.
그래서 최대한 빨리빨리 하려고 하는 거죠. 그렇죠 지금 여기 보면은 제가 이거 강의 자료 올려놓은 거 있잖아요.
맨 처음에 올려놓은 거 그거 다시 올렸거든요. 사실은 이것도 추가해가지고 이런 거 그냥 새로 다운로드 받으면 돼요.
여러분 별거 지난번에 사실은 추가 시 변경 기는 별로 나간 것도 없고 해서 그냥 새로 올렸어요.
그래서 여기 지난 시간에 얘기했던 설명할 때 좀 이렇게 약간 보충해 놨어요.

참석자 1 42:46
그렇죠 그다음에 여기 이거 방금 얘기했듯이 지도 학습 기지도 학습 그쵸 지도 학습에서는 레이블이 있는 거 정답이 있는 거고 강화랑 비지도 이런 거 처음에 지난 시간에 좀 했죠.
그쵸 비지도 좀 약간 다 가지고 교과서에 있는 것도 보여주면서 좀 했었어요.
그렇죠 말아 먹으라고 그랬었고 이제 여기 처음 하는 거죠.
여기 이슬라이드부터 그렇죠 그래서 머신러닝에 세 가지가 적혀 있는데 이거는 지금 딥러닝이랑 아무 상관없이 얘기하고 있는 거예요.
그쵸 딥러닝에 근데 다 적용이 돼요 안 돼요. 이게 여기 머신러닝 얘기지만 이게 딥러닝이 적용이 되는 거예요.
안 되는 거예요. 머신러닝에서 얘기하는 모든 것에 딥러닝이 적용이 되는 거예요.
안 되는 거예요. 되는 거예요. 무조건 여러분 나중에 우리 자바 배웠죠 하이 클래스의 모든 특성은 하이 클래스가 받아요 안 받아요 갖고 있어요 없어요.
갖고 있어야 돼. 머신러닝이랑 지금 딥러닝이랑 어느 게 상위 플래스예요?
머신러닝이야 그쵸?

참석자 1 43:51
알겠죠? 이 그래프를 이렇게 그리는 거에서 밖에 있는 게 또 상이 클래스예요.
알겠죠 머신러닝에서 나오는 거 하나하나 다 중요하다 안 중요하다.
엄청 중요하지. 딥러닝에도 다 똑같이 적용되는 거라고요.
알겠죠? 여러분 슈퍼 바이즈 러닝 어슈퍼 바이즈 러닝 이런 거 있잖아요.
딥러닝도 똑같아요. 그냥 머신러닝 딥 뉴럴 네트워크 안 쓰고 그냥 머신러닝으로 할 수도 있고 이렇게 쓸 수도 있고 이해되죠 여러분 그런 식이에요.
그러니까 되게 중요하다고 알겠죠. 그래서 딥러닝 리얼 네트워크 하는데 이게 뭔지 모르면은 이제 또 하나도 모르니까 제가 이 복습해 주는 거야.
알겠죠? 그래서 여기 지금 세 가지 이거 나오는 거에 제가 예를 들어 놓은 거예요.
이것도 여러 가지가 있는 방법이 있는 건데 근데 왜 나눠져 있냐면 기본적으로 보면은 프라블럼이 자체가 달라요.
보면은 여기 제가 영어로 내가 귀찮아서 영어로 적어놨어요.

참석자 1 44:43
너무 우리나라 말로 너무 애매해서 컨티뉴어스 밸리 프리딕션 디스크립 마이더리 프레시피케이션 이런 거 적어놨죠.
여러분 그다음에 디스크릿 멀티 프레스 프레시피케이션 적어놨죠.
이게 이게 뭐냐면 하는 일이 다른 거지 이해돼요. 여러분 와닿아야 돼.
이거 모르겠으면 질문하셔야 돼요. 여러분 창피할 거 없어요.
여기서 창피하는 게 낫지 나중에 들어서 듣고 나서 모르는 게 낫지 시험 망치는 게 더 창피한 거야.
알겠죠? 여기 커팅스 밸리 플리스는 연속적인 값을 예측한다는 거예요.
그쵸? 연속적인 값이라는 게 뭐예요? 여러분 예를 들어서 키 있으면 몸무게 예측하고 이러는 거 그쵸 사실 정확히 맞지는 않지만 키랑 그럼 체지방률 한번 보물 예측 가능하잖아요.
그렇죠 이런 거 그렇죠 그러면 이게 몸무게 같은 거는 굉장히 퍼티어스 값이잖아요.
몸무게 48kg 49kg 48.2kg 이런 식으로 그렇죠 여러 가지 있잖아요.

참석자 1 45:35
그쵸 이해되죠 여러분 무슨 말인지 100kg 이런 거 그렇죠 100.1kg 다 있잖아.
그게 커트러스 리지 그래요. 그다음에 그런 문제에 대해서는 이런 이런 방법을 쓰고 그쵸.
그다음에 디스크립 바이너리 클리시피케이션 요거는 무슨 뜻이냐면은 여기 일단은 클래시피케이션 클래스피케이션 적혀 있죠 그쵸 분류 우리나라 말로 분류 우리나라 말로 다 알아야 돼요.
여러분 그쵸 분류 분류 분류를 하는데 바이너리 분류 멀티플레스 분류 돼 있죠 그쵸 바이너리 분류는 뭐예요?
여러분 예스 노 분류하는 거 또는 영일 분류하는 거 이해되죠 여러분 멀티클래스는 종류가 많은 거 그쵸 저 첫 번째 문제는 이제 개냐 고양이냐 개냐 개가 아니냐 이런 거 구분하는 거고 두 번째는 개냐 고양이냐 사람이나 이런 게 구분하는 거 이해되죠 여러분 그런 문제들 그다음에 디스크릿은 뜻이 뭐냐 클래스피케이션 자체는 다 디스크릿 하지 왜냐하면은 이 중에서 뭐냐 이런 거니까 다 디스크 타잖아요.
떨어져 있는 것들이잖아요.

참석자 1 46:34
이상 그쵸 커티니스 하지 않지 아까 몸무게 같은 것들은 가격 값이 예측하는 값이 굉장히 연속적인 값인데 얘는 연속적인 게 아니라 0 아니면 1이다 아니면 0 1 2 3 4 5 6 중에 하나다 이런 식의 디스크릿이죠.
그쵸 그런 거에 따라서 기법이 달라지는 거예요. 그렇죠 그래요.
그리고 보면은 여기 지금 이 용어들이 다 중요한 용어인데 이거는 제가 수업에서 아니 이거 알고 있어야 돼요.
여러분이 이게 진짜 이거는 지금 여기 적어놓은 것들을 제가 딥러닝 아닌 것만 다 적어놨어요.
여기서 여기 적어놓은 건 지금 기본적으로 리뉴얼 리그레션, 라지스트 리그렉션, 소프트리스 리그레션 다 채운 거 있죠.
그리고 서포트 벡트 머신 디시전 트리 랜덤 포리스트 있죠 이런 거 다 뉴럴 네트워크를 안 쓰고 하는 방법들이에요.
알겠죠 기본적으로 그리고 근데 문제는 뉴럴 네트워크에서는 이거를 사라지는 게 아니라 이거를 기반으로 해서 뭐 더 하는 거예요.

참석자 1 47:26
그러니까 이거를 알아야지 제대로 이해하지 그쵸 그래요.
그리고 제 수업이 아니라 이제 3학년 1학기 수업 한진희 교수님 수업에서 열심히 했었겠지 아니면 지금 동시에 듣고 있는 친구도 있을 거고 동시에 들더라도 동시에 두들어도 괜찮아.
어쨌든 열심히 좀 하세요. 여러분 알겠죠 그래서 이거 어 용어 자체가 뭔지 알아야 되는데 이게 그래서 이 문제 자체가 리뉴얼 리그레션 라지스트 리그레션 소프트웨어스 리그레션 이렇게 돼 있잖아요.
그쵸 리그레션이라는 말 자체가 따라가기 헬기 하기 뭐 이런 뜻이거든요.
우리나라 말로 그래서 리뉴얼을 리그레션 하겠다는 거는 내가 뭔가 아까 기본적으로 이제 뭔가 정답 값이 나오게 하기 위해서 역시 슈퍼바이즈 러닝이니까 내가 뭔가 함수를 매핑 함수를 만들어야 되는데 함수라는 거 항상 그런 거는 뭔가 입력값을 수록 특별 값 내놓는 건데 그게 리니어 선형 함수가 나온다는 거야.
선형 함수 선형 함수 별거 아니고요.

참석자 1 48:17
그냥 더 x 원래 입력 값에다가 뭘 곱해 그다음 바이러스 값 주고 이 정도예요.
그쵸 그렇죠 여러분 입력 값에다가 뭘 곱해? 입력 특성 값에 데이터에다가 뭘 곱해가지고 그리고 거기다가 뭔가 선이 여러분 1차 함수가 이렇게 생겼잖아요 그쵸 선이 그렇죠 아까 봤던 거 그런 거 그쵸 그게 리니어 함수예요.
그쵸 별거 아니고 리니 함수가 뭔지 정확히 알아야 돼요.
여러분 이거는 어디서 배웠어? 초등학교 때 배웠지 리니어 함수 아니에요 미니어 펑션이 뭔지 알겠죠 여러분 비선형은 뭐냐 그거 아닌 거 다 알겠어요 여러분 이 선이 아닌 건 다 비선형이에요.
알겠어요 선형 비선형 정확히 이해하죠 일단 대충 선형은 선 비선형은 선이 아닌 거 약간 좀 뭔가 다 변환이 되는 거 그렇죠 230도는 선형이 아니야 그쵸 이해됐어요.
다른 거 다 그래요. 그래서 여기 리뉴 리릭션을 그냥 원래 특성 값에다 곱하기만 해가지고 모든 해결하겠다는 거야.
알겠죠? 이게 바이어스를 더하잖아요. 보통 몰랐으면 지금 알았어.

참석자 1 49:18
바이어스를 더하고 원래 여러분 as 피처가 하나면은 특성 값이 하나면은 y는 AX 더하기 b잖아.
a가 곱하는 값이고 b가 뭐예요? 바이어스지 우연히도 a하고 b 했는데 b가 바이어스가 좋아요.
그렇죠 바이러스를 약간 더 올릴 수도 있는 거죠. 선이 항상 항상 원점 지나는 게 아니잖아요.
그쵸 그렇게 하는 거지 그쵸 지금 제가 하는 거라 못 알아들었다 질문해도 좋아요.
여러분 나 창피할 거하나도 없어. 지금 시점에서는 그렇죠 나중에도 분화 지난 다음에도 이거 물어보는 거 창피할 거 없어요.
여러분 학생이니까 다 못해 알겠죠? 막 물어봐요.
알겠죠? 나이프로 잘하면 되지 그쵸 그쵸 그래서 어쨌든 리니어 함수로 하는 거고 그다음에 라지스틱 그립션은 라지스틱 함수라는 게 있어요.
여러분 라지스틱 펑션이라는 게 라지스틱 펑션은 모든 세상의 모든 자연수 값을 0하고 1 사이로 매핑하는 놈이에요.
0하고 1 사이로 그런 함수가 있어요. 여러분 아직 우리나라 다른 말로 시그모이드라고 불러요.

참석자 1 50:21
시그모이드 시구 모이드 라지스틱 같은 말이에요.
그러니까 시구 모이드 리그이랑 똑같은 말인데 미리 들어놨다가 넣으면 나중에 또 모르겠으면 질문하세요.
여러분 라지스틱이 리글리 시노비도 같은 말이고 라지스틱 뜻이 모든 세상의 모든 값을 실수 값을 영하 14일 매핑하는 거 영하 11이 뭔데요?
영어 1 사이의 값이 뭔데요? 여러분 커플이잖아 확률을 매핑하면 데이터 하나도 아 그렇죠 확률로 그러니까 모든 세상이 모든 누가 뭔가 훈련을 시켰는데 뭔가 더 이상한 값이 나왔는데 무조건 확률로 바꿔버리는 거예요.
그러면 1에 가까우면은 그냥 이거는 예스다 여유 합법은 노다 이렇게 하면 편하잖아요.
그쵸 그래서 우리가 뭔가 원래 값에서 원래 나온 값에다가 뭘 곱해가지고 곱하고 더해서 어쨌든 무조건 0하고 의사를 시키겠다는 거지 그래서 구분하겠다는 거지 왜냐하면 이익증 분류니까 이해되잖아요.

참석자 1 51:13
여러분 당연히 여러분이 그냥 원시시대로 돌아가서도 이제 또 살다 보면 이렇게 이게 나올 수밖에 없어요.
그렇죠 여러분 그러니까 은근히 세상에 있는 많은 것들이 당연해 그렇죠 그렇죠 그래요.
그다음에 소프트 넥스는 소프트맥스라는 것들은 여러 개의 출력 값이 여러 개의 값들이 나오는데 하나가 아니라 출력값이 걔네들이 전부 다 더해가지고 1이 되게 만드는 놈이에요.
원래 이제 아무리 여러 가지 이제 출력 값들이 나와서 전부 다 확률로 만들고 싶은 거지.
그러니까 예를 들어서 소프트맥스 함수라는 거는 그렇게 나중에 보는 게 더 빠를 것 같아요.
어쨌든 소프트맥스는 멀티플렉스 쪽에 쓰는 거고 어쨌든 뭔가 결과 출력 값이 나온 것들이 전부 저희가 일로 만드는 놈이에요.
알겠죠 별 거 아니고 그래요. 그래서 이 정도만 알고 있고 그리고 저거는 자세한 거는 제가 교과서에서도 나올 거 나중에 필요하면 보충해 줄게요.

참석자 1 52:10
그리고 지금 슈퍼바이저 러닝에서 되게 열심해지겠는데 무슨 육군이 지금 막 하고 있으면 미치다 그렇죠 돼가지고 선출했는데 여러분 이렇게 하면 안 되고 쉬는 시간을 보장해 주세요.
3분밖에 안 남았지만 여러분 11시 5분에 오세요.
11시 5분에 내가 이제 알았죠 11시 5분에 쉬는 시간 가졌다가 왜냐하면 쉬는 시간 맞춰져야 되는 시간 다른 수업이라서 미안해요.
여러분 미안해요. 11시 5분에 와요. 11시 5분.


clovanote.naver.com


딥러닝 day2
2025.03.10 월 오전 10:19 ・ 30분 59초
심승환


참석자 1 00:00
뭔가 뭔가 여러분들 지금 훈련을 하고 있는 거잖아요.
그쵸 나 때문에 계속 뭔가 값이 변하고 있는 거죠. 그쵸 그래서 나 갑시다 말하고 난 다음에는 사실 여러분 자전거 타는 건 비슷한데 그러니까 공부할 때는 천천히 대고 막 막 이거 아닌가 보다 막 이러면서 이게 맞나 저나 이러면서 하는데 나중에 이제 실제 쓸 때는 거의 그냥 자동으로 쫙 나가게 되잖아요.
그쵸 문제는 이제 이게 또 문제인데 업 러닝이라는 게 되게 중요해요.
요즘에 잘못 학습 시켜놓으면은 잘 안 바뀌잖아요.
그쵸 뭐 예를 들어서 자전거 운전도 습관 잘못 들이면 되게 이상해지고 자전거도 잘못 습관 들여놓으면 되게 위험하게 타고 습관이 되게 중요하잖아요.
3번 한 번 외우고 나면 하드 와이어 돼버리잖아요.
사실 그 사람도 자전거를 타기 시작하면 다 못 하기 힘들어요.
그쵸 여러분 자전거 타지마 그런 게 있어요. 그래서 걷기 시작하면 못 걷기가 힘들어요.
그렇죠 근데 처음에 걷는 거 되게 힘들잖아요.

참석자 1 00:51
그쵸 근데 뭐냐면은 이거 훈련은 되게 힘든데 훈련된 거를 그냥 쓰는 거는 굉장히 자원을 별로 안 쓴다고요.
이 빠르고 근데 이것조차도 근데 사실은 좀 더 빠르게 하기 위한 방법이 여러 가지가 필요하고 그래서 온 디바이스 AI 이런 걸 열심히 하고 있는 거고 그렇게 하는 걸로 일단 감이 잡혀 계세요.
여러분 그래요. 그리고 여기서 언급하고 싶은 게 이 인퍼런스라는 용어가 원래 전문 용어로 이제 사실 테스팅 하는 거랑 같은 용어를 쓰고 있거든요.
그래서 이제 그리고 우리나라 말로 번역을 추론이라고 하는데 문제는 이 추론이 우리나라 말로는 하나인데 영어로는 추론을 번역을 이제 리즈닝이라고 하기도 하고 리즈닝이라는 거 여러분 들어봤어요.
요즘에 직급 나 딥시큐가 자기가 잘하는 게 뭐 프로그래 근데 그건 영어가 리드닝이에요.

참석자 1 01:42
이 임플런스가 그 아이야 그 제가 약간 좀 조심스러운 게 실제로 지금 머신러닝이나 딥러닝 쪽이 막 계속 발전하고 있는데 더 이상 우리나라 사람들이 새로 발전시키는 건 별로 없고 당연히 영어 쓰는 사람들이 발전시키는데 용어가 새로 사주 나오잖아요.
번역은 다 똑같이 하는 거야. 이해돼요. 여러분 구조랑 아기처럼 다른데 계속 구조를 번역하면 사실은 오해가 있잖아요.
그러니까 영어로 알아야 되는 면이 있다고요 여기 지금 이번에 챗gpt 45 나온 거랑 딥시크 새로운 버전은 추론 기능이 강화됐다고 이제 뉴스가 나와요.
근데 그게 인플러스 기능이 아니잖아요. 리즈닝 기능이지 리즈닝은 뭔가 이렇게 체인 오브 띵킹이라고 그래가지고 다음에 뭐가 필요한지 생각하고 또 생각해보고 생각하고 하는 거 있잖아요.

참석자 1 02:31
그거를 이제 리즈니이라고 하는데 체인 오브 띵킹이라고 그래서 cot라고 부르기도 하고 이러는데 그런데 그거는 번역을 인퍼런스라 인퍼런스가 아니거든요.
근데 그걸 추론이라고 번역하니까 똑같은 걸로 추론은 이미 하고 있는 건데 갑자기 뭔 소리인가 그런 생각이 들잖아요.
그리고 조심할 건 이제 기자들이나 뉴스에 나오는 거 보면은 트랜스포머는 딥러닝이 아니고 딥러닝과 달리 이런 다 있어요.
되게 이상한 거예요. 그렇죠 남자 사람은 사람과 닮은 거 이해돼요.
여러분 그런 식으로 말하게 하는 거지 남자 사람은 사람이 아닌가 그렇죠 웃기잖아요.
그쵸 너무 웃기죠. 그런데 그런 식으로 만일 여러분은 그렇게 얘기하면 어떻게 되겠어요?
같이 안 놀겠지 사람들이 그렇죠 아니 뭔 얘기인지 알겠죠.
같이 안 는다는 게 이사가 안 된다는 뜻이에요. 여러분 그래서 그래도 말을 잘해야 돼요.

참석자 1 03:25
그렇죠 특히나 요즘 같은 시대에는 AI가 거의 다 해주는 시대는 왜냐하면 AI가 환각에 시달리기 때문에 여러분이 정확한 개념을 가지고 있는 게 되게 중요하다고 알겠죠.
특히 단어도 우리나라 말로만 이해하면 큰일 나요.
여러분 그쵸? 영어로 다 뭔지 알아야 돼요. 추론이라고 이해하지 말고 인퍼런스라고 이해하지.
추론이 리즈닝이라고 생각해야 되고 지금 나온 새로운 거라고 알겠죠 여러분 이 그래요 그래요.
그다음에 이제 여기 머신러닝의 3종류 이런 거 유명해서 이거 이 용어는 이따 알아야 돼서 교과서에서도 그냥 아무렇지도 않게 막 나오거든요.
근데 그걸 그때마다 설명하는 게 별로라서 지금 이제 다 해버리는 거예요.
알겠죠? 지도 비지도 강화가 유명해요.

참석자 1 04:07
꼭 보통 얘기 개 세 가지 이렇게 얘기할 때 사실 이게 엄밀하게 맞는 건 아니지만 그냥 보통 학습에는 크게 이렇게 세 가지가 있다 얘기하는데 이것도 사실은 좀 그렇게 바람직하지는 않아요.
왜냐하면 지도의 반대말은 비지도잖아요. 여러분 강화가 여기 또 있잖아 이거 이상하잖아요.
그쵸 이상하다고 이게 별로 안 예뻐요. 사실 이렇게 논리적으로 아름답지 않은 상황이에요.
알겠죠 그냥 그렇다고 근데 그렇게 사람들이 쓰고 있다는 거 알고 계시고 여러분도 이상하다고 생각하시고 이상하다고 생각하는 게 되게 중요하겠죠.
이상하다고 생각 안 하고 넘어가기 바쁘지 비지도 비지도 하고 끝났잖아.
사실은 여러분 지도 아니면 비지도잖아요. 그렇잖아요.
여러분 강화는 그러면 도대체 좀 다르다고 또 해놓은 거지.
사실은 사실은 지도적인 것도 있고 비지도적인 것도 있는데 사실은 그래 갑시다.

참석자 1 04:56
여기서 일단 사람들이 뭘 이렇게 분리하고 있는지 지금 명확히 분리하고 있는 체계가 있으니까 어쨌든 지도 학습은 정답 레이블을 주는 게 핵심이에요.
레이블 레이블이라고 해서 정답 출력과 이거는 거의 이 교과서 전체에서 거의 이것만 할 곡이라서 은근히 제가 이 슬라이드 별로 없어요.
이 슬라이드 지도 학습 내용이 안 나오는 이유는 거의 이 책에서도 거의 레이블이 있는 데이터만 그러니까 아까 예를 들어서 사람이면 사람 사진 갖다 놓고 사람 적혀 있고 고양이 사진 갖고 고양이 적혀 있고 이런 식으로 레이스 잡혀 있다는 거죠.
이미지 분류한다 그러면 그런 것들이 그런 그렇게 이제 입력 데이터가 이미지 같은 경우는 거기 정답으로 레이블이 붙어 있는 거 그러면 저 결괏값으로 만약에 사람 보고 고양이 이러면은 아니네 이러고 고치고 그치 그렇죠 그렇게 지도해 주는 거죠.
그렇죠 이게 되게 쉽죠. 여러분 보통 딥러닝 하는 건 거의 딥러닝이 아니라 머신러닝이고 딥러닝이고 다 지도 학습이에요.

참석자 1 05:53
그리고 분류도 있지만 온갖 여러 가지 보통 이제 키 갖고 몸무게 예측하고 이러는 것도 키가 얼마인데 몸무게가 얼마 데이터가 있을 거 아니에요?
그렇죠 데이터 기반으로 원래 이제 입력 데이터의 정답 출력 값이 같이 있는 거예요.
그리고 그거를 계속 보정하면서 이제 그 웨이트 값 아까 전에 얘기했던 파라미터 값들을 제대로 될 값을 찾아 나가는 거지 감은 오죠 여러분 이렇게 크게 감을 잡으세요 알겠죠?
그거고 이건 딥러닝 상관없이 모든 머신러닝이 다 이렇다고 딥러닝이 뭔지도 모르는데 그쵸 이 설명하고 있으니까 또 별로기도 하죠.
그냥 일단은 이렇게 일단 한 다음에 다시 넣어볼게요.
그다음에 비지도 학습은 반대니까 지도 안 한다는 거잖아요.
레이블이 없는 거예요. 그쵸? 레이블이 없어 그쵸?
레이블이 없는 정답 레이블이 없는데 정답 정답 출력 값을 레이블이라고 보통 불러요.
여러분 레이블이 꼭 이름표 같은 거잖아요. 이름표.

참석자 1 06:50
그런데 지금 이 출력 결과를 가지고 계속 얘가 비슷한 것끼리 모아보고 이러는 걸 하는 거예요.
특성을 가지고 분석해서 좀 뭔가 스스로 파악을 해서 폭력 목표 같은 걸 파악을 해서 뭔가 분류를 해보는 거죠.
스스로 모아보는 작업을 하는 거예요. 이것도 감이 잘 안 오죠 여러분 그쵸 나 먼저 보여주고 하는 게 어떨까요?
여러분 표정이 안 좋아서 먼저 보여주세요. 표정이 안 좋아서 이거 보여줄게요.
여기 제가 뒤에 새로 갖다놨는데 비지도 학습에 대표적인 게 베리에이션 오토 인코더랑 그다음에 프린서플 컴포넌트 어네렉시스랑 근데 케이미지 클러스터링 이런 게 있거든요.
많이 쓰는 거예요. 여러분 케이미지 클러스터링은 이거는 딥러닝이 뭔지 설명 안 타고나는 게 아니라 어쨌든 이거 이거 먼저 보여드리니 내가 VI가 있는데 되게 재미있어가지고 교과서에 있어야 사실 교과서 몇 페이지에 있는지 보여줄 수 있지만 그냥 여러분 보여줄까요?

참석자 1 07:46
그래도 관심 있어 하는 교과서 갖고 온 사람들은 저 그림이 교과서에 있는 거예요.
이건 완전 논문에 있는 거라요. 사실은 515쪽에 있어요.
515쪽 교과서 515쪽에 있어요. 잘 보이잖아.
그래가지고 이거 비싼 건데 우리 딴 말로 이렇게 인사하려면 그렇죠.
여러분 515쪽에 있어요. 그렇죠 515쪽 12.4절에 12.4.1절에 있어요.
이게 비지도 학습의 대표적인 건데 이게 뭐냐면 데이터를 잔뜩 때려 박아 넣어주면 이제 형량 포에 의해서 사람 얼굴은 기본적으로 여기 머리카락 있고 눈 있고 코 있고 입이 있는 거 알잖아요.
그쵸 얼굴 선 있고 이거의 분포들을 학습을 해서 얘네 비슷한 확률적으로 비슷한 것끼리 모아보는 거예요.
이렇게 그리고 생성도 하는 거지. 그래서 사실 사람 얼굴에 잔뜩 넣어주면 여러분들 얼굴 잔뜩 넣어가지고 학습시키잖아요.
v 탭 그러면은 지금 예를 들어서 여기 앉아 있는 두 사람 있죠.
두 사람 사이의 얼굴도 만들어져요.

참석자 1 08:46
특성이 비슷한 게 있고 다른 게 있잖아요. 그리고 머리카락이 점점 변하고 눈도 변하고 안경 쓴 게 있다가 없어지고 이런 식으로 이해되시죠?
여러분 그런 식으로 다 뭔가 비슷한 게 공통점과 차이점이 있잖아요.
그쵸 그게 다 확률적인 거잖아요. 그쵸. 절대로 여기 눈이 달리지는 않는데 이제 눈이 달리면 이제 기묘의 칼날인가 그런 거지.
그쵸 이해되죠 여러분 네 그래요. 그래서 창의적이라고 그랬지.
우리가 하여튼 VA를 쓰면 현재 있는 데이터에서 가장 뭔가 비슷한 것 안 비슷한 것끼리 잘 이렇게 정리를 해가지고 모아줘서 보면은 지금 이 사람이랑 이 사람이랑 머리 이렇게 보면 왼쪽 오른쪽 다 비슷비슷하잖아요.
그쵸 인접한 것끼리는 점점점 변하는 거 보이죠. 여러분 그렇죠 여러분 교과서 보는 김에 다음 페이지 한번 볼래요.
여러분 교과서 보는 김에 어디 있냐 이거 교과서 보는 김에 어디 있지?
여기 525쪽 있죠 525쪽 제가 이거 그냥 올릴게 이니까 이거 이거예요.

참석자 1 09:50
여기 숫자 숫자 0부터 9까지 숫자 있잖아요. 걔도 비슷한 것끼리 모아보는데 어떤 거 보면은 이게 여기 보면 비슷한 것끼리 모아놓은 거 보면은 9랑 9에서 8로 변하는 게 보이고 7에서 9로 변하는 게 보이거든요.
7도 이상하게 쓰면은 9 같이 보이기도 하고 여러분 사실 시력 검사할 때 7인가 9가 맞히고 싶은데 그렇죠 그런 거 있잖아요.
여러분 그런 거 뭉개지는 거 그거도 실제로 보여주고 있는 거죠.
숫자로 학습해 놨더니 비슷한 분끼리 이렇게 확률적으로 이렇게 생길 수밖에 없다는 거죠.
그렇죠 이게 재미있죠. 여러분 이거는 우리가 라이브를 주거나 정답 값 준 게 아니라 실제 존재하는 데이터들의 특성을 비교해서 공통점 차이점을 통해서 이제 보관이라고 그래서 중간중간 단계에 뭐가 있는지를 보여주면서 이렇게 하니까 이게 이것도 대표적인 생성형 AI 중에 하나예요.

참석자 1 10:38
왜냐하면 뭐 만들어낼 수 있겠죠. 이걸로 자동차를 디자인해보라고 저런 차가 이런 변형이 있을 수 있으니까 하면서 만들어낼 수 있겠죠.
그쵸 약간 더 샤프하게 만들어서 샤프 하는 쪽으로 가고 이게 더 재미있는 게 여기 다음 페이지 보시면 이거 슬라이드 이것도 교과서에 있는 건데 이거 몇 쪽에 있나요?
여기 아까 516쪽 한번 보시면 돼요. 5 16일 다른 국가들보다 이 국가 훨씬 좋긴 좋아요.
전 세계에서 제일 미국에서 제일 많이 버는 국가소라서 이것도 여전히 보면은 아까 그 사람 같은 사람이었던 어요.
같은 저자가 쓴 건데 여기 이 사람 얼굴 보이죠. 여기 하게 환하게 그냥 무표정 한 건데 이쪽으로 환하게 웃죠.
그쵸 그러잖아요. 여러분 여기는 똑같은 얼굴인데 여기 되게 막 너무 우울해 보여 죽을 것 같아요.
그쵸? 이해되죠? 여러분 이게 스마일 벡터라는 걸로 스마일 벡터를 강화시키는 방향 이거는 약화시키는 방향으로 하면은 이렇게 산성이 가능한 거예요.

참석자 1 11:34
여러분 어차피 웃으면 일이 벌어지는 거 알고 있고 눈이 이렇게 옆으로 터지는 거 아니고 그렇죠 우울하면은 입꼬리가 내려가고 눈이 좀 어두워진다는 거예요.
의해서 이렇게 만들어낸다는 거죠. 이게 이것도 지도를 한 게 아니라 저절로 이제 있는 특성을 모으다 보니까 이런 벡터를 찾아낸 거예요.
벡터가 이제 변한 풍경 포상 이쪽으로 가면 웃는구나 이쪽으로 가면 우는구나 이러면서 굉장히 재밌죠.
여러분 되게 이거 이거 지금 굉장히 유명해서 지금 뭔가 새로운 좀 생성해야 괜찮은 게 나오면 더 이거 기반으로 하는 게 많아서 되게 재미있는 것 같아요.
이게 진짜 지금 원래 개니 할 때 되게 떴잖아요. 개니라고 들어본 사람 있을 거예요.
근데 걔보다 사실 얘가 더 재밌는 거 걔는 잘 학습이 잘 안 돼요.
학습이 잘 돼요. 그리고 뭔가 의미 있는 데이터를 찾을 수 있기 때문에 이거는 교과서에 있는 건데 그리고 데이터를 이렇게 막 다 때려박으면은 이거에 뭔가 잠재적인 뭔가 확률 분포를 찾는 거예요.

참석자 1 12:32
레이턴트 스페이스라고 돼 있는데 이게 뭔가 내재적인 뭔가 우리도 이제 사람 얼굴에 이렇게 생겨야지 이런 감이 있잖아요.
눈이 제일 중요하고 일단 그렇죠. 눈은 이렇게 양쪽으로 있고 이런 거 있잖아요.
그런 거에 대한 우리가 후포가 생기는 거잖아요. 그쵸 규묘리 칼라처럼 안 되는 거지.
그쵸 규묘리 칼라도 너무 이상하지 않게 문이 달리면 이렇게 달리기.
어쨌든 절대로 여기 다른 데 가지는 않잖아요. 그렇죠 손으로 가기도 하고 동그랗게 어쨌든 그래서 그거 보고 이제 신기하니까 또 보는 거지.
이 사람들이 어쨌든 뭔가 브로드한 이미지에 대한 뭔가 이렇게 학습을 하는 거지 그래서 여기서 아무 이제 뭔가 특정 랜덤 변수를 만들어내는 이 공간에서 거기에 해당하는 이미지가 나오는 거죠.
그런 식으로 해요. 그리고 그걸 여기 이거 얘네들이 만들어낸 방법은 그리고 잠재 공간 학습한 다음에 인접한 값들로 주면서 생성하니까 이렇게 된 거예요.

참석자 1 13:26
이것도 마찬가지고 근데 이게 차원이 2차원 1차원 이게 아니라 굉장히 크기 때문에 사실 그리고 또 가오시 분포 이런 거 쓰기 때문에 원래 여러분들 평균 가가하고 분산이라는 거 있잖아요.
그걸 조정하면서 나오는 거라서 다양하게 나올 수 있는 거지 그래요.
이것도 교과서에 다 어떻게 실제 코드 다 있어요? 이거를 하는 코드까지 굉장히 훌륭해야 할 책이 진짜 우리 거의 중요한 건 다 있는 거예요.
그리고 요것도 요거는 PCA도 교과서에 언급이 되는데 자세한 내용은 사실 머신러닝 책에 나오고 이거 머신러닝에서 굉장히 중요한 부분인데 이거 여기 다 딥뉴럴 네트워크 써야 되는 거고 이건 딥뉴럴 네트워크라는 거 안 쓰고 아직 안 배웠지만 이따가 또 해줄게요.
PCA는 필수 프로퍼포먼트 어널리시스라고 이거 보면은 여러분 패션 앱리스트라는 데이트가 있어요.
그게 뭔지 뭐냐면은 패션 이스트 제가 인터넷에 보여줄게요.

참석자 1 14:20
그냥 패션 리스트 앱 리스트는 사실 원래 아까 제가 숫자에 있는 거 있죠.
숫자 보여줬죠. 숫자 0 1 2 3 4 이런 거 숫자에 있는 데이터를 모아놓은 건데 그게 워낙에 머신러닝 하다 보니까 그거를 이제 패션 쪽에다가 적용해가지고 이미지들을 이렇게 막 샘플로 뭔가 숫자가 10가지 있잖아요.
0부터 9까지 그것처럼 옷도 패션 쪽에서도 10가지 아이템을 한번 그러니까 분류하는 걸 만든 거 이런 거

참석자 1 14:53
보이죠. 여러분 이게 보면 가방도 있고 옷도 있고 그렇죠 신발도 있고 이런 게 10가지가 있는 거예요.
구분이 되는 거 이게 딱 보면 이거 신발인데 이 신발 이 신발 좀 다르잖아요.
그쵸 그리고 얘는 원피스고 얘는 바지고 이거 우리 알잖아 그쵸.
근데 사실 확률 포장으로도 얘도 비슷한 놈들이잖아요.
사실은 그렇죠 어쨌든 그래서 여기 이것도 VR도 아까 베리셜 오토 커도 보여줬던 걸로도 할 수 있지만 기본적으로 프레스퍼 컴포넌트 어너리스라는 거는 가장 프리스터폴이라는 게 뭐예요?
뜻이 중요한 컴포넌트 구성 분석이라는 뜻이잖아요.
중요한 요소 분석 구성 요소에서 그래서 여기에서 가장 다른 그런 요소들이 있을 거 아니에요 공통점 말고 차이점 차이점을 기반으로 뭔가 분류를 하기 시작하는 거예요.
그러니까 이게 이 바지 같은 경우에는 여기에 가운데가 분명히 퍼져 있고 그렇죠 여기 이런 거 있잖아요.

참석자 1 15:48
이게 이게 다른 애들이랑 다르잖아요. 그쵸 여기가 있고 여기가 구멍이 쫙 있다는 게 여기랑 다르잖아요.
그런 식으로 이제 모아보는 거예요. 수학적으로 그래서 차원을 사실은 축소를 시켜요.
얘는 지금 차원이 이게 차원이라는 게 뭐냐면은 데이터의 개수예요.
여러분 그러니까 이게 우리가 지금 3차원에 살고 있잖아요.
1차원 2차원 3차원 알죠? 여러분 1차원은 데이터가 얼마예요?
항상 그냥 값 하나잖아요. 그쵸? 2차원은 가로 곱하기 세로잖아요.
그렇죠 근데 가로의 개수와 세로의 개수에 따라서 정보가 달라지는 거죠.
그렇죠 그래서 이게 지금 얘는 지금 2차원이잖아요.
근데 2차원이지만 사실 데이터가 가로 곱하기 세로가 지금 28 곱하기 28이거든요.
28개의 픽셀 28개밖에 안 돼요. 여러분 28개면은 여기 28개 점이에요.
28개 점 여기 28개 점으로 다 표현된 거예요. 전부 다 28개 성정 때 이해되죠 여러분 그래서 별로 그것도 충분히 구분 가능하죠.

참석자 1 16:46
28 부하 28인데 그거를 좀 더 작은 데이터 더 작은 정보를 바꿔서 제일 중요한 것만 남기고 다른 것만 수학적으로 만들어서 매핑을 하고 나면 이렇게 그리고 아까 변이를 한 거긴 한데 그 제가 아까 보여준 것처럼 이런 식으로 잘 안 보이지 되나 이게 된다 안 되네.
이거 보면은 여기 믹도오리가 몰려 있고 가방이 몰려 있고 이거 보이죠.
여러분 비슷한 것끼리 뭐 하니까 얘는 확실히 얘랑은 먼 정도에 따라서 더 멀리 갖다 놓은 거예요.
느낌이 와요. 여러분 가방은 좀 멀잖아 그리고 신발끼리 모아놓고 그쵸?
이해되죠? 여러분 그리고 우도리까지 모여 또 바지끼리 모여 있고 이런 식이에요.
여러분 알겠죠 이렇게 저절로 된다는 거죠. 우리가 사람들이 뭐 분류하라고 시키지 라벨을 안 알려줘도 얘가 이건 비슷한 거다 안 비슷한 거다 하고 분류할 수 있을 거 아니야 수학적으로 그렇게 하는 게 비지도 학습들이라고 이해돼요.
여러분 확률 분포를 주로 하는 거예요. 알겠죠? 그래요.

참석자 1 17:49
얘는 확률 포라기보다는 이제 해당 차이점들 별로 한 거고 다 있구나.
그리고 이제 케인 밍지 플러스틱이라는 거는 워낙 유명한데 이거는 이제 머신러닝에서 배 지금 이것도 항상 유용해요.
뭐냐면은 이게 뭔가 3개의 평균을 가지고 뭔가 분지타 해봐라 모아봐라 그래서 여기 지금 이거 종류는 잘 모르겠지만 세 가지로 분류해요.
하면 세 가지로 제일 비슷한 끼리 모아주는 거예요.
5개라고 하면 5가지로 분류해 주고 정해줘야 되긴 하는데 굉장히 가벼운 금방 학습되는 비지도 학습이라서 여기 이런 거죠.
여러분을 예를 들어서 케미지 클러스팅으로 제가 이 두 가지로 보면 가장 반은 두 가지로 분류해 줄 수 있다고요.
근데 그거를 이제 외형을 가지고 갈지 만약 목소리를 가지고 갈지 다를 수 있잖아요.

참석자 1 18:33
여러분 그렇죠 목소리를 가지고 하면 남녀로 구분될 수도 있을 것이고 그렇죠 그냥 모형 갖고 하면 안경 쓴 걸로 구분할 수도 있을 것이고 무슨 얘기인지 알겠죠 여러분 분류를 해준다고 근데 이렇게 두 가지로 하든지 5가지로 하든지 그러면 또 어떻게 하겠지 옷을 벗어서 입었는지 안 입었는지 이런 식으로 얘네들 얘가 분류한다고요 이해되죠?
여러분 그래서 이런 게 다 비지도 학습이에요. 이건 뭐예요?
비지도 학습이 지도 학습은 은근히 뻔하잖아요. 사실 여러분 원래 정답 값이 있는데 그래서 그가 그걸 하는 거 그렇죠 데이터를 가지고 그러는 거고 비지도 학습 이런 것들이고 비지도 학습이 굉장히 재미있는데 교과서에서 거의 안 다뤄요.
지금 보면 아까 나오는 것들 이거 어디 있어요? 거의 끝에 있잖아요.
그쵸 밑에 있잖아요. 그래서 좀 그만큼 이제 덜 다루기 때문에 제가 더 열심히 강조하는 면이 있었어요.
알겠죠 아 저 수학 시간에 첫째 주에 그래요.

참석자 1 19:25
그리고 강화 학습이라는 게 있는데 이것도 재미있는데 강화 학습은 이거는 왜 맨날 자율주행도 사실 이거 갖고 했지롱 이러고 맨날 이게 뭐냐면은 이거는 여기는 이제 실제 공부시킬 때도 여러분 지도 학습을 계속 선생님이 옆에서 이거 답은 이건데 너 이거라고 대답하지 않으면 안 돼 이런 거고 비지도 학습은 니가 알아서 좀 뭔가 분류해 봐 이런 거고 그렇죠 강화 학습은 그런 거 정답 답 알려주지 않고 니가 이제 공부해 오면서 점수를 많이 따게끔 어떻게 하나 뭔가 점수가 있는 거예요.
점수 리워드 펑션이라는 게 있죠. 보상 보상 그러니까 정답을 알려주지 않고 뭔가 잘하면은 점수를 높게 주고 못하면 점수를 낮게 줘서 점수만 높게끔 훈련시키는 거예요.
실제로 여러분 뭔가 좀 자기주도 학습이 저희 보상 함수 자기주도 학습이 다 그렇거든요.

참석자 1 20:15
그냥 뭐 이렇게 자꾸 막 하라 하라 이러면서 이거 아니다 틀렸다 이렇게 하는 것보다 스스로 이제 장기적으로 뭔가 장기적으로 뭔가 잘 되는 쪽으로 학습하는 게 바람직하잖아요.
그래서 이게 이쪽 지도 학습은 정답 값이 사람이 만들어주는 것 같아 데이터에 의해서 여기도 여기 강화 학습은 데이터를 사람이 주지 않겠죠.
정답 값을 특히나 이쪽은 의사결정이나 예측 쪽에서 많이 하는데 그러니까 뭐냐면 예를 들어서 여러분이 알파고 같은 거 있잖아요.
알파고 바둑 뜨는 거 바둑 뜨는 거 정답 가스는 어떻게 되겠어요?
여러분 지도 학습을 한다면 흙이랑 백이랑 이렇게 판이 있을 때 다음에 흙은 어디로 가야 될지가 옛날 기호에 의해서 나오겠죠.
옛날 사람들이 했던 거 여기다 둬야 되는데 여기다 두면 이상하게 이럴 수 있잖아요.
근데 이 가마수면 어떻게 하겠어요? 일단 둬봐 그래서 나중에 점수 이기면 그만이죠.

참석자 1 21:04
그쵸 이기면 되는 거잖아요. 그러니까 희한하게 두는 거죠.
얘가 근데 이기지 이해돼요. 여러분 사람보다 더 나을 수도 있잖아요.
여러분 그러니까 예를 들어 주식 투자를 하는데 이상하게 투자하는데 결국 잘 돈 벌면 그말 아니에요.
그쵸 근데 이 정답 지도 학습을 하면 어떻게 돼? 지도 학습을 철저하게 인간이 뭔가 만든 데이터에 의해서 하는 거잖아요.
그러니까 좀 더 창의적인 게 나오기 힘들잖아요. 그렇죠 더 대단하게 나오기 힘든데 학습 강화 학습은 고상 뭔가 점수 결국은 진짜 원하는 그 고를 위해서 뭔가 학습이 되기 때문에 굉장히 더 재미있는 게 나올 수 있는 건데 문제는 강화 학습은 여러분 선불로 조절하기 힘든 게 결국은 영원히 학습이 안 되는 경우도 있어요.
헤매기만 하다 끝날 수도 있어요.

참석자 1 21:48
그러니까 옛날에 알파고 같은 거는 사실은 성공을 못했지 바둑이 인간보다 바둑은 인간이 맨날 이긴다고 그랬어요.
그러니까 도저히 안 될 거라고 컴퓨터는 근데 이제 워낙에 지금 NVIDIA가 잘해가지고 사실 된 거죠.
사실은 하드웨어가 너무 빨라져가지고 그런 면이 있어요.
이해되죠. 여러분 그래요. 지금은 근데 강학습 또 이제 이것도 방법이 여러 가지가 나왔는데 굉장히 또 알고리즘적으로 좋은 게 많이 나와서 지금은 그리고 또 강화 학습도 사실은 처음부터 시작하는 게 아니라 요즘에는 인간 데이터 같은 걸로 뭔가 잘 된 것들만 모아서 뭔가 트라이어 레러 하다가 좋은 것만 모아서 그걸로 그걸 기반으로 시작하는 게 많이 있어요.
그래서 어쨌든 학습 강화 학습은 영원히 제일 항상 사할 수 없는 분야겠죠.
그렇죠 왜냐하면 컴퓨터 니가 잘해 봐 이건데 실제로 그런 게 많잖아요.
게임 같은 것도 그렇고 그래요.

참석자 1 22:38
그래서 이렇게 일단 분리가 되고 이거 지금 다 하면 너무 정신없을 것 같아서 제가 일단 이거 먼저 해줘야 지난 시간에도 이거 먼저 해줬던 것 같은데 머신러닝 딥러닝 그쵸 근데 이게 딥러닝이 뭔지 먼저 좀 알려주긴 해야 될 거 진짜 딥러닝은 8분 남았네.
그래서 어쨌든 머신러닝의 일종이에요. 머신러닝 중개 딥러닝이 있는 거예요.
그러니까 모든 딥러닝은 머신러닝이에요. 여러분 그러니까 얘도 뭔가 프라이터 값을 주고 그걸 학습시키는 게 맞다고요?
그런데 머신러닝하고 다른 뭔가가 있는 거죠. 그쵸 여기 이거 그냥 보통 머신러닝 중에 딥러닝이 아닌 것도 있고 딥러닝은 다 학습을 한다는 거죠.
그렇죠 그래요. 그래서 딥러닝의 핵심이 뭐냐면은 여기 보면은 심층 신경망이 적혀 있죠.
그쵸 심층 신경망이라는 그런 자료 구조를 쓰는 거예요.

참석자 1 23:29
근데 심층 신경망이라는 게 도대체 뭐냐 그러니까 이거 여기는 심층 신경망이 아니라는 거지 머신러닝은 알겠어요 일단 딥러닝은 심층 신경망 심층 신경망은 영어로도 알아야 되잖아.
그사람 그쵸 딥 뉴럴 네트워크이겠다. 그쵸 번역하면 딥 심층 딥 신경 뉴럴 네트워크 그쵸 이거 적어봐요.
여러분 딥 뉴럴 네트워크 딥 뉴럴 네트워크 영어로 약자로 보통 하는 거 아니에요?
그 기면 뭐라 그럴 것 같아요. 여러분 DNN 외워야 돼.
DNN DNN이라고 그래 DNN DNN 하면 이제 여러분 그게 뭐예요?
이러면 안 되지 그쵸 딥 뉴럴 네트워크에 알겠죠 DNN 심층 신경관 그리고 그래서 여기 보면은 여기 지금 적어놨어요.
뒤에 적어놨네. 디밀러 적어놨네. 그렇죠 예 적으라고 뒤에 나오네.
여기서 신경망 중 딥러닝이 아닌 것도 있다고 제가 적어놨죠.
신경망 신경망이 뭔지도 모르는데 신경망이 뭔지 설명한 다음에 다시 돌아올게요.

참석자 1 24:31
여러분 신경망 이거부터 뉴럴 네트워크 신경망이에요.
여러분 신경망 슬라이 다 있잖아요. 그쵸 뒤에 가면은 신경망 이거는 신경망은 사람이 이렇게 신경망이 있어요.
근데 여기 뇌 핸드라고 적어놨는데 뇌 말고도 여기도 다 신경망이에요.
여러분 여러분 다 여기 지금 신경만 있잖아요. 그쵸 신경이 없으면 우리가 정답이 없잖아요.
그쵸 뇌에 특히 뭔가 신경망이 좀 빼곡빼곡하게 있어가지고 이렇게 부르는데 뉴러는 신경세포라는 뜻이고 사람 뇌에 천억 개가 있다.
이거 지금 항상 볼 때마다 재미있어요. 사람한테 천억 개가 있는데 개수가 사람에 따라 차이가 없고 새로 생성되지 않아요.
태어날 때부터 거의 그대로 있는데 중요한 건 이제 이게 학습을 하면 여기 이게 얘가 이런이 있는데 여기 연결이 되는 거 있죠 연결이 공고를 하면 연결이 되고 여기다가 뭔가 값이 저장이 되는 거예요.
중간 연결 부분에 연결이 강화되기도 하고 이제 약해지기도 하고 그러면서 학습이 된대요.

참석자 1 25:30
근데 여기 이렇게 이게 이렇게 붙은 거를 시냅스라고 부르는데 이게 필리스 한 유런에다가 이렇게 많이 붙는데요.
그러면 유런 하나가 이렇게 연결이 많이 높아 그래서 뇌가 이제 이렇게 실내 수가 많이 생기는데 공부를 하면 이게 많이 생기고 안 하면 점점 사라진대 또 어때요?
어쨌든 뇌를 많이 쓰면 이게 이제 생기고 안 쓰면 없어지고 그래서 학습하고 기억하는 거 지금 여러분 공부하는 것도 이걸 만들고 있는 중이에요.
그렇죠 그리고 이게 이제 자야지 계속 남아 있는데요.
그래도 잠을 자야지 잠을 계속 새면 안 되는지 그래요.
어쨌든 이게 이건 뉴럴 네트워크이고 이거 사실 운동 신경 운동 세포 그다음에 감각 세포 이런 거 있잖아요.
그것도 다 누런들이에요. 사실은 그래서 어쨌든 자전거 타는 것도 이런 식으로 다 배우는 거고 일단 걔네 말고 손해도 있고 그래요.

참석자 1 26:24
재미있지만 그냥 넘어가면 다 이제 여기 보면은 이제 여기 이거는 여러분 사람의 신경 휴먼 유발 유라 에서 휴먼이나 아니면 동물도 다 있겠죠.
근데 아티피셜 리브레트 네트워크라는 거는 이제 이것도 줄여서 ann이라고 부르거든요.
여러분 ann aln이라고 불러요. 그래서 이제 DNN 말고 ann이라고 부르는 사람들 많아요.
ann 이것도 외워요. 알겠죠 알아야 돼요. ann ann도 왜 눈물이 안 올지다 이렇게 나오겠네요.
aln이 인공신경망인데 뭐냐면은 이게 뉴런에 따라서 혈압 뉴런에 따라서 이제 사실 만든 거죠.
프로그래밍 사실 사람 이런 따르지 않고 원래 이렇게 만들었다가 사람이랑 비슷해서 이렇게 치면 변하고 있는데 그래서 이 책에서는 전혀 안 나와요.
이 책은 그런 거 이 책은 이 사람은 또 비유하는 거 되게 별로라고 그래가지고 책에는 이런 거 사람 얘기 하나도 안 나오거든요.
이 책은 근데 나는 그게 별로라서 또 이거 넣었어요.

참석자 1 27:16
그래서 중요한 게 뭐냐면은 여기 이제 여기 이건 뉴런 이게 뉴런인데 뉴런에 여러 가지 여러 유런이 연결돼 있다고 그랬잖아요.
아까 몇 개 했어요. 여기 천에서 만 개 붙어 있댔잖아요.
그래서 여러분이 이제 예를 들어서 내가 이렇게 여기 감각을 했어요.
손이 감각을 했어 눌렀잖아요. 누른 걸 여기서 알잖아요.
그쵸 아는데 이게 여기 쫙 타고 오는 거예요. 근데 이게 만약에 여기서 중간에 끊어지잖아요.
그럼 안 오겠지 그쵸 그리고 이제 여기서 지금 뾰족한 거 누르면 제가 뭔가 반응을 해서 이제 뭔가 좀 위기 신호가 발동해야 되잖아요.
그쵸 뜨거운 거 누르거나 그런 것들이 다 감각이 증폭되는 거잖아요.
사실은 그래서 이게 여기서 막 오는 신호 중에서 어떤 거는 무시해야 되고 어떤 거는 가야 돼요.
그쵸 근데 사실 여러분이 지금 시간이 별로 안 남았다 이게 재미있으니까 할게요.

참석자 1 28:05
계속 그러면은 여러분 여기서 아무도 돌아다니고 있지 않죠 노래 부르고 있지도 않고 근데 사실 여러 노래 부르고 싶은 욕구도 있고 어떤 쪽에서는 막 어떤 이런 담배하고 막 노래 불러야지 어떤 데 돌아다니고 싶어 자고 싶어 이러는데 지금 참으면서 여기 앉아 있잖아요.
사실 차단하고 있는 거지 그게 여러분 어떻게 지금 이 나이 여기 우리 대학교 입학하려면 그거 안 하면 입학을 못해요.
돌아다니는 사람이 입학을 못해 걸르잖아 우리가 이해돼요.
여러분 그런 게 다 이제 여기 이렇게 오는 신호 중에서 일로 보내는 거 있죠 다른 데로 보내는 거 그러니까 아까 막 돌아다니고 싶은 욕구는 다 차단시켜버리는 거지 그리고 지금 나의 말을 듣는 거는 열심히 전파시키고 있는 거잖아요.
이해돼요. 여러분 자전거를 타면서 이제 일로 가는 거는 차단시키고 저기로 가는 거 강화시키고 이런 걸 학습을 시키는 거죠.

참석자 1 28:50
그래서 이게 중요한 게 뭐냐면은 신호들이 오는 거에 신호들이 오는데 여기 지금 원래 값들이 올 거 아니에요?
이게 신호들이 신호들이 올 거 아니에요? 그렇죠 신호들을 어떤 거는 0으로 만들어야 되고 어떤 거는 그대로 보내야 되고 이럴 거 아니에요?
그쵸? 여기 지금 이게 곱하는 걸 비슷하잖아 그 웨이트라고 그러는 거죠.
그쵸 제가 여러분 주소 적을 때는 5표 0.05만 곱한다니까 진짜 만들 거예요.
굳이 0.5만 곱하는 거지 그거를 100으로 만들어 버리면 이상했잖아요.
수동을 열심히 해야겠지 아무것도 안 하고 그렇죠 아니 그런 식으로 이제 뭔가 중요한 거는 열심히 여기 세게 만들고 다 여기 값을 가벼운 파라미터를 하고 있다고요.
지금 우리가 CS를 연결하고 안 하고가 다 그런 거라고요.
신입스로 연결했는데 여기 얘기 듣고 공부했다라 홍보할 때마다 이렇게 듣고 하는데 이거를 강화시켜야지 이거를 약하게 만들어야지 이러고 있는 거라고요.
여러분이 알겠죠.

참석자 1 29:40
그리고 그게 웨이트 값을 학습시키는 거랑 비슷하다고 그리고 또 중요한 거는 그냥 그런 다음에 이제 다 오르는 신호들을 다 합해요.
다 해 다 합한 다음에 그거를 또 여기다 이걸 적당한 값으로 이제 뭔가 트랜스포드를 뭔가 변환을 시켜야 되는데 그거를 이제 액티베이션 활성화 함수라고 불러요.
활성화 함소가 너무 신호가 이제 좀 미미한 거는 그냥 또 없애버리는 게 나을 수도 있고 그렇죠 어떤 신호는 더 증폭시킬 수도 있고 그렇죠 어떤 바운더리 안에 넣어야지 모든 신호가 다 크게 하면은 이제 어떻게 되냐면은 자폐 여러분 알아요.
아니면은 조울증 신호가 막 이상하게 갑자기 막 갑자기 귀에 뭔가 환청이 들리고 정신이 하나도 없는 거예요.
왜냐하면 그게 차단이 안 되니까 그게 액티베이션 포션이 제대로 되는 게 아니고 그래서 약을 먹으면 좋아지는 것도 있는 거지.

참석자 1 30:26
진짜로 그래서 이게 어쨌든 인공신경망 그러니까 ADHD도 그래 ADHD도 지금 뭐냐면은 이게 이 활성 감소가 제대로 안 돼가지고 그냥 막 하고 싶은 대로 하는 거죠.
그렇죠 나는 마음은 이제 하지 말라고 그러는데 이쪽에서 판단이 안 되고 막 움직이고 난리가 아닌 거죠.
그렇죠 점점점 이제 나이가 들면 학습이 돼가지고 멀쩡해지는 거지.
그렇죠 그래요. 50분이 다 돼 버렸다. 어쨌든 다음 시간에 이어서 어쨌든 여기 약간 긴 테일에서 잡는 거니까 더 할게요.
여러분.


clovanote.naver.com
