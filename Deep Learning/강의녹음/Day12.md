딥러닝 day12
2025.04.16 수 오전 10:02 ・ 48분 0초
심승환


참석자 1 00:00
할 수도 있을 것 같은데 5장이랑 5 6장은 진짜 내용이 얼마 근데 5장 내용이 은근히 있어서 일반화 이런 데도 있고

참석자 1 00:13
그리고 모델 평가에서 훈련 검증 테스트 하는 거 얘기했고 그다음에 여기 원래 또 실 폴드 클로스 밸리데이션이라는 게 이거 원래 앞에 나왔던 건데 또 나오거든요.
그래서 똑같은 거예요. 넘어갈 거예요. 알겠죠 했어요.
그래서 이거는 추가로 뭔가 셔플링 몇 쪽이냐 셔플링에 대한 내용이 나오는데 정말 데이터가 부족할 경우에 또 썩고 썩고 섞어가지고 여러 번 하는 그런 걸 말해요.
그러니까 정말로 잘하고 싶어서 이러는 거죠. 몇 쪽인지는 티셔트라는 내용은

참석자 1 01:01
197쪽 197쪽에 5.2.2절 위에 셔플링을 사용한 단법 체견 및 격차 검증이라는 거 여기 있어요.
그냥 k 교체 검증을 여러 번 하는 건데 이제 그날 나누기 전에 또 섞어가지고 또 한 번 더 해보는 거죠.
왜냐하면 이게 검증 데이터 섞이는 것도 되게 다양하게 하고 싶은 거죠.
이해되죠 여러분 정말 데이터 부족하면 이렇게 해보는 거지 검증 데이터가 섞이는 것도 또 우연인데 그것도 또 다르게 해보고 싶어서 또 한 번 더 완전 섞어가지고 데이터를 하는 방식을 의미해요.
어쨌든 정말 데이터 부족한 경우가 근데 많거든 여러분들 사실 요즘에는 시간이 될지 모르겠지만 이거 안 될 것 같은데 요즘에 최신 기술들이 많죠.
이런 데이터가 너무 부족한 경우에 하기 위한 방법들이 일단 전의 학습도 그런 거고 사실은 데이터 조금 갖고 하려고 하는 거고 기존 학습도 하고 메타 러닝이라는 것도 그런 거고 메타 러닝은 학습 방법을 학습해 가지고 또 뭔가 하는 방법인데 여러 가지가 있어요.

참석자 1 02:05
그리고 믹스트오 맥스토스도 사실 그렇고 데이터 부족한 거 갖고 해결하려는 게 되게 여러 가지가 많이 있어요.
어쨌든 이건데 이것도 이제 기본적으로 케이 골드랑 토스트 플 셔플 반복 케이 콜드 겹쳐 골드도 사실 많이 사용된다는 거죠.
효과가 있을 수 있어요. 여러분 실제 프로젝트 할 때 한번 써보세요.
네 어떻게 하는 건지 알겠죠 여러분 알겠어요 요거 요거 만들기 전에 이거도 만들었잖아요.
이걸로 끝나는 게 아니라 또 막 섞어가지고 또 이 짓을 하는 거예요.
알겠죠? 이게 두 번 할 수도 있고 세 번 할 수도 있고 반복 세수를 교과서에서 p라고 해놨지.
그쵸? p 곱하기 k이게 되겠지 그쵸 그만큼 모델이 많이 만들어지는 거지 그렇죠 훈련을 여러분이 해 굉장히 시간이 많이 걸리겠죠.
근데 시간이 많이 걸려도 비용이 많이 많이 든다고 했지만 데이터가 워낙 적어가지고 금방금방 끝나는 놈이라 이러고 있는 거야.
알겠죠?

참석자 1 03:00
그래요. 그다음에 5.2.2절 5.2.2절에서 상식 수준의 기준점 넘기라는 게 있죠 그렇죠 예.
상식 수준의 기준 수준 이게 영어로도 되게 중요한데 기준점이 이제 베이스 라인이 베이스 라인 베이스라인이라는 말을 써요.
여러분 기준점 베이스 라인 현지 치료는 커먼 센스 커먼 센스 커먼 센스 베이스 라인이라는 거를 넘어야 되는 건데 그래서 전에 전에 제가 얘기했듯이 그리고 제일 이걸 기억하면 좋지.
그리고 그게 많지는 않아요.

참석자 1 03:41
교과서에 어디 나와 있냐면 예를 들어 엠리스트 왜 없죠?
여기 여기다 이진 분류 문제에서 90% 샘플이 a 클래스고 10%가 b에 속한다면 항상 a로 분류하는 맨 마지막 문단에 있어요.
197쪽에 그렇죠 전에 얘기했던 거죠. 그쵸 제가 말로 떨어졌던 거 데이터가 90%가 있으면 그렇죠 90%가 원래 90% 확률로 있는 샘플 샘플이 90% 확률로 있으면 무조건 a라고 하는 놈이 항상 0.9에 정확도가 나올 거 아니에요 그쵸 잘하는 게 아니지 베이스 라인이 원래 0.9인 거지 이거는 그렇죠 이거보다 잘해야 되는 거예요.
그쵸 오해하면 안 되지. 그쵸 이따가 10분 뒤에 맑을 거야.
이거 예측하는 거는 거의 100%인데 그쵸 이해되죠?
여러분 뻔한 걸 베이스 라인이 되게 중요하죠. 베이스 라인 베이스 라인을 여러분 오해하면 안 되지 그쵸 굉장히 항상 베이스 라인이 이진 분6이라고 해서 0.5가 아니야.
그쵸 이해되죠 여러분

참석자 1 04:45
남자 고등학교 가가지고 남자 남자 여자 맞춘다고 그러면서 정말 남자라고 그러면 거의 다 맞추잖아요.
여러분 무슨 얘기인지 알겠죠? 피부 잘하는 거야 그쵸 근데 그런 개가 마치 두 문제 카리스마 있는 사람들이 그러면 안 돼요.
여러분 그렇죠 그다음에 198쪽에 넘어가서 또 조심해야 될 거 데이터가 대표성 있어야 된다.
데이터가 여러분 잘못된 데이터 학습하잖아요. 그쵸 대표성 없는 데이터 숫자를 배우는데 교과서에 나와 있는데 이거 뭐 훈련 세트 테스트 세트 만드는데 훈련 세트에는 영 칠만 담고 테스트 때는 팔고만 담는 그런 경우도 있을 수 있어요.
여러분 진짜 완전 발못됐죠 그쵸 이렇게 학습하면 곤란하잖아요.
그렇죠 공부를 이상하게 한 거죠. 그렇죠 데이터를 잘 줘야 돼요.

참석자 1 05:34
그렇죠 그다음에 시간의 방향이라는 거는 이것도 굉장히 여러분 진짜 제가 누가 졸업 논문 심사하러 박사 전업도 보면 어느 대학교 심사하러 가는데 이러고 있는데 진짜 학기를 맞더라고 시간의 방향인데 과거로부터 미래를 예측하는 건데 미래 데이터를 썼어요.
학습할 때 그래놓고 미래를 예측해야 그러면 안 돼요.
여러분 그쵸 주식 있죠? 여러분 맨날 백 트레킹이라는 거 하잖아요.
그거 잘될 수밖에 없어요. 그렇죠 미래는 몰라 그래요.
그래요. 여러분 주식 그거 맨날 논문 나오는데 사실 좀 그래 그렇죠 그렇다.
원래 이거 당연한 건데 사실 뭐 알겠죠? 여러분 그래요.
절대로 미래를 보고 데이터를 학습하면 안 되지. 이제 지나간 거 있으면 그렇지 날씨 일기 예보 예측하는 것도 마찬가지지.
그쵸 미리 거를 학습시킨 다음에 그거 다시 맞추게 하는 거는 잘못된 거예요.
그쵸 아무리 정확히 그 데이터를 안 썼다고 하더라도 그렇죠 그래요.

참석자 1 06:34
그다음에 왜냐하면 그건 있을 수 없는 일이니까 그다음에 데이터 중복을 같은 데이터를 2개씩 있으면 안 되죠.
그렇죠 여러분 그렇죠 비슷한 거 써야지. 그렇죠 데이터를 부족할 때 나중에 데이터 늘리는 방식이 있는 데이터 오그멘테이션 해서 늘리는 방식이 있는데 똑같은 데이터를 다시 쓰면 안 되고 그런 것들을 바꿔서 써야 돼요.
그렇죠 똑같으면 조금 당연히 잘 풀 거 아니야 그쵸 그래서 그렇게 안 된다는 얘기를 하고 있어요.
당연한 얘기들이지만 이거 해놨어요. 그리고 사람들이 은근히 많이 간과하고 있어요.
정말 여기도 여러분 옛날에 진짜 이런 옛날이 아니라 가끔도 지금도 이런 논문이 그냥 막 실려 있어요.
SCI 논문에도 마찬가지고 그러니까 거기 숨었다고 보는 거냐 아니지 여러분 인스터 디스플리너리 저너리 그런 거 나와요.
엑세스 같은 거 mdpi 여러분 논문 볼 때 저널이 어딘지 잘 봐야 돼요.

참석자 1 07:27
저널이 리뷰가 제대로 안 되는 저널이 상당히 있어서 그다음에 이제 5.3절에서 훈련소 향상하기 여기 내용 들어가면은 어쨌든 중요한 거 맨 첫 번째 줄 여러분 199줄 맨 첫 번째 줄 최적 적합을 얻기 위해서는 먼저 과외 적합이 되어야 된다는 거 있죠 여러분 과외 적합이 돼 봐야지 이제 적이 적합이 어디서 끝내야 될지 알잖아요.
여러분 이제까지 계속 그랬죠 4장에서도 그렇죠 거기까지 넘어가 봐야지.
이게 최적 적합 찾을 수 있는 거잖아요. 그쵸 그 얘기예요.
여러분 경계가 어딘지 알 수 없으니까 밑에 넘어가 봐야 되는 거지.
그쵸 그렇죠 예 그래요. 그리고 이제 가래 적합하고 싸워야 되는 거죠.
그쵸 가래 적합까지 가면 안 되는 거지 그런 거예요.
그리고 중요한 게 이제 여기 보면 일반으로 일반적으로 문제 발생하는 게 이제 세 가지를 적어놨는데 훈련이 안 되거나 훈련이 됐는데 일반화가 잘 안 돼.
그러니까 밸리데이션 로스가 크다는 거죠.

참석자 1 08:32
그렇죠 그다음에 세 번째는 베이스 라인을 뭐야 여기 여기 세 번째가 어디 저렇게 했어 순서가 있나 계속 가슴 적합이다.
가소 적합이 계속된다는 것도 사실은 결국은 훈련이 되는 기준점은 넘어섰지만 가소 적합인 상태다 이런 거죠.
그렇죠 약간 이런 세 가지가 있어요. 여러분 일반적으로 그래서 이런 거 해결하기 위해서 이제 여기 5.32 1절이랑 2절이 있는 거죠.
그래서 첫 번째 나오는 게 나오는 게 이제 여러분들 공부할 때 항상 공부 어떻게 하느냐 항상 뭐 전체적으로 뭐 뭐가 있는지 봐야 돼요.
그렇죠 일단 5.4.1절은 병사가 보면 핵심 토란테 튜닝학이고 그다음에 206쪽에 5.3절 2절이 구두에 대해 더 나은 가정 하기 이렇게 되어 있겠죠.
그렇죠 무슨 말인지 모르겠지 그다음에 5.3.3절이 용량 늘리기 이렇게 돼 있죠 여러분 그렇죠 여러분 이게 다예요.
그렇죠 그래요.

참석자 1 09:40
이거에 대해서 먼저 여러분 좀 깰 필요가 있겠지 여러분 시험을 잘 보는 게 중요한 게 중요하지만 시험이 문제가 이상할 수도 있어요.
그쵸 저는 여러분 큰 그림 되게 잘 찾는 거 중요하게 여기거든요.
여기 세 개의 절이 있잖아요. 그거에 대해서 파악하는 게 중요하다고 알겠죠.
세 가지가 있다는 거 그쵸. 그리고 여러 미리 얘기하면은 5.3.2절의 핵심 파라미터라는 거는 같은 그림으로 먼저 이게 학습 러닝 레이트랑 배치 사이즈에요.
여러분 적어놓든지 여기 보면은 199쪽에 바로 그냥 두 번째 줄에 학습 요가 배치 q기획이라고 돼 있잖아요.
그쵸 199쪽 아래에서 두 번째 줄에 학습 효과 배치 크기 나와 있죠.
학술명 크기 그게 그거예요. 여러분 신 프라미터는 다른 거보다 이게 이게 핵심이에요.
여러분 이 두 가지 잘 조정하는 거예요. 알겠죠?

참석자 1 10:34
그거랑 그다음에 먼저 큰 그림 5.3.2절은 10 201조 5.3.2절은 구조에 대해 더 나은 가정 이렇게 적혀 있는데 이게 뭐냐 구조에 더 낫다 이렇게 모델 자체를 바꾼다는 얘기예요.
모델 자체를 여러분 사실 이거는 지금 배우면 좀 약간 이상하긴 한데 그러니까 이거 사실 어떤 식으로 할 수 있냐면 구조에 더 나은 과정이라는 게 아예 선형 일기를 쓸 거냐 딥러닝을 쓸 거냐의 문제부터 시작한다고 알겠어요.
그러면 딥러닝으로 갈 거냐 말 거야. 딥러닝으로 왔으면 지금 우리가 배운 폴리 컬렉티드 네트워크 있죠 이런 거랑 또 뒤에 뒤에 안 배웠다는 거 맞죠?
제가 그렇죠 CNN이 이런 거 있다고 그랬잖아요.
그쵸? 트랜스포머도 있고 막 그렇거든요. 어텐션 뭐 이런 거 있거든요.
그럼 모델을 뭘 쓸 거냐에 대한 문제예요. 여러분 알겠어요 모델 선택의 문제야 알겠어요 아직 잘 안 배웠지만 근데 모르는 선택이 여러분 SVL 이런 것도 있고 있죠.

참석자 1 11:25
여러분 디시전 트리도 있고 랜덤 폴리스도 있고 그냥 머신러닝 거 쓰냐 딥러닝을 쓰냐 딥러닝 중에서 뭘 쓰냐 이런 문제라고요.
알겠어요 여러분 이해되죠 그래요. 그다음에 5.3.3절은 뭐겠어요?
모델 용량 연이 이거 파라미터 배수가 부족한 것 같으면 더 늘려야 될 거 아니에요?
그쵸 여러분 사실 지금 딥러닝이 잘 되는 것도 파일 형태가 많아서 그런 거잖아요.
여러분 오픈 AI 채색 피칠하면 잘하는 게 오늘 엄청 많이 써가지고 그렇잖아요.
그쵸 그래요. 그렇죠 용량이 많다는 건 파라미터가 많다는 거예요.
그쵸 하이트 많으려면 층을 많이 하든지 유전 개수 늘리든지 그런 거죠.
그쵸 그 얘기예요. 여러분 알겠죠 그게 다예요. 알겠죠?
됐죠 그러면 5.3.1절 다시 볼게요. 여러분 이거 공부할 때 항상 DFS 한 다음에 DFS 하세요.
여러분 DFS 하고 DFS 하라고 DFS만 계속하면 안 돼.
그렇죠 내가 뭐 하는지 몰라 했나요? 그렇죠 DFS 하고 DFS 해 알겠죠?

참석자 1 12:25
이게 이게 뭔지 모른다 2학년을 안 들었나 그런 거죠.
그렇죠 여러분 레스퍼 퍼스 서치하고 베스퍼 서치해야 돼요.
그래서 지금 데스 DFS 했고 지금 다시 DFS로 5점 3점인지 봅시다.
그래서 여기 이거는 여러분 여기 적혀 있는 게 여기 이제 다 좋은 말인데 이 보면은 여기 보면은 이게 두 번째 단락에 이런 상황이 발생하면 잘 안 돼.
확실히 잘 안 되는 거예요. 경사 급부 과정의 설정에 문제가 있는 거고 그쵸 두 번째 단락에 어디 있는 거야 여기 있다.
경사 학원법 과정의 설정에 문제가 있고 옵티마이즈 선택 모델 소개가 분포 학습 칙 네 가지 나와 있죠.
그렇죠 바로 문장에 옵티마이저 여러분 맨날 교과서는 아리 스트라 쓰고 있는데 그쵸 아담도 있고 있다고 그랬잖아요.
그쵸 그거 여러 가지 써봐서 잘 되는 거 쓰면 되는 거고 SGD 있을 수도 있고 그래요.

참석자 1 13:19
그다음 가중치 초기값 포포 있죠 이거 요거 여러분 이거 한 번도 우리 안 했죠.
모델 가중치 초기 값을 우리 만드는 거 한 번도 안 했잖아요.
여러분 맨날 랜덤하게 추해야 되는데 이거는 지금 사실 이거 되게 중요한 내용인데 우리 교과서는 잘 안 나오죠.
벌써 이게 학문적으로 익어서 원래 영어로 투기하면 망하는 거예요.
영어로 전부 다 영어로 초기화하면 망한다는 게 유명 저널이 되게 많아요.
논문이 그리고 근데 옛날 사람들이 모르고 영어로 초기화 했어.
전부 다 영어부터 시작한 거지. 근데 그러면 안 되고 이상한 지점이 많아지는 거 봤죠 여러분 전에 플래투니 아니면 이렇게 여러분 로컬 아티머 하면 되고 글로벌 아티머 안 되는 거 봤잖아.
그림이 이렇게 생겼을 때 이렇게 생기고 이랬을 때 여기서 다 이렇게 여기서 왔으면 여기까지밖에 못 가잖아.
여기서 시작해야지 여기 갈 수 있지 절대로 여기서 못 벗어나요.
여러분 그렇죠 초기 값이 무조건 여기서 시작하면 안 돼.

참석자 1 14:11
그쵸 여기서 시작해서 가면은 당연히 여기밖에 못 오잖아요.
그쵸 근데 더 가서 여기 있을 수도 있으니까 여기저기서 시작해 봐야 돼요.
그렇죠 여기서 시작하는 건 절대로 안 되고 여기저기서 시작해 봐야 되는 건데 그거에 대한 온갖 이론들이 있고 우리 교과서에는 안 나와요.
알겠죠 왜 안 나오냐면은 지금 여러분이 쓰는 라이브러리 다 해주거든 그거를 저절로 알겠죠.
근데 우리 교과서에는 안 나와 일기 교과서 되게 실용적이야.
여러분이 영어로 투기하는 미친 짓을 안 하면 돼. 알겠죠 랜덤하게 랜덤 시드 바꿔가면서 하는 거 되게 괜찮아 랜덤 시드 여러분 모델 만들 때 랜덤 제어하려고 그러면 랜덤 시드 만드는 거 있잖아요.
여러분 시드 랜덤 스 시드라는 게 있어요. 나도 해줄게.
근데 그거 있잖아요. 여러분 일단 그거라서 중요한 거는 그걸 그래서 학습률하고 배치 학습률하고 배치 크기만 거의 튜닝하는 걸로 다 해결된다.
알겠죠?

참석자 1 15:00
여러분 그래요. 그리고 그거에 대한 의미를 알아야 될 거 아니에요?
그쵸?

참석자 1 15:07
여기 보면은 그래서 200쪽에 코드가 있는데 200쪽에 200쪽에 코드가 5 7 5 8이 있는데 5 7에서 여러분 볼 거는 요 숫자 이 숫자 1.0 그다음에 58에서 볼 숫자는 요거 1 2 마이너스 2 이거거든요.
이게 뭐냐? 학습률이에요. 여러분 아티마이저에서 학습률을 줄 수 있거든요.
파라미터로 원래 여기 이제 러닝 레이트라는 파라미터가 있는 거지 이렇게 주는 간에 이렇게 생략된 거야.
알겠죠 여러분 그래요. 그래가지고 다른 거 이거밖에 없어요.
여러분 코드가 알겠어요 여러분 되게 뭐 말인지 알겠죠 이거 공부할 때 아 이것도 교과서가 친절로 있으면 여기다 형광펜이라도 칠해주지.
그쵸 다른 건 이거밖에 없는데 이해되죠 제가 문영 체제 교과서가 좋은 게 제가 여러분 거기 참관 다 지어줬잖아요.
그러니까 얼마나 친절하셔요 그렇죠 그랬더니 이 책은 그렇지가 않아요.
그렇죠 이것만 다 도움 되는 거예요.

참석자 1 16:08
어쨌든 1이고 여기 뒤에는 0.0 이거 얼마예요?
0.01이지 그쵸? 여러분 이거 0.01인지 모르고 이러면 여러분 이번에 제 시험 망하는 거예요.
아니 정말로 인간적으로 그런 거 모르면 안 돼요. 여러분 큰일 나요 알겠죠?
1 2의 마이너스 1은 0.1인 거 알겠죠? 됐죠 그래서 0.01 정도가 이제 괜찮다는 거지 일은 너무 커요.
알겠죠 그래서 어쨌든 지금 학습이 잘 안 되는 상황이면은 학습률을 낮추거나 높이는 거 있죠 여기 보면은 여기 200조 세 아래에 분리 학습률을 낮추고 높이는 거 알겠죠?
학습률을 낮추거나 높이는 건 뭔지 사실 설명 되게 많이 했지 그치 여러분 학습률이 낮으면 뭐가 어떤 일이 높으면 어떤 일이 일어나는지 여러분 되게 많이 했잖아요.
그렇죠 알죠 공부하세요. 여러분 그다음에 시험에 나와요.
여러분 그거 모르면 여러분 이상한 거야 알겠죠?

참석자 1 17:01
그다음에 201쪽 201쪽에 배치 크기 배치 크기 배치 크기를 보통 학습이 잘 안 되면 줄여야 되나 감소시켜야 되나 늘려야 되나 늘려야 돼.
학습이 잘 안 되면 왜냐하면 그거를 좀 너무 프로젝트 업데이트할 때 너무 적은 데이터를 써가지고 문제 되는 거니까 실수 잘 안 되고도 늘리는 게 검사이에요.
알겠죠

참석자 1 17:27
그래서 여러 가지 해보라는 거지 다 안 되면 알겠죠.
그리고 그거 이게 이게 기본이고 모델이 멀쩡하다면 5.3.2절은 모델이 이상할 수 있다는 5.3.2 201쪽 5.3.2절 그다음에 이제 모델에 대한 나중 부장 제가 제가 기본적으로 여기 지금 단락이 여기 이게 이 이쪽에 카메라 둘 셋 넷 다섯 개가 있죠 여러분 그쵸 근데 핵심적인 단락은 세 번째 네 번째 단락이에요.
여러분 세 번째 네 번째 세 번째 네 번째 1번 2번 동그라미 1번 1번 표시하세요.
여러분 완전히 무슨 학원 강사 같은데 좀 너무 정리 안 해 주셔서 좋은 내용이 많은데 약간 좀 알아서 찾으라 하시는 데 있어.
그래서 이게 뭐냐면은 먼저 단순하게 이렇게 돼 있는 문장 있죠.
먼저 단순하게 비교 했어요. 여기 요요기 먼저 단순하게 요기 요 문장이라 그다음에 어디 갔어 여기구나 현재 사용하는 모델의 종류 두 가지예요.
여러분 요거 요거 요거 요게 핵심이에요. 알겠죠?
그래요.

참석자 1 18:34
그래서 핵심이 뭐냐면은 여기 입력 데이터가 충분하지 않다.
피처가 충분하지 않다는 거지 이게 중요하다는 거예요.
여기는 아까 이제 모델 얘기만 했는데 사실은 이것도 있는 거지.
피처가 충분하지 않아 여러분 뭐냐면은 이거 진짜 중요한데 주식을 예측하겠다고 할 때 사실은 여러분 주식에서 우리 주식이 불가능하다는 걸 얘기하고 싶은데 정치적인 이유 트라프가 자꾸 뭐라 그러면 바뀌잖아요.
여러분 그게 입력 데이터에 들어가냐 안 들어가냐 되게 중요하겠죠.
그쵸. 근데 트럼프 생각을 어떻게 알아? 우리가 못하지 그러니까 불가능한 거야.
사실은 이해돼요. 여러분 그거 하겠다는 게 뭐 이상한 거지 사실은 이해돼요.
여러분 그래요. 아니 그게 중요한 피처가 입력이 없는데 어떻게 그걸 하냐고 그쵸 그래요.
우리나라 주식들도 마찬가지 작전 세력 같은 것도 갖고 들어오고 싶고 들키지도 않은 거 되게 많은데.
그쵸.

참석자 1 19:26
그래서 저희 협상 기술이라는 드라마가 있는데 그 마지막 해 봤는데 진짜 좋더라고 마지막 회가 거기에 작전이 어떻게 이루어지는지를 잘 설명해 주더라고.
되게 좋더라고. 어쨌든 나중에 그 협상 기술 마지막 회만 여러분 한번 보고 진짜 당하지 않기 위해 주식을 여러분 어릴 때 많이 할 것 같아요.
진짜 이거 찾기 힘들겠더라고요. 안 걸리고 넘어가니까 엄청나게 많을 것 같아요.
그래서 그런 것 때문에 그건 입력 피처에 넣을 수가 없는 거야.
이해돼요. 여러분 그런 거는 못하는 거지 못하는 문제가 있는 거지.
그렇죠. 여러분 이해되죠. 그래요. 그래서 용감하게 다 해보겠다고 그러는 게 어떻게 돼?
그쵸. 입력 데이터에 회계 출 정보가 충분하지 않을 수 있다는 거 이거 이거 되게 중요하다고 알겠죠.
그냥 이거는 그다음에 너무 당연한 거지만 사실 은근히 많이 용감하게 그냥 뭘 해봐 그쵸 없어 의미가 없어.

참석자 1 20:09
그렇죠 그다음에 두 번째 두 번째는 제가 얘기하는 모델 모델의 종류 맨날 댄스만 썼는데 이거 말고 여러 가지 RNN 나중에 배울 거 CNN 이런 거 있죠 여러분 뭔가 적합한 걸 써야 되고 옛날에 뭐야 자연어 처리가 그렇게 안 됐는데 트랜스포머 나오고 나서 너무 잘 되거든요.
트랜스포머 안 쓰면 결국 잘 안 되는 거야. 아니 그러니까 왜냐하면 자연어 처리가 그렇게 안 되다가 어텐션 나오고 다 되기 시작했어요.
아니 그러니까 내가 말은 지금 자연어 처리하는데 또 이상한 걸로 쓰는 건 미친 짓이고 트랜스포머를 써야지 딥시크도 트랜스포머고 다 트랜스포머예요.
여러분 트랜스포머 교과서에 잘 나와 있어요. 알겠죠?
그래요 갑시다. 이해되죠? 여러분 이상한 모델 쓰면서 안 되네 안 되네 하고 있으면 이상한 거라고 알겠죠 이미자 처리하는데 그냥 데스 네트워크로 한다.
그것도 이상한 거지 리스트 데스 네트워크 쓰면 안 되고 CNN 다 해야 돼요.

참석자 1 21:00
그건 당연히 2차원 정보를 사는데 왜 CN을 안 쓰고 댄스를 쓰냐고 처음부터 이상한 거예요.
그런 거를 이제 알고 아직은 잘 몰라요. 여러분 알겠죠?
그다음에 5.3점 3절은 우리 202쪽 202쪽은 202쪽이 얘기하는 거는 여기 202쪽에 여기 202쪽에 코드 59 있죠 코드 59 보면은 여기 지금 모델이 모델이 봅시다.
여러분 모델 이렇게 적혀 있죠 이렇게 돼 있어 액티베이션 소프트믹스로 끝났죠 됐어요.
그쵸 여러분 여기 지금 여러분 소프트맥스 함수는 여러분 난 리니어 펑션이 아니에요.
아니 이게 이거는 그냥 라지스트 리그렉션으로 끝나 난리 역이 맞긴 맞는데 이거는 출력 대칭 내는 거죠.
여러분 그렇죠 출력만 내는 거잖아요. 그러니까 이건 라지스트 리그렉션이에요.
사실은 여기 적혀 있지 교과서에 라지스 리글진이라고 적혀 있죠.
이거 이렇게만 모델을 만들면은 이거는 히든 레이어도 없고 그냥 출력 레이어만 덜렁 있잖아요.

참석자 1 22:01
입력해서 그쵸 라디스 이뮬레이션 맞죠? 여러분 그쵸 여기 만약에 액티베이션 이거 없어 그 댄스가 1 1이야 10을 할 수도 있지 이러면 여러분 뭐예요?
이거는 이거 이렇게 하면 이거 이거 액티베이션이 없어 이러면은 리뉴얼 리그레션이고 그쵸 액티베이션이 소프트맥스나 10개인데 하나 하나인데 하나이면 이제 여기 소프트 시그모드 하면은 시그모이드 하면은 라지 리글렉션이고.
그쵸 얘도 라이즈 리그렉션이고 이해돼요. 여러분 뭔지 잘 모르는구나 잠깐만요.
요거 여기 10이 나왔죠 10 10이 나오면 10개의 숫자가 튀어나와요.
그쵸 원래 라짓이야. 근데 그거는 라지 실수들이라고.
근데 소프트맥스를 통과시켜 그러면 전부 0하고 1 다 더해서 0하고 1 사이가 다 더해서 1이 되고 모두 숫자가 0하고 1 사이가 되는 거죠.
그쵸? 확률이 나오게 되는 거예요. 그쵸? 이것도 라지스트 리그렉션의 일종인 거예요.

참석자 1 22:54
여러분 라지스트 리그렉션을 멀티플 밸류를 위한 라지스트 리그렉션이기 때문에 아스트리게이션 별 게 아니라 다 모든 숫자 라짓을 0하고 인사로 만들어주는 거잖아요.
코드로 만들어주는 거지 라짓을 실수로 표현할 확률이라고 생각하면 돼요.
그쵸 알겠어요 여러분 라짓을 소프트 베스트 통과시키는 거지.
그쵸 라티스 취급으로 통과시키는 거고 그 이걸 다 라티스 킥이라고 불러요.
알겠죠 이거는 딥러닝이 아니야. 그렇죠 이것만 갖고 안 되는 거 많다는 거지.
이렇게 하면 절대로 안 되는데 결국 용량을 늘리려고 그러면은 결국 뭐 해야 되냐 여기다가 앞에다가 추가해야지.
그쵸 데스 데스 이런 거 그쵸 알겠죠? 여러분 그래요.
그다음에 이제 5.4절 갑시다. 여러분 5.4절은 204쪽 리뷰 안 하고 다시 복습 안 하고 넘어가도 되겠죠.
여러분 기억하세요.

참석자 1 23:49
여러분 다시 원래 내 업 해야 되는데 지금 제가 그냥 알겠죠 여러분 뭐 하는지 훈련 잘 시키려고 그러면 세 가지를 연겨둬라.
그쵸 되게 되게 주옥 같은 얘기들이에요. 여러분 알겠죠 알겠죠?
여러분 그래요. 그다음에 5.4절은 일반화 성분 향상시키 향상하기 위해서 또 또 보면 또 데스커스로 5.4.1은 데이터 세큐레이션 5.4.2 15점 특성 공학 그다음에 5.4.3은 조기 종립 5.4.4는 모델 기준으로 적혀 있죠 그쵸 그래요 여기 다 금방 금방 금방 끝나는데 오토 4.4절이 좀 길어.
그쵸 오토4.4절은 전에 제가 한 번 보여준 적 있는데 레글러라이제이션이라고 했죠.
좀 내용이 많아요. 그쵸 그래요. 그래서 보면은 오픈 4.1.1부터 봅시다.
여기 데이터 세큐레이션은 데이터가 큐레이션이 여러분 뭔가 잘 조직하는 거잖아요.

참석자 1 24:50
여러분 큐레이터라고 해서 전시회 막 꾸미고 이런 사람이죠.
그쵸 뭘 구성할지 데이터를 어떤 걸 가지고 데이터 큐레이션이라는 건 데이터 입력에 훈련을 사용한 데이터랑 테스트를 사용한 데이터를 잘 수집하는 걸 의미해요.
알겠죠 데이터 수집을 잘해야 돼. 그렇죠 일단 그렇죠 여러분 그리고 그래서 여기 뭐냐면 205쪽 맨 위에 있는 글 볼까요?
여러분 205쪽 여기 이게 되게 많은 경우에 데이터가 좋은 경우가 모델을 열심히 예쁘게 만드는 게 중요한 게 아니라 모델은 사실 이미지 다 잘 돼 있는 거 많고 데이터셋이 좋은 게 보통 학습이 잘 되는 경우가 많다는 거죠.
그쵸 그쵸 여러분 딥시크 있잖아요. 그거 되게 좋았잖아.
모델 되게 작은데 그거 왜 그랬는지 여러분 유명하잖아요.
사실은 걔가 채채 피티한테 맨날 데이터 얻어왔어.
데이터 씨 너무 좋았어.

참석자 1 25:45
챗gpt는 인터넷에서 막 이만한 데서 데이터 긁어가지고 했는데 걔는 챗gpt한테 맨날 뭐 물어가지고 다음에 답 뻗어서 학습했거든 사실은 그래서 챗gpt가 또 이제 화나가지고 약간 이해됐죠.
여러분 데이터셋이 수집을 사실 그래서 딥시크는 채티피 때문에 잘 된 거야 사실은 그래요 안 비밀 그쵸 비밀 아닌 거지 다 알아 그래요.
그래서 이제 CGPT가 이제 막았다잖아요. 특정 사이트에서 d에서 많이 가져가는 거 일반 메인 막았대요.
이제 왜냐하면 억울해서 그렇죠 나는 그렇게 힘들겠는데 그래서 어쨌든 그런 게 있는 거지 그다음에 그다음에 여기 뭐냐면 여기 이따은 처음에는 불가능해 보이는 문제가 대응 네티스를 해결한다는 게 분리 첫 번째 맨 마지막 문제 분리 첫 번째 아까 방금 그 문장 밑에 이다은 처음에는 보이죠.
필리 첫 번째 마지막 문장이 그렇죠 대용량 데이터가 데이터가 대용량이든지 데이터가 되게 좋던지 그쵸 그것도 여러분 알고 있어요.

참석자 1 26:47
근데 이거를 진짜 딥시크가 똑똑하게 한 거지 사실은 어쨌든 막아놓은 것도 아니었는데 그다음에 특성 공학은 피치 엔지어링이라는 건데 5.4.2절 피처 엔지니어링 유명한 표현인데 피처 엔지니어링은 여기 뭐 예를 들어서 아니 이거 뭐 이렇게 해서 몇인지 맞추기 하는 것보다는 이렇게 그냥 x축 y축 여기 얘네들 잡혀 주는 게 더 좋던지 아니면은 그냥 축에 반한 각도를 주든지 그렇죠 이런 훨씬 더 쉽잖아요.
그쵸 대기 바로 되잖아요. 여러분 이러면 바로 산수로 되지 그쵸 수리하는 각도 안 주면 돼 그렇지 사실은 금방 나오잖아.
그렇죠 그럼 굿모닝도 필요 없어. 그래서 특성이 좋은 게 좋은데 그래서 이게 좀 재미있는 거는

참석자 1 27:42
뭐 그랬지 딥러닝 이거 어디 있냐면 이 단락이 206쪽에 맨 마지막 단락 볼래요?
여러분 여러분 다행히 최신 딥러닝 대표성을 필요하지 않는 요거 있죠 206쪽이 맨 마지막 단락 시작 단락을 시작하지 다행히 최신 딥러닝은 206쪽 맨 마지막 단락 최신 딥러닝 대부분 특성 보강이 필요하지 않대.
그 위에 단락이 딥러닝 이전에는 딥러닝 이전에는 수성 음악이 중요했다고 그쵸 딥러닝 전에는 맨 마지막 단락 바로 위에 딥러닝 이전에는 특성 공강이 중요했대.
그쵸 딥러닝 이전에는 그쵸 근데 맨 마지막 단락에서 딥러닝에서는 특성화가 필요하지 않다고 나와 있죠 그쵸 근데 진짜예요.
여러분 오히려 우리가 특성을 자꾸 만들어주는 게 더 잘 못할 수가 있어요.
예를 들어서 아타리 게임하는데 아타리 게임 벽돌 게임 있잖아요.
특성화합 하면은 벽돌의 위치 같은 거 이런 거 막 데이터를 만들어 줄 수 있잖아요.

참석자 1 28:38
그리고 이 내가 위치가 어딘지 뭐 이런 거 그런 줄 수 있는데 그러지 말고 그냥 이 픽셀 그냥 전체 줘가지고 하니까 더 잘해 픽셀에 그냥 다 줬어요.
그냥 거기 이미지 이렇게 줬어. 진짜 게임을 이렇게 주는 게 더 잘한다는 거지 우리가 괜히 막 이렇게 만들어주는 것보다 그냥 로 데이터를 주는 게 우리가 사람이 원래 보고 하는 데이터를 그대로 주는 게 더 많은 어려운 문제는 훨씬 잘하는 거예요.
왜냐하면 얘가 우리가 사람이 생각하지 못하는 걸 더 잘할 수 있기 때문에 그래요.
여러분 특성 보강이 별로 안 필요해. 딥러닝 쪽에서는 아니 그러니까 문제가 너무 심플하면 잘 안 그렇지만 문제가 좀 복잡해서 사람이 잘 못한다 그러면은 그냥 모든 데이터를 그대로 두는 게 낫다는 거죠.
알겠죠 다양한 데이터 전부 다 괜히 변형시키지 말라는 거지 변형시키지 말아라 알겠죠 그래요.
그렇지만 그래도 이제 빨리 락스 시키기 위해서는 여기 207쪽에 분리 2개 있죠.

참석자 1 29:37
좋은 특성을 주면은 적은 자원 그다음에 적은 데이터 보이죠.
여러분 적은 자원 적은 데이터 그쵸 적은 자원 적은 데이터 적은 자원이나 적은 데이터는 이제 모델이 약간 작더라도 아니면 데이터가 좀 작더라도 그래도 잘 되는 경향이 있다는 거죠.
열악한 환경에서는 이렇게 해야겠지 그쵸 그래요.
그래서 이게 사실 이게 딥시큐가 이렇게 한 거지. 그쵸 오픈 API는 그냥 오픈 API 오픈 AI지 오픈 AI는 그렇죠 그렇게 한 거고 없으니까.
그렇죠 그렇게 한 거고 됐어요. 그다음에 5톤 4.3절은 조기 종료 얘기인데 조기 종료 얼리 스타핑은 이거 뭔지 알죠?
여러분 배웠어요. 저한테 그쵸 근데 이거 사실 맨 마지막 달 맨 마지막 줄에 7장이라고 돼 있어.
7장 7장에서 7장에서 콜백 쓰는 걸로 7장에서 콜백 사용한다는 거 여기에 복사서에 여기 강의 자료가 없네.
7장에 콜백이 있을 거 있어요.

참석자 1 30:42
여러분 7장에 거기까지 시험 범위를 넣고 싶은데 오늘 여기까지 해주려고 직장 선배 이거 얼리스타핑 여러분 이제까지 했던 거 어떻게 했어요?
그냥 할까? 콜백 안 쓰고 어떻게 했어요? 여러분 눈으로 보고 막 그냥 그렇죠 이상했잖았죠.
그렇죠 그 자동으로 될 텐데. 그렇죠 당연히 자동으로 되는 게 있지.
콜백이 있다는 거를 배워놔야 될 것 같아. 그쵸. 그거까지 진도 나가려고 그래요.
안 돼도 알겠죠 여러분 미안해요. 이거 하는 게 좋아.
미안해요. 여러분 이거 아니 같이 배우는 게 좋지 시험 볼 때 시험 보면 확실히 뭔가 이렇게 여러분 뭐 해요?
이게 프리지가 되잖아 그 시리즈가 되잖아. 시험 볼 때 있는 게 좋지.
그래서 별로 안 남았어요. 갑시다. 됐죠 7장에 5대 95까지 해주라고 딴 거 안 해도 그다음에 모델 규제하기 내용은 이거 저번에는 사실 이 영어로 여러분 규제하기가 레주러 라이제이션 레그 라이제이션 번역하면 규제라는 거 알아야 돼요.
여러분 알겠죠?

참석자 1 31:45
그래요. 그다음에 이거 규제하는 거는 여러분 모델의 너무 용량이 너무 모델이 뭔가 좀 복잡해지는 걸 막는 거지.
그쵸 모의 복잡도를 줄이려고 하는 거예요. 그쵸 그쵸 여러분 이거 학습이 잘 안 되면 모델 용량 늘리는 거 5.3.3절이랑 반대되는 거예요.
여러분 이해돼요. 여러분 5.3 202쪽에 모델 용량 늘리기랑 반대되는 내용이에요.
이거는 알겠어요. 여러분 얘는 모델을 어떻게든 좀 간단하게 만들려고 그래 왜냐하면은 일반화가 잘 되려고 그러면 모델로 복잡하면 안 되는 거 알잖아요.
어느 정도 복잡해야지 또 학습이 되는 것도 있지만 되게 힘들잖아요.
여러분 적당히 해야 되잖아. 그 얼리 스테핑 하고 이러는 거잖아요.
그쵸 이거 이거 이거 여러분 제 강의 자료 이거 다시 한 번 이거 이거는 여러분한테 보여드리겠어요.

참석자 1 32:35
강의 자료 35페이지 그래서 트윈 베이스에서 개발하는데 왜 안 떠 이 이 분이 너무 좋아 나는 이 분이 너무 사랑스럽게 이거 보면 진짜 마데카폴리스트가 너무 높으면 여기 적당한 여기 입지 지점 차야 되잖아요.
너무 이게 이거 이거 막으려고 규제하는 거고 여기서 또 여기서 멈추려고 얼리 스타킹 하는 거고.
그쵸 근데 어쨌든 여기서 멈추면 안 되잖아요. 이렇게 그래서 모델을 계속 늘려야 되는 거고.
그쵸 이 그림 그쵸 알겠죠 이 그림이랑 또 여러분 여기 이게 에러가 y 축인데 여기가 4인 것도 있죠.
그쵸 그거는 이제 밸리데이션 로스랑 트레인 루스 같이 해가지고 그쵸 달라요.
그렇죠 여기 복잡도고 그래요. 여러분 달라요. 그림 그다음에 그래서 x축이 뭔지 항상 여러분 설명하고 봐야 돼.
그쵸 그래프에서 그래프 설명하는데 x축 뭔지 설명 안 하고 시작하는 것도 되게 이상한 거지.
x축이 아무것도 안 적혀 있어 그때 미치는 거지. 그렇죠.

참석자 1 33:33
진짜로 저는 제가 여기 또 학회 갔는데 x축에 안 써 있길래 그거 뭐냐고 물었더니 잘 모르겠대요.
진짜 그 사람이랑 놀지 말아야지. 그렇죠 그런 거죠.
여러분 그래요. 그래서 심지어 박사야 그래서 여러분 박사가 이상한 사람들이 와서 그래서 계속하면 무대 규제하기에 208쪽 넘어오면은 모델 교재에서 지금 여러 가지가 제가 여기 적혀 있는데 과대 적합 완화시키는 방법은 여러 가지 모델 크기 줄이고 막 막 있잖아요.
그쵸 이 문장 있는데 복잡해서 이거 여러분 읽어보시고 결국은 얘기하고 싶은 거는 여기서 이게 여기서 이렇게 해야겠다.
너무 작은 모델은 반대적합되지 않는다면 이미 배웠다고 적혀 있죠.
여러분 그쵸 그 부분 너무 작은 모델 모델 규제가 내용이 어떻게 이걸 어떻게 정리해야 되냐 전에 했었나 내가 이거 어디 교과서에 이게 208쪽에 세 번째 단락이 너무 작은 모델은 가제 접촉되지 않는다 이렇게 적혀 있죠.

참석자 1 34:51
그래서 가리자가 알아서 가장 간단한 방법은 모델 크기를 줄이는 거라고 적혀 있죠.
왜 이거를 제목을 안 달아놨는지 참 불만인데 나도 그래요.
그래서 이게 모델 규제라고 적혀 있는데 이거 정리를 사실은 제가 저 나름대로 슬라이드 만들까 하다 말았는데 합시다 합시다.
여기 여러분들 1번 2번 3번 알려줄게요. BFS로 해야 되니까 항상 여기 여기 요거죠.
요거 요거 요거 여기 여기 적혀 있죠. 그쵸 모델 크기 파라미터 개수 그쵸 그래서 1번이 뭐냐면은 모델 용량 규제라고 할 수 있죠.
그쵸 용량 규제야 그쵸 용량 영어로 커피스티라고 하기도 하고 영어로 파라미터 개수를 줄이는 거예요.
그쵸 알겠죠? 여러분 이게 1번이고 그쵸 첫 번째 그다음에 여러분 볼게요.
여러분 뒤에 가서 2211쪽 211주의 가중치 규제라고 적혀 있죠.
가중치 규제 가중치 규제에 적혀 있잖아요. 가중치 규제 이게 2번이야.

참석자 1 36:09
가중치 규제가 가중치 규제는 웨이트를 레귤라제이션 하는데 모델 웨이트 레귤라제이션이라고 할 수 있어요.
여러분 알겠어요 여러분 아까 파라미터 개수를 뭔가 좀 조절하려고 하는 거였고 이거는 파라미터 값을 규제하려고 하는 거예요.
여러분 값이 너무 커지지 않게 만들려고 하는 있는 거예요.
알겠어요 여러분 그다음에 3번이 어디 있냐 3번이 드라아웃이라고 했죠.
드라아웃 3번 3번은 드라바웃이 여기 3번 이게 3번이에요.
드라바웃 드라바웃은 뭐냐면은 네트워크를 규제하려고 하는 건데 선입 덴스틀리 컬렉티트 있잖아요.
덴스틀리 컬렉트 안 하고 해보려고 하는 거예요. 좀 스팟 사이 컬리티해 보여서 모델 학습시킬 때 전부 다 웨이트 다 쓰는 게 아니라 웨이트 일부씩만 써서 학습시키려고 하는 거예요.
그러니까 이게 여러 가지로 뭔가 여러 가지 문제를 잘 풀게 하기 위해서 이게 사실 굉장히 중요한 거죠.

참석자 1 37:10
드라바웃이 이게 이게 앙상블 비슷하기도 하고 랜덤 폴리스 비슷하기도 하고 그다음에 나중에 지금 전부 다 나오는 게 사실 거의 드라아웃에서 나온 아이디어 갖고 어떤 웨이트가 바보 짓을 해도 문제 잘 풀게 하기 위해서 드라바우이 그림이 교과서에는 제대로 안 그려져 있는데 한번 보여줄게요.
여러분 너무 유명해서 드라바웃은 드라바웃이 드랍 아웃이 됐죠 드랍 아웃 이미지 보통 이렇게 여기 보면은 원래 데스트 리얼이 다 연결돼 있잖아요.
그쵸 근데 여기 일부만 연결해서 지금 여기서 반만 연결해 보통 반씩만 알겠죠 반씩만 쓰는 거예요.
노드를 학습할 때 이렇게 얘는 없는 셈 치고 하는 거예요.
얘네들만 학습되겠죠 그쵸 그래서 이게 일반화 잘하려고 하는 거예요.
머리를 얘만 일하고 또 얘만 일하고 이런 식으로 써가지고 학습시킨 다음에 나중에 이제 다 같이 써가지고 제일 잘 되는 쪽으로 하려고 하는 거거든요.

참석자 1 38:13
그래서 이거는 네트워크를 좀 줄였잖아요. 그쵸 그래서 이건 네트워크 규제라고 부를 수 있어요.
내 마음대로 내가 잘 정리 잘 한 것 같아요. 근데 안 그래요 여러분 이거는 그러니까 모델의 디자인 게 세 가지인데 하나는 컴페스티 두 번째는 웨이트 세 번째는 네트워크 모델 커패시티 모델 웨이트 모델 네트워크 어때요?
괜찮지 않아요? 그쵸? 이렇게 정리해 주면 좋지 않나 나중에 책을 쓸까?
어차피 너무 교과서가 좀 마음에 안 들어 아니 그렇잖아 딱 세 가지로 정리하면 좋잖아요.
맞아 그거 맞잖아 선을 줄이든지 웨이트 자체를 값을 하든지 아니면 용량을 줄이든지 이렇잖아요.
그쵸? 이렇게 정리를 해 주지 아무도 정리를 안 해주네.
그렇죠? 그래 언젠가 책이 나오더라고요. 나 기다려야겠습니다.
딴 짓 해야지 뭐 어쨌든 이해됐죠? 여러분 여러분이 책을 쓰던지 그래요 알겠죠?
그래서 그 세 가지가 어떻게 되는 거냐?

참석자 1 39:14
첫 번째 그러면 다시 이 모델 교제하기에서 문을 규제하기 위해서 첫 번째 208쪽에 나오는 거는 208쪽에 나오는 거는 208쪽 209쪽에 나오는 거는 결국은 원본 모드에 있는데 이게 지금 여기 5 11에 작은 용량의 모델로 적혀 있죠.
작은 용량의 모델 그쵸 작은 용량의 모델은 여기가 4 4 여기가 4개 4개 돼 있죠 그쵸 그쵸?
4개 4개 그리고 여기 원고는 왼쪽은 28쪽에서는 뭐라고 돼 있어?
16 16 돼 있죠? 그쵸 그리고 여러분 어휘 동그라미 치는 게 좋겠죠.
이 사람들이 현관표에 하나도 안 치워져 있으니까 이 교과서는 다른 것만 뭔지가 중요하잖아요.
사실 여러분 안 그래요? 공부할 때 이렇게 해야 되지 않나?
그렇죠 화도 좀 칠해주지 그다음에 여기 210쪽에서는 어떻게 돼 있어요?
210쪽에 여러분 이쪽에서 뭘 동그라 쳐야 되겠어요?
50면 여기 있으면 이쪽으로 그렇죠 알겠죠?

참석자 1 40:18
여러분 원래 16이었다가 16 16 했다가 4 4 했다가 그다음에 55 22 했어요.
그렇죠? 여기서 누가 제일 잘 나가느냐? 이걸 보면은 실제로 로스 같은 경우 보면은 209쪽 그림 볼까요?
209쪽 209쪽 그림 보면은 원래 오리지널 만에 16 16으로 했던 놈이고 얘는 4 4 했던 놈인데 4 4 했던 놈이 더 잘하지.
그쵸 더 더 잘한다는 게 더 오히려 밸리 스가 작다고 오히려 모델이 이렇게 문제 잡음이 있거나 약간 이상한 거였나 보지.
원래 트레인 데이터가 너무 이게 계속 뭔가 잘 안 되는데 이게 오히려 모델을 적게 쓰니까 더 잘 되는 거야.
그쵸 그래요. 그다음에 여기도 여기야 다음 페이지에 이제 210쪽에 그림 보면은 220쪽에 보면은 이제 라즈 모델에서는 라즈 모델은 이렇게 되고 아니 라즈 모델이 맞지 이거 그다음에 오리지널 모델은 이렇게 되네.
그쵸 아저 모델이 약간 더 낫긴 해요.

참석자 1 41:20
하는데 나중에 막 이상해지지 그렇죠 이게 왜냐하면 굉장히 가족하게 적합됐나 보지 그쵸 510 여기에 210쪽 210쪽에 그림 보여주고 있습니다.
210쪽 보이죠. 여러분 이게 점선이 원래 아까 16 16짜리고 이게 실선이 50c 오선이 모델이 너무 크다고 잘 되는 게 아니라는 걸 얘기해 주고 싶은 거예요.
그리고 실제로 여러분 제일 작은 게 제일 잘 됐죠. 너무 복잡한 거 쓸 필요가 없었던 거지.
결국 이게 여러분 20 9쪽의 그림에서 이게 제일 잘하잖아요.
여러분 그렇죠 이게 이게 여러분 환경 친화가 얼마나 좋아 그렇죠 괜히 쓸데없이 모델 크게 만들어 가지고 이게 잘 못을 수도 있다는 거지 이해되죠 여러분 im DB에서 이게 뭐냐면 imdb가 여러분 이거 뭐 하는 놈이었냐면 이게 센티먼트 나스라는 거였죠.
긍정 조장 판별하는데 그냥 작은 데이터가 잘했다는 거예요.
대충 보고 하는 게 너무 많죠. 그렇죠 너무 열심히 봐가지고 하네요.
이상해졌다는 거죠. 거죠. 그렇죠 알겠죠 그래요.

참석자 1 42:26
그다음에 이게 이제 모델이 그래서 용량이 작은 모델 메이트의 개수가 적은 게 오히려 더 잘한다.
이걸 봤어요. 그쵸 근데 이거 유명한데 나중에 이제 제가 오늘 진도 나갈 것 중에 이제 양자화 진도가 더 나갈지 모르겠네.
양자화랑 약간 좀 뭔가 온디바이스 AI를 하기 위해서 굉장히 줄이는 게 많거든요.
뭔가 용량을 근데 그게 더 잘 되는 경우가 많이 있어요.
요금이 ebct 쪽 워낙 워낙 많이 하니까 저는 저 이베시 쪽 하는 건데 오히려 모델의 커트 줄이거나 아니면은 비투수를 줄여놓으면 더 잘 되는 경우가 많아요.
실제로 메트로시가 재미있어요. 그래요. 그래서 여기도 그런 걸 보여주고 있는 거고 그래서 기억할 필요가 있겠죠.
여러분 그렇죠 모델을 규제하는 데 첫 번째가 컴플레스트 규제라고요 알겠어요 용량 규제 두 번째가 뭐라고요?

참석자 1 43:14
여러분 그리고 211쪽 웨이트 그쵸 모델 웨이트를 규제한다는 건데 이거는 여러분 그냥 에러 투라고 외우세요 에러넬2 유명하니까 에러넬2 여기 212쪽에 진한 블릿으로 에러 l2 적혀 있죠 그쵸 에러벨2 그쵸 제가 이거는 텐서 플로우 3 그라운드 하나 설명 있어요.
사실은 먼저 그쵸 이거 뭐 에러는 그냥 웨이트 패러티로 추가해 주는 건데 웨이트랑 여기 잘 적혀 있네.
왼쪽에 여기 그래서 이제 가중치를 다 비용이라는 게 여러분 로스 펑션에다가 가중치를 전부 다 더할 때 값을 더하는 거예요.
에런이 그리고 2는 가중치 제곱을 더해요. 그리고 가중치가 이게 크면 클수록 어떻게 되겠어요?
로스가 커지니까 가중치를 값을 작게 만들어야지 웨이트 로스가 작아지잖아요.
가중치를 자체를 절대값을 작게 만들려고 노력하겠죠.
절대값을 작게 만들려고 노력하겠지 숫자 너무 크게 안 하려고 그렇죠 이해되죠 여러분 그래요.

참석자 1 44:19
그런 내용이고 근데 이게 또 조심할 거는 요거 조심할 게 아니 알아둘 필요는 뭐야 213쪽에 213쪽에 213쪽에 뭐야 여기 작은 딥러닝 모델에서는 많이 효과가 있는데 큰 대규모 딥러닝 모델에서는 어차피 제약해봤자 별 볼 일 없다.
그렇죠 했수 드려요. 그렇죠 213쪽에 코드 5 14 밑에 있는 글이에요.
여러분 작은 러닝 모델에서는 이게 효과가 있는데 특히나 이제 우리 멋있러닝 쪽 효과가 많겠지 선형회이나 라지 스펙이나 이런 데는 거기서는 근데 딥러닝 모델 워낙 프로젝트 많아가지고 별 볼 일 없다는 거야.
알겠죠?

참석자 1 45:06
거기는 사실 또 애널레이션 같은 거 워낙 많이 해가지고 그리고 에런 규제랑 원래 에런 규제 l2 규제 쓸 수 있다고 했는데 에런 규제 l2 규제 어떻게 쓰냐면은 교과서 그래도 코드가 어떻게 쓰는지 보면은 오픈북으로 볼 때는 여러분 할 수 있어야 되니까 보면은 여기 보면 교과서 212쪽에 코드에 이렇게 l2 l2 이렇게 이런 거 보이죠 여러분 레귤러 라이제이션 해가지고 그쵸 레귤러 라이저 해가지고 보통 레귤러라이제이션 하면 이걸 의미하는 거야 에런 l2를 의미하니까 굳이 말도 안 나오고 그렇죠 이렇게 해놨죠 그쵸 보통 레귤라이제션 하면은 보통 이것만 의미하는 경우도 많아요.
알겠죠 근데 이제 교과서에서 오히려 깨끗하게 브라바우까지 이대로 얘기해 준 거고 됐죠 그리고 l2하고 이때 에론도 있다.
여기 보면은 여기 214쪽에 코드 4 213쪽에 코드 5 13에 에런도 있어요.
그렇죠 당연히 되겠지 에런도 있고 l2도 있고 이런 것도 있어요.
에러 네투 같이 섞어 쓸 수도 있어요.

참석자 1 46:09
여러분 에러네2를 쓰는데 에런에다가 0.01 l2에다가 0.001 서로 웨이트를 곱해가지고 그러니까 원래 이제 가중치를 다 더해서 그대로 하는 게 아니라 거기다 다시 또 가중치 여기다 웨이트를 주는 거예요.
이해되나 여러분 가중치를 더 다 에러는 이제 절대값 더하는 거잖아요.
2는 제곱 더하는 거잖아요. 거기에다 더한 값에다가 이걸 또 곱하는 거예요.
숫자들 알겠죠? 여기 여기도 내가 설명 안 했네. 여기도 마찬가지였어요.
여기 0.002 곱한 거였어요. 알겠죠? 뭔 얘기인지 알아요?
여러분 로스 값에다가 그냥 또 다 더하면 곤란하잖아요.
거기다가 이만큼 메이트 줘서 정말 천분의 2를 만들어가지고 천분의 2만큼 줄여가지고 더해준다고 알겠죠 그래야 0.002 정도로 보통 0.002를 많이 써요.
알겠죠? 에러라도 섞어 쓸 수도 있다고 심지어 알겠죠 그래요.
그다음에 마지막으로 이렇게 나오네.

참석자 1 47:14
바웃 바웃은 드라바웃 드라바웃은 아까 얘기했듯이 그 그림을 기억하는 교과서에 안 나오는 교과서에서 이제 이분이 좀 색다르게 하려고 아까 그 그림이 되게 유명한데 원래 드라바우시 논문이 이번에 노벨상 받으신 제 프린트 그 두분이 쓴 거예요.
그것도 그분이 만든 거예요. 그 사람 하여간에 만든 게 되게 많아 알겠죠?
모트리 대학교에 제프리튼 그분은 여러분 인공지능이 이제 미친 짓 할 거라고 조심하라고 계속 그러고 있어요.
그래요. 시간 다 됐네. 그래요 어쨌든 10분 쉬었다가 여러분 계속할게요.
여러분 10분 쉬었다가.


clovanote.naver.com

딥러닝 day12_1
2025.04.16 수 오전 11:00 ・ 50분 20초
심승환


참석자 1 00:01
교과서 지금 드라바우 하고 있어요. 저거 드라바하고 있어요.
드라하고 네트워크 교재라고 얘기했죠. 제가

참석자 1 00:13
그래서 이게 보면 이게 지금 싸게 이 그림이 뭐냐면 이 그림 봅시다.
여러분 이 그림이 214쪽에 있지 214쪽 이게 갑자기 이게 뭔 말인가 싶겠지만 어쨌든 이게 여기 지금 그림 보여주는 게 뭐냐면은 활성화 함수가 지금 출력들이 결국 이렇게 있다고 치자를 만약에 이 첫 번째 레이어, 두 번째 레이어, 세 번째 레이어 네 번째 레이어 이렇게 있다고 치잖아요.
예를 들어서 원래 이렇게 있었어요. 근데 드라아웃을 시킨다는 거는 여기서 50%를 랜덤하게 못 쓰게 만드는 거예요.
랜덤하게 0으로 만들어버리고 진행을 하는 거예요.
0으로 만들어서 없는 셈 치고 하는 거지 아까 이 그림이 적당한데 훈련시킬 때 훈련시킬 때 얘네들은 없는 셈 치고 이들을 업는 해주고 진행하는 거예요.
알겠어요 여러분 그런데 이게 교과서에 너무 친절하게 돼 있는데 이거 당연한 거지만 드라마도 여러분이 쓰면 드라아웃이 워낙에 지금 텐서플로우나 타이 터치 다 많아서 여러분 API 쓰면 그냥 저절로 되거든요.

참석자 1 01:31
50% 정도 한다 50% 한다 이런 식으로 50% 한다면 50%를 훈련에 이제 얘네들을 참여하지 못하게 하는 거고 20% 한다 20%는 훈련에서 참여하지 못하게 하고 그러는 거거든요.
약간 작은 용량의 모델을 훈련시키면 비슷해지겠죠.
여러분 내용 사실 이해되죠 여러분 웨이트 수 줄이는 게 용량 적게 하는 거랑 똑같잖아요.
자금 용량의 모델을 여러 개를 한꺼번에 나중에 짬뽀시키는 거죠.
그쵸 나중에 이제 실제로 이 훈련시킨 다음에 맨 마지막에는 어쨌든 다음 훈련에서 얘가 사용 될 거 아니에요 그렇죠 안 썼던 것들도 전부 다 벽이 다 촬영이 돼서 데이터가 살아 있어요.

참석자 1 02:06
여러분 그래서 만약에 50%를 했으면은 나중에 이제 출력은 시킬 때 두 배를 해가지고 출력을 예측하게 만들던지 나중에 다시 살아날 테니까 나중에 살아날 때 만약에 3분이었으면 나중에 원래 투어 나오는 거 3배는 3을 곱해가지고 심하게 나오게 하든지 이런 식으로 해야 될 거 아니에요?
그쵸 이해되죠? 무슨 말인지 교과서에 그거를 아주 친절하게 설명해 놨어요.
수식으로 이해돼요. 안 돼 안 되지 그러니까 3분의 1로 처리하게 했어.
만약에 그러면 나중에 원래 결과값을 3분의 3을 곱해가지고 만들어내야 나중에 다 써도 문제가 없을 거 아니야 이해되죠?
여러분 그 얘기예요.

참석자 1 02:50
아니면은 나중에 훈련 나중에 방법이 여러 가지지 이렇게 해서 나중에 원래 훈련시킬 때부터 내가 하는 참여 안 한 놈까지 안 한 놈의 개수만큼 곱해가지고 결과를 나오게 하든지 아니면 나중에 이제 그냥 그대로 쓰다가 나중에 나온 값에서 3분의 1을 하든지 그쵸 3명이 배 3분의 1만 쓰고 있었으면 그걸 교과서에 수식으로 보여주고 있어요.
말로 이해되잖아요. 여러분 그렇죠 시험에 잘 아는지 모르는지 물어볼게요.
근데 어쨌든 이거는 원래 드라아웃에서 여러분 API 쓰면은 그냥 저절로 돼 그렇지 당연히 그래야 될 거 아니에요 실제로 여러분 원래 제대로 하려고 그러면 그렇죠 그래서 그러고 있어요.
네 어쨌든 여기 드라바웃을 쓴다고 해서 드라바웃 쓴다고 해서 뭔가 모델이 용량이 줄어드는 건 아니야 훈련시킬 때 모델 용량을 줄이는 것뿐이지 그쵸 최종적으로 다 살아 있어요.
알겠어요 그것도 중요해요. 제대로 이해하세요.
그것까지 설명 열심히 돼 있어요.

참석자 1 03:50
제가 지금 말로 다 때웠는데 괜찮지 그쵸 교수님 오버 피팅을 시킨 다음에 그러면 이 드라아웃을로 또다시 학습을 시키는 거요.
이거 처음부터 그냥 학습시킬 때 처음부터 이렇게 학습시키는 거 오피팅은 같이 또 병행하는 거예요.
알겠어요 다 한 다음에 하는 거 아니고 처음부터 그래서 여기 보여줄게요.
미안 215쪽 215쪽 215쪽 보면은 캐러스 혀 있으면은 여기 보면은 댄스 한 다음에 드라아웃 드라아웃 중간에 층으로 넣어요.
0.5 0.5 였잖아요 그쵸 그럼 0.5%만 웨이트를 쓰겠다는 거예요.
참여한다는 거고 다음에 넘어갈 때 그래서 드라아웃이 돼버려요.
알겠죠? 드라웃은 여기서 이제 나중에 이제 원래 우리가 이제 아까 얘기했듯이 2분의 1 구매하든지 2분의 1 하든지 이런 거 있잖아요.
그러면 내부적으로 다 해주고 있어요. 그래요 그리고 드라아웃은 보통 당연히 하는 게 좋아요.
안 하면 좀 더 안 좋고 보통 아까 모델 기자하면서 좋은 거 봤잖아요.

참석자 1 04:54
아까 모델 웨이트 웨이트가 아니라 저 파라미터 이거 딥러닝에서 가중치 규제는 아까 메이트 에론 l2는 별로 안 써도 되지만 드라아웃을 쓰는 게 정석이에요.
알겠어요 여러분 그래서 사실 앞에 적은 용량 모델 만드는 것보다 드라바웃 쓰는 게 더 좋지.
여러 드라바웃 쓰는 모델 여러 다 커버려서 쓰는 거 마찬가지라서 아까 원래 오리지널 208쪽에 오리지널 모델은 16개였고 209쪽에 작은 용량의 모델은 댄스가 4개였잖아요.
드런이 그쵸 이건 드랍아웃을 얼마나 시킨 거예요?
0.25 0.25가 아니지 0.75로 드랍시켰네.
그쵸? 0.75% 날려버렸어요. 그쵸 4분의 1 그렇죠 그래요

참석자 1 05:48
네 그러면은 그다음에 정리는 다 했고 됐어요. 그래서 이제 넘어갑시다.
5장은 끝났고 6장이 6장이 진짜 짧아요. 여러분 6장이 기장이 짧은데 그래도 뭐 중요한 내용이라서

참석자 1 06:15
보면은 작업 정의 모델 개발 모델 배포 이렇게 돼 있잖아요.
진짜 여러분 실제 일할 때 할 워크 플로우잖아요. 그치 일할 때 여러분이 딥러닝 프로젝트가 생겼어 그러면 먼저 작업 정의하고 그쵸 모델 개발하고 배포해야 될 거 아니에요 그쵸 세 가지에 이제 나와 있어요.
작업 정의는 221쪽부터 나와 있는데 문제 정의하고 데이터 수집하고 대표성은 데이터 주의하고 하는 게 지금 나와 있는데 당연한 얘기들이고 인프라 투자하고 이러는 건데 데이터 구하는 게 힘들죠 221쪽 가볼까요?
여러분 226쪽에 226쪽에 기표성원 데이터 주의하기 거기 가면은 대표성은 데이터 주의하기.
여기서 226쪽에 개념 이동 이거 있죠 컨셉 드리프트 요거를 여러분이 항상 신경 쓰세요.
여러분 요거 요거 외웁시다. 한 영어로 한 드립트 드리프트가 이렇게 미끄러져 내려가면서 여러분 뭔가 좀 미끄러지는 거죠.

참석자 1 07:17
그렇죠 드리프트가 이렇게 전전할 때 드리프트 하면 이렇게 막 커브 될 때 이렇게 막 거의 떠다니듯이 막 움직이는 거.
그렇죠 드리프트가 개념이 갑자기 막 움직이는 거예요.
그쵸 드리프트가 개념 이동이 뭐냐면은 우리가 시간 알지도 못하는 사이에 갑자기 개념이 바뀌어버리는 거예요.
무슨 사건이 일어나는 거에 따라서 언제부터 그랬는지 모르겠지만 허리라는 단어가 나타나기 시작했고 그렇죠 허리라는 단어는 여러분 이제 어쨌든 좀 새로운 단어가 자꾸 많이 나와요.
그쵸 단맛이라는 거 여러분 진짜 맛도 안 좋은 거잖아요.
그쵸 그럼 맞죠? 여러분 무음도 이렇게 생기잖아 힘 받는다.
그쵸 몸에 힘 받는다는 말에서 뭐 알아줬다가 어쨌든 이 개념이 이동하고 있어요.
그렇죠 근데 그게 무슨 사건이 생길 때마다 이동하기도 하고 그렇죠 함부로 막 갑자기 새로운 일에 의해서 개념이 바뀌기 때문에 어떤 걸 학습을 했어도 계속 학습을 새로 해야 돼.

참석자 1 08:11
그렇죠 이런 폭삭 소각 수단 이런 거 요즘에 유행해서 그 육사 수가 수다가 이제 또 온가 옛날하고 다르잖아 이제 다 속았다는 것밖에 안 되는데 이제는 쓰고 있다는 뜻이라며 원래 제주도 방원이라서 아니 어쨌든 이제 사람들이 다 많이 쓰잖아.
그쵸 지원미 저는 처음에 그게 무슨 말인지 진짜 되게 미스터리 해가지고 도대체 저게 뭔데 다 지원미라고 그러지 이래서 아니 진짜로 안세도 특징 어쨌든 그런 거 있잖아요.
자꾸 새로운 나오는 거 그런 거 때문에 학습을 계속 끊임없이 해야 돼.
그렇죠 그런 얘기예요. 학습을 한 번 하고 끝나는 게 아니야 그쵸 특히 사람과 관련된 쪽에서는 거의 그리고 이게 아니 거의 사실 머신러닝 하는 게 예측 불가 불허에서 하는 것들인데 워낙에 개념이 바뀌는 게 많아서 계속 어휘 모든 게 다 변해요.
그래요. 그다음에 샘플리 편향 문제 226쪽에 또 샘플릿 편향 문제가 있죠.

참석자 1 09:08
저기 박스로 그쵸 샘플링 변화 문제 이것도 이제 조심해야 될 게 이제 인터넷에서 뭔가 데이터 긁고 와가지고 했을 때 여러분 진짜로 정치적인 얘기하지 맙시다.
정치적 얘기 얘기하면 안 되는데 어쨌든 그거 있잖아요.
저 미국으로 얘기하지 우리나라 얘기하면 좀 그렇고 정말 온 세상 사람들이 다 트루건이 트럼프를 지지하는데 왜 이런 거죠?
그렇죠 예를 들어서 그러니까 내 주위에 있는 사람들은 다 그런 거 그렇죠 샘플링 편이 그렇죠 저도 이제 만약에 제주도 제주도가 아니라 어쨌든 주위 사람들만 다 믿으면 안 돼요.
그쵸 실제로 그리고 여러분 인터넷도 인터넷에서 맨날 맨날 트럼프랑 저번에 누구였지 대통령 선거 같은 거 있잖아요.
되게 틀리잖아. 많이 그 이유가 이제 인터넷이 정확하지 않아서 그래 그렇죠 거기서 쓰는 말하는 사람들이랑 실제랑 다르니까 그쵸 침묵한 자수가 있기 때문에 그렇죠 그 얘기예요.
됐어.

참석자 1 10:09
그다음에 데이터를 잘 이해하고 성공 지표를 선택하는 게 228쪽까지 나와 있고 잘 나와 있지 항목 지표를 잘 정해야 되겠지 중요한 내용이 많이 있고 근데 이게 앞서 했던 거니까 넘어가서 모델을 개발을 해야 되는데 일단 모델 개발할 때 첫 번째가 229쪽에 데이터 준비 그다음에 평가 방법 선택 6.2.4례 기준 모델 뛰어넘기 이런 식으로 되어 있죠.
그리고 둘이 있는데 앞에 있던 거 다 겹치는 거라서 강조할 내용만 하면은 데이터가 어쨌든 여러분 229쪽에서 첫 번째가 220페이지 6.2절로 넘어가네.
여기 첫 번째가 벡터화 정규화 이런 거 해야 돼. 그렇죠 그렇죠 여러분 벡터 정리 이런 거 해야 돼 그렇지 벡터가 여러분 저번에 imbb 영합형 할 때도 그냥 텍스트 데이터 쓰지 못하고 벡터로 만들어서 했죠 그쵸 딥러닝 시키려고 그러면 그렇죠 그리고 심지어 원하드터로 만들어버렸어요.

참석자 1 11:14
멀티 아펙터로 만들어버렸죠 그것도 그냥 구입할 수 없어서 그쵸 여러분 그냥 텍스트 데이터 숫자를 막 줘가지고 하면은 학습이 잘 안 되는 거야.
댄스에서는 근데 이제 그거를 그냥 그 숫자가 아니라 또 뭔가 임베딩이라는 걸 바꾸고 한다는 것도 교과서에 있었어요.
제가 녹화해 놨었죠 그렇죠 벡터로 바꾸는 것 또 뭔가 여러 차원의 나중에 배울 거예요.
벡터화시켜야 돼 알겠죠? 뭔가 숫자들이 집합을 만들어야 되는데 그냥 학습이 잘 되는 여러 기법들이 있는 거를 여러분 알아야겠지 그쵸 그냥 두면 안 되죠 이 그래요 텍스트 데이터에 대한 전처리 작업 이미지 데이터 전처리 작업이 달라요.
알겠죠? 텍스트 데이터에 대한 거 여러분들 멀티 아티카만 배웠어요.
지금 그쵸 사실은 임베딩이라는 게 훨씬 더 많이 쓰이는 거기 때문에 잘 되는 거야.
그거는 그냥 데스트에다가 주로 주입하는 게 아니라 트랜스포머 같은 데 어텐션에서 잘 쓰는 것들이고 나중에 할 거예요.
알겠죠? 그래요.

참석자 1 12:07
그다음에 데이터 준비하고 데이터 준비해서 벡터화하고 그다음에 정규화 노멀라이제이션 이거 전에 했었죠 그렇죠 여러분 230조 넘어가 볼까요?
230조 블리스로 뭐라고 돼 있냐면은 작은 값을 취해야 되는 군인이 된다라고 적혀 있죠.
그쵸 작은 값을 취해야 되면은 기본적으로 이제 입력 표차를 0하고 1 사이로 만든다고 돼 있죠 노멀라이제이션 하면 사실 여러분 0하고 이 사위가 아니라 마이너스 1하고 2 4위가 되죠 그쵸 보통 표준 편차가 1이고 1이니까 보통 마이너스 1하고 이 사이가 되겠지 그쵸 대부분이 그리고 비슷한 범위를 가지게 되겠죠 그쵸 균일하게 된다는 게 그런데 보통 0하고 1 사이로 하면 그 밋닉스 노마레이션 써야 돼요.
그쵸 밋닉스 스탠다드에이션 그쵸 알죠 여러분 민미스 뭔지 알죠?
여러분 보세요. 제가 p2 스케일링에서 했던 거죠.
그렇죠 그래요. 그다음에 요거 데이터 준비할 때 중요한 게 또 누락된 거 처리하기.

참석자 1 13:09
여기 부드럽게 다 처리 아니고 여기 누렸던 거 처리를 하기 위해서 누락된 게 이제 여러분 데이터가 뭔가 정보가 빠져 있는 거예요.
그럴 때 어떻게 할 거냐 이게 첫 번째 1 2 30쪽에서 이제 저기 분리시나 있고 231쪽에 분리 시도 위에 있죠.
범죄형 특성이라면 231쪽에서 수치형 특성이라 이렇게 적혀 있죠.
요거 그쵸 범죄 특성이라는 게 카테고리죠 카테고리 카테고리가 뭔가 빠져 있다는 의미지.
그쵸 그러니까 이게 범주가 없는 거야. 이게 0 1 2 3 4 5 6 7 89가 예를 들어 넷리스트 하는데 이거는 이상하게 써가지고 도대체 무슨 숫자인지 모르겠어 이런 게 있잖아요.
그럼 걔는 어떻게 할 거냐 그 데이터를 날려버릴 거냐 아니면은 그런 데이터도 뭔가 살릴 거냐의 문제였는데 뭘로 살려야 되냐 이런 거죠.
그쵸 이해되죠 여러분 그래서 절대로 새로운 범주를 만드는 것이 일단 암정을하죠.

참석자 1 14:04
그러니까 여기 서 우리 집8 9 있었으면 10이라는 거 만드는 게 낫지 차라리 그냥 언디파이 된다고 여러분 텍스트 데이트 할 때도 우리가 오오브라든지 따로 만들었잖아요.
oov가 뭐였어요? 노라 주세요. 그거 아웃오브 케블러리 있는 게 있어야지 그게 없으면 곤란하다는 거지 있어야지.
그쵸 누락된 거잖아 사실은 그게 이제 데이터가 너무 많아서 빼버린 단어들인데 그렇죠 어려운 단어 그거는 어려운 단어라는 걸 표시해 줘야지 그냥 놔두면 안 된다는 거죠.
그렇죠 그다음에 뒤에 이제 231쪽이 수천성이라면 숫자로 그냥 나온 거야.
예를 들어서 저번에 범죄율 보스턴 집값 있었잖아요.
거기에 어떤 어떤 집은 범죄에 노고가 안 돼 있는 거야 이 지역에 대해서는 이해되죠 여러분 그러면 어떻게 할 거냐 거기다 무조건 0 두면 곤란하죠.
여러분 범죄율이 0이 아니잖아. 실제로 모르는 거지.

참석자 1 14:57
근데 많은 경우에 여러분 막 영 넣어버리고 막 한다고 0이 그냥 디폴트 값이니까 근데 0은 진짜 값일 수도 있잖아요.
여러분 그렇죠 곤란하죠. 그래서 이거를 어떤 식으로 하는지 적혀 있는 게 이게 231쪽에 그러니까 네 번째 줄에 평균 중답값 또는 예측하는 모델을 훈련할 수도 있고 이런 식으로 여러 가지 방법을 써야 되는 거예요.
평균을 넣는 게 제일 안전한 편이에요. 여러분 오히려 이상한 거 만드는 것보다 범죄율은 또 마이너스 1 내버리면 또 범죄율이 되게 없는 것처럼 보이잖아.
없는 숫자 쓴다고 해서 또 범죄율이 10회 변에 또 또 또 문제 되게 범죄율이 큰 것처럼 보이잖아요.
굉장히 곤란하지 그쵸 그래서 안전하게 투명감 잡는 게 좋다 이 얘기하고 있어요.
이해돼요 여러분 알겠죠? 그래요. 시험에 낼게요.
왜냐하면 진짜 여러분이 이거 맨날 하는 일이라서 0 쓰면 풀라나잖아.
풀 나는 거잖아. 그치 근데 만약에 0 쓴다니까 진짜 오르면 그냥 0이야.

참석자 1 15:53
또 그리고 마이너스로 넣어버려 없는 안 쓰는 숫자라고 근데 마이너스 1을 걔가 알아먹겠어요 이 수치형이면 어떻게 되겠어요?
여러분 뭔가 표준 편차 시켜가지고 막 이렇게 만들어버렸는데 마이너스 가 의미가 있는 거잖아요.
그쵸? 곤란하지 그래서 여기 보면은 진짜 평균 중간값을 평균 중간값 아니면 그냥 예측하든지 뭐 이래야지 그쵸 알겠죠 그냥 임의의 숫자를 넣는 거 되게 위험하다.
안 쓰는 숫자로 넣는 것도 위험하다. 알겠죠 엄청 중요한 내용이에요.
여러분 진짜로 시험에 낼게요 알겠죠? 그다음에 평가 방법도 여러분 아까 홀드 아웃 여러 가지 알죠?
여러분 그렇죠 교과서에서 이거 되게 강조하니까 6.2.2점 231쪽에 앞으로 가 버렸구나 231쪽에 요거 홀드아웃이라는 이거 되게 되게 좋아하죠 그쵸?

참석자 1 16:49
케이 폴드랑 반복 케이 폴드 그쵸 되게 되게 좋아하셔 그쵸 그래요 그다음에 앞에 나왔던 거 다시 또 나와 있고 똑같은 거 또 나와 있어요.
여러분 너무 중요하니까 여러 번 하셨어요 그렇지 반복했어요.
이게 사실 이쪽 위에 있는 거 앞에 했던 거죠. 그렇죠 이게 다 뭐가 있는지 다 외울 수는 없어도 뭐 물어보면 알 수 있게 그다음에 오라 손치락 선택하기에 여기 보면은 이거 다 했죠 여러분 시그모이드 소프트맥스 이런 거 그쵸?
그렇죠 여러분 분류해서 그쵸 마지막 층의 활성화 함수가 시그몬 쇼 텍스트에서 봤죠?
그쵸? 다중 레이블 다중 분류는 다중 레이블이 다중 분류를 한 번도 한 적 없어요.
여러분 레이블이 여러 개인데 또 여러 개 분류하는 그런 거는 없었거든요.

참석자 1 17:42
여러 개를 분류하는데 그 여러 개가 또 각자 또 레이블이 다양해 그런 건 한 적이 없는데 그래서 이거는 뭐 이거는 제가 시험 문제 낼 것 같지 않으니까 나중에 나중에 필요하면 할까 일단 여러분 이거 이거만 일단 기억하세요.
두 개 알겠죠? 이거만 할 수가 있나요? 그렇죠 이 두 개 발견했어요.
그렇죠 그래요 그다음에

참석자 1 18:12
그리고 이제 모델 배포로 넘어갑시다. 여러분 이게 한 예산 44쪽 234쪽에 모델 배포에 넘어가서 이게 36쪽으로 갑시다.
여러분 이것만 합시다. 이거 이거는 되게 편하면 좋겠어요.
여기 36쪽 136쪽에 추론 모델 대체학이라는 게 추론 모델 추론 여러분 영어로 뭐예요?
인퍼런스 그렇죠 인퍼런스 인퍼런스 모델 이거는 훌륭한 모델이 아니라 그렇죠 나중에 배포하는 거죠.
그쵸 그래서 배포할 때 교과서에 지금 236쪽에 1번 레스트 API 그쵸 1번 레스트 API 그다음에 2 37쪽에 2번 장치로 모델 더하기 그다음에 238쪽에 3번 브라우저의 모델 구하기.
그렇죠 이렇게 있어요. 그쵸 그다음에 여기서 239쪽에 추론 모델 최적화라는 거 적혀 있잖아요.
그쵸 추론 모델 최적화 요거는 다에 해당하는 거라고 볼 수 있지만 특히나 2번에 중요한 거예요.
2번은 2번에 특히 중요한 거예요. 그리고 최적화라는 말 대신에 경량화라고도 써놓으세요.
여러분 경량화 사실은 경량화 내용이 답입니다.

참석자 1 19:26
여러분 일단 경량화 경량화가 뭐예요? 이렇게 작게 만들려고 하는 거죠.
그래서 이게 보면은 레스트 API랑 1번 레스트 API 3번 브라우저 모델 배포 이거 전부 브라우저도 사실은 PC 같은 건데 2번이 장치 모델프 한다는 거 있잖아요.
이게 온 디바이스 AI예요. 여러분 2번에다가 온 디바이스 AI 여러분 교과서에 안 나와 있는데 지금은 너무 유명한 문어가 되잖아요.
그렇죠 여러분 이게 이 책이 20 21년에 나왔나 하여튼 그래가지고 지금 온디바세율 우리나라가 거의 온디발세 되게 열심히 하고 있잖아요.
지금 알겠죠 온디바스 AI 그래요 여기 보면은 그래서 이거 온디머스 AI부터 먼저 봅시다.

참석자 1 20:09
여러분 중요하니까 237쪽에 보면은 이따은 모델 fp 실행되는 동안 동일한 장치 스마트폰 로봇에 있는 임베디드 암 CPU 이런 거 적혀 있죠 여러분 그렇죠 이거 마이크로 컨트롤러 이일이죠.
여러분 이게 지금 인베리 시스템이죠. 그쵸 제가 제가 임베리 시스템 하는 사람이니까 특별히 중요하다고 그러는 거죠.
알겠죠? 여러분 그래요. 자동차 고학케이스들 맨날 이거 그렇죠 열심히 열심히 하겠죠.
온디바이스 AR 보드에다가 라즈베리 팔 같은 데 보일라 넣는 거예요.
여러분 그런 레버 업무에서 돌아가요. 근데 문제는 잘 돌아가 라즈베리 파일을 기반으로 한 온갖 보드들이 나오고 있고요.
인베리드 AI 보드들이 NVIDIA 자비어 이런 것도 비싼 놈이고 되게 싼 라인 나와 있어요.
그래서 여기 이걸 하기 위해서 여기 적혀 있지만 결국은 239쪽에 마지막 추론 모델 최적화 있죠 이거 같이 배워 같이 나와요.
이게 추론 모델 최적화를 해야 돼 알겠죠?

참석자 1 21:03
여기 분리수로 뭐가 나와 있어요? 여러분 가중치 가중치기랑 가중치 양자가 적혀 있죠 여러분 요거 외워요.
가중치기랑 양자화 중요해요. 알겠죠? 가중치랑 양자화가 뭐냐 가지 아셨죠?
가지치기라 이게 메이트 푸르는 영어도 외워요. 여러분 메이트 컨터이게이션 알겠죠?
프로닝은 여러분 웨이트 프로닝은 웨이트를 날려버리는 거야 진짜로 없애버리는 거예요.
어떤 웨이트는 별로 중요하지 않아 잘 안 사용하는 숫자가 같고 그런 거는 없애버리는 거예요.
여러분 그래서 진짜로 모델을 결국은 용량을 줄여버리는 거나 마찬가지가 돼요.
알겠죠? 알겠죠? 그리고 가중치 양자화는 원래 이제 가중치 자체가 보통 플로트 포인트 32비트로 쓰고 있거든요.
점 d타입 하면 이렇게 나왔어요. 여러분 그냥 디폴트가 64비트인데 32비트로부터 줄여서 우리가 한 1년 시켰거든요.
계속. 그런데 이것도 여러분 다 얼마씩 해요? 한 웨이트가 4바퀴씩 차지하잖아요.

참석자 1 22:10
근데 이제 보통 INT 탈로 인테리어 에잇으로 만드는 거예요.
알겠어요 여러분 지금 이거 8비트 그리고 여러분 지금 딥시크인 이런 애들을 4비트로 만들었어요.
심지어 때에 따라서 거의 뭐 어느 위트가 많으니까 줄이라고 알겠죠 그게 그래야 PC에서 돌아가는 거지 그래서 그리고 요즘 NVIDIA 보드 중에서 그런 것도 되게 많고 4비트로 해도 인테리어 4비트짜리 나왔어요.
어쨌든 이래도 어쨌든 분명히 정보가 있으니까 알겠죠 여러분 그래서 이게 지금 아까 온디바이스 얘기하는 데 중요한 기준들 중에 하나예요.
알겠죠? 여러분 이거 말고도 또 최신 기술들이 있지만 이게 기본이고 이거는 여러분 은근히 신경 쓸 게 없는 게 펜스 플로우 라이트라는 거 쓰면 지가 다 알아서 해줘.
걔가 이거 저절로 해줘요. 알겠죠? 테스트 라이트라는 거 교과서에 어디 있나 여기 있다.

참석자 1 23:01
238쪽에 38쪽에 브라우저 모델 배포하기 위에 단락에 캐러스 모델은 스마트폰에 이그런트 배포하기 위한 솔루션은 텐서플로우 라이트 적혀 있죠?
여러분 뭐예요? 외워 외워야 돼요. 여러분 여러분이 이거 모르고 안 쓰면 이상한 사람이 돼요.
알겠죠? 스마트폰이나 인베딩 장치에다가 여러분이 훈련시킨 거 넣으라 그래 텐서플로 라이트를 쏴야 돼요.
알겠어요 여러분 이거 안 쓰고 모른다 그러면은 나쁜 놈이야 알겠죠?
시험에 내겠다고 알겠어요. 이거 외워 텐서플로우 라이트 알겠죠?
이거 사실 이름 바꿨거든요. 얘네들 RT 라이트로 바꿨어요.
리얼 타임 라이트로 언제 바꿨냐 작년 12월인가 바꿨어요.
얼마 안 됐죠 딱딱딱딱 하신 지 그래서 여러분 이거 딥시 카트 부르면 딕시트가 아니라 그냥 프로 다트 부르니까 모른대 테스티나 안다고 그러더라고요.
신스트를 그래서 다 맡았죠. 아니 어쨌든 일단은 여러분 텐서플로우 라이트라고 알아도 돼요.
알겠죠?

참석자 1 23:56
이름 자꾸 바꾸는 거는 이제 좀 멋있어 보이려고 그러는 거니까 알겠죠 센서플로 RT RT는 다음 기말고사 보네.
그럼 여러분 알겠죠? 여러분 교과서에 나오는 대로 외워요.
시험에 냈다고 그랬죠 제가 적을 수 있어야 돼. 알겠죠?
여기 문제 어떻게 나오냐 이렇게까지 스마트폰에 인물의 장치에다가 배포하려고 그러면 어떤 엔진을 써야 되는 소프트웨어를 써야 되냐 핸드 플로우 아이p 알겠죠?
그래요. 외워요. 여기 다 적혀 있네. 온디바이스 여기서 효율적인 온디바이스 딥러닝 추론을 위한 프레임워크 그쵸 보이죠 여러분 그래요 여기 보면은 변하기가 다 있다고 그쵸?
iOS까지 다 된대 적혀 있죠 여러분 여기 보면 안드로이드랑 iOS에다가 올리는 변환기가 다 있어요.
그럼 파일 토출 훈련시켜놓고 변환시키면 돼요. 알겠어요?
여러분 뭐든지 이거 모른다 그러면 안 된다고 알겠죠 여러분 프랫트할 때 당연히 이거 써야 돼.
알겠죠?

참석자 1 24:50
온드sai 하는 거 해봤다고 그러면 당연히 좋아하는데 지금은 너무 쉬워져가지고 이제 할 수 있는 사람이 너무 많은데 옛날에 어려웠는데 알겠죠 그다음에 나머지 합시다.
레스트 API는 236쪽에 레스트 API는 랑 레스트 API는 여러분 이 서버 쪽 쓰는 거겠죠 서버 서버에서 그냥 노드 제이스나 아니면은 스펙이나 이런 것들 있죠 써가지고 하는 거예요.
알겠죠? 그렇게 하면 됐지 뭐 알겠죠? 여러분 클라우드에서 하는 거예요.
외워요. 모르면 안 돼 알겠죠? 원격 API 쓰는 거 여러분이 그렇게 배포하면 되는 거지 이거 저기 예를 들어서 아마존 웹 서비스 쓰거나 레이저 쓰거나 오라클 쓰거나 이런 거 있잖아요.
구글 클라우드 이런 데다가 배포해도 다 레스 PI 쓰는 거예요.
알겠어요 여러분 됐죠 다른 거 알겠어요 여러분 다른 거 알겠어요?
원격 서버에다가 쓰는 거 나 내 서버에다가 해서 해도 돼요.
마찬가지예요. 그쵸? k8s 우리 과에 있는 거 거기다 포해도 마찬가지예요.

참석자 1 25:49
그러고 2번 237쪽에 장치 모델 배포하는 거는 온디바이스예요.
알겠죠? 그다음에 그때 뭐라고요? 테이프 라이트 쓰는 거 테스 플로우 라이트 알겠죠?
마지막 238쪽 장치 모델프하고 이렇게 이렇게 브라운드의 모델프 하면은 이것도 이것도 여러분 알고 있어야 되는데

참석자 1 26:16
자바스크립트 모델 하는 거예요. 이거는 자바스크립트로 그래서 이거는 더 모델이라는 거는 또 맨 제일 중요한 게 맨 뒤에 있는데 239쪽 위에 자바스크립트를 모델 배포하기 위한 생태계에서는 여기 보면 텐스 플로우 JS라는 거 보여요.
여러분 텐스 플로우 닷 JS 텐스 플로우 요거 요거 외워요.
또 아까 텐스 플로우 라이트처럼 텐스 플로우 닷 JS가 중요한 거예요.
여러분 이거 이 라이브러리를 써가지고 배포하는 거예요.
여러분 이걸로 또 임의의 모델을 변화시킬 수가 있어요.
이것도 이걸로 플랜 자체도 이걸로도 할 수 있고 훈련할 때는 여러분 브라우저에서 훈련하는 게 아니라 노드 제이스에서 훈련하는 게 맞겠지 노드 제이스에서 훈련한 다음에 배포는 텐서플로우 JS로 하는 거야 알겠죠?
노드js의 텐스 플로우 JS를 쓸 수 있어요. 텐스 플로우 JS 자바스크립트니까 자바스크립트를 브라질에서 돌 수도 있고 노드js를 돌 수도 있잖아요.

참석자 1 27:09
여러분 훈련할 때는 그냥 빠방하게 하려고 브라우저는 아무래도 한계가 있으니까 노르제이스로 하고 알겠죠 여러분 그리고 심지어 이제 노드js로 그냥 프로 모델을 ST API처럼 또 쓰는 경우도 있는데 보통 그래도 자바스크립트는 브라우저를 쓰게 하기 위한 거지 그렇죠 근데 브라우저를 쓰잖아요.
만약에 또 출연 모델로 이것도 온 디바이스에 가능하긴 해요.
왜냐하면 저도 그냥 돌아가니까 이해돼요. 여러분 근데 덜 유행하고 있지 어쨌든 여러분 웹앱으로 만들면 되게 편할 거 아니야 그쵸?
웹앱으로 왔으면 앱을 웹으로 만들었으면 이것도 테스플로 제이스 온디바이스에 쓸 수 있는 거잖아요.
이해돼요. 여러분 그래요. 그걸 명확히 이해하고 계세요 그래요.
그다음에 그래서 다 했다. 6장 끝났어요. 여러분 6장 끝났고 7장에 아까 제가 예고한 대로 얼리 스타킹을 합시다.

참석자 1 28:07
여러분 저도 이제 애들이 대학생 들어가니까 계속 진도 나간다고 미쳤다고 교수님하고 이러는데 지금 그래서 여러분 중요한 거는 공부할 때 보라 있잖아요.
그래요 여러분 자기 그래요 여기 요거가 있어요. 여러 가지가 그래서 지금 제가 내장된 훈련 로퍼의 평가 로표 이런 거 있잖아요.
그렇죠 어리 스텝핑이 여기 있거든요. 이걸 하고 싶은 건지 알겠죠?
얼리스타핑은 뭐가 있다는 걸 아는 거 근데 미리 좀 해놓으면 좋지 그래가지고 20분 남았는데 여기 보면은 워크플로우에 7.2절 들어가면은 245쪽에 이렇게 이 그림 있고 그쵸 여기에 적혀 있죠 그쵸 크게 이제 클래스 모델 API가 시퀀셜 모델 245쪽 245쪽 시점이죠.
시퀀셜 모델 삼성 API 마이드 서버 표시 이렇게 세 가지가 있어요.

참석자 1 29:10
여러분 시퀀스 모델 이제까지 여러분이 본 건 다 시퀀셜 모델로 이렇게 맨날 어레이처럼 만들어가지고 댄스 테스트 썼어요.
계속 근데 그거 말고 함성 API라고 그래서 뭔가 점 점 에드 점 에드 에서 할 수 있어요.
모델 만들어서 그래서 이거 이 두 가지를 제일 많이 써요.
알겠죠? 시퀀셜 쓰다가 또 함수형 쓰다가 막 왔다 갔다 하면서 쓸 수도 있고 이걸 제일 많이 쓰고 모델 서브 클래싱은 모델 원래 있는 거를 서브 클래스 만들어서 쓰는 건데 이거 잘 안 쓰는 게 좋아요.
이거 웬만하면 이런 거 좀 오픈 소스도 갖고 와서 이거 쓰면은 뭔가 웨이터도 잘 안 살아나고 불편하더라고요.
굉장히 저 사람들이 여러 가지 노스 플레이스 그래서 안 하려고 이거는 알겠죠 출점을 설명해 줄게요.
그다음에 시퀀셜 모델은 72.1점인데 245쪽에 보면은 항상 이렇게 시퀀셜 해가지고 이렇게 맨날 쓰던 거 있잖아요.
이렇게 그렇죠 여러분 이거 맨날 마다 이렇게 했죠 우리 계속 그쵸 이해되죠?
여러분 그래요.

참석자 1 30:18
그리고 이제 여러분 처음 보는 건데 모델 점 빌드라는 걸 부르면 빌드라는 거 여러분 커피 하나 불렀잖아 맨날 그쵸 커피 하나 불렀잖아요.
모델 만들고 저렇게 정의한 다음에 맨날 컴페이 불렀죠.
그쵸 빌드는 그냥 컴파일 할 때 아티마이저랑 로스 값 줘야 되잖아요.
그런 거 하기 전에 그냥 웨이트 값만 초기화시키는 게 이 빌드예요.
이런 게 있대요. 여러분 알겠죠? 안 써 거의 안 쓰는 건데 웨이트 가끔 막 구현시켜주려고 한 거예요.
알겠죠 근데 모델 웨이트 하면 이런 거 외우는 건 불가능할 거예요.
제가 보기에 좀 지금은 그냥 오픈소스로 오픈북으로 낼 수 있겠지.
만약에 내는지 모르겠지만 알겠죠? 여러분 웨이치 하면 이렇게 쭉 나와요.
웨이트가 뭐가 있다고 그렇죠 그리고 모델 점 서머리 하면 이렇게 뭐가 쭉 있는 게 다 보여요.
구조가 이제 이거 쳐 받았잖아 이제 그쵸 알겠죠? 여러분 모델 점 서머리 하면은 빌드나 컴파일 한 다음에 들어오면 된다고요 알겠죠?

참석자 1 31:19
그래 그러면은 지금 댄스가 2개 있었고 쉐입이 604 10 이렇게 돼 있었잖아요.
그쵸 난이 여기 왜 있겠어요? 여러분 이게 난이 여기 앞에 왜 있겠어요?
배치 사이즈겠지 배치 사이즈 모르니까 그쵸 인풋에는 배치가 몇 개 들어올지 모르니까 나이 있는 거예요.
그쵸 64는 뭐예요? 여러분 출력 계층의 출력 그쵸?
피처 개수 그쵸 어떻게 되는지 알겠죠? 여러분 그래요.
이 파라미터의 개수가 256개라고 나와 있는데 256개인지 어떻게 알았지?
여기서 이렇게 이제 어떻게 알았냐면은 여기 인풋을 여기 아까 첨부한 건데 여러분 이거를 여기 볼까?
아까 모델 로그에서 인풋 어디 갔어? 미안해요. 잠깐만요.
이거 어디 갔지? 인풋이 입수 줬어야 되는데 점수 어디서 했지?
빌드 뭐 해드 빌드 할 때 했어요. 여기 여기 인풋이를 여기 줬잖아.
그쵸 빌드한테 인풋이 앱을 줬어요.

참석자 1 32:31
인풋이 앱을 인풋 쉐입이 난 3 돼 있죠 난이 이게 뭐냐?
배치 사이즈 모르는 거 그렇죠 아무나 들어올 수 있는 거 그쵸 배치는 3이 이제 피처가 3개 들어오겠다라는 거죠.
그쵸 값이 숫자가 3개 들어온다는 거죠. 그쵸 3 피터 그래요.
그래서 그러면 이 3개 들어 와서 그러면은 바이어스까지 하면 4개가 되잖아요.
그쵸 4 곱하기 64면 256이 나온 거예요. 알겠죠?
알겠어요 여러분 3 곱하기 64면 256 맞잖아요 알겠죠 그리고 이제 여기에서의 파라미터 개수는 여기서 64이가 튀어났으니까 또 65 곱하기 10을 하는 거지 그쵸 그래서 650이 되겠다.
그쵸 알겠어요 여러분 어떻게 됐는지 알겠죠? 그래요.
그리고 이게 나중에 분석하기 쉬우려고 그러면 여기 네임을 주는 게 좋아요.
여기 데스 할 때 네임 네임 주면은 여기 나중에 이 나와요.
알겠죠? 그죠? 이게 47쪽에 있지 그래요.

참석자 1 33:38
그리고 뭐 다 됐죠 그러면 아 그리고 아니 마델점 에드도 내가 한 참배한 적이 없구나 이게 모델을 시퀀시를 만들고 이것도 시퀀셜 모델 만들 때 이렇게 하는 거 말고도 설명 안 했는데 미안해요.
이렇게 이렇게 하는 거 말고도 수 투자실 코드 실라이스에서 이렇게 하는 거 말고도 코드 7 es처럼 이렇게 시퀀스 정해 놓은 다음에 이렇게 에드 에드 할 수도 있어요.
여러분 어레이 추구하듯이 알겠죠 에드 에드 알겠죠?
여러분 이것도 시퀀셜이에요. 알겠죠? 이거 어펜드 하는 거 비슷하죠 그쵸?
원래 어펜드 파이썬에서 하는 거 그렇게 하는 거 된다는 거고 그다음에 뭘로 불러야 되냐면은 이렇게 시퀀셜 한 다음에 이게 7 7이 어디 있어?
코드가 248쪽에 시퀀셜 한 다음에 맨 처음에 이제 테라스 인풋을 맨 처음에 에드 할 수도 있어요.
인풋이에을 알려주는 거예요. 미리 들을 수도 있다고요?
알겠죠? 이렇게 하는 게 더 사람들이 좋아하는데 알겠죠?

참석자 1 34:49
베스 말고 인풋 처음 보죠 여러분 그쵸 이렇게 할 수 있다고 이게 인풋 이을 알려주는 거예요.
배치 사이즈는 안 알려주고 배치는 몰라도 되는데 몰라주는 고 3 주면은 이렇게 수입이 이제 개 3개짜리 벡터가 들어간다는 거지 그렇죠 한 배치가 한 데이터가 그래요.
이해됐죠? 여러분 그다음에 드디어 삼성 API는 심플해요.
삼성 API는 249쪽 이 코드 78에 보면 캐러스 점 인풋에서 인풋이라는 거 함수를 여기 봐 여기 뭔가 리턴을 받아서 만들어놓은 다음에 그다음에 여기다가 이걸 여기다가 이렇게 인풋을 여기 데스 데스 아까 제가 썼던 거 있죠?
걔를 에드 에드 했었었잖아요. 맨날 그러지 말고 여기다가 이렇게 괄호 해가지고 이프지를 줬어요.
앞에 나눴던 거를 이걸 입력을 주겠다는 거야. 그리고 또 여기 이를 만들었죠.

참석자 1 35:47
그쵸 그리고 또 다음 계층 이렇게 하면서 여기다 또 이렇게 주고 이런 식으로 알겠죠 혹시 제가 이거 시험에 낸다고 그러면 이런 거는 절대적으로 오픈북으로 내겠지 이런 거를 넣으라고 안 나겠지 그쵸 알겠죠?
여러분 그래요. 그거 뭔지 이해하시면 됩니다. 따라 있으면 되는 거야.
알겠죠? 맨 마지막에 인풋을 아웃풋 잡아서 인풋에다 이거 이거 하는 거 알고 시작할 때 이걸로 시작하고 끝날 때 얘로 끝난다는 거 알려주는 식으로 해요.
알겠죠? 그렇게 해서 정의하면 앞에랑 똑같아져요.
앞에 방금 했던 앞에는 시퀀셜 API였고 이건 함수형 API예요.
여러분 이게 함수 얘 이게 함수 같잖아. 여기 함수 함수잖아 함수 커라는 것처럼 했잖아.
다 레이어를 그쵸 근데 이거 함수형이라고 불러요.
이게 왜 이 함수형이라고 부르는지 알겠어요? 여러분 각 계층을 함수처럼 보고 있잖아.

참석자 1 36:39
그쵸 앞에서는 각 계층을 함수를 안 보고 그냥 뭔가 시퀀셜하게 넣었죠 그쵸 에드 에드 이런 식으로 그래서 앞에는 시퀀셜 얘는 함수죠 알겠죠?

참석자 1 36:52
그리고 다중 입력 다중 출력 이런 거 있는데 시간이 없어서 다음에 해야겠을 것 같아요.
여러분 그러니까 왜냐하면 251쪽에 다중이 다중력이 있거든요.
시간이 없어 밑에 그래가지고 내가 얼리 스타핑을 한 줄 알려주고 싶다 그랬잖아요.
여러분

참석자 1 37:17
메리 스타틴이 어디 있냐면은

참석자 1 37:27
264쪽 264쪽에 12분 남았는데 한 번 너무한가 너무한가 얼리 스타킹이 있다는 것만 알려드리고 그거는 기말고사 범위를 뺄게요.
여러분 있다는 기말고사 범위를 뺄게요. 여러분 기말고사 범위로 제대로 못 가르칠 것 같아서 얼리스타핀 얼리스타핀 있다는 거 보여주고 그냥 지나갈게요.
여러분 일단 여러분한테 보여주고 싶은 거 64쪽 보세요.
64쪽에 멀리 아니 어디 갔어 이게 지금 어무 깊숙이 너무 길어 멀리서부터 어디 갔어 여기다 얼리 스타핑이 여러 개가 있는데 얼리 스타핑 마제 체킹 이런 게 있는데 얼리 스타핑은 얼리스터킹 콜백은 설명 265쪽에 우리 265쪽에 블록 2개가 돼 있잖아요.
그쵸 블락 엔타트 단락 2개죠. 얼리스타트 콜백은 정현이 포크 동안 모인 조지표 형성되지 않을 때 훈련을 중지 이런 걸로 돼 있죠.
여러분 그쵸 그런 거예요. 이게 원래 저절로 되면 좋은데 그쵸 보통 이거는 마델 체킹 마델 체크 포인트 콜백과 함께 사용하도록 돼 있죠.

참석자 1 38:40
그쵸 마델 체 포인트 챗 포인트라는 말 들어봤어요.
여러분 게임할 때 여러분 갑자기 죽어버리면 차 다시 시작하려면 너무 억울하잖아요.
그쵸 보통 게임의 챗 포인트는 스테이지죠 그쵸 레벨 근데 어떨 때는 레벨이 또 굉장히 한스케이지가 길면 중간에 어디 특정 유통 관절 다시 시작하고 그러잖아요.
책 포인트는 그래서 뭐냐면은 저장해 놓는 시점이에요.
그쵸 마르체크 포인트라고 하면은 콜백을 모델에 저장해 놓는다는 뜻이에요.
여러분 모델 저장시키는 거야 저장을 시켰는 거를 우리 살려야 될 거 아니에요 저장을 여러 점 해놨다가 그렇죠 그래서 얼리 스타핑 콜백은 그래서 이제 나중에 원래 이제 여러분 해보려고 그러면 이게 얼리 스타핑 하려고 그러면서 어떻게 돼야 돼요?
과적합이 일단 돼야 되잖아 지나쳐야 되잖아요. 그렇죠 지나쳤다가 이거 계속 향상이 안 되는구나 그래 왜냐하면 항상 안 된다는 게 뭐예요?

참석자 1 39:36
여러분 밸리데이션 에러가 형성이 안 돼 밸리데이션 그쵸?
루스가 형성이 안 되는 거잖아요. 그러면 옛날에 밸리데이션 루스가 제일 작았던 때로 살려야 될 거 아니에요?
채권이 돌아가야 될 거 아니에요? 그걸 의미하고 있어요.
알겠죠? 있다는 것만 알아두시라. 알겠죠 시험에 이거 뭐 내는 건 좀 애매한 것 같아요.
지금 시점에서 너무 좀 그래 그렇죠 시험에 안 나오는데 있다는 거 기억하세요 여러분 알겠죠?
됐죠 그리고 여러분 이제까지 모델 세이브 한 적이 한 번도 없었죠 그렇죠 그것도 좀 이상했잖아요.
그렇죠 나중에 여러분 지금 이제까지 배웠던 이 소스 코드 갖고 하면은 맨날 다시 훈련시켜야 돼.
옛날 훈련도 했던 거 다 날아가고 그렇죠 이상했지 제 포인트 쓰면은 저장이 돼요.
알겠죠? 파일로 마르시 포인트에서는 파일 패스 같은 거 줘요.
그래서 그러니까 이게 파일 패스라고 주잖아.

참석자 1 40:28
그쵸 교과서에 265쪽에 파일 패스 이 현재 폴더에다가 저장하는 거고 여기다가 이제 드라이브 패스 주면은 진짜 다른 데 앞에 다른 디렉터로 저장할 수도 있고 이해되죠 여러분 파일이 있어야지 뭔가 사라지지 않잖아 그렇죠 난바라 타일로 그쵸 이해되죠?
여러분 그래요. 그리고 그냥 세이브 하는 것도 교과서에 225쪽에

참석자 1 40:54
265쪽에 마델 점 세이브 하고 이렇게 이게 이렇게 여기 파일 이름 265쪽에 맨 마지막에 있잖아요.
마델 점 세이브 그쵸? 표출하는 거 보이죠? 여러분 이렇게 하면은 그 모델 웨이트가 저장이 돼 핏 한 다음에 그럼 세이버스 나중에 써야 될 거 아니에요?
모델에서 그렇죠 여러분 이거 모델 다 훈련시킨 다음에 피터 나이 들면 뭐 불렀어요?
여러분 이벨류에이트 부르던지 이벨류에이트 부르는 건 테스트 데이터 갖고 하는 걸 그렇죠 프리딕션 플린트 불렀죠.
그거 그 내용을 여러분이 그거 하려 할 때 이거 세이브 한 거를 로드 해가지고 로드 마델 교과서에 225쪽에 적혀 있죠.
저장했던 걸 로드 마델 해가지고 그 모델 가지고 프린트하고 하겠죠 그쵸 이해되죠?
여러분 파일에 있는 거 갖고 여러분 여러분 시크 같은 거 공개했다고 그랬잖아요.

참석자 1 41:47
웨이크를 이렇게 해서 하는 거예요. 여기 이거 인터넷에 올라와 있고 허기 페이스 같은 거 다 올라와 있고 다운 받아가지고 이렇게 쓰는 거예요.
진짜로 이해되죠? 여러분 그리고 이거를 이제 본 디바이스에 올리려고 그러면 어떻게 한다 펜스 플로우 라이트로 변환시켜가지고 올리면 되고 알겠죠 iOS나 안드로이드나 이런 데 맞춰가지고 알겠죠 이거는 시험에 냅시다.
세이브 라이드 요거 말고 이거는 이것도 오픈북으로 오픈북으로 알겠죠 이거 모르는 건 좀 그렇잖아 안 그래요?
여러분 오픈복으로 내겠다고 한번 여러분 머릿속에 넣어보셔 알겠죠 세이브랑 로드 모델은 오픈복으로 내겠다고요.
알겠죠 알겠죠 여러분 이거 알고 있어요. 모르는 건 좀 그렇잖아요.
그쵸 중간고사 보는데 이거는 중간고사에 무시 안 돼요.
퍼스트 하지 마시고 알겠죠 이걸 알고 있어요. 알겠죠 오픈 국으로 이걸 어떻게 외워 근데 있다는 걸 찾아 쓸 수 있어야 된다고 알겠죠.

참석자 1 42:43
난 몰라 이러는 게 아니라 그렇죠 여러분 시험 볼 때 정리한 노트 같은 거 있잖아요.
못 보는 거 아주 대화형 정리하는 게 좋지 그쵸 뭐 이거 꼭 헤드 가도가도 된다 그랬잖아요.
여러분 하이프 소리 되게 더럽게 못해가지고 저도 맨날 타이핑해서 한 거죠.
저도 그러니까 손으로 쓰는 게 글씨가 내가 못 알아봐 내 글씨를 그래서 아니 만들어 하세요.
인쇄를 해오던 그쵸. 근데 컨트롤 f 같은 거 하는 게 편하니까 당연히 이제 패드 하는 게 더 좋지 안 그래요 여러분 파이팅 하는 게 근데 여러분 그게 본인 실력이잖아.
진짜로 실제로 여러분이 할 수 있는 거잖아요. 찾아가서 할 수 있으면 알겠죠 그건 나쁜 짓이 아니야 정말 좋은 짓이야 알겠죠 채팅이 아니고 여러분 제가 문제 엄청나게 많이 내기 때문에 시간이 없어요.
거기서 공부하고 있으면 안 돼. 빨리빨리 풀어야 돼.
알겠죠 이리 고민되는 거 있을 것이고 그렇죠 그래요.
월요일 날 수업 시간 중에 있을 테니까 질문하러 오시고 6분 남았네.

참석자 1 43:42
그래요. 그래서 이거 이거 이거 이거 어떻게 쓰는지는 시험에 안 내는데 있다는 거 알고 계세요?
알겠죠? 그래요 그러면 우리 뭐 할까요? 리뷰 해야지 리뷰 리뷰 리뷰를 여러분 시험 내는데 여러분 선배들이 되게 많이 틀리던 게 녹화본에 있어요.
세상에 그래서 제가 얼굴 보고 한번 더 떠들어줄게요.
뭐가 있냐면은 너무 안타까워서 녹화고 사장이 있지.
사장 사장이

참석자 1 44:18
화장의 분류와 회기에서 분류가 앞에 두 개 있지 분류로 제가 다 녹화했잖아요.
그쵸 다중 분류 다중 분류 4.2점 가중 분류에

참석자 1 44:37
자성률 4.2절이 4.2절이 몇 쪽인가?

참석자 1 44:44
4.2절 4.2절에서 여러분 예측하기 여기 가세요.
167쪽

참석자 1 45:04
요거 이거 보면은 오픈북에 오픈북 문제죠. 당연히 이거는 보면은 새로운 데이터 예측하려고 그러면 이제 항상 이제 모델 아까 세이브 한 거 갖고 하는 게 좋겠죠.
이런 거 세이브가 되는 바로 핀타서 받아 쓰는 게 아니라 로드 한 다음에 이렇게 쓸 수도 있겠죠.
여러분 이해되죠? 그쵸 프린트를 이렇게 부르잖아요.
그쵸 엑스 테스트 이렇게 부르잖아요. 그쵸 엑스 테스트에 이거 쉐입은 반드시 배치 디멘전이 있어야 돼요.
그쵸 배치 디리전이 없으면 쉐입이 안 맞으면 이거 에러나요?
알겠어요? 알고 계시고 그쵸 쉐이 배치 디메던 없이 그냥 원래 데이터 그대로면 안 돼.
여러 m 리스트에 여러 리스트 m 리스트에서 만약에 우리가 데스트를 쓰면은 그냥 28 곱하기 28을 그냥 플랫하게 만들어버리겠죠 그쵸 플레틴이라는 거 내가 좀 여러분한테 가르쳐줬나 배운 적이 없네.
리쉐이 있으니까 리쉐이 있잖아요. 여러분들 리쉐 배잖아 리쉐입은 배웠죠?

참석자 1 46:03
프레트는 안 배웠나 안 배웠지 교과서에 없었죠 없었나 없었으면은 미시를 제가 시험에 낼게요.
때 혹시 제가 그리고 이상한 거 냈다 그러면 질문하세요.
제가 계속 앉아 있을 테니까 집으로 하면 시간이 없을 텐데 그러네.
플래튼 플래튼이 리쉐입이랑 똑같은 거예요. 여러분 됐어요.
그냥 리쉐입이라 얘기해요. 실수 안 하고 그다음에 엑스 테스트 있잖아 엑스 테스트 이게 만약에 이게 지금 여기서는 지금 무슨 데이터였냐면은 뉴스 기사였으니까 텍스트라서 또 원화 터 멀티 벡터로 와 있었고 그렇죠 그렇죠 여러분 만약에 이미지였으면은 이미지를 다 이제 펼친 거였지 그쵸 28 곱하기 28 이렇게 벡터가 있는 거죠.
그쵸 이미지 픽셀들이 그래서 쭉 근데 그게 항상 이제 여기는 차원이 항상 이 두 개 이상이어야 돼.
그쵸 2개여야 돼.

참석자 1 46:53
두 개 데스에서는 그렇죠 왜 이렇게 두 개냐면 차원이 왜 액시스가 2개인 이유는 배치 때문에 그렇죠 됐죠 여러분 그다음에 그래서 결과도 여기가 어떻게 배치 사이즈만큼 나오기 때문에 왜냐하면 한 개만 있어도 만약에 엑스 테스트가 이 데이터 1개만 있어도 차원이 2차원이었고 이렇게 0을 해야 첫 번째 게 나와요.
그쵸 그쵸 쉐입이 46개의 멀티아 벡터였나 봐. 그쵸 멀티엣이 아니지 이거 이거 이거 뭐야 이게 프리시전이 이게 내용이 뭐 있지 이게 이게 토픽이 46개구먼 여러분 이거 이거 이게 문제가 뭐였냐면은 뉴스 기사 분류인데 46개의 토픽에서 스포츠 이런 거 여러 가지가 있는데 46까지 나 놨으니 이 사람들이 너무 많아요.
그렇죠 그랬어요. 2분 남았네. 그다음에 이거 이거 다 썸 하면 당연히 0 1 내야 되고 그렇죠 실제 여기 있는 데이터들이 어떻게 생겼냐 0.001 이렇게 막 있어 그쵸 제가 녹화는 열심히 해놨어요.

참석자 1 47:52
그쵸 정성 들여서 지 어쨌든 이거 실제로 여기 클래스를 뽑아내려고 그러면 클래스가 이거 여기 어떻게 되냐면은 클래스가 0부터 45까지 있겠죠.
실제로 클래스는 그쵸 그러면은 나온 프리딕션즈 0에다가 만약에 이게 1 2 3 이렇게 있을 텐데 네 번째 클래스면은 여기 3이 튀어나 이게 3이 튀어나오는 게 아니라 실제로 여기 제일 큰 확률이 튀어나오잖아요.
0.999 이런 식으로 그쵸 국어에서 여기 실제 찾아야 되는 게 인덱스잖아요.
인덱스 그래서 이게 arg 레스를 써야 돼요. 그쵸 여러분 이해돼요.
여러분 arg 렉스라는 함수를 써야지 넌 파이 arg 렉스를 쏴야지 3이 튀어나온다고 알겠어요 여러분 그거 아닌데 오픈북인데 모르면 이상하잖아요.
그쵸 제일 큰 여러 개 값이 있는 것 중에서 값이 있는 것 중에서 제일 큰 값의 인덱스가 AI 배수라는 거 튀어나와요.

참석자 1 48:57
알겠어요 설비 냈다고 알겠어요 너무 당연히 해야 되는 거예요.
만약에 그리고 레이블이 0부터 이렇게 돼 있지 않고 이게 하나씩 증가돼 있는 상태야 숫자 같은 경우에 그렇잖아요.
여러분 그럼 더하기는 해야죠. 여기다가 그렇죠 이해되죠 그렇죠 그게 실제 문제에서 많이 쓸 일이 많아서 그래요.
알겠죠 원래 레이 페이 1부터 시작하거든요. 보통 스팟으로 해도 알겠어요 여러분 그럼 더하기 해야 될 거 아니야 레이블이 0부터 시작해 만약에 신자 같으면 0부터 9까지니까 더하기를 할 필요 없죠.
그쵸 바로 인덱스 자체가 그냥 끝나잖아요. 그렇죠 근데 숫자가 만약에 그렇지 않다 그러면 그렇죠 숫자가 레이블 1부터 시작한다.
로이터 이거 46개 토픽이 1부터 이렇게 46까지로 한다고 쳐요.
그러면 더하기는 해야 될 거 아니야 토픽이 이제 얘기하려고 그러면 그렇죠 알겠죠.
그리고 여기 파이썬 디시노 이런 거 막 교과서에 나오잖아요.

참석자 1 49:59
그쵸 키벨 이런 거 그거 당연히 알고 있다고 생각해.
이건 내 거예요. 알겠죠? 파이슨 딕셔너리 같은 거 모르면 안 돼.
알겠죠?

참석자 1 50:09
그래요 여기까지 할게요. 여러분 월요일에 복사한 보고 다른 거 할 사람 다른 거 하시고 하세요.


clovanote.naver.com
