딥러닝 day20
2025.05.26 월 오전 10:02 ・ 49분 23초
심승환

참석자 1 00:00
처럼 동영상 만들지는 않는데 뭔가 만담하는 식으로 서로 이렇게 바뀌어 있는 거예요.
내가 보여줄게 다음 주에 여러분 다음 시간에 준비해서 보여줄게요.
여러분 이게 정말 여러분 공부할 때 획기적으로 도움이 될 거예요.
콘텐츠가 되게 좋다면 콘텐츠가 되게 좋은데 논문이 되게 이해하기 힘들잖아요.
여러분 근데 이걸 하면 되게 이해가 어쨌든 모티베이션이나 이런 거 보고는 확실하게 통찰력 같은 거 주는데 디테일한 건 따로 공부해야 되잖아.
어차피 집에 그런 건 수학이니까 어쨌든 근데 약간 문제가 이거 3개밖에 안 되더라고요.
가지만 동영상 하루에 3개만 만들 수가 있어요. 동영상이나 음성 파일을

참석자 1 00:43
아무도 못 그러면 일단 진도를 나갑시다. 정말 지금 지난 시간에 식장 다 한 거 맞나 다 했다고 보셨죠?
딥러닝은 그게 없으면 아 시장을 부장이 장이 아닌지 두 장을 다 했다고 보는 거는 제가 부장이 이제 컴퓨터 비전 CNN 쪽이었는데 CNN 보그죠 대주고 CNN은 원래 제가 이제 어떻게 된 거냐면 사실 시장에서 많이 안 했는데 2장에서 제가 폴리 콤플루미나 뉴럴 네트웍이라고 안 한 거는 명확히 뭐냐면 시간이 계속 움직이는데 그러니까 내가 노트북 예행 같은 것도 이렇게 약간 떠들기 시작하면 시간을 확 잡아가 되거든요.
근데 나는 그런 것도 좀 아는 게 좋을 것 같거든 세상에는 그러니까 지금 굉장히 이게 거의 뭐 한 달 단위로 엄청나게 변해요.

참석자 1 01:39
그렇죠 엄청나게 변해 사실 작년이랑 너무 다르고 그렇죠 한 달 단위로 너무 변해가지고 그래서 이든 근본은 근본인데 이것도 되게 339.2절에 이제 원래 이미지 분할 자체는 했는데 코드는 안 했지 333쪽 이거 사실 근데 이것도 되게 재밌는데 3주 334쪽 컴퓨터 트랜스 포즈 요거 이거 우리 할까 어떡하지 하고 금방 얼마 안 걸릴 것 같긴 한데 우리 해버리자 그냥 그렇죠 해버립시다.
여러분 찝찝하니까 나 이거 그럼 먼저 강의 자료를 좀 할 테니 강의 자료 이거 하려면 교과서 갖고 이해가 안 돼서 너무 그것도 훌륭하거든요.
그러니까 어디 있냐면 제 강의 자료에 CNN 제가 나 없앤 거 CNN에

참석자 1 02:41
제가 뭘 안 했냐면은 텐션 플로우 컨버션 이거 여러분 강의 자료 똑같은 거 맞나 다 제목이 이거 똑같은가 DS 이랑 맞지 여기서 쭉 하다가 제가 여기까지 다 하고 나서 강의를

참석자 1 03:03
유지 스윙 그냥 뭐라고 했고

참석자 1 03:11
여기 CN 아키텍처 이것도 그냥 제가 안 하고 교과서에 있는 걸로 퉁 치고 넘어가는데 그쵸 강의자료 줬는데 CNN보다 다른 게 더 관심이 생겨가지고 제가 약간 좀 하셨는데 CNN 여전히 유용하죠 여러분 다른 교과서에 이제 CNN 그게 중요한 거 갖다 놓고 여러분이 지금 트랜스퍼 러닝할 때도 이 중에 하나 골라 쓰는 거고 얘네들이 다 이제 보통 라이브러리에 보통 얘네들이 다 올라와 있는 거죠.
펜스 플로우 파이프스고 다 워낙 유명해서 갖다 쓸 수 있게 웨이트도 이미 특정 데이터셋에 대해서 예를 들어서 이미지 넷이나 시파나 사이파인가 발음 어떻게 하는지 모르겠네 걔네들에 대해서 뭔가 있는 거지 그래서 이게 르네 되게 옛날 거고 그다음에 알렉스넷 둥글렛 있는데 사실 VGD 레즈넷 익셉션 이런 애들이 이제 지금 쓸만한 오는 26페이지에서 디글렛도 있고 모바일 넷도 있고 여러 가지가 있어요.

참석자 1 04:01
그래서 리셉션도 있고 그런데 이제 생긴 모양이 이렇게 생겨 먹었고 보면은 여러분 지금 우리가 VTG랑 익셉션을 교과서에서 하고 있잖아요.
그쵸 교과서에서 이제 우리가 트랜스퍼 러닝 할 때 얘기했었잖아요.
VGG랑 익셉션은 제가 해보라고 그랬고 그쵸 여러분 뭔지 알아요 이거 이거 다 데스 와이즈 컨버레션 디발프 하는 게 익셉션이고 이건 좀 알아야 되는데 시험 시험에 약간 뭔지 알아야 돼요.
액시션이 됐어야지. 저자가 만든 거 했잖아. 손 보는 느낌 표정을 그렇죠 이렇게 챙겨 먹었어요.
사실은 아까 다르게 그렸지만 그쵸 그리고 드리시지 여러분 했던 거 있잖아요.
그 점점 VG 16 했지 우리가 구지 16 점점점 이렇게 간단하게 완전히 시퀀셜하잖아요.
여러분 이해되죠? 완전히 시퀀스 라니 착착착착 그쵸 컴버 CNN CNN 맥스플링 CNN CNN 맥스플링 이런 식이잖아요.

참석자 1 04:59
그쵸 이거 뭔지 몰라요. 여러분 이거 했던 거 계속 알죠 그림도 크게 나와 있고 그 근육 있는 거 옆에 있잖아요.
그렇죠 이 그림 뭔지 알죠? 여러분 아니에요 왠지 몰라요 여러분 트랜스포 러닝 할 때도 얘 갖고 있잖아.
교과서에서 VD 16 갖고 했어요. 교과서 얘에서 아니에요 모르겠어 됐죠 그냥 안 보여줘도 되겠지 됐지 그다음에 여기 레즈넷은 왜 이렇게 유명하냐면 레지 듀얼 이거 이거 있잖아요.
여러분 레지 듀얼 커넥션 그쵸 스틱 커넥션이라고 하기도 하고 걔가 있어요.
그쵸 보이죠 여러분 스키커넥션 다시 EP이 또 넘어가잖아.
CNA 끝나고 나서 또 넘기고 넘기고 하는 거 이거 다 유명하잖아요.
그쵸 보이죠. 순서가 교과서에 이렇게 교과서가 아니라 지금 제가 여기 다른 데서 지금 갖고 왔는데 사실 좀 약간 굉장히 심플한 게 이 VGG고 되게 간단하죠.
구조가 그쵸 레드넷은 여기 약간 좀 복잡하기 시작하지.

참석자 1 05:51
약간 여기 스티커니션 같은 것도 있고 그쵸 그리고 엑스션은 뎁스 와이즈가 또 새로 들어갔고 그쵸 인셉션도 복잡해 그쵸.
근데 사실은 순서가 이게 왼쪽부터 왼쪽부터 이게 약박스 왔다 갔다 하는데 이게 실제 순서고 제가 정리해 본 거고 VGG 인셉션 이런 게 있는데 인셉션이 인셉션도 사실은 이걸 레지도 레터가 벌써 있지.
레지디얼이 넘어가 넘어가서 나오니까 레지도열 자체는 아니구나.
여기도 레지듀얼이 있지. 사실은 보면은 여기 익셉션도 중간에 막 넘기고 하잖아요.
그쵸 옛날 게 넘어오잖아. 앞뒤로 그쵸 인셉션도 우리가 익셉션 그쵸 그래요 이런 게 있었는데 인셉션보다 익셉션이 잘 되는 걸로 유명해요.
훨씬 더 성능이 좋은 걸로 익셉션 되게 복잡하죠 여러분 복잡하고 긴 거는 사실은 되게 메모리도 많이 쓰고 성능 메모리 많이 쓰는 놈이에요.
되게 느리고 짧으면 짧을수록 되게 빠른 놈이지 사실은 그래요.
여러분 알겠죠?

참석자 1 06:58
그래요 그런 게 있었고 그다음에 이제 원래 컨볼루션에 우리가 컨볼루션을 맨날 2D만 했잖아요.
2D만 했지 그쵸 항상 컴프 2D만 했잖아요. 기억나세요?
그거 말고도 먼지도 가능하고 3도 가능해요. 컨플루션의 필터를 만들 때 거기 모양 맞춰서 하는 건데 원디를 내가 갖다 놨나 원디 아에 안 갖다 놨네.
원d는 그냥 사실 가로 길이로 만들어 놓으면 되는 거고 사실 우리가 원래 원 곱하기 1 컴플레이션도 심지어 했잖아요.
그쵸 원 곱하기 원 커버레이션 해봤잖아. 우리가 그 원 곱하기 3 4 뭐 이런 거 하면 되겠지 뭐 그렇죠 똑같은 거지 그쵸 별로 따로 보여줄 필요 없겠지.

참석자 1 07:44
그러니까 그렇죠 이해되죠 그다음에 3D는 거기에 대해서 또 한 번 더 하는 거고 우리는 이거는 지금 동영상 이런 쪽에서 하는 거고 그다음에 다이얼레이션이라는 거가 또 있는데 이거 이거 그림으로 보여주는 게 좋은데 별거 아닌 건데 아니 안 할게요.
여러분 필요하면 나중에 보세요. 이 다이어레이션 레이트 나오는 게 원래 웨이브 쪽이나 이런 쪽에 하는 건데 오디오 쪽 하는 건데 제가 스타에 안 갖다 놨네.
넘어갈게요. 그냥 뎁스 와이즈 뭔지 알죠? 여러분 뎁스 와이즈 뎁스 와이즈 이렇게 그냥 말로 하면 되겠냐고요.
봤죠 우리 그렇죠 스와이즈 커모션은 알죠 우리 회차 봤잖아요.
개죠 교과서 나오는 거 동상 트랜스퍼 러닝을 위한 이거 하는 거는 제가 지금 이거는 강의 자료 갖다 놨어요.
제가 따로 지금 강의 자료가 아니라 따로 이거 갖다 놨어요.
이거 여러분 이 페이지 두 개는 그거 하라고 저기 숙제하라고 그렇죠 따로 또 뽑아놨죠 그쵸 그거예요.

참석자 1 08:43
그 내용이고 그다음에 오지 디텍션하는 내용이 원래 이제 욜로 때문에 너무 유명해서 지금 사실 욜로가 여러분들이 굉장히 제일 유명한 욜로가 지금 계속 잘 나오잖아요.
욜로가 지금 9가지가 나왔나 고든 2부터 완전 센터 조사를 했죠.
지금도 여러분 웬만하면 프로젝트 할 때 욜로 쓰면 제자 된다고 그러면 있는 이미지 갖고 처리하는 거는 근데 어쨌든 이게 욜로 전환해서 어떻게 되면 항상 여러 번 그래서 여러분 맨 마지막에 나오는 거 데스 네트워크로 어떻게 됐어요?
무조건 여러 개 중에 하나라는 거 하려면 그 분류하는 개수만큼 나오고 그게 픽셀별로 나오려고 그러면 전부 다 이제 여러 번 출력하는 식으로 했어요.
근데 그렇게 안 하고 한 번에 나오는 걸 만든 게 이게 이게 용어가 솔릭 컨볼루셔널 네트웍이라는 건데 이게 헷갈리는 게 제가 이제 폴리 커넥티드 네트웍이라는 거 있잖아요.
데스 마트에 있는 거 맨 앞에 있던 거 그것도 FCN이라고 번역이 돼버리잖아요.
그쵸 용어가 그럼 헷갈리잖아요.

참석자 1 09:50
그쵸 그리고 또 댄스만 있다고 덴스넷이라고 불러도 문제는 여기는 안 나와 있는데 여기 레즈넷 말고 또 댄스 넷이라는 것도 유명한 걸 학급들이 차지하고 있어요.
여러분 댄스가 사실은 레지듀얼렛을 약간 겨냥하고 댄스 넷이라고 부르는 게 또 있어요.
어쨌든 그래서 용어가 너무 힘들어요. 사실 여러분 조심해야 되고요.
그래서 댄스넷은 못 부르고 사실 그래서 약간 제가 전에 얘기해 줬나 여기서 여기도 이제 FCN이 플리 커넥트 네트웍이 아니라 플릿 컨퍼러스 네트워크를 의미해요.
이것도 정해졌어요. 이미 어렵죠 여러분 너무 별로지 별로인데 이렇게 정해졌어요.
아래서 약간 색색해줘야겠다. 그래서 또 아는지 모르는지의 문제니까 강의 자료에다가 용어 정리하는 데 있잖아요.
용어 정리 그래서 어떻게 용어 정리한 터미널로지 있죠 터미날러지에 중간고사까지 여기 있었죠 그렇죠 여기 있었죠 그쵸.

참석자 1 10:46
그리고 여러분 하이퍼 파라미터라고 부르는 게 이제 웨이트 말고는 데이트 바이스 말고 다 하이퍼 파라미터라는 거 알고 계세요?
여러분 하이퍼 파라미터라는 거는 원래 파라미터 말고 나머지들이에요.
알겠죠 이거 여러분 모르면 안 되니까 시험에 낼게요.
그러면 이것도 조금 클로즈 북으로 하이퍼 프라이스의 개념도 용어는 여러분들도 약간 뭐랄까 외워야 되는 거지 이거 찾아봐서 하는 게 아니라 약간 나중에 찾아봐도 되지만 시험 볼 때는 당장 외우고 있어야 말이 대화가 되잖아.
이게 용어를 모르면 대화가 안 되고 이해를 못하잖아요.
강의 들으면 서로 소통이 안 되니까 이거 외워야 된다고 말겠죠.
맨날 서 보고 있으면 언제 알아들어 그렇죠 소통이 안 되니까 외워야 돼요.
알겠죠? 카톡 파라미터를 외우면 이렇다는 것도 다 정리해 놓은 거.
그다음에 제가 조심할 게 FCN은 폴리 아까 컴벌루셔널 네트워크이에요.
이게 폴리 컬티드 네트웍이 아닌 거예요.

참석자 1 11:38
폴리 커넥트 네트웍은 MLP라는 게 있어 멀티 AP가 아니고 멀티 네 레이어 퍼 세트론이 이게 풀리 커넥티드 뉴럴 네트워크이요.
이거 댄스

참석자 1 12:11
알겠죠? 여러분 이거 보통 이제 이거를 이렇게 부르면 헷갈리잖아요.
그쵸? 이렇게 부르면 얘 때문에 헷갈리잖아요. 이렇게 불러도 댄스라는 게 또 뭔가 있어 댄스 넷이라는 게 원래 CNN 쪽에 적어놓을게요.
댄스 넷이라는 게 CNN CN 아키츠

참석자 1 12:42
용어가 이쪽은 정말 지저분할 수밖에 없어요. 왜냐하면 세상이 막 지금 변하고 있으니까 그래요.
그래가지고 MLP라고 불러야 돼요. 정해놓는 거 없네.
정현우가 하는 게 MMP라고 하는 게 정확한 용어였어.
정현우 훌륭했는데 멋있네. 좋은 논문에서 댄스만 있는 거를 그걸 우리가 MMP라고 부르더라고.
그래가지고 정현우가 헛소리하는 애는 아니니까 찾아봤지.
진짜 MC가 맞더라고. 오늘 학생회 때문에 울었나 보지.
진짜 무슨 얘긴지 알겠죠 MLP라고 불러야 돼. 진짜로 댄스 넷이라고 부르면 안 되고 알겠죠.
그다음에 저기 FCN 지금 이거 때문에 FCN이라는 용어가 풀리.
컨버셜 네트워크이라는 게 되게 특이한데 CNN밖에 없는 거예요.
마파에도 CNN인 거야. 보통 막판에는 어떻게 돼요?
여러분 CNNS는요. 댄스로 끝나죠. 그쵸 근데 얘는 막판에도 댄스로 안 끝나고 CNN으로 끝나요.

참석자 1 13:33
거꾸로 한 CNN 거꾸로 하시는 게 도대체 뭐냐 이게 그래서 이게 욜로 같은 데 핵심이라서 이것도 안 배운 게 별로긴 해요.
진짜 그래 갑시다. 2015년 논문이에요. 볼게요.
이게 일단 여러분 처음 배우는 건데 트랜스포트 컨볼루션이라는 게 있어요.
트랜스퍼 컨볼루션 트랜스 포드 트랜스포즈가 여러분 받은 게 전치 이런 뜻이잖아요.
그럼 트릭스는 트랜스코드 매트릭스 있잖아요. 그쵸?
저치 전치 설명을 되게 여러 가지로 하는데 어쨌든 문제가 뭐냐면은 이런 애들 있잖아요.
이런 애들 이런 애가 원래 입력이 있어요. 코너에 이렇게 있어요.
그런데 이게 나오게 하고 싶은 거야 이상하죠 그쵸?
어떻게 하는 거냐면은 이 원래 0이 있는데 얘를 이렇게 커널 크기만큼 뻥트시켜요.
풀려 그런 다음에 이 코너를 여기 뭐야 적용을 튀기 시키고요.
다시 잠깐만요. 그리고 나머지 0을 채워 0이 있다고 또 보는 거죠.

참석자 1 14:39
그러고 아니 아니 이거 이거 하는 방식이 0을 이만큼 펌핑 시키고 얘랑 얘랑 곱해가지고 이렇게 만들고 이를 똑같은 자리에 이렇게 곱해가지고 여기다 만들고 이를 여기다 똑같이 얘랑 곱해가지고 이렇게 만들고 3도 이렇게 만들고 이해되죠 여러분 어떻게 하는지 알겠죠?
같은 원래 크기가 원래 이제 원래 이게 이거 3 곱하기 3짜리가 컨버레션 하고 나면 1 곱하기 1이 되잖아요.
2 곱하기 2가 되잖아요. 그쵸 그것처럼 이제 원래 컨볼루션은 거꾸로 하고 싶은 건데 컨볼루션 거꾸로 하는 방식이 이 원래 인풋을 커널 크기만큼 뻥 튀기 시킨 다음에 원래 자리 유지시키면서 커널이랑 곱해서 다 한 장 다 더하는 거 한꺼번에 다 더하는 거예요.
나머지 자리는 영어로 패딩 한 다음에 그러면 이게 뭐 하는 짓이냐 원래 정보를 어 원래 원래를 이제 이거를 이제 컨버레션 하면 이게 나 원래 이제 이이 나온 게 거꾸로 하는 거예요.
거꾸로 전체 연산이에요.

참석자 1 15:49
트랜스포드 연산 수학적으로 증명이 된 거고 여러분 이렇게 이해하시면 돼요.
오픈북으로 낼 수 있겠지 이 정도는 따라 할 수 있으면 되는 거니까 그런데 이게 결국은 뭐 하는 거냐면은 옛날에는 컴플레이션 하면 항상 뭐였어요?
다운 샘플링 되는 거였잖아요. 줄어드는 거였잖아요.
패딩을 안 하면 다운 세플링 되는 거였고 매시플링 타임이 다운 세플링 되는 건데 얘는 억셉플링 되죠.
정보가 늘어나잖아. 그쵸 그래서 이거 뭐 하는 짓이냐 그래서 이렇게 원래 그라우트루스가 원래 이렇게 있는데 얘를 이렇게 이미지를 줄였어요.
4분의 1로 그러면은 이걸 그냥 만약에 그대로 이걸 이렇게 줄인 다음에 정보가 이제 이거 이렇게 보이게끔 그대로 유지하면서 픽셀 수로 이렇게 이렇게 줄인 다음에 늘리면 이렇게 돼요.
그 흐리멍당해지거든요. 근데 아까 컴브리던 뉴럴 네트워크 쓰잖아요.
트랜스 포즈 그럼 이렇게 살아날 수가 있는 거예요.

참석자 1 16:43
그 코너를 잘 쓰면 그래서 이거 슈퍼 레졸루션을 할 때 레졸루션 키우고 할 때 저걸 많이 쓰고요.
스트레스트 그는 마법 같은데 학습을 잘 시키면 이게 가능해요.
그래서 이게 지금 여기 여기 이거 이거 외워야 돼. 욜로는 플레임이 유 온리르 원스라는 거 외우시고요.
이거 시험에 나와요. 여러분 시험에 안 내도 하셨지만 내면 알아야 돼.
알겠죠? 욜로라고 그러면 이게 욜로는 uo 26 원스라는 거를 알고 있어야지 뭔가 소양이 있는 거예요.
알겠죠? 이게 뭐 하는 놈이냐 2016년에 나왔고 2018년에 버전 3 나왔고 지금 버전 나이까지 있는 건데

참석자 1 17:28
노래 모르는 사람이 별로 없긴 해요. 그쵸 그러면 이렇게 이 강아지 있으면 강아지가 이만큼 차지한다는 게 나오면 강아지랑 트레이가 달리지 그런 거 알고 있죠 여러분 이거 이제 슬라이드 이거 옛날 거지만 이게 워낙에 좋아서 여기 가서 보면 알 필요가 있어요.
이건 진짜로 진짜 없어 보였다. 언제 또 없앴어 금방 없앴네.
없앴네. 욜로가 너무 옛날 거니까 없앴나 보다.

참석자 1 18:01
이거 구글이 구글이 구글이 아니네

참석자 1 18:10
그이 아니네. 그거 나오는 워낙 그 사람 걸 여기저기서 다 뺏겨가지고 가만히 있을 걸 이 사람 뺏겨놨나 봐요.
이 사람 거기가 아까 그거 사라진 거를 이 사람이 뺏긴 것 같아요.
우리나라 워낙 많이 뺏겨놓기 때문에 다 보면은 어쨌든 이게 재밌는 게 안 나오나 딱 말았구나 이게 중요한 게 있는데

참석자 1 18:40
아닌데 이거 아닌데 얘가 이건 없었지

참석자 1 18:50
그게 참 내가 그냥 다 뺏겨놓을 걸 그랬네. 아깝다 슬프다 슬프네 없네 이게 욜로가 사실 뭐냐면 이게 여러분 우리 맥스 플릭하고 막 이렇게 하잖아요.
그쵸 여러 가지로 보면 지금 이 보면 자동차가 이렇게 큰 것들은 잡히는데 자동차가 멀리 있는데 요만한 자동차가 있다 쳐 요만한 요만한 거 그러면 이제 잘 안 잡히는 거예요.
그렇죠 이해돼요. 왜냐하면 자동차 특징을 이제 어쨌든 우리 욜로로 하는 거는 막 맥스플링 해가지고 이제 처음에 나오는데 나중에 사라질 거 아니야 그게 맥스플링 다 해버리면은 조그만 거는 이해되죠.
그래서 처음부터 보지 않으면 잘 안 보이잖아요. 그래서 그런 것 때문에 욜로가 특이하게 한 게 뭐였냐면은 보면 이렇게 여기 조그마한 거 해서 더 비슷하고 더 큰 거 더 이제 스피트 하면 또 디테 디트하고 큰 것도 디스하려고 큰 거 중간 크기 작은 거 다 디펙트 하려고 여러 번 이제 디텍트를 하는 걸 해요.

참석자 1 19:53
스토리 이해되죠 여러분 피트는 마지막에 연상하는 게 아니라 중간중간 계속한다고 이해되죠.
이해되지 그거 일단 여러분 그다음에 또 재미있는 게 이게 뭐냐면은 막판에 어쨌든 원래 이미지가 이렇게 생겨 먹었으면 막판에 결과가 원래 배수로 끝나는데 우리는 항상 여기는 보면 마지막에 이렇게 이렇게 이렇게 이게 지금 결과가 세 번 나왔잖아요.
세 번이 전부 다 하나가 전부 다 이제 이미지 크기면서 보면은 한 이미지 이미지 픽셀 안에 이런 데이터가 있어요.
엄청 많지 그게 뭐냐면은 이게 이게 이게 뭔가 이게 내가 확신하는 정도를 나타내는 스코어고 그다음에 클래스가 만약에 천 개가 있으면 천 개만큼 이렇게 되는 거예요.
천 개 중에서 내가 뭘 얼마큼 확률이 확실하는지 그다음에 요 픽셀에 대해서 박스를 내가 이렇게 칠 수 있다라고 나오는 거예요.
txty TWTH 이해되죠?

참석자 1 20:47
여러분 TXT y는 이 가운데 점이고 twti는 이 가로 세로 가로 세로 가로 세로가 이해되죠 여러분 그러면 딱 그림을 그릴 수 있잖아요.
그렇죠 이해되죠 여러분 가로세로 그러면 딱 박스를 칠 수가 있잖아.
이 정보가 다 박스를 칠 수 있겠죠 박스를 치고 여기 있는 것 중에 제일 큰 걸로 AIG 맥스로 라이브를 딱 표시할 수 있겠죠.
이해돼요. 여러분 각 핏셀별로 그리고 이 po가 po가 만약에 숫자가 0.5가 안 넘어가 확신하는 정도가 그러면 박스를 칠 필요가 없고 0.5가 넘어가면 박스로 쳐야겠구먼.
이거는 확신하는 거네 이러면서 그쵸 이렇게 하면 좋은 점이 그래서 어쨌든 한 군데에서 2개 2개 동시에 나올 가능성이 별로 없으니까 보통 이제 어쨌든 강아지하고 옆에 이렇게 쳐가지고 또 고양이 나오고 막 막 벅스가 겹치면서 겹치는 거 보이니까 겹치는 거 그림이 겹치는 거 보잖아요.
카바 여기 또 있는 거 또 알았어.

참석자 1 21:45
여기 카버 또 있는 거 또 알았죠 여기 중앙점이 여기 있는 거고 여기 픽셀에서 한 거고 얘는 여기 픽셀에서 한 거고 가로 세로가 나오고 이해돼요.
이해되나 이게 XY는 왜 필요하냐면 이게 맥스 플림이기 때문에 원래 정보가 남아 있어야 되잖아요.
이게 원래 그림에서 맥스 y가 뭔지 알아야 되잖아요.
각 픽셀에 나의 원래 x y 크기랑 XY 좌표랑 가로 세로가 다 나오게 그렇게 예측을 해요.
네 중앙점 가로 세로 크기 그다음에 내가 얼마나 확신하는지 그리고 이 각 클래스별로 얼마나 확률이 어느 정도 되는지 그렇게 해서 이 한 결과가 나오는 게 어마어마하게 많죠.
이해돼요. 요 픽셀 하나에 이게 이렇게 두껍게 나온다.
CNN 마지막 막판에 우리가 어떻게 하냐 옛날에는 다 한 다음에 훈련시켜가지고 뭔가 다시 대시켰잖아요.
그게 아니라 그냥 나오는 대로 그대로 두는 건데 막판에 나오는 거를 저런 식으로 훈련시키는 거예요.

참석자 1 22:44
두껍게 하는 거는 이 확률 값과 x값 y 값 넓이 값 이렇게 넓이 값 넓이 넓이가 간다.
가로 세로 그렇죠 확신하는 정도 이렇게 훈련시킨다고 그래가지고 하는 이 설마 잘 될까 싶어서 너무 잘 되는 그래서 이렇게 한 여기 한 픽셀이 이렇게 내놓고 보여요.
여러분 이게 b가 있고 여러 개 세 개가 있죠. 3개 b가 1이라는 게 있지 b가 왜냐면 아까 중간에 튀어나오는 한 번 두 번 세 번 튀어나오잖아요.
이게 유지가 돼요. 세 번 이번 세 가지가 있는 거지 일반적으로 이런 거는 이해되죠.
작은 레졸루션에서 큰 레졸루션에서 더 큰 레졸루션에서 이 세 가지가 나오죠.
여기 지금 이게 예를 들어서 이게 만약에 클래스가 천개고 천 개면은 그리고 이렇게 지금 세 번 하죠.
여러분 이런 거 하나 둘 셋 그러면 한 픽셀의 두께가 얼마나 되겠어요?
4 더하기 다 적혀 있네. 4 1 천 더하기 더하기 해서 그렇죠 곱하기 3 이렇게 이렇게 많은 결과가 탁 튀어나온다고 두께가 이해되죠.

참석자 1 23:57
그렇게 많이 튀어나온다고 이해되죠 그렇게 만들어내는 거예요.
이런 걸 하고 있는 거고 이런 게 여러분 이렇게 마지막 막판까지 평면으로 나오잖아요.
면 평면이 아니지 평면 여러 개 이게 평면 하나 하나하나 다 해당하겠죠 그쵸?
이 평면 하나하나에 해당하지 평면이 이만큼 있는 거잖아요.
지금 그쵸 뭔 말인지 몰라요. 여러분 내가 뭐 말하는지 이게 이렇게 저렇게 이게 이렇게 있는 거야 이 안에 이 정보가 들어간다고 그리고 여기 요거 하나 박스가 3개 있고 이거 하나 이렇게 들어가 있다고 박스가 3개인 거는 이 박스 2 박스 이 박스 됐어요.
이게 막판에 어쨌든 댄스로 나오지도 않고 어떻게 됐어요?
완전 또 CNN처럼 나왔잖아요. 원래 입력 모양처럼 비슷하게 나왔죠.
그렇죠 3차원으로 이해됐어요. 심지 샘플이 여러 개를 하면 어떻게 되겠어요?
배치로 하면 또 하나 더 있겠지. 디맨드는 4차원이지.

참석자 1 25:09
그쵸 동영상으로 하면 이렇게 될 거 아니야 코로나에 프레임별로 이게 엄마 어쨌든 이 출력이 되게 많아.
그쵸 재미있지 않아요 여러분 훌륭하지 이런 것도 된다는 게 안 되는 게 없다는 거예요.
여기는 안 되는 게 없다는 거예요. 그게 되게 훌륭하고 이게 이게 또 중요한 게 만약에 옛날처럼 계속 그런 네트웍을 썼으면 여러 번 해야 되는 처리를 얘는 한 번만 쫙 하면 나오는 거야.
유 온리 원 유 온리 원 유 원스 그쵸 욜론을 외워야 돼.
너무 이름 장명 센스도 참 훌륭하지 않아요. 유릴루 원스래 참 진짜 멋있잖아.
안 그래요 별로 신난다. 이럴 때도 있고 그렇죠 되게 좋잖아요.
신나지 진짜 이 사람은 그리고 참 대단하네. 진짜 앞에 다 옮겨버리고 그래요.
그리고 원래 다크넷이라는 걸 원래 기반으로 해서 만들었고 이 씨로 짰어요.
처음에 예산이 근데 이제 파이토치로도 막 버전이 있고 그래요.
됐지 뭐예요?

참석자 2 26:10
55페이지

참석자 1 26:11
95페이지 여기

참석자 2 26:13
그럼 그 한 픽셀당 4 더하기 1 더하기 곱하기 3이라

참석자 1 26:21
그렇지 한 픽셀당 맞아 한 픽셀당 픽셀이 이게 만약에 32 곱하기 322m지면은 거기다 곱하기 2 32 곱하기 32를 해줘야지.
전체 딱 나와 전체 결과가 나 결과가 틀린 거지 이해돼요.
됐어요. 정확히 이해했어요. 여러분 욜로가 그래요.
여러분 나중에 욜로는 여러분 나중에 욜로를 가지고 구경해서 실제 일할 때 롤러 아웃풋 갖고 중요해서 제대로 됐는지 검증하고 이러는 것도 하고 막 그렇기도 해요.
여러분 실제 일하면 여러분이 이상하게 나오면 알겠죠.

참석자 2 26:59
근데 한 픽셀량인데 왜 가로 세로의 값이 필요한

참석자 1 27:03
이게 맨 처음에 얘네들이 할 때 이게 여기 여기서 할 때 이 정보를 유지하기가 힘들어서 가지고 오는 것 같더라고요.
이게 어디 건지 이게 실제로 여기 얘는 조그매진 거잖아요.
원래 여기서의 가로세로 원래 픽셀을 유지하고 계속 가야 되기 때문에 필요한 것 같더라고요.
얘가 이렇게 만들었을 때 이렇게 만들어 이렇게 만들었을 때 있잖아요.
요 픽셀이 실제 여기서는 어디에 해당하는지를 하려고 하는 것 같아요.
혹시 없애고 잘할 수 있으면 그 논문 쓰던지 아니 진짜로

참석자 2 27:32
진짜 기억하려고

참석자 1 27:34
그런 거 프라프 게이션 하려고 쓰는 것 같더라고

참석자 1 27:40
그래요. 됐죠 그리고 이것도 옛날에는 맨 처음에 이렇게만 끝났다가 나중에 이제 이런 기능이 막 나오는 거야.
두 번 세 번 작은 것도 찾지 달라고 이렇게 하면 작은 디테트가 잘 안 되거든요.
그러니까 이런 거 있잖아요. 여기 아까 한 30분 정도 됐어요.
아 여기 보면 요하를 잡아내는 거는 미안하잖아요.
그게 그렇죠 타이트성이 딱 보이잖아. 우리가 사람이 봐도 참 헷갈리겠다.
여기 보이는 차를 전부 다 체크하라 그러는데 여기 하나 또 있어요.
또 하나 또 있어요. 이게 보면은 인스턴스까지 다 되고 너무 좋지 않아요 진짜 사실 원래 시맨틱 세그멘테이션 있잖아요.
그거는 인스턴스를 못 보고 그냥 다 종류만 구분하고 있잖아요.
근데 얘는 인스턴스까지 다 나오잖아 사실은 다 몇 개인지 그렇잖아요.

참석자 1 28:29
원근법에 의해서 멀리 되는 것까지도 되고 원근법이 문제 아니죠 여러분 멀리 있는 거 우리 조그맣게 보이잖아 우리한테 그렇죠 그렇죠 그런데 그것까지 다 지금 하나 둘 이거 안 돼 차가 몇 개야 보이잖아 여기 왼쪽에 하나 둘 세 개 있네 이런 거 보이잖아 그쵸 좋지.
사람들 몇 명 가고 있는지 다 보이고 그렇죠 얼마나 좋아 그래요.
그래서 교과서 다 읽고 교과서에서 그래서 이제 뭐야 지 9장에 부장님 지금 제가 안 한 것 중에 이미지 분할하는 거 있잖아요.
이거는 이게 바로 풀리 컨볼루셔널 뉴럴 네트워크이에요.
플리 컴볼루션 네트워 이미지 분할하기 위해서는 봐봐요.
이렇게 결과가 나와야 되잖아요. 그래서 이걸 보면 여기 예를 들어서 이런 샘플 이미지가 입력이 이렇게 들어왔는데 얘가 이제 지금 시맨틱 세그멘테이션 하는 예제거든요.

참석자 1 29:28
시멘틱 세그먼테이션 이미지 세그멘테이션에 시맨틱 세그멘테이션 인스턴스 세그먼테이션이 있지 그쵸 점점점점 더 힘든 거야.
그쵸 시멘틱 이미지 세그멘테이션에 시멘틱과 있죠.
인스턴스가 있고 시멘틱에서 더 나아간 게 인스턴스지 그쵸 근데 여기서는 그냥 인스턴스 세그먼트 이거 실제로 훈련시킬 때 어떻게 하냐면 이거를 이런 이미지에다가 이걸 쌍으로 줘야 돼요.
이렇게 쌍으로 이걸 입력을 줘야 돼. 이거 다 이거 전부 다 여러분 슈퍼바이즈 런닝이에요.
지도 학습이야 이렇게 답을 줘야 돼. 이렇게 저걸 저런 입력을 주면 이런 입력이 와야 돼.
출력이 나와야 돼. 이렇게 해야 된다고요. 이걸 누가 전체가 만들었겠어요?
사람이 무식하게 해주겠지 사람이 무식하게 했다고 옛날에 지금은 이것도 자동화시키는 게 다 나와버렸지.
왜냐하면 다 먼저 해놓은 게 있으니까 옛날에는 이거 다 이거 알바에 있었어요.
이거 알바 섹션에 표시하는 알바가 있었어요.

참석자 1 30:24
이 비둘기 비둘기 비둘기 갈매기 이렇게 표시하고 이런 알바 하나 하는데 이거 그냥 딱지 치는 거는 20원이고 이렇게 점 찍는 거는 한 100원 하고 그랬었어.
여러 분 한 15년 전에. 근데 지금 드라이브 다 사라졌지 참 유망한 직종이라고 그랬는데 날에 하원도 있고 그랬지 요령도 엄청나게 블로그도 많고 그랬는데 어쨌든 무슨 얘기인지 알겠죠.
여러분 옛날에 여러분의 선조들의 핏덩어리 이렇게 이런 거 다 주는 거고 그다음에 이것도 실제로 여러분 이미지 넷 하면 이런 것들 다 돼 있는 게 있어요.
누가 해놓은 거 이해되지 무슨 말인지 알겠죠 이거 그다음에 그리고 이런 거 있잖아요.
지금도 제가 하는 거 봤는데 어떤 박사 논문 심사하러 갔었는데 그 사람 보니까 반도체에서 공정하는데 이렇게 뭔가 시료가 떨어지는데 시료가 바깥에 흐르지 않아 있는지 검증하는 거 있잖아요.
근데 그거 하기 위해서는 뭔가 노하우가 필요하대.
그래서 사람이 다 했더라고요.

참석자 1 31:23
보통 사람이 이렇게 초 찍는 거 있잖아 이게 신호가 흘렀는지 안 흘렀는지 그러니까 이거를 이제 사람이 안 하고 자동화시키고 싶은 거야.
사람이 원래 이거 기가 막히게 이제 뭔가 센스로 이거는 시료가 밖으로 흘렀네 안 흘렀네를 방지하고 있었거든요.
근데 이제 머신러닝을 하기 위해서는 다시 데이터가 필요하잖아요.
이 짓을 하고 있는 거 이해되죠. 그리고 자동으로 못하니까.
근데 그다음에 끝나고 나면 이제 자동화가 되는 거잖아요.
근데 그 사람은 자기 편하게 살려고 그렇게 하고 있는 거지 이해되죠.
내가 자동화시키고 싶은 거야 뭐든지 그리고 진짜 이상한 거 되게 많았는데 소리가 이상하게 들리면 그게 기가 막히게 예쁘다는 걸 알았어.
근데 그 사람만 알아먹어. 근데 그 사람이 데이터를 주면 이제 그거 그 사람 일자리를 잃어버리겠네.
어쨌든 그런 거지 그렇죠 근데 그 사람이 이제 쉬고 싶대.
그러면 이제 그렇게 해서 그 정보를 주고 이제 가면 참 신기하죠.

참석자 1 32:09
여러분 이런 보면 정말 센싱이 굉장한 능력인데 어쨌든 이게 고양이가 이렇게 생겨 먹었다는 걸 해주고 이런 걸 하기 위해서 여기에 이제 331쪽에 재밌는 거 한번 보면은 331쪽에 이건 여러분 실제로 신고할 때 쓰게 331쪽에 3 4분이니까 해봅시다.
이거는 하는 김에 이미지 분할의 맨 밑에 여기 보면은 맨 위에서부터 337쪽에 여기 데이터셋에 여기 전경 배경 윤곽 보이죠.
여러분 그렇죠 전경 영어 이게 한문이 정경이라는 말을 잘 안 쓰고 포어 그라운드라는 말 더 많이 쓰죠.
여러분 그렇죠 포어 그라운드 백그라운드 이게 정경이 포어 그라운드고 배경이 백그라운드지 그쵸 알죠?
여러분 그리고 윤곽 이렇게 그렇죠 이런 식으로 숫자를 쓰기로 했어요.
그래서 아까 보면 여기 여기 보면 여기 보면 여기 보면 그렇죠 이게 백그라운드 그렇죠 이게 포 그라운드 그렇죠 이게 유각이죠.

참석자 1 33:10
그렇죠 이게 그죠 그리고 여러분 이거 디폴트 칼라 인데 비즈니스라는 거 초록색 그쵸 얘가 이게 중간 값이고 얘가 제일 작은과 노란색이 제일 큰 값이죠.
그쵸

참석자 1 33:26
그래서 그다음 데이터셋을 지금 데이터셋이 어디 여기 있대요?
로버츠 aos ac 이렇게 있대요. 달아가지고 했고 지금 그걸 저장을 하고 그다음에 실제로 이미지를 가지고 바로 하는 게 아니라 이미지를 가지고 조작하기 쉽게 하기 위해서는 로드 이미지에 이게 331쪽에 보면 332쪽 맨 마지막에 어디 갔나 여기는 없어.

참석자 1 34:03
여기 로드 이미지 이미지 투어에 이런 이런 라이브러리가 되게 유용하게 쓸 수 있어요.
그래서 보면은 그 다음 페이지 332쪽에 이미지 쇼할 때 그냥 이미지 저거 하면 안 되고 원래 인풋 이미지 패스를 준 다음에 걔를 로드 이미지를 부르고 로드 이미지를 불러줘요.
인풋 이미지 테스라는 것도 이거는 앞에 선택시 이쪽에 만들어 놓는 거예요.
선수 선택시 이쪽에 만들어 놓은 함수고요. 그래 이 정도면 알아서 하겠지 됐죠 여러분 넘어갈게요.
로드 이미지라는 걸 불러야지만 이렇게 잘 그림이 보여주고 그다음에 그다음에 뭘 보여주나 로드 이미지가 어쨌든 유용하고

참석자 1 34:55
됐나 그래서 이게 이게 타겟이 이렇게 생겨 먹었다는 거 여러분 알고 있어야 돼요.
그러니까 타깃이라는 게 결국 학습시키는 데이터가 이렇게 생겨 먹었고 그다음에 이게 결국은 이제 이 3 3 4쪽에 3 4시 4 34 3쪽 4쪽에 이어져가지고 얘를 가지고 학습시키는 코드가 나와 있어요.
그래서 맨 처음에 33쪽에 시작하는 게 이렇게 여기 보면 이 코드죠.
저쪽에 이거 박스를 안 쓰고 있잖아요. 원 파이하고 포트부터 시작해가지고 이미지 사이즈를 20,200으로 만들어 놓고 그다음에 파일 경로를 뒤섞기 위해서 뭔가 랜덤하게 뭔가 시드 값을 우리 정확하고 하고 이런 게 있죠.
그리고 여기 보면은 이제 뭐가 있냐면은 이게 다 이제 뭔가 함수를 만들어놓은 거지 여기 제가 하게 이런 거 저도 헷갈렸는데 이거 보면 여러분 인풋 이미지즈 한 다음에 이거 코드 보이죠.
여러분 인풋 이미지즈 하고 나오는 거 어디 있냐면은 중간에 포문 위에 있죠.
그렇죠 보이죠.

참석자 1 36:03
여러분 이거 보면 MP 지로즈하고 플러스 플러스 이렇게 돼 있잖아요.
여러분 이렇게 하면 여러분 이게 정체가 뭐가 되냐면은 이거 여러분 나도 헷갈렸는데 여기 이게 MP 지로즈하고 요렇게 싸고 이렇게 싸고 있죠.
처음에 이렇게 지로즈하고 그렇죠 뭔 얘기인지 알지 MP 지로즈하고 이렇게 하고 있잖아요.
그쵸 이렇게 그렇죠 여러분 이미지 사이즈 이게 뭐냐면 이미지 사이즈가 아까 200 200이었거든요.
그쵸 이거 요거 더하고 이거 더하고 이러잖아요. 넘 이미지가 얼마였냐면은 7390이에요.
7390이네. 이거 앞에 나오는 거 중요한 건 아닌데 예를 들어서 이게 그렇죠 여러분 이러면 이거 이렇게 더하잖아요.
이게 이 트플들을 더하고 있는 거잖아. 그럼 어떻게 되는지 알아요 여러분 그럼 이 결과가 73 90 콤마 2000 콤마 200 콤마 내가 헷갈렸던 거라서 그다음에 3 이렇게 돼요.

참석자 1 37:06
그 결과가 이거라고 이거랑 똑같은 거예요. 이거를 쓸 거를 저렇게 해놓은 거예요.
이해돼요 여러분 튜플 듣기 더하면 연결돼. 원래 여러분 리스트 더하면 연결되는 거 있잖아요.
똑같이 알죠? 여러분 나 몰라 이거 3 4 이렇게 더하잖아 그러면은 이거 상품만 4 나오잖아 스트링 더하는 것처럼 스트링 더하면 이렇게 연결되잖아요.
여러분 뒤로 붙잖아. 그냥 저도 이거 그것도 그렇다고요.
사이즈가 다른 거 더하잖아요. 원래 런 파일만 서로서로 더해지지 다른 건 다 브로드캐스트 해 가지고 그렇죠 그냥 이렇게 뭔가 큐플 더하고 이러면은 원래 이렇게 트프린드로 만들어진다고 알겠죠 4차원 쉐입을 만들어준 거예요.
알겠죠? 그래요. 그런 거를 인풋 이미지가 이런 사이즈라는 걸 알려준 거예요.
근데 너 MP 지로로 해서 뭔가 가짜로 채워준 거지 이렇게 해놓은 다음에 그 인풋 이미지에다가 실제로 그 밑에 포그룹에서 실제로 이렇게 익힌 이미지 시인 맞춰놓은 거에다가 다시 또 채워주고 있어요.

참석자 1 38:09
얘네들은 아까 인키지 이미지 테스트 해가지고 이해되죠?
여러분 모양 만들어 놓은 거에다가 다시 또 넣어주고 있어 그쵸?
하나씩 하나씩 그리고 샘플별로 넣어주고 있는 거지 아이에게 딱 해주면 여기 요거 하나하나씩 들어가는 거예요.
전체 차원이 이게 4차원인데 여기다가 아 해 주면 여기 하나씩 하나씩 들어가는 거라고 알아요.
여러분 인풋 이미지 여기 여기 있는 건 이거고 이 사이즈고 알겠죠?
파이썬인데 내가 헷갈렸기 때문에 여러분한테 알려주는 거예요.
알겠죠? 됐죠 나만 헷갈리는 게 아니겠지 그다음에 여기 겐 마델이라는 코드가 맨 밑에 있는데 g 마델 31분 이내에 할 수 있어요.
그 겐마델 보면은 쉐이비 이미지 사이즈에다가 이렇게 3 해놨죠.
여러분 이것도 옛날에 맨날 우리가 류쉐이하고 막 이랬는데 이게 되게 편하네.
이렇게 보면 시험에 이렇게 나오려 내라고 다니면서 더 멋있잖아.
20,200 더하기 3 이렇게 되는 거잖아요.

참석자 1 39:08
20,002미지 사이즈가 2200이니까 이해돼요.
여러분 이해되지? 유니 사이즈가 200 콤마가 200이었거든요.
그리고 얘가 정체가 뭔 값이에요? 이게 이게 200 200 3 이렇게 되는 거지 그쵸 1억이 되는 거야.
트풀끼리 더하면은 이렇게 딱 이게 멋있잖아. 이게 멋있는 됐죠 다 그렇죠 됐죠 그래요.
그래서 이제 한동근 씨 여러분 이제 드디어 드디어 이제 폴리 폴 포트 라트 구경합시다.
맨 처음에 인풋하고 리스케일링 했어요. 여기까지 똑같죠 그쵸 컴퓨트 되는 건 똑같아요.
그쵸 근데 스트라이드를 2로 하기로 했어요. 스트라이드를 2로 하면 어떻게 돼요?
여러분 사이즈가 줄어들지 그쵸? 그쵸? 반으로 줄어들지 액티베이션 해줬고 바로 패딩을 또 세임을 했는데 스트라이드 했으니까 반으로 줄겠구먼.

참석자 1 39:56
그쵸 어쨌든 그리고 이제 다음에는 패딩 세임 하면서 스트라이드 안 했으니까 사이즈가 유지될 것이고 그다음에 또 다음 페이지 넘어가 334쪽 또 스트라이드 하고 또 하고 스트라이트 하고 이렇게 해서 반으로 줄이고 반으로 줄이는 거를 여러 번 했어.
그쵸 그런데 맥스 클릭은 또 안 했어요. 그쵸 엑스플링 안 하고 스트라이드 하는 걸로 줄여버렸어요.
그렇죠 그렇죠 약간 다르게 했어요. 여기서 그렇게 했어요.
보통 이쪽에서 할 때는 스트라이드 안 하고 이렇게 엑스플링 안 하고 스트라이드 하는 거를 많이 쓴대요.
그건 그렇고 그렇다고 맥스 플리고 안 되는 거냐 그거는 뭐 해봐야 알지 나도 모르는 거야.
그쵸. 일단 그랬는데 이럴 경우에 이제 뭐 하려고 그러냐면 여기까지는 이제 여러분 계속 전통적인 CNN 방식이었잖아요.
이제 뭐 하냐 아까 욜로도 그렇고 이제 원래대로 다운 샘플링 했어.
여기까지는 그렇죠 여기까지 다운 샘플링이라는 거 이해되죠?
여러분 다운 샘플링 이해되지?

참석자 1 40:51
다운 샘플링 여기까지 그쵸 사이드 2를 통해서 그쵸 이제 뭐 해야 돼요?
업샘플링을 해야 되잖아요. 업샘플링하는 함수가 컴브트리 트랜스포즈인 거야.
거꾸로 하는 거 거꾸로 하는 거.

참석자 1 41:07
그런데 여기서 억셉플링이 일어나는 방식은 이 커널 크기가 있을 때마다 요거마다 3이잖아요.
3이면은 하나씩 뚫어놔. 그쵸 3이면은 하나씩 줄어나지.
원래 3이면은 원래 그냥 밸리데이션 하면은 하나씩 줄어들었죠.
1 20 원래 필터 크기 터널 크기가 23이잖아요.
3 곱하기 3이면 1 곱하기 1이 줄어들었죠 1 1이 이 필터 크기에서 1 뺀 거가 그쵸 2인데 거기서 또 이를 뺀 거 아닌가 2가 줄어들었나?
이가 줄어들었나? 1 줄어들었나 이가 줄어들었네.
미안 실수 근데 여기 그랬죠 원래 이가 줄어들었네.
실수 2가 줄어들었어. 맨 마지막에 이렇게 떡 3 차지 하고 나면 이게 다 날아가니까 2가 줄어들었네.
실수 2가 줄어들었는데 여기서는 3을 하고 나면은 일이 늘어나지.
그렇죠 아까 보니까 3을 하고 나면 1이 늘어나. 그렇죠 여러분

참석자 1 42:15
그래가지고 그래서 이거 보면은 잠깐만요. 나기 봅시다.
아니 아니 아니 어쨌든 줄어드는 게 줄어드는 게 중요한 게 아니라 패딩을 세임을 했기 때문에 하나도 안 줄어들어요.
여러분 미안해요. 내가 이거 지금 바로 직전에 안 뽑아서 헷갈린 거예요.
됐죠 여러분 일단 원래는 줄어들 수 있는데 지금 패딩을 엔드로 할 수 있는데 패딩을 세임을 했기 때문에 그대로 그대로 유지가 되는데 여기 보면 스트라이즈를 2로 하죠.
여러분 스트라이즈 2 사이드 2를 통해서 아까 거기도 스트라이드가 있었나 보다 사이드 2를 통해서 2배가 늘어나요.
두 배가 여기 두 배가 다시 5분 남았는데 할 수 있어요.
보면 여기는 아까랑 똑같이 하는데 아까 여기에 여기에 해당하고 반대로 되니까 스펙처럼 이렇게 이렇게 대응하는 거예요.
이렇게 앞에 두 개가 더 있겠지 두 개 더 있잖아 이렇게 대응하게 해요.
여러분 그러니까 이게 왜냐하면 다시 번복시키는 거니까 줄었더라도 다음 세플링했던 업세플링 하는 거잖아요.

참석자 1 43:17
그래서 여기에 해당하는 게 똑같이 똑같이 하고 여기 똑같이 하고 여기 스트라이드 2였으니까 우리 스트라이드 2예요.
보여요. 여러분 그런 식으로 다 하면은 똑같은 사이즈로 다 바뀌어요.
트랜스포드 역함수를 하면서 앞에랑 다 이렇게 대응되게 하면 원래대로 그 원복이 돼요.
뭐 어떻게 하는지 알겠죠? 여러분 이렇게 해서 플리 컨버션 룰러 네트워크를 만들 수가 있어요.
되게 재미있잖아요. 되게 재미있잖아요. 여러분 여러분 이거 할 수 있으니까 재밌지 않아요 실제로 해보면 재밌겠다.
그렇죠

참석자 2 43:58
지금 근데 x값 업데이트할 때

참석자 1 44:01
x 값 업데이트할 때

참석자 2 44:03
컴프 2D 하는 과정에서 x 값을 그냥 중간에 저장하지 않고 뒤에서 트랜스포즈로 다시 연산을 하는 이유로

참석자 1 44:10
아니 계속 이제 뭐 메모리 아끼려고 그러는 거야. 옛날부터 그랬지 함수형 API 할 때 계속 그랬어요.
이제까지 다시 안 쓸 거니까 그냥 넘기는 거야. 어차피 넘어가니까 다음에 넘어가서 파라미터 카피가 되니까 어차피 필요 없어서 넘긴 변수 다른 것도 안 쓴 거죠.
변수 쓰고 있으면 또 괜히 가이 리플레이션 하고 해야 되니까 넘겼으면 카피가 일어나거든요.
그래서 그냥 냅둔 거죠. 이거 옛날이랑 똑같아 이거 특별히 다른 게 아니야 무슨

참석자 2 44:38
근데 여변산을 하는 이유가 이전에 x 값을 복원하려고

참석자 1 44:42
익숙해서 복원한다기보다는 이건 뉴럴 네트워크 자체가 연산을 그렇게 하는 거예요.
값을 복원하는 게 아니고요. 값을 살리려고 하는 게 아니라 연산을 새로 하는 거예요.
이것도 아까 아까 그 컨볼루션 연산 있잖아요. 트랜스포스 컨볼루션 연산을 하려고 하는 거라서 이것도 그냥 계층이에요.
이것도 학습하는 거고 이게 무슨 값을 보고 한다는 게 아니라 크기가 원래대로 돌아간다는 것뿐이지 똑같아 그냥 이것도 그냥 레이어에 레이어 되돌리는 게 아니라 다시 그냥 업샘플링 하는 거죠.
업샘플링 이것도 학습시켜가지고 컬러 값이 여기 지금 있잖아요.
요거 학습일 대상이 아니고 이거 학습하는 거야 이거 그리고 이게 이거 여러분 이걸 봅시다.
지금 7분인데 봐요. 이거 이거 이거 중요 봐봐요.

참석자 1 45:26
맨 처음에 이거 앞에 대응하는데 인풋은 200 곱하기 2 곱하기 3이었죠 그쵸 그러면은 이제 리스케일링 할 때 똑같은데 허브 2D 하고 나면은 컴포트디 맨 처음에 하는데 스타이드를 했으니까 이제 반으로 줄어드는 거 알죠?
여러분 64가 튀어나온 이유는 터널이 피터가 64개였겠지 그쵸 이해되죠?
여러분 그다음에 또 24개 있었으니까 또 이렇게 나오고 그쵸 그다음에 또 스트라인 됐으니까 150 곱하기 50이 되고 그렇죠 반으로 스물고 128이 되지 이해 되죠 이건 128이어야 된다면 필터가 보통 이제 필터 개수 막 늘리는 거 알죠?
여러분 그쵸 그쵸 이걸 안 했구나. 근데 마지막에 마지막에 여기 계속 나오다가 64일 하다가 끝날 때 막판에 여기 넘버 로브 클래스이즈로 여기 끝났잖아.
피터 리스가 여러분 넘버 로브 클래스이즈를

참석자 1 46:27
이게 얼마예요? 클래식에 종류가 몇 개 나와요 여기

참석자 1 46:34
영화 클래스가 몇 개예요? 여러분 몇 개였어? 여기 클래스가 몇 개 나와요?
여기 보면은 나귀마야 말판 구분의 색깔이 몇 가지였어요.
여기는 세 가지만 하면 되니까 이게 이게 넘버 클래스가 3인 거야.
여기가 3 그쵸? 막판에 3 3개가 나오게 필터 개수가 딱 나오게 이렇게 해준 거예요.
여기는 여기 3개의 범주 중 하나로 한다는 게 여기 넘버 러브 클래시즈에 해당하는 거지.
그래서 3개를 썼다는 거야. 3개의 유닛을 썼다는 거지.
그리고 막판에 여기에 얘가 3인 거는 여기 3이랑 상관없는 거야.
2 3 여기 3 3은 여기 2 3이라는 사람 0번 클래스에 가 있는 거야.
알겠어요? 여러분 알겠죠? 됐어요

참석자 1 47:27
됐지 그래요. 여기는 그냥 필터 개스는 또 그냥 3개를 해 준 거고 아니지.
필터도 마지막에 3장이 나오긴 했지만 3장이 나오는 건 여기 세 장이 나오게 한 건 여러분 세 장이 나오게 한 거는 이게 결국은 여기 피터 개수고 이거는 커널의 크기잖아요.
그쵸? 커널의 크기지 여기는 허프 2D로 커널 크기 해준 거잖아요.
그쵸? 코널 크기 3으로 그렇죠 그냥 그거는 웨이트 이렇게 쓴 것뿐이고 그렇죠 어쨌든 다시 강조하고 싶은 거는 여기 3이랑 여기 3이랑 상관이 있는 게 아니라 요 3은 여기 3이 여기랑 상관이 있다는 거지.
그쵸 알겠죠? 50 딱 돼버렸네. 다시. 그래서 이것만 보면 여기에 아까 컴퓨터 트랜스포즈 하면 딱딱 두 배로 늘어나는 거 아까 스타일 2할 때마다 이해되죠.
막판에 어떻게 되겠어요? 맨 마지막에 2224로 만들어져요.
2203으로 마지막에 마지막 223으로 원래 이미지랑 똑같이 내야 되는 거야.
그쵸 알겠죠?

참석자 1 48:23
여러분 이게 20 200이 되는 이유는 아까 똑같이 스트라이드 하는 거 반대로 똑같이 했기 때문에 그런 거고 이게 3이 되는 거는 왜 그랬냐 아까 그 마지막 편에 컴브 2D 할 때 3으로 했기 때문에 그런 거예요.
마지막 필터 개수를 그쵸 이해되죠? 그렇죠 막판에 그래서 어쨌든 이렇게 해서 약간 되게 재밌죠.
굉장히 재밌지 이래가지고 뭐 이렇게 트레이트하게 이걸 입력으로 줘가지고 훈련시켜서 하는 훈련이 된다는 거야.
진짜 잘 돼요. 여러분 저게 욜로드 욜로도 되게 잘 돼.
훈련 욜로 대박 욜로도 여러분 트랜스포 러닝 해야 돼.
욜로 할 때마다 그냥 맨땅에 헤딩하는 게 아니라 비슷한 문제에 합치된 거 같고 여기까지 하고 다음 시간 이어서 할게요.
여러분 다음 시간에 노트북 제가 부탁드릴게요.


clovanote.naver.com