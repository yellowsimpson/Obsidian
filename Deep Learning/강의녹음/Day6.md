딥러닝6
2025.03.24 월 오전 10:02 ・ 47분 14초
심승환


참석자 1 00:00
손실 함수 자체가 이제 사실 이거 말고 지난 시간에 사실은 인스한 것 같은데 한글 함수가 이제 보통 이렇게 오차에다가 제공 방법이 돼요.
인스케에러를 많이 쓰죠. 그리고 그랜드 디스트라는 개념이 이렇게 원래 파라미터에 이제 여기 사실 변화율이 있어 사실 변화율 여기 이제 다 옛날 다음 값 여기 옛날 값이니까 그 차이를 이 특정 파라미터 j에 대한 보합 변화율에 대해서 그렇죠 어떤 그냥 바로 고치는 건 좀 위험하니까 러닝 레이트 해서 좀 작게 자르다 고쳐서 조금 조금씩 조정해 나가는 거죠.
그쵸 그래서 이렇게 진행하는 거고 러닝메이트 값이 따라서 이렇게 왔다 갔다 그쵸 한다는 것 같고 그리고 이제 사실 피처가 입력 값이 하나가 아니라 보통 여러 개죠.
그쵸. 그리고 여기 기본적으로 이제 여기 이런 식으로 모델링 된다는 거를 좀 기억할 필요가 있어요.
그쵸 요렇게 식으로는 이제 행렬을 쓰면 이렇게 되고 그렇죠 이런 거 좀 헷갈리기 때문에 이제 오픈북으로 봐도 오픈북으로 본다고 그랬죠.

참석자 1 01:09
제가 이런 부분에 대해서는 그렇지만 이제 개념에 대해서 좀 알아야 될 거는 일단 항상 바이러스라는 게 존재해요.
그렇죠 바이러스라는 게 없을 수도 있지만 바이러스를 없애면 세트 제로가 없어지는 거고 그쵸 바이러스를 있으면 이제 이거 바이러스는 보통 이제 이렇게 세탁 드어라는 게 생기는 거죠.
그쵸 원래 이제 피처 개수에 비해서 하나 더 그냥 상수 값으로 뭔가 파라미터를 주는 게 가이어스라는 거고 그쵸 그래서 이제 원래 피처의 개수가 특성의 개수가 n개면 n 플러스 1개의 이제 파라미터가 존재하는 거지 화면이 다르지 그렇죠 이해되죠 여러분 그래요.
그리고 또 조심할 게 항상 이렇게 여기 지금 로테이션이 막 복잡하게 많아지는데 이게 이제 피처의 개수 피처 인덱스가 여기 있고 0부터 n인데 0은 이제 바이러스고 그쵸 1로 그렇죠 보는 거고 피처를 이렇게 상수 값으로 그렇죠 이해되죠 여러분 몰랐으면 지금 알았을 거 모르면 질문하세요.
여러분 지금 질문하는 거 좋아요.

참석자 1 02:07
쉬는 시간에 질문하면 창피하네요. 어쨌든 질문한다 고 모르겠어요.
여기 가로하고 아이 적혀 있는 개념이 중요하지 이게 이게 값이 이게 중요한 게 이제 우리가 러닝하는 거고 실제 나중에 이제 테스트하는 것 트레이닝 테스트할 때 항상 여러 개의 데이터에 대해서 하는 거기 때문에 이게 이제 특정 어떤 다양하게 들어올 수 있기 때문에 이제 여기 아이라는 인덱스가 있죠.
여기 데이터의 인덱스인 거지 그쵸 피처 인덱스 데이터 인덱스 다른 거 알겠지 여러분 그쵸?
그렇죠 여러분 그거 헷갈리지 마시고 그렇죠 복습하는 거예요.
지금 계속 그래서 여기 앞에서도 여기 이제 여기 n 적혀 있는 거 있잖아요.

참석자 1 02:46
여기 여기 여기 지금 로스트 펑션 계산할 때 데이터 한 개에 대해서 하는 게 아니라 데이터 여러 개에 대해서 이렇게 트레이닝 데이터 여러 개에 대해서 그렇죠 용어 앞에서 했지만 사실 트레이닝 데이터 데이터 전체 원래 데이터가 우리가 이렇게 뭔가 얻은 게 있으면 데이터 셋이 있으면 데이터셋을 트레이닝 데이터랑 더해 이제 트레이닝 데이터 테스트 데이터 나눠 보죠.
트레이닝 데이터도 일반적으로 또 그 안에서 다시 또 그냥 또 트레인 데이터 내부적인 진짜 트레인 데이터랑 밸리데이션 데이터 나눠가지고 그렇죠 하죠 그쵸.
그래서 근데 그냥 막 찾으면 사람들이 그냥 두 개 트레인 데이터 밸리데이션 여기 트레인 데이터 밸리데이션 데이터 테스트 데이터 이렇게 세 가지로 부르는 거지 그냥 그치 그러고 있어요.
용어가 왜 이렇게 되는지 알겠죠? 여러분 그래요.

참석자 1 03:34
그래서 지금 얘기하고 싶은 거는 지금 여기 보통 이제 트레이닝 할 때는 여기 이렇게 한꺼번에 이제 여기 지금 예측값에 대해 예측값을 구할 때 트레인 데이터 트루 트레인 데이터지 전체에 대해서 구하고 그쵸 밸리데이션 데이터에 대해서도 따로 구하고 밸리데이션 하려고 밸리데이션이 왜 하는 거냐면 도대체 언제까지 내가 공부를 해야 돼 이런 개념이라고 그랬잖아요.
그쵸 그것도 이제 언제 하느냐도 정할 수 있다고 밸리데이션을 학습 학습 파라미터 고치자마자 또 하는 거냐 아니면 좀 여러 개 고치다가 할 거냐 이런 것도 정할 수 있다고요 여러분도 이제 진짜로 실무에 투입되면은 밸리데이션 너무 자주 하는 건 좀 아닌 것 같아서 실제로 여러분 공부할 때도 그렇잖아요.
맨날 모의고사 또 보나 그렇죠 사실 근데 여러분 모의고사 보고 답은 안 알려주는 거야.
계속 우리 고사에 대한 너 얼마 정도 한다 이렇게 하는 거지 사실은 내가 왜 틀렸는지 모르겠지.

참석자 1 04:23
내가 왜 틀렸는지 알려주는 거는 트레인 데이터에 대해서만 하는 거고 그쵸 그런 거죠.
그래요. 어쨌든 완전히 똑같진 않은데 조심할 게 사람은 이제 모의고사 같은 거 보면은 약간 그래도 뭔가 배우는 게 생겨버리잖아.
근데 이쪽은 안 그렇죠. 완전히 몰라. 밸리디 데이터를 적용해봤자 전혀 영향을 주지 않기 때문에 진짜로 처음 보는 데이터 같이 되는 거지.
그래요. 이것도 무슨 얘기인지 모르겠으면 또 질문하시고 어쨌든 이거는 이제 벡터 형태로 표현한 거고 그리고 지난 시간에 얘기한 게 지금 아까 계속 업데이트 이거 결국 브랜드 디센터 적용할 때 데이터 몇 개 쓸 거냐에 대한 문제예요.
그쵸 앞에서는 그냥 당연히 모든 데이터 다 쓰는 것처럼 설명했어요.

참석자 1 05:05
m이라고 그러는 거 그쵸 데이터가 아 7만 개면 7만 개 가고 이런 식으로 트레이닝 데이터가 근데 그렇게 하면 너무 느리니까 그쵸 그래서 이제 극단적으로 한 개만 해보고 그쵸 1개 스토캐스틱 원래 그러다가 너무 한 개 갖고는 좀 아닌 것 같아서 미니 배치로 그쵸 뭉탱탱이 끊어가지고 가는 거 그러다가 이게 워낙에 이게 거의 이걸 쓰니까 gd 이 SGD로 변질돼가지고 이 의미로 변해버렸어요.
그래서 결국은 얘가 모든 걸 다 커버하거든. 사실은 미니 배치에서 배치 사이즈를 정하는 건데 배치 사이즈라는 말 안 적어놨구나.
여기 배치 사이즈라는 말은 근데 이제 미니 배치의 크기인 거지.
미니 배치 데이터의 크기 한 번에 이제 저 프라베트 업데이트할 때 쓰는 그 데이터의 개수를 의미하죠.
스트레이닝 데이터의 개수 그게 이제 하나면 테스트 원래 트루 스터 패스티 트루 SD 교과서 이 교과서에도 트루 HD라는 표현을 써요.

참석자 1 05:55
그래서 헷갈리니까 이 교과서도 은근히 엄밀해서 근데 여러분 많은 한글 교과서는 이제 그냥 웹에 나오는 거 보면은 그냥 이것처럼 얘기하고 있어요.
알겠죠? 됐어요. 여러분 그래요. 됐지 그다음에 이거는 또 사실 이거는 또 브레드 디스트를 리뉴얼 일렉션에 대해서 적용한 걸 의미해요.
리얼 리그렉션이 아니라 미 미안 실수 리뉴얼 리그렉션뿐만 아니라 어쨌든 mse로 저기 로스트 펑션 쓰는 게 많아요.
루스 펑션을 꼭 루스 펑션을 mse로 쓰는 데가 많거든요.
딥러닝 가도 루스 펑션은 mse로 쓸 수 있다고 어떤 부스 식하려고 그러면 그럴 거 아니에요?
여러분 알타모드 엘리스 있었어요. 여러 봐요. 아이그 그럴까 MSA라는 건 굉장히 뭔가 원래 필터에서 벗어난 그 값을 잘 모델링 하잖아요.
그렇죠 스케어 에러가 좋지 더 좀 어지럽게 확 늘려버려요.
그쵸 그러니까 그래서 그거를 이제 미분화했을 때 이렇게 된다는 걸 보여주고 있어요.

참석자 1 06:57
여러분 사실 이거는 고등학교 교사 차 내용인데 행렬로 표현하니까 좀 복잡해 보이지 그래요.
그리고 이거를 여러분이 몰라도 된다고 얘기하기 좀 그런 게 채색 키티가 제일 못하는 게 이런 거거든 이런 거 되게 실수 많이 해요.
그래서 이런 것 좀 잘한다는 놈이 이제 브로크랑 그다음에 뭐 부치카 이런 것을 잘한다고 나랑 잘하면 레스 잘한다고 매스자라는 놈은 또 말을 잘 못하고 좀 그런 게 있어요.
여러분 사람들도 다 그런 것 같아요. 되게 디테일 잘 하면은 그렇게 발표를 못 하 다 잘하게 좀 약간 조정 할 필요가 있지 그렇죠 소통이 돼야 되니까 그렇죠 그래요.
그리고 참고로 이제 농담 삼아 얘기하면 농담이 아니라 진짜로 그런데 클로드가 진짜 잘했었거든요.
근데 올해 들어와가지고 거의 바보가 돼버렸어요.

참석자 1 07:44
업데이트하면서 뭔가 공부를 공부를 하면서 할수록 걔는 바보가 되는 느낌이야 진짜로 못 쓰겠더라고요.
진짜 아니 나 저만 하는 얘기가 아니라 다른 사람들한테 많이 얘기해봐 프로들 다 끊어버렸어.
그래서 근데 왜 끊냐고 물어볼 때 그냥 뭐 돈 없다고 이렇게 이렇게 뻥 쳤는데 미안해서 이게 사실 사실이야 이렇게 얘기해 주셔야 되는데 체스 피티는 똑똑해지고 가서 어쨌든 내 말은 이제 여러분도 공부를 이상하게 하면 더 안 좋다는 느낌이 들어요.
걔도 공부를 좀 잘못한 것 같아 저기 프로드가 옛날보다 더 못해 어제요 어쨌든 어쨌든 그런 거고 BGD랑 이제 BGD가 배치 그레디센트랑 미니 배치 이거 이것도 얘기했죠 여러분 지난 시간에 그렇죠 여기서 뭐 이해 안 되는 거 있으면 물어보세요.
여러분 내가 했었어요. 근데 이것도 보여봤지 그렇죠 그리고 요거 이제 3차원으로 표현한 것도 얘기했었고 했던 거야.

참석자 1 08:31
지난 시간에 근데 이제 다시 하는 이유는 여러분도 발표할 때 그런 게 있는데 졸업 발표할 때 중간 발표한 다음에 이제 제안 발표하고 중간 발표할 때 제안 발표 있는 데는 다 빼버리고 그냥 바로 중간부터 자기 건 발표하는 거야.
그 사람들이 알아듣겠어요 못 알아듣지 근데 그걸 몰라 나만 알고 남들은 모르는 걸 모르는 게 문제죠.
여러분 그렇죠 발표할 때 그 일은 일주일 만에 여러분 지도 교수님이랑 미팅이 4학년 많잖아요.
지금 그리고 지난 시간에 했던 거 결론이랑 어떤 상황인지 얘기한 다음에 들어가야 되잖아요.
근데 바로 들어가 우리 지도 교수가 알겠어요 다른 데라도 왔는데 자기는 b라도 왔지만 지도 교수는 알겠냐고 상사도 마찬가지예요.
여러분 같이 일하는 사람도 약간 좀 뭔가 이 워닝업을 한 다음에 뭐 어떻게 쓰고 이렇게 하기로 했었는데 그중에서 이걸 했고 이걸 안 했고 이래야 될 거 아니야 그쵸 여러분 발표할 때 항상 그런 거를 은근히 공부 잘하는 애들이 못하는 경우가 있어.

참석자 1 09:23
그냥 몰입돼 가지고 그게 이제 좋게 말하면 이제 노벨상 받은 사람들 수업 못 해가지고 되게 유명하거든요.
아무도 수업 안 듣잖아. 지 혼자만 떨어져 있어가지고 그렇죠.
아 그게 그럴 수도 있는데 이제 우리 요즘 세상에서 그렇게 살기 힘들어요.
여러분 그렇죠 이제 여러 가지 강조하는 거지 지금 내가 왜 이 짓을 하고 있냐 사실 바로 들어가면 여러분 또 뭔 소리인지 잘 모르잖아.
그래가지고 지금 제가 이러고 있는 거죠. 그쵸 그래요 그래가지고 어쨌든 이것도 지난 시간에 했던 거야.
그쵸 특정 섹터에 대해서 이렇게 이게 그랜드 디스트 이게 이게 이게 중요한 게 이거죠.
항상 이게 그레디언트라는 거의 의미가 변화율인데 변화율이 줄어들게 항상 업데이트하는 거잖아요.
그쵸 그 파라미터를 이 파라미터가 용어를 이제 웨이트라고 부르기도 하고 누가 질문했는데 좋은 거예요.
웨이트라고 부르기도 하지 그쵸 곱하는 거니까 웨이트지 뭐 그쵸 가중치 똑같은 말 그렇죠.

참석자 1 10:17
그래서 요 얘 이 수정 파라미터에 대한 코스트의 변화율이 중요해서 얘를 계속 업데이트하는 거잖아.
그쵸 얘는 어쨌든 지가 변할 때 어쨌든 자기가 다른 건 다 픽스된 상태에서 이게 편미분이지 사실은 편미분이 뭔지 알죠 여러분 사실 파라미터는 되게 많은데 다른 건 다 그대로인 상태에서 내가 변하면 어떻게 된다는 것만 보는 거잖아.
사실 그렇죠 그런 거지 그래가지고 근데 이게 보통 이유리 그렉션은 이렇게 생겼는데 그냥 이렇게 생겼잖아요.
여러분 그냥 이렇게 볼록 함수로 끝나잖아요. 그렇죠 사실 실제로 뉴럴레트이나 이런 데는 이제 막 이렇게 다양하게 생긴 거지 아까 여기 나오는 것처럼 뭔가 그죠 그래서 여기 내가 사실은 여기 여기 여기 만약에 여기 와서 진짜 코스트가 작아지는 걸로 봤어요.
얘가 근데 나만 중요한 게 아니라 사실 다른 프라미터들도 많잖아요.
사실은 그렇죠 여러분 항상 그래서 이게 사실 힘든 거기도 하고 그쵸 진짜 코스트는 얘 말고도 다른 애에 대해서 변하더라.

참석자 1 11:16
얘는 어쨌든 얘는 나름대로 최선의 최선을 다해서 어쨌든 여기 와야 되는 거지 그쵸 그렇게 하는 거지 그래요.
선호에 대해서는 항상 가능해 소니케이션이 항상 가능한데 다른 데서는 항상 못 가능할 수도 있다고 항상 로컬 아티업이 글로벌 아티멈도 아니고 거기다가 또 다른 파라미터의 영향도 있기 때문에 정말로 모든 파라미터가 한 곳을 향해 제일 다 조합해가지고 정말 낮은 데가 있을 수도 있지만 아닐 수도 있거든 생긴 모양이 그렇잖아요.
여러분 얘는 이 파라미터는 도저히 안 될 약간 좀 희생을 해야지 다 진짜 글로벌 아트몬이 나올 수도 있잖아요.
여러분 크로드 같은 느낌인 거죠. 그래서 걔는 뭔가 잘못됐어 근데 어디 어디 최적화 하다가 뭔가 잘못됐어 그래요.
바보가 아 내가 원하는 쪽은 바보가 돼서 뭐 다른 걸 잘하는 것 같아 그래요.

참석자 1 12:00
그래가지고 그런 얘기를 하고 있고 그다음에 리믹스 이게 피츠 스케일링도 지난 시간에 했었어요.
그쵸 이게 학습하려고 그럴 때 스케일이 비슷해야지 학습이 잘 된다고 했죠.
그래서 여러분 미리 이제 얘기해 두면은 이게 사실 이렇게 우리가 피처를 입력을 줄 때마다 이렇게 맨날 만들어 가지고 예쁘게 이렇게 리믹스로 아니면 스탠다 정규 표로 만들어서 줄 수도 있지만 이거 귀찮으니까 이제 어떻게 하고 있냐면 지금 뉴럴 네트워크에서는 계층이 있다는 얘기 여러분 봤잖아요.
항상 계층이 보통 계층이 사실은 여러분 그냥 웨이트 곱하는 것들이랑 그다음에 액티베이션 포 하는 거 두 가지잖아요.
사실은 그걸 한 계층으로 보고 있는데 추가로 이제 막 막 넣기 시작하는데 배치 노멀라이제이션이라는 게 있어요.
여러분 배치 노멀라이제이션 느낌이 오죠. 이게 노멀라이제이션이 사실은 스탠다드다이제이션 노멀라이션 같은 말이죠.

참석자 1 12:47
사실 노멀라이제이션 그쵸 그렇죠 여러분 그 배치 노마레이션은 있어요.
나중에 볼 거예요. 그래서 배치 노마레이션은 계층은 항상 넣어버려요.
요즘에는 항상 그래가지고 항상 그 중간중간에서도 이제 항상 그렇게 이렇게 값을 예쁘게 만든 다음에 항상 학습시키는 걸로 된 거지.
사실 이것도 뭔가 근본을 알면 나 내가 그거 만들었을 것 같아 이런 느낌 그렇잖아요.
여러분 내가 만들어서 대박인데 그러면은 아니 어쨌든 그렇죠 무슨 얘기인지 알겠죠 여러분 사실 이거 지금 알고 보면 당연하잖아.
사실 은근히 근데 지나고 보면 당연한데 먼저 알아내는 게 중요한 거죠.
항상 교수들은 내가 먼저 알아낼 수 없을까 이렇게 이제 그러고 있는 거지.
그렇죠 여러분도 마찬가지고 그렇죠 이해되죠. 여러분 지금 보면 당연하잖아 이게 계속 학습을 여러 번 하면 그러니까 할 때마다 계속 이렇게 해주는 게 좋을 거 아니야 그쵸 그런 걸 하는 게 배치 노멀레이션이라는 게 있고 이름이 배치 노멀라이션이잖아.

참석자 1 13:40
배치 배치 들어오는 배치가 여러분 뜻이 뭐냐면 배치 데이터가 학습시킬 데이터 셋이잖아요.
그쵸 미니 배치지 사실은 그래서 배치하는 거죠. 하여튼 그런 게 있고요.
나중에 봅시다. 나올 거예요. 그다음에 여기서부터 알았던 내용이에요.
그쵸 안 하는 내용이에요. 여러분 그렇죠 이거는 사실 여기 이거 사실 이거 다른 책에서 갖고 온 거라서 거기에 많은 앞에 내용이 나오고 한 건데 그냥 제가 재미있어서 갖고 왔어요.
이거는 뭐냐면은 원래 세타 원이나 세타 제로가 있는 이 리뉴얼 리그레션 문제예요.
여러분 딱 2개만 있는 파라미터가 리뉴얼 리그레시션 쪽에서 그렇죠 예를 들어서 세타 제로가 이제 바이러스 세타 원이 웨이트겠구먼.
그렇죠 그런데 실제로 이제 여기 지금 이 그래프는 지금 로스 값을 안 보여주잖아요.
그러니까 알 수가 없어요.

참석자 1 14:29
원래 등고선으로 하려면 3차원처럼 이렇게 색깔을 표시하든지 이런 게 있어야 되는데 여기도 지금 대충 알아먹으라고 여기가 미니멈으로 표시해 놨으니까 그렇겠지 하는 거잖아요.
그쵸? 여기도 안 보여주지만 사실 여기가 여기가 정답 값이에요.
여러분 여기가 여기 여기 세타 재료가 4고 세타 1이 3인 이게 이게 진짜 정답이 아니라 제대로 된 찾아야 되는 루스가 제일 작아지는 지점이 여기인 거예요.
예를 들어서 예로 그렇다고 여러분 그냥 재미로 보는 거예요.
여러분 그런데 사실 맨 처음에 이제 한 번 트레이닝에서 이제 어떻게 변했는지 어떻게 너에 가까워지는지 보여준 거예요.
여러분 트레이닝 할 때 그래서 배치 있죠. 배치 보면 여기를 가까이 가야 되는데 여기 가까이 가면 이제 이제 로스가 작은 거 아니에요 여러분 그런데 스토키스티 이거 배치부터 볼까?
배치는 모든 비 다 쓰는 거잖아요.

참석자 1 15:19
여기 한 번 한 봉열 나오면 일로 가고 두 번쯤 일하면 일로 가고 이러는 거예요.
여러분 이해돼요. 예. 그렇게 가다가 이게 금방 가요.
여러분 금방 여기서 막 머무르고 있는 거 보이죠. 여러분 그렇죠 근데 근데 여기서 문제는 1로 갈 때 시간 오래 걸리고 2로 갈 때 시간 오래 걸리는 거지 계산할 게 많아서 느낌이 오죠.
여러분 봐요. 이해돼요. 그리고 미니 배치 미니 배치 배치는 배치는 이렇게 굉장히 하나씩만 하는 거예요.
스토캐스팅을 하나씩만 신문 실수 스토캐스틱은 딱 한 데이터 갖고 한 거야.
그랬더니 겨우 여기 왔고 운이죠. 운 그쵸 운이야.
사실은 겨우 여기 왔고 이랬다는 거예요. 알겠죠?
3 4 거의 안 변하고 있죠 그쵸 이러다가 이렇게 갔다는 거고 그렇죠 미니 배치는 여기 대치 사이즈가 얼마인지 안 나왔지만 적당히 해서 했는데 문이 이거 처음에는 여기 딱 갔잖아요.

참석자 1 16:08
이렇게 오히려 배치보다 더 잘하기도 한다는 거지 데이터가 운이 좋았지 사실은 이거는 꼭 이런 건 아니고 보통은 배치가 더 잘해 이 보통 1로 이렇게 되는 게 배치가 더 가능성이 높은데 여기는 이렇게 됐어요.
우연이야 이거 사실 알겠죠. 근데 그럴 수도 있다고 미니 배치가 더 훈련이 잘 되기도 해요.
그러니까 공부를 너무 많이 하는 것보다 조금 하는 게 오히려 훈련이 더 자신 있게 한다고 그래가지고 보면 여러분 유명한 거 있는데 골프 같은 거 배우면은 처음에는 막 잘하다가 점점점 망하고 볼링도 처음에는 되게 잘하다가 점점 망가지고 그런 거 있어요.
그렇죠 잠깐만 해보면 더 잘하다가 그런 거 있죠. 마지막에 여기 도전하는 거 되게 힘들지 어쨌든 그런 걸 보여준 거고 그다음에 이것도 같은 책에서 갖고 온 건데 이 책은 이제 옛날에는 여러분이 옛날에 우리가 우리 과에서 머신러닝이나 딥러닝 동시에 개설됐거든요.
머신러닝 딥러닝을 머신러닝 안 듣고 들어야 되니까 제가 머신러닝을 같이 했었어요.

참석자 1 17:03
처음 배우니까 지금은 이제 그냥 듣고 오라고 그러고 같이 듣던지고 그러니까 또 하나 나는 건데 거기 그래서 이 책이 머신러닝 책이에요.
우리는 팬존 머신러닝 사이킬런으로 주로 해요. 사이킬런으로 좋아요.
괜찮아요. 이것도 그래서 어쨌든 얼리 스타킹이라는 개념도 미리 알려주고 싶은데 여기 데이터를 지금 계속 트레인 데이터만 얘기하면 제가 얘기했죠.
주로 배치 얘기하는 거 전부 다 근데 밸리데이션 데이터를 제가 쓴다 그랬잖아요.
그럼 어디 쓰는지 보여주는 거예요. 보면은 그 트레이닝 셋으로 학습을 하면 이렇게 되고 있죠.
그렇죠 이게 지금 왜냐하면 에폭이라는 게 한 의 폭이 아까 한 미니 배트를 쓴 걸 의미해요.
이해되세요? 여러분 또는 이게 에폭이 텐서 플로우에서는 미니 배치를 다 훑은 거를 하나 에폭이라고 부르기도 해요.

참석자 1 17:49
미니 배치로 쪼갰으면은 이런 거지 이걸로 이렇게 쫙 하나 둘 이걸 한 스텝 스텝 스텝 해가지고 다 붙여서 모든 데이터를 다 써버린 걸 한 f이라고 부르기도 하고 이게 용어가 막 왔다 갔다 해요.
사실은 에폭이 뭐 어쨌든 한 종목 세대가 지난 거잖아 세대잖아 세대 그쵸 제너레이션 이런 느낌이잖아.
한 시대에 그래요. 용어가 잃어버 퍼 어쨌든 이거는 그래서 보통 보통 여기서는 어떻게 이해하시면 좋냐면 이 그림에서는 실제로 이 에폭은 데이터 풀로 다 쓰면서 미니 배치 여러 번 해가지고 어쨌든 한 번 한 다음에 이제 밸리데이션 데이터로 적용해가지고 그런데 펜스 플로우도 기본적으로 다 하는구나.
어쨌든 항상 보면은 여기 이제 첫 번째 폭 들어왔을 때 이제 밸리데이션 셋에 RMSE 값이 있고 트레이닝 셋에 RMSE 값이 있는데 항상 밸리데이션 쪽에 로스 값이 더 크죠.
이게 로스 값이죠 여러분 그쵸? RMC가 로스 값이에요.
그쵸?

참석자 1 18:50
RMBC가 이제 여러분 뭔지 아나 플레이 루트민 스퀘어 에러죠 그쵸 이거 플레임 알아야 돼요.
여기 에러니까 로스랑 같은 말이죠 그쵸 스포스도 같은 말 그쵸 이 2가 에러잖아 에러 에러 로스 코스트 다 같은 말 그쵸 이게 이거 지금 포스트 값이구먼 에러 값이구먼 로스 값이구먼 이렇게 알아야지.
그쵸 RMS는 이제 민스케에라에다가 루트까지 씌워버렸네.
그렇죠 루트를 씌워버려서 그리면은 그림이 좀 너무 아프니까 노트 많이 알겠죠 RMS 뭔지 알겠죠 MSD 계산해서 다시 노트 씌운 거지.
그래 어쨌든 이거 보면 이게 보면 트레이닝이 쭉 해가지고 이렇게 점점점 내려가고 있는 거 보이죠.
그쵸 이게 사실 트레인 데이터는 어떻게 해서든 항상 굉장히 작아질 수밖에 없어요.

참석자 1 19:40
여러분 보통 왜냐하면 그거에 대해서 학습하니까 그런 시간이 지나면 무조건 잘 처인 데이터에 대해서는 보통 이상한 것도 막 배우니까 틀린 것까지도 근데 밸리데이션은 어쨌든 한 번도 본 적이 없는 데이터인데 이렇게 지금 내려가다가 어느 순간에 올라가죠.
클로드 같은 어쨌든 이게 이렇게 하다가 사실 여기가 좋았는데 더 학습을 하니까 뭔가 데이터가 좀 별로인 것 같아.
그래가지고 이렇게 더 나빠졌어요. 그쵸 이거 진짜로 쓸 때는 밸리데이션 데이터가 더 중요하잖아요.
여기보다 뭔가 여기서 멈춰야 돼. 그렇죠 더 이상 훈련을 하면 안 돼 업데이트하면 안 된다고요.
그쵸 그래서 밸리데신 데이터가 필요하다는 거예요.
이게 여기를 사실은 오버 피팅이라고 그래요. 오버 피팅 과대 적합이라고 나중에 나오는데 뭔가 좀 그 테스트 트레이닝 데이터에 대해서만 잘하는 거야.
내가 공부하고 있는 거에 대해서 실제로 실제 일반적인 거는 못 풀기 시작하는 거지.
그쵸 그래서 밸리데이션이 필요하다는 거예요.

참석자 1 20:35
알겠죠 여러분 그래서 얼리스터팅

참석자 2 20:38
그러면 오버 피팅이 일어나기 전에 저 베스트 모델 지점을 알

참석자 1 20:42
아니 그러니까 미리미리 저장해 놨다가 제일 낮아졌을 때 저장했다가 화면 올라가면 이제 그거는 그게 아니라 저장 했던 값으로 웨이트 값을 써야지 이해되죠 어차피 모르잖아.
이게 진짜 이게 최저일지 이게 최저일지 할 때는 모르잖아.
업데이트할 때는 그러니까 복사본 만들어 놔야지.
게스트 모델 이렇게 제일 점점 낮아질 때마다 저장을 해놓는 거지.
항상 여기 계속 저장을 하는 거죠. 여기서는 그쵸 앱마다 그러다가 여기서는 저장 안 하고 또 기다려보고 사실 이게 이렇게 그림이 예쁘게 안 나오면 보통 요동쳐요.
막 이렇게 막 요동쳐요. 사실은 왜냐하면 이게 우연에 따르기 때문에 잘하다 못하다 막 이럴 거 아니야 사실은 밸리데이션도 그렇고 트레인도 그렇고 그래요.
약간 이렇게 느끼게 한다고 제일 작아졌을 때부터 저장을 해놔요.
이해되죠? 얼리 스타핑이라는 걸 하는데 얼리스타핑은 모든 에폭을 너무 많이 하지 말고 빨리 끊어야 된다.
스톱 스톱시켜야 된다 그런 얘기예요. 알겠죠 그래요.

참석자 1 21:39
여러분 보통 그냥 여러분 그냥 딥러닝 같은 거 공부 안 하고 이론적인 거 안 하고 하면은 그냥 무조건 f은 500이 기본 이러고 그냥 해.
그냥 무조건 공부 많이 하면 좋은 줄 알아. 공부 많이 하면 좋은 게 아니에요.
여러분 공부 많이 하면 더 나빠진다니까 여러분들 공부 적당히 해야 돼.
이해되죠? 여러분 진짜로 많이 한다고 좋은 게 아니에요.
알겠죠 모든 걸 위해서 그래요. 어쨌든 계속 하면 그다음에 여기 다른 이거 유명한 거라서 또 이제 나 갖다 놨는데 베이싱 그레디언트 프라블러는 베니싱이 사라지는 그레디언트 그레디언트가 변화값이잖아요.
여러분 변화율 변화율이 있어야지 업데이트하잖아 그치 알파를 곱해가지고 러닝 메이트 곱해서 그쵸 빼잖아요.
원래 값에서 이해되죠? 여러분 원래 어떻게 업데이트하는지 알지?

참석자 1 22:26
자기 원래 각 파라미터에다가 그 파라미터에 대해서 원래 로스 값이 얼마큼 변했는지 변화율을 구해서 거기다 알파 곱해서 해야 되는데 알파도 조그맣게 하잖아요.
그쵸 0.001 이렇게 하는데 베니싱이라는 건 사라진다는 뜻이에요.
사라지는 그래서 이게 뭔가 그레디언트가 있어야지 변화율이 있어야지 변화율을 업데이트하는데 변화율이 거의 안 나와 보면 사실 변화율이 거의 없으면은 그거 학습 못하는 거잖아요.
원래 그래도 디스트가 변화율이 거의 없으면 끝나는 거잖아요.
학습을 근데 이게 문제가 우리가 이제 원래 전에 얘기했지만 액티베이션 펑션이라고 쓰기로 했는데 액티베이션 펑션을 비상용성을 위해서 기억나죠.
여러분 앞에서 기성형 서 그래서 근데 그때 시드 모이드라는 것도 알었다.
s 자 모양으로 일로 가는 거 근데 걔가 여기가 기울기가 기울기 값이 이제 여러분 여기 이렇게 있잖아요.
그렇죠 이렇게 기울기 값이 여기 기울기잖아요. 그쵸 근데 여기 기울기가 없잖아요.

참석자 1 23:26
계속 에러가 크면은 뭔가 변화되는 게 없으니까 학습이 안 일어나는 거지 이해돼요.
여러분 기울기가 0이잖아 여기가 시그모이드나 당연히 그렇잖아요.
여러분 기울기 0이잖아요. 여기 여기도 기울기 0이니까 계속 에러가 이렇게 발생하고 여기서 어쨌든 전파가 될 때 요건데 여기 줄 좀 맞추자 했는데 줄 좀 줄 좀 줄 좀 하면서 이렇게 없어진다는 걸로 표현했는데 재미있어서 갖고 왔어요.
그냥 사실 이거랑 저거랑이 맞는 건 아닌데 이게 이게 시그모이드라는 함수가 중간에 끼어 있는데 걔가 변화율이 없어가지고 변화율이 없어서 거의 줄어드는 거 사실은 더 정확하게 말하면 수식을 보면 더 나중에 나중에 보여줄까 그래 이게 소수를 계속 곱해가지고 되게 낮아지는 경향이 있어요.
호수를 1 마이너스 와랑 와랑 곱해서 주든지 어쨌든 그래서 피팅이 잘 안 돼서 렐루는 이게 무조건 기울기가 1이거든 그대로 전파돼요.

참석자 1 24:19
그래서 이걸로 하니까 소리가 안 작아져서 좀 잘 된다는 게 유명해서 재미있어서 보여줬어요.
여러분 엄밀하지는 않지만 그거요. 그래서 어쨌든 지금은 시그모이드는 중간에 액티베이션 펑션으로는 아무도 안 써 옛날에 중간에 액티베이션 펑션 딥 뉴얼 네트워크에 들어갔잖아요.
비서 열수 많이 넣기로 했는데 비서 열수 중간중간에 다 시공 오일을 썼었거든요.
무조건 영하고 이사회를 만들어버렸지 근데 그러지 않아야 된다는 걸 알았다고 사람들이 학습할 때 그래서 지금은 아무도 안 써 시그모드를 맨 끝에는 쓰는 거지만 중간에 쓰지는 않는다는 거예요.
중간에는 거의 다 멜론 아니면 다른 거 써야 돼. 어쨌든 이렇게 기울기가 확실히 살아 있는 놈이 살다 동요하는 게 더 머리를 바꿔버리는 게 이런 게 좋을 수 있잖아요.
그쵸 그런 식으로 가속화시키는 가속화시키려고 하는 여러 기법들이 나왔어요.

참석자 1 25:06
그래서 미니 일치하면 이렇게 되는데 뭔가 좀 이거 이렇게 여기로 가려고 그러면 좀 더 더 기울기를 더 확실히 해가지고 더 확실히 가는 게 좋지 않아요.
더 빨리 배우게 그런 거를 열심히 연구해서 이렇게 러닝 메이트도 가지고 러닝 메이트도 마찬가지고 그리고 이제 파라미터가 여러 개라고 그랬잖아요.
워낙에 걔네들을 잘 조합해서 뭔가 거기서 뭔가 좀 여러 가지 요소가 많은데 거기서 좀 기울기가 큰 놈을 좀 많이 반영해야 될 거 아니야 다른 데는 별로 변화가 없으면 그냥 냅두고 똑같이 러닝 레이트 적용하는 게 아니라 그런 생각이 들잖아요.
그래서 그런 거를 여기 복잡해 방향과 보폭이라는 건 이제 방향이라는 거는 보폭은 이제 값을 하는 거고 방향이라는 건 어느 그레디언트를 더 집중하느냐 이런 내용이에요.
어느 파라미터로 대지 말고 그냥 파라미터가 많잖아.

참석자 1 25:50
워낙에 지금은 그래서 그런 거를 그래서 이거 보면은 이게 실제 이쪽 이쪽이 이제 되게 굉장히 혈압 로스까지 작은 쪽이고 이로 가면 좋은데 여기서 막 헤매는 것보다 이렇게 빨리 가면 좋잖아요.
빨리 가기 위해서 여러 가지 기법들이 유명한 것들이 있어요.
SGD가 지금 제일 느린 거 보이죠. 여러분 원래 하던 거 이거는 사실 미니 배치야 알겠죠 되게 느리게 가잖아.
근데 이게 이렇게 모멘텀이니 이런 거 니까 이런 모멘텀이 여러면 느낌이 오죠.
뭔가 모멘텀을 가지고 뭔가 운동량 그가 그렇죠 뭔가 관성을 가지고 가자는 그런 거잖아요.
그래서 보면은 여기 뭐 이게 반복되고 있는데 얘는 느려 터지고 딴 애들은 휙 가버리는 거 보이죠.
여러분 그래요. 근데 또 이게 휙 가버리다가 또 사실 다른 체제가 있는데 못 갈 수도 있으니까 여러 개 보통 해봐요.

참석자 1 26:36
그래서 잘 모르겠으면 아담이라는 건 좋겠지 언제 나 이게 아담이라는 게 제일 많이 쓰여요.
여러분 아담이라는 거랑 이 책은 나 맨날 아라스 트롬 쓰고 있고 그래요.
여러분 제가 전에는 제가 이걸 하나하나 다 가르쳤거든요.
지금 이제 여기 뭐 그냥 안 배워도 될 것 같아 이 정도만 알고 그냥 여러분 이거 갖고 할 일 없어.
사실 너무 너무 오래 완전히 머추어한 그러지 여기서 뭔가 나올 게 없어요.
알겠죠? 얘는 그냥 이거 써보고 저거 써보고 이거 잘 때 이거 써보다가 다 돼 가서 이거 써보고 여기 나와 있는 거 있잖아요.
그냥 그냥 SGD도 써볼 때가 있어요. 진짜 뭔가 잘 못 찾았다 그러면 이것도 써봐야 되고 사실 사라진 게 아니고 뭔가 자꾸 뭔가 더 최적화가 될 수 있을 것 같은데 못한다 그러면 조심스럽게 움직여야 되는 거거든요.
사실은 얘는 되게 느리게 움직이는 거 봤잖아요. 맞잖아요.
맞잖아요.

참석자 1 27:28
그래서 이게 더 좋을 수도 있어요. 사실은 그래서 여러 가지 다 해보는 게 맞아요.
사실은 알겠죠 그래요. 마지막 내용이 바이오스랑 베리언스랑 트레이드 오픈인데 이게 여러분 바이오스라는 의미랑 베리언스라는 의미가 우리나라말로 내가 적어 어서 잡아 어서 평양 군산 여기가 30페이지 올 평양이랑 군산

참석자 1 27:53
커진 정도지 베리언스는 그렇죠 여러분 그렇죠 바이러스는 이렇게 몰린 정도잖아 그렇죠 반대말이잖아 그쵸?
바이어스는 여기 보면 이거 이거 볼까요? 여기 보면 이거 이거 볼까요?
여러분 여기 왼쪽 이 이 세로축이 바이어스인데 그리고 여기가 이렇게가 바이어스고 이렇게 베리언스예요.
여러분 보면 지금 베리언스가 이제 작다 크다 바이어스가 작다 크다 이렇게 돼 있잖아요.
이해되죠? 여러분 일단 다이어스도 작고 베이러스 작으면 이렇게 과녁이 있으면 과녁에 여기다 다 빨간색에 맞춰야 돼.
이게 양궁이에요. 이렇게 하는 게 제일 좋지. 그런데 이제 페인제 페인제 페인제 바이러스 바이러스가 크면 이제 막 이쪽으로 막 가는데 베리어스는 없는 거예요.
다 여기다가 쏘고 있는 거지. 근데 바이러스는 되게 커 별로 퍼지지 않았잖아.
베리어스가 작은 건 다 안 퍼졌죠. 일단 보면 여러분 안 퍼졌어.
베르스가 크면 퍼졌죠.

참석자 1 28:49
베리어스가 크면 커진 거고 베리어스가 작으면 베류지 자체는 본인이 이제 뭔가 예측하는 값의 평균값이 기본 평균값으로부터 떨어진 정도가 작은 거예요.
그러니까 이거는 원래 정답 값이 아무 상관없이 베리언스는 내가 프로드 값에 뭔가 퍼티 노드가 작은 거지 이해돼요.
여러분 그러니까 이 베리언스라는 것 자체는 저는 정답 값에 상관없는 거야.
그냥 내가 여기 예측기가 예측기가 내놓고 있는 답에 퍼진 정도야 알겠어요.
그런데 바이러스는 뭐냐면은 정답 값으로 상관 있는 거예요.
정답 값으로부터 얼마나 떨어졌느냐의 문제예요.
평균값이 평균 값이 또 이번에는 그래서 바이어스는 보면은 결국은 이게 막 베리어스가 커도 결국 평균적으로는 바이러스는 작은 거지 평균적으로는 그렇죠 여기는 여기는 완전히 전부 망한 거고 그렇죠 여러분 훈련 잘못하면 이렇게 될 수 있지.
맨날 여기만 쌓여서 그렇죠 절대로 이로 못 가 그쵸 그래요.

참석자 1 29:59
그래서 계속 여기다가 막 쏘다가 여기다가 막 쏘다가 그치 몰려가지고 쓰긴 쓰는데 여기는 못 맞춰.
그렇죠 그리고 차라리 이게 나을 수도 있어. 그렇죠 어쨌든 얻어걸리는 거라도 있잖아.
그러니까 여기 여기 있어야 죽는다고 생각해 봐요.
여러분 사람 죽는다 그러면 적군이 있어 적군을 쏴서 죽여야 되는데 이렇게라도 쏴야지 사실은 이렇게 쏘면 무슨 소용이야 답이 나오는데 이해돼요.
여러분 총을 버리는 게 있어도 이렇게 나는 게 낫잖아 이건 망한 거고 그쵸 그래서 이게 바이어스랑 베리언스랑 트레이드오프가 있어요.
너무 베리언스를 줄이려고 그러면 줄이려고 그러면 이런 일이 생기는 거지 얻어 올리는 것도 없이 그렇죠 그렇죠 그렇다고 알겠죠 그래요.
그래서 그리고 또 그래서 이제 이게 수학적으로 보면은 이게 여기 나오는 지금 재밌는 내용인데 실제로 지금 원래 이제 펑션을 이렇게 모델링 할 수가 있어요.

참석자 1 30:54
원래 예측 값이 원래 이제 FX를 항상 입력값이 x가 입력 피처들이고 FX 예측하는 거잖아요.
결과 내놓는 거 거기다 항상 플러스 에러가 이게 어쩔 수 없는 에러가 좀 섞일 수 있잖아요.
그래서 원래 이제 실제로 여러분 뭔가 실제로 이해도 우리가 원래 이제 x와 상관없는 에러가 들어갈 수 있어요.
여러분 그래서 여기 y는 FX 더하기 2로 모델링 할 수가 있는데 에러 에러 근데 노이즈를 이제 랜덤 별로 해서 이제 여기 이렇게 노이즈라고 노이즈라고 했지 여기 노이즈 노이즈라고 했어요.
여러분 그렇게 해서 모델링 하면은 여기에 그림으로 이게 노이즈의 이제 평균은 0이고 베리언스는 이렇게 정해놨는데 mse가 이제 민스케어 에러가 이렇게 될 거 아니에요 여러분 예측 값이 이렇게 다 이렇게 책값이 우는 거 알죠?
이거 이거 MSD가 이거잖아요. 여러분 알죠 그러고 바이어스는 원래 이거잖아요.

참석자 1 31:49
여러분 바이어스 이거는 거 이해되지 않아요 여러분 바이어스는 원래 여러분 예측값의 평균에서부터 원래 지금 원래 예측 값을 평준으로부터 원래 값을 빼는 거잖아요.
그쵸 정답 값을 베리언스는 예측 값이랑 원래 정답 값이랑 아무 상관없이 자기들끼리 그냥 평균 낸 거잖아.
그런데 재밌는 거는 이 mse에다가 다시 또 평균 내는 건 데이터 여러 개에 대해서 얘기한 평균 낸다는 뜻이에요.
여러분 데이터 여러 개에 대해서 그렇게 하면은 이게 정식이 깔끔하게 이렇게 떨어져요.
여러분 여기 옆에다가 정해놨는데 재밌어요. 이렇게 하면 진짜 바이어스의 제곱에다가 요거 제곱에다가 베리언스 더하고 그 베리언스가 제 제곱이니까 이렇게 되는 건데 에러 자체가 제곱이라서 바이어스의 제곱에다가 바이어스가 뭔지 알잖아요.
여러분 정확히 그거잖니까 근데 그냥 정답 값이랑 원래 값의 예측 값의 평균의 차이예요.

참석자 1 32:45
그래서 걔의 바이어스 제곱에다가 베리언스 더한 거에다가 원래 절체로 못 없애는 에러 있잖아.
얘 얘 요 요 요거 요거 여기 얘의 베리언스죠. 노이즈에 그거 더한 값으로 정리가 되니까 너무 아름답지 않아요.
여러 되게 놀라웠다는 거 되게 훌륭하잖아. 진짜 진짜 그리스도 그 에로라는 게 사실은 MST 계산하면 이렇게 딱 예쁘게 정리가 되는 거예요.
이렇게 모델링 하고 나면 너무 재미있잖아요. 수학적으로 되잖아.
너무 이거 되게 좋은 직관을 주잖아요. 절대로 못 없앨 수 없는 에러가 노이즈 때문에 생기고 에러 값을 이제 우리가 계산하잖아요.
그럼 노이즈가 끼어 있으면 항상 이런 게 생기고 그다음에 걔는 뭐로 구성돼 있냐면 바이오스가 베리어스로 구성되어 있는데 바이오스의 제곱이랑 베리언스는 원래 제곱 값이니까 그쵸 베리어스 제곱이고 뭐야 스탠다드 디비에이션은 요거 베리언스를 이렇게 씌운 거잖아요.
그쵸? 알죠?

참석자 1 33:38
여러분 베리언스 원래 이렇게 계산했으니까 제곱으로 그쵸 알겠죠 이걸로 더해지니까 여러분 로스 값을 계산하면 실제로 이렇게 이게 여기 에러가 이제 MSG라고 치고 여기는 이제 사실 이거 마델 콤플레스티라고 생각할 수도 있고 또는 이게 마 콤플렉스라는 건 여러분 파라미터의 개수라고 생각할 수도 있고요.
마델이 복잡해진다는 거잖아. 또는 훈련을 너무 많이 해가지고 파라미터 늘리는 파라미터가 되게 많아지는 거라고 생각할 수도 있어요.
여러분 훈련을 많이 하는 경우도 약간 다르긴 하지만 모델 컴플리스티가 높아지면 높아진다는 높아진다는 건 프랑트 되게 많이 쓰는 거 컴플리티가 낮은 거는 파랑 별로 안 쓰는 걸 의미해요.
극단적인 게 리뉴얼 리그레이션이 프랑트 거의 안 쓰는 거잖아요.
그리고 딥러닝은 프로트 되게 많이 쓰는 거지 이해돼요.

참석자 1 34:23
여러분 이게 막 이렇게 이렇게 하려고 그러잖아요.
우리 딥러닝은 근데 리뉴얼 션이 이렇게 해버리잖아요.
그쵸? 이렇게 하든지 그쵸? 이해되죠? 프라이트 계수 작은 쪽 큰 쪽이에요.
그래서 실제로 이게 항상 국물이라고 그러지 항상 항상 있는 규칙인데 원래 에러는 이제 모델이 복잡한 원래 제일 앞티멀한 뭔가 컴플렉스 모델이 잘 나오면 이게 에러가 이렇게 줄어들 수 있다는 거지 이렇게 요만큼으로 제일 작은 에러가 이건 거지 이니어르트 에러 때문에라도 이렇게밖에 할 수 없는 거예요.
예를 들어서 모델이 아직 너무 단순하면 이렇게 될 거고 굉장히 에러가 클 거고 모델이 너무 복잡해도 에러가 엄청 커진다는 거지 적당한 그 모델 콤플레스트에서 적당한 프라미터 개수로 이렇게 나온다는 거예요.
이 에러가 이 정도로 작아진다는 거지 이해돼요.

참석자 1 35:12
여러분 그런데 이거는 뭘로 구성돼 있냐면은 처음에는 굉장히 이제 바이어스 값이 크고 베리언스는 되게 작고 베리언스라는 건 이제 예측하는 데이터의 베리언스보다 작은 거지 여기 모델이 간단하잖아요.
그럼 베리언스가 거의 없어 선형 대수 선형 회기하면은 그냥 별로 평균 값에서 벌어지지 않게 그렇게 예측한다고요.
지들끼리 값들이 근데 이제 모델이 붙잡히는 베런스 점점 커진다고 그러니 계속 막 이렇게 이렇게 이렇게 막 하면서 막 되게 이상하게 맞추고 할 거 아니에요 그쵸?
꼬불꼬불하면서 이해되나요? 여러분 무슨 말인지 그래요.
그런데 이제 막 거꾸로 바이러스는 점점점점 모델이 복잡해질수록 줄어든다는 거지 확 고정값으로 들어 있는 게 거의 안 하면은 하이 바이어스이면서 사실은 로우 베리언스지.
맞아 베리언스는 되게 작아 맨 처음에 훈련 안 하면 이렇게 되는 거예요.
오히려 하나도 못 맞추는 거지.

참석자 1 36:12
훈련을 잘하면 여기서 멈출 수 있고 훈련을 너무 심하게 잘못하면 이렇게 된다는 거지.
바이러스가 바이러스가 거의 없는데 이렇게 된다는 거죠.
너무 심하게 하면 나중에 좀 이제 맞는 것도 있지만 이상하게 틀리기도 시작해.
너무 여기서 잘하다가 더 하면 이렇게 될 수 있단 말이지 뭔가 걸 고려하고 막 이러다 보면 오히려 그렇단 얘기예요.
여러분 이건 여러분 저기 사실 비유니까 그래서 실제로 내가 말한 걸 여기 정리해 놨는데 바이어스 내가 느끼기에 이거 내가 정리한 건데 저기 채팅 피터에다 물어볼까요?
이거 인지 맞을 것 같아요. 바이어스는 시뮬레이티를 찾는 거 비슷한 점을 찾는 거고 베리언스는 디퍼런스를 찾는 거라고 생각해요.
저는 베리언스는 이제 켜진 정도니까 차이점에 대해서 적응하려는 거고 뭔가 실제 일에서 바이어스는 뭔가 같은 거에 대해서 그냥 한 방에 딱 갈라놓은 거거든요.

참석자 1 37:05
뭔가 포스 값을 줘가지고 그래서 그런 걸로 이해하시면 될 것 같고 그래서 로 바이어스랑 하이어 바이어스 서로 이제 서로 이제 대구를 이루고 여기 전부 다 위키피에서 뺏겨 온 거예요.
사실은 지금 최초 피트 해달라는 게 낫겠다. 업데이트할 거 있네.
데이트 너무 잘하고 있어 피트 진짜 너무 잘해 걔가 나빠지기 전에 얼른 데이터 써야지 하이어 그다음에 어쨌든 너무 하이어 여기 언덕 피티 OPT는 그냥 거기 써버렸는데 언덕 피티 OPT 언더 피팅이 뜻이 여러분 뭐냐면은 오피스의 반대말이겠다.
오피스는 너무 꽉 맞는다는 거잖아요. 딱 맞는 거고 언더 피트는 전혀 안 맞는 거잖아요.
여러분 오해하지 마실 게 옷이 뭐 오버 사이즈 뭐 이런 거 있죠 그거랑 상관없이 오버 사이즈 그런 거 옷을 박스형으로 입는 거 그런 거 오비팅이 아니고 그거는 언더 피팅이야 안 맞추는 거잖아 하나도 알겠어요.

참석자 1 37:57
여러분 언더 피팅은 거의 맞추지 않아서 학습이 안 된 걸 의미해요.
너무 학습이 제대로 학습을 거의 안 한 거 너무 간단한 거고 오피팅은 너무 복잡하게 돼서 너무 컴플렉스 한 거죠.
그쵸? 그래요. 그래서 이거 제너럴한 문제 잘 못 푸는 거고 그래가지고 어쨌든 뉴럴 네트워크 이건 이건 이건 저기 밑에 나오는데 아직 이거 잘 모르니까 넘어갑시다.
그냥 됐어. 어쨌든 요거 요 두 문장은 지금 내가 제대로 안 가르쳐서 그냥 놔두는데 일단 나중에 합시다.
여러분 질문 나중에 할게요. 여러분 미안해요. 지금 설명하기 너무 길어 진행할게요.
여러분 일단 이렇게 해서 여기 다 끝났지 또 하나 더 있네.
그래서 이제 여기 하나 더 만들어 놓은 게 로우 바이어스이면서 하이 베리언스인 거가 이제 이런 것도 있고 그다음에 하이 바이어스 로우 베리언스인 거 이런 게 있어요.

참석자 1 38:53
그러니까 리니어리 브리스 라이지스트중에 제일 대표적인 걸로 로베리언스로 이거고 얘가 이제 로바이러스는 하이 베이언스인 게 이런 게 서포트렉트 머신이나 디션징이나 이런 거는 막 문제 맞춰가지고 막 막 계속 꼬아버리는 거거든 그래요.
그래서 뉴럴 네트워크는 중간에 가보려고 노력하는 거고 그래서 여러 개 다시 훈련시켜서 잘 쓰는 거고 사실 근데 디시전 트리 같은 것도 앙상블에 쓴다고 그랬잖아요.
랜덤 폴리스트라는 거 쓴다고 그랬잖아요. 사실은 그래서 그런 거를 해결하기 위해 이런 문제를 해결하기 위해서 이렇게 하는 거고 그리고 지금 제가 뭘 하고 싶었냐면은 이거 다음에 뭐가 들어가기 전에 이거를 텐서 플로우 시간이 지금 8분 남았는데 재밌으라고 그냥 그래도 지금 부르겠다.
텐서 플로우 플레이 그라운드라는 게 있어요. 우리 교과서에는 소개가 안 돼 있는데 저 아까 머신러닝 책에 소개가 돼 있는데 되게 인사이트를 주는 게 좋은 것 같아서 이게 이런 게 있거든요.

참석자 1 39:51
보면은 플레이 그라운드 텐서플로우더가 이렇게 돼 있죠 보면 이게 에폭이라는 개념이 있고 러닝 메이트라는 것도 있고 액티베이션도 있고 레이션 레글라이션이 뭔지 안 알아들었지 레글라이제이션은 아 아니 여러분 안 배웠어요.
레브라이제이션은 로스 값에다가 파라미터들을 뭔가 파라미터들의 값을 반영해서 파라미터가 너무 복잡해진 파라미터 때문에 뭔가 되게 복잡해지는 바이러스가 아니야 저기 커런트 때문에 모델 컴 플레스트가 더 높아지잖아요.
커런트가 많으면 많을수록 근데 걔가 너무 조정이 잘 안 되게 파라미터가 값이 너무 너무 크게 안 만들려고 노력하는 게 있어요.
바이러스 웬만하면 크게 만들려고 그래서 여러 가지 지금 에러넬트라는 게 있거든요.
여기 설명할 못하게 나중에 할게요. 여러분 보여주면서 할게요.
어쨌든 레글라이이션 뭐를 하지 나중에 설명해 줄게요.
알겠죠 그리고 브레이브 이건 모르겠고 러닝 레이트랑 액티베이션만 일단 알아봐 이거 안 하고 있어 봐요.

참석자 1 40:50
그런 건데 6분 남았는데 되려나 어쨌든 지금 이렇게 디폴트로 들어가면은 러닝 메이트 0.03으로 돼 있고 막 더 막 바꿀 수 있죠.
그쵸 이러면 이제 되게 느리게 학습되는 거고 이렇게 하면 엄청 빨리 가수 되는 건데 잘못하면 이제 수렴이 안 될 수도 있고 그런 거지.
그리고 이게 정체가 뭐냐면은 데이터가 셋이 지금 이렇게 생긴 데이터 이렇게 적혀 있는데 이게 무슨 의미냐면은 여기 아웃풋이 나오는 게 있잖아요.
근데 여러분이 지금 여기도 계속 그림이 여러분 여기 입력 입력에 이제 피처가 하나인 게 아니라 두 개인 걸 얘기하고 있어요.
여러분 입력 피처가 2개 그래서 여기 이거 보여 이거 눌러볼게요.
일단 이거 이거 이거 지금 자체가 뭘 의미하냐면은 데이터가 이게 x 값 x1 이게 라 값의 x 2라고 생각하면 돼요.
여러분 x1 2 이해되죠?

참석자 1 41:47
여러분 이게 x1이고 x2야 그 파란 피처가 2개인 거야.
이해돼요. 여러분 피처가 2개인 거를 표시를 한 거예요.
그리고 여기 색깔도 나오잖아요. 파란색이 지금 1이고 주황색이 마이너스 1이에요.
사실 3차원 그림을 보여주고 있는 거예요. 이해돼요.
여러분 이해되나 퀴처 각각의 하나하나가 요 파란색들은 어디에 몰려 있어요?
요 요 데이터는 요 데이터는 마이너스 3 0인 거야.
예를 들어서 마이너스 30인데 입력 디처가 근데 실제 이제 정답 값은 뭐냐면은 파란색이니까 1인 거지 여기는 마이너스 4 주황색 이거는 마이너스 4 0인데 정답 값이 마이너스 1인 거지 이해돼요.
여러분 입력값 위치에 따라서 여기 걸려 있는 그런 입력 값들 피처들은 전부 다 플러스 1이고 여기 몰려 있는 건 마이너스 1이 그런 데이터예요.
지금 사실 이게 3차원 그래프예요.

참석자 1 42:42
알겠어요 필라 때문에 여기 이런 데이터는 뭔지 알겠죠?
이것도 여기 있는 데이터는 1이라고 그래야 되고 여기 있는 데이터는 마이너스 1이라고 그래야 된다고 이해되죠.
여러분 근데 이런 거를 이거 선용 얘기가 안 되는 데이터들만 잔뜩 모아놨어.
그쵸 이건 선영이가 된다. 이게 선영이가 되는 겁니다.
이건 선영이가 되죠 왜요? 이렇게 딱 해버려가지고 그렇죠 선형 회의가 완전 선형이 되지는 않지만 그래도 좀 약간 좀 에러가 적게 나올 수 있잖아요.
그렇죠 여기 어쨌든 이렇게 얘기하면은 여기는 여기라고 그러고 여기는 여기고 그렇죠 좀 더 적게 나올 수 있겠지 분류기로 한다고 그러고 그래요.
그렇게 되는 거고 그래서 지금 여기가 또 이제 여기서 이거를 피처즈라고 적혀 있잖아요.
피처즈 요걸로 요걸로 이게 제일 간단하니까 이걸로 해볼게요.
여러분 피처즈 보면 x1 x2가 x1 값들은 지금 아예 그림이 이렇게 나왔지.

참석자 1 43:31
x1은 이게 지금 x1 값이 0 이상이면 전부 다 파란색이고 0 이하 마이너스 1 주황색이 마이너스 1이라서 이렇게 표시가 된 거예요.
지금 얘는 거꾸로 이해돼요. 여러분 여기 이런 애 눌러볼까 이런 애들은 x1 누르면 잘 안 되네.
어쨌든 x2 그런 거죠. 여러분 그래요. 어쨌든 이렇게 돼 있는 거고 그다음에 여기가 여기 지금 여기 이게 아웃풋인데 이게 결괏값이 나온 건데 결괏값이 지금 이제 어떻게 돼 있어?
여기 전부 포란색으로 돼 있죠. 전부 다 이렇게 디폴트로 뭔가 여기 지금 뉴런들이 있는데 예측 값이 전부 다 플러스 1로 플러스 1 근처 요 정도 0.2 정도로 예측하고 있다는 거지.
대충 디폴트 값이 0점 예측하고 있다는 거야. 이해되죠?
일단은 그렇게 결과가 나오게 지금 뉴런 이게 지금 히든 레이어가 2개라는 뜻이잖아요.
그러니까 이거 디미럴 네트워크이네.

참석자 1 44:28
그쵸 근데 우연히 그냥 여기 4개의 뉴런 쓰고 두 개의 이론을 쓴 거예요.
이거는 인풋 레이어 이건 아웃풋 레이어인 거지 뭐 이런 걸 설명하는 시간이 다 갔는데 이해되죠?
여러분 대충은 한 번 더 들으면 더 이해가 잘 되겠지 이런 식으로 해서 더 이해를 잘 해보려고 하는 거예요.
여러분 일단은 그래서 지금 이게 이 선형 얘기를 하려고 그러면은 여러분 액티베이션 펑션 여기 누런이 일단 휴든 레이어 몇 개 있어야 돼 선형 회계는 선형 회기는 선형 얘기를 한다고 그러면 어떻게 하는 건가 이따 여기서 선형 회기도 가능해 이걸로도 그가 뉴런이라 생각해 봐요.
여러분 이 뉴런이라고 생각하면 사실 잘 보면 여러분 이게 보면 각 뉴런들이 뉴런들의 모든 값이 다 연결돼요.
여기 입력들이 그렇죠 여기도 다음에도 여기 다 연결되는 거 보이죠.
다 연결돼 이렇게 해서 여기 지금 여기 선들이 색깔이 또 있잖아요.
이 선들이 정체가 뭐겠어요?

참석자 1 45:17
여러분 웨이트 값이겠지 여기 웨이트 값이 0.24 이거 디폴트 값 이게 처음에 맨 처음에 랜덤하게 초기화된 거예요.
그리고 여기 보면은 항상 이렇게 바이어스라고 돼 있죠.
바이어스 바이어스도 이제 여기 얘가 입력이 없는데도 불구하고 1 다고 일이 있다고 보고 곱하는 값으로 0.10이 지금 있는 거지 입력이 지금 3개 2개잖아요.
여기 근데 여기 들어오는 값이 하나 둘 세 개 곱하는 게 3개인 거야.
사실 얘는 그대로 더해지는 거지. 여기 x1에다가 이걸 곱하고 x2에다가 요 값을 곱하고 그다음에 여기 이 바이어스를 더하는 거지 사실은 일이 여기 일이 있는 거나 마찬가지지 일이 있고 선이 있는 거나 마찬가지라고 얘기해.
이거를 머릿속에 확실히 넣으시라고요. 알겠죠.

참석자 1 45:57
그래서 이게 여러분 이것만 봐도 뭐가 좋냐면은 이거 입력에 들어오는 피처가 2개면은 사실 웨이트가 3개구먼 하나는 바이어스구만 그걸 깨달으라 하는 거지.
그리고 이게 다음에 가면은 똑같이 또 4개니까 여기 또 한 4개 4개 잘 안 보이네.
여기 4개 그러니까 4개의 웨이트에다 바이스가 있는 거지.
그래서 여기 여기 이 각각 뉴런에서 나온 결과값에다가 이 웨이트를 다 곱해서 더한 다음에 이 바이스 값 더하는 게 여기 나오는 거라고요.
이해되죠? 그래요. 그런 다음에 항상 여기 뭐 하고 있는지 알면 여러분 액티베이션 통장 통과시키는 거잖아.
여기도 다 이 값 곱하기 이거 얘 값 곱하기 이거 그다음에 얘 더하고 그다음에 다 더해 3개를 그렇죠 그다음에 에큐레이션 통신을 통과시키고 그리고 이 값이 다 들어가가지고 또 이거 곱하고 이해되죠?
여러분 그렇게 한다는 거를 그림으로 이해하면 좋다고 생각해요.
저는 도식화해서 이해하는 게 정말 잘 알면은 다 그림으로 표현할 수 있어야 돼.

참석자 1 46:57
여러분 뭔가 발표하거나 논문 쓰거나 아니면 회사 가서나 할 때 그림으로 표현하면 굉장히 여러분의 이해도 높이고 뭔가 인사이트를 얻을 수가 있어요.
진짜로 뭔가 큰일 날 수 있다고 그래서 여기까지 하고 다음 시간에 이어서 할게요.
이걸로 시작하려고 알겠죠.


clovanote.naver.com