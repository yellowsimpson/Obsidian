딥러닝 day25
2025.06.11 수 오전 10:01 ・ 58분 1초
심승환


참석자 1 00:00
그래요. 이게 회사 이게 잘 안 보이는 거네요.

참석자 1 00:19
어텐션 하고 있는데 나중에 이제 제가 그냥 어쨌든 제가 슬라이드 따로 만들어가지고 떠들어가지고

참석자 1 00:32
묶어서 읽는 거를 이제 설명을 이해를 마지막으로 시키고 어쨌든 반복을 여러 번 해가지고 이게 어쨌든 11장 텍스트에 대한 거고 백업은 월주를 할 수도 있고 그래서 기본적으로 텍스트 리트로 입력을 주는데 집합으로 구성할 거냐 아니면 시퀀셜하게 좀 구속할 거냐 두 가지가 있죠.
그쵸. 근데 이제 bow 방식으로 하는 게 이제 간단한 문제들이 잘 되는 면이 있어요.
그리고 최신 지금 잘 되는 유명한 라이징이든 뭐든 이런 거 전부 다 시퀀스로 하는 거예요.
그쵸 내가 여기 목소리가 이상하지만 알아들을 수 있지 미안해요.
내가 옛날에 박기영 님 목소리가

참석자 1 01:17
이게 이제 고정이 되면 그렇게 되는 거지 어쨌든 알아들으면 알아들을 수 있는 것 같아요.
계속할게요. 그냥 여러분 미안하지만 그래가지고 어

참석자 1 01:35
지금 교과서 어디에 있냐면 우리가 일단

참석자 1 01:45
어쨌든 그 RNN으로 하는 거 했었고 스티커스로 해서 그다음에 이제 어텐션이라는 거를 가르쳐줘서 어텐션 어텐션이라는 걸 해서 셀 포텐션이라는 것도 했고 교과서에 이렇게 나오는데 제가 강의 자료 준 것도 있잖아요.
여러분 그렇죠 제가 강의 자료 이거 QKV 그쵸 QKV 그쵸 교과서랑 매칭시킬게요.
여러분 여기서 이게 여러분 요게 이게 이제 문장 하나인데 뭐냐면

참석자 1 02:40
449쪽이에요. 449쪽 449쪽인데 이게 문장 하나고 그쵸 문장 하나예요.
그쵸 그리고 여기 지금 이 문장 하나가 사실 이제 시퀀스 데이터를 표현하면은 토큰의 벡터들이지 그쵸 토큰에 하나하나가 토큰이야 그쵸 뮤니 그랜드 그쵸 니네 그리고 이게 이제 만약에 창원이 64라고 그러면은 94개짜리 하나 64개짜리 하나 이런 게 있는 거죠.
그쵸 인베딩 차원이 그리고 이 단어가 지금 하나 둘 셋 넷 다 7개니까 7개가 있는 거지 그쵸 시퀀스 그쵸 그리고 이렇게 이렇게 연산하는 게 이제 이게 q랑 케이랑 슈얼리랑 키로 전부 다 똑같은 거 써서 셀프 어텐션 하고 있는 거죠.

참석자 1 03:22
그쵸 그래서 여기 나온 게 어텐션 스코어죠 그쵸 유사도죠 유사도 각 단어들이 자기들 주위에 있는 거랑 유사도 점수 계산해서 여기 예를 들어서 여기서는 근데 이제 집중하라고 여러분한테 스테이션 하나만 봤을 때 이렇게 유사도가 다른 나라가 이렇게 나온다는 걸 보여준 거지 그쵸 그래가지고 이거를 제가 확률 모포로 만든다고 그랬잖아요.
확률 모포 그래서 소프트맥스 먼저 이제 스케일링 하고 소프트맥스트 하는 거지 그쵸 스케일 조정 소프트맥스도 좋겠지만 사실은 먼저 스케일 조정하고 루트 dk로 나누는 거죠.
그쵸 디맨전으로 그쵸 그쵸 여러분 64에 루트 씌우면 얼마예요?
8이죠 그쵸 8로 나누는 거죠. 예를 들어서 그렇죠 김 회장도 탈이었어요.
임베딩이 그런 다음에 소프트맥스 시켜 성용품 만들고 그쵸 그거를 주위에 있는 단어들인데다가 다 웨이트 곱해가지고 다 더해버려서 자기가 다시 스테이션이 칸테스트롤 인베딩이 되는 거예요.

참석자 1 04:17
그쵸 스테이지뿐만 아니라 모든 단어 다 그러는 거예요.
그쵸 그렇죠 그런 거였어요. qk 그게 여러분 셀 포텐션의 핵심이고 그렇죠 아세요?
알겠죠 그리고 그래서 제가 설명 다 했고 이거를 교과서에 이제 450쪽에 실제로 스토 텐스 플로우에는 이런 멀티 어텐션이라는 이런 API가 있고 이렇게 3개의 입력이 있는데 큐어리 밸류라고 그랬죠 여러분 이거 큐어리 밸류라고 해서 여기는 생략할 수 있다.
그 키는 키는 생략하면 뭐라 그랬어요? 여러분 밸류랑 같다고 기본적으로 QKV에서 사실은 세 가지 다 쓰지만 유사도 검사할 때 얼이랑 밸류 큐얼이랑 키랑 매칭하는 거잖아요.
키가 없으면 어떻게 한다고 밸류를 쓰는 거야 그냥 그쵸 밸류랑 키랑 카테고리 감상 알겠죠 생략 얘를 많이 해.
세 번째 거 알겠죠 세 번째 생략되는 거는 키라는 거 외워요.
클로즈드 북 알겠죠 유명한 중요한 거야. 키는 생략이 될 수 있어요.
생략되면 뭐다 밸류라는 거죠 알겠죠 알겠죠 교과서에도 다 나와 있어요.

참석자 1 05:29
그리고 멀티 에드 어텐션 돼 있는 거는 이제 사실에 저도 했지만 사실은 원래 어텐션 그냥 쓰는 게 아니라 이렇게 그냥 쓰는 게 아니고 여기다 리니퍼레이션까지 해가지고 써요.
여러 개 만들어 가지고 여러 개 왜냐면은 정말 뜻이 여러 가지일 수가 있으니까 한 단어가 어떤 건지 모르니까 여러 가지로 다 생각해 보는 거야.
그리고 그게 주주주에서 뭐가 제일 맞는지 보는 거지.
그쵸 배란 단어가 들어왔으면 이게 무슨 배인지 먹는 배인지 먹는 배인지 이 배지 타는 배인지 생각을 해보면서 다 여러 가지 만든 다음에 그렇죠 그런 거지.
그래서 멀티 헤드도 기본적으로 멀티 헤드는 디폴트가 저렇게 돼 있고 그냥 오케를 쓰는 일은 없다는 거지.

참석자 1 06:11
그리고 여기 지금 교과서에 이제 수식은 안 나오고 이렇게 결론 나와 있는데 이게 이게 헤어지고 여기 유사도 그렇죠 이렇게 그다음에 인풋이랑 원래 다시 여기서 얘가 이제 q키겠지 QKV겠지 그쵸 당연히 이제 넣어 알겠지 그쵸 q키 v 그리고 지난 시간에 이것도 설명했었고 사실 여기 값이 이게 진짜 원래 이러면 값의 키가 녹아들어 있다 이런 거 이 안에 값의 바이 여기 다 붙어 있다고 생각하시면 돼요.
키가 생각할 수 있는 거고 그다음에 멀티 인더텐션이 이렇게 커져 나오는데 설명했지만 여기 댄스가 리니어 프로젝션이라고 그랬죠 댄스 그렇죠 여기 댄스 이렇게 되고 실제로 그래서 멀티 디텐션 층은 이렇게 454쪽에 이렇게 되어 있는데 멀티 데텐츠 여기 있으면 이거를 또 굳이 여기 다시 또 댄스 프로젝션 해주고 중간에 댄스 네트워크 MLP를 좀 더 섞어주고 LGD 이제까지 배웠던 게 다 들어가요.
거의 CNA 빼고 CNA 병 다 들어가는데 이게 만약에 비전 쪽에 쓰기 시작하잖아요.

참석자 1 07:26
지금 우리가 트랜스포머를 어디 쓰고 있냐면 이게 지금 언어 쪽에 쓰고 있잖아요.
기존 쪽에 쓰면은 다시 거기 또 CN을 써요. 그냥 왜냐하면 2차원이니까 우리는 그렇죠 2차원 안에서 사람 나와 있는 거는 데스보다는 CNN이 낫잖아요.
그렇죠 그렇죠 그렇게 이해하시면 될 것 같고 그리고 그래서 어쨌든 지금 여기 지금 우리 트랜스포머 인코더가 350t쪽에 다시 나오는데 인코더 이게 제가 지난 시간에 이미 보여줬잖아요.
트랜스폼 마케이처는 인코더 디코로 두 가지로 이루어져 있다 그랬죠 그쵸 그쵸 인코더가 여기 인코더만 나와요.
이 챕터에서는 이 챕터는 인코더만 나오는데 이거 지금 계속 여러분 사실 이게 너무 얘기가 막 밑으로 내려가서 정신없었는데 여기서 지금 계속하고 있던 거는 뭐였냐면은 그게 imdb 데이터베이스에서 센티멘탈라스 하는 거였죠.

참석자 1 08:23
positive negative 그쵸 그거 하는데 백업 워즈를 계속 하고 있었잖아요.
단어 원어로 정하고 그리고 이걸 RNN으로도 한번 해보고 그렇죠.
그 트랜스포머 쪽에서 만약에 그렇게 뭔가 이게 긍정 부정인지 판단하려고 그러면 디코드는 필요 없고 인코드만 필요하면 인코드만 있으면 되거든요.
셀프 포텐션만 하면 돼 그거 갖고 이제 나중에 판별하면 되는 거지 인코더밖에 없어요.
알겠죠 인코더만 써가지고 인코더가 생긴 모양은 진짜 아까 했던 아까 얘기했던 요 모양 요 모양 이것만 가지고 해가지고 이거 하고 있었죠.
그걸 코딩을 해가지고 하는 거예요. 그래서 보면은 여기 여러분이 지금 여기 트랜스 포브 인코더라는 거를 저는 사실 이제 7장에서 이런 강의를 하지 않고 넘어갔는데 레이어즈 점 레이어 해가지고 그냥 상속을 해서 여기 콜이라는 거 있죠 콜 클래스가 있잖아요.
여러분 클래스 클래스 있죠? 클래스 파이썬이에요.
파이썬 파이썬에서 클래스 만든 다음에 여기 이렇게 콜을 부르잖아요.

참석자 1 09:27
여러분 콜 콜 있죠 콜을 정의해 그러면은 이 클래스트를 나중에 클래스를

참석자 1 09:41
만들어요. 예를 들어서 여기 트랜스포머 인코더 보이죠 여러분 요거 이게 몇 쪽이냐면은 11 20인데 좀 길긴 하는데 11시 22가 457쪽 457쪽 여기 지금 여기 트랜스포머 인코드 보이죠.
여러분 아까 만들었던 클래스 보이 트랜스포머 인코드 게 클래스 아까 봤던 거 걔를 그냥 여기 지금 이렇게 해서 지금 생성을 했죠.
그렇죠 생성을 할 때는 뭐라 부르냐면 이미지 불리는 거 알죠?
여러분 파이썬에서 원래 생성할 때는 인지 불리지 지금 몰랐으면 알았어요.
알겠어요 여러분 파이썬 강의야 파이썬 강의 생성을 할 때 이거 생성하는 거죠.
대문자 그쵸 나중에 이제 이게 실제로 인스턴스화가 되잖아요.
지금 x로 그쵸 여기 들어갔죠 그쵸 여기 이게 생성이 됐을 때 들어갔잖아 x로 그쵸 레이어 이쪽이 된 거잖아요.

참석자 1 10:42
지금 이쪽에 그쵸 그렇게 됐을 때 들어갔으면은 실제로 나중에 이제 생성이 된 게 분리 때는 앞에 콜 했던 거 있죠 콜 여기서 생성 여기서 여기 트랜스포밍 코드 여기 했었잖아요.
여기 11 21에서 이게 분리해야 돼. 이게 콜이 분리가 돼요.
객체가 있을 때는 이게 분리가 돼요. 몰라요. 지금 알았어요.
여러분 알겠죠? 이거를 파이썬 강의지만 여러분 좀 생소하죠 그쵸 이거 사실 파이토치 하면 맨날 이러고 있었어요.
파이토치는 내가 파이토치랑 원래 댄스 플로우랑 일대일 매핑하는 거 슬라이드 올려놓는데 강의 못 했죠 시험에 안 나오는데 못 내죠.
제가 강의 못 했으니까. 근데 실제로 여러분 실무할 때 거의 파이트 쳤을걸 오픈 소스가 되게 많아가지고 맨날 그러고 살 거예요.
아마. 근데 알고 있으라고 알겠죠 알겠어 콜 하면은 이탈 클래스 맨 처음 생성할 때 객체 생성할 때는 이닛이 불리고 실제로 불릴 때 콜이 불려요.
알겠어요 모르면 곤란하지 파이토치 같은 거 하나도 못 쓰지 그랬어요.

참석자 1 11:53
여러분 질문 많이 하세요. 모르는 거 있으면 채찍 피티할 때 물어봐도 되지만 명세치 PT도 잘해.
혹시 모르는 거 있으면 물어보세요. 질문 공부하면 해 물어볼 거 있어요.
그러면은 홀 정의 안 하면 정의 안 하면 220 아니요 아니요 안 돌아가지 안 돌아가 디폴트가 있어요.
원래는 디폴트가 그냥 레이어 거 그러니까 원래 근데 우리가 의도하는 대로 해줘야 되니까 실제로 우리가 짜주는 게 진짜 짜주는 건 여기 있거든요.
콜 정리 안 하면 디폴트가 물론 이상하게 되겠죠 그쵸 홀로 실제로 뭘 할지 다 정해주기 때문에 콜이 있어야 돼.
무조건 알겠죠 여러분 레이어를 뭔가 만드는 이유는 초기 화면에서 뭐 하나 우리가 뭐 할지 계속 정해줘야 되는데 콜을 해줘야 돼.
또 불러줘야 돼. 디파인 해줘야 돼. 여러분들 파이토치는 맨날 콜 써가지고 하고 있어요.

참석자 1 12:46
파이토치 그거를 또 강의를 했으면 좋았는데 시험 보면 그거 알아서 할 것 같아요.
여러분이 4학년인데 알아서 하세요. 나중에 파이토치 워낙 맛있으니까.
그리고 지금 나도 지금 뭘 강조하고 넘어갈지 고민하고 있는데 여기서 나머지는 따라 하면 될 것 같고 중요한 거 이제 여기 위치 여기 트랜스포머에서 459쪽 볼게요.
459쪽에 459쪽에 위치 인코딩이라는 게 메뉴에 적혀 있죠 그쵸 359쪽에 위치 인코딩 그러니까 이게 시퀀셜 데이터를 시퀀셜 우리 RNN만 정보를 여러 번 루프를 돌려서 하고요.
대부분 다 그냥 한꺼번에 줘요. 근데 위치 정보를 주긴 줘야 될 거 아니에요 그쵸 그래서 위치 정보를 주기 위해서 아예 그냥 인코딩하는 거예요.
원래 워드 우리 카테스트로 인코딩하고 있잖아요.
지금 그쵸 컨셉트 인베딩 바꾸고 있죠 여기 아예 그래서 컨텐츠 인베딩 어떻게 하고 있어요?

참석자 1 13:42
맨날 더해가지고 하고 있잖아 다른 거랑 웨이 웨이트 줘가지고 그쵸 다른 단어들이랑 섞어서 주고 있잖아요.
위치 정보까지도 더해서 주기로 했어요. 더 진짜 더해 1 2 3 4 5 6 28 순서 있죠 그걸 더 해요.
그래서 보면은 교과서 459쪽에 11 24 보면은 여기 포디셔널 인베딩이라는 거 있죠 여기 보이죠.
여기 여기 이 탈 때 그렇죠 보이죠. 여기 여기에 이런 거를 아예 정의를 해놓고 여기 보면은 콜 할 때 포지션즈라고 돼 있죠.
롤포지션즈 콜할 때 보이죠. 만드는데 그냥 레인지로 만들어 레인지로 만들면 어떻게 돼요?
여러분 여기서부터 시작해서 랭스까지 그 원래 단어 길이만큼 그냥 숫자 1 2 3 4 5 0 1 2 3, 4 5로 나오는 거야.
그냥 숫자를 덜렁 더해요. 정말로 더해가지고 토큰들에다가 넣어줘요.
진짜로 그냥 모든 벡터들에다가 답을 자기 위치만큼 숫자를 더해준다고 여러분 되게 무식하죠.

참석자 1 14:46
그쵸 근데 이게 실제로 이렇게 여기서 이렇게 했는데 잘 돌아가 진짜 잘 돌아가 이게 원래 진짜 트랜스포머에서는 이거를 코사인 사인 값으로 숫자 숫자로 해줬거든요.
코사인 사인으로 하면 여러분 우리 우리 맨날 공업수석에서 하던 프리 트랜스폼 같은 거 있잖아요.
거기로 연속적 값으로 만들 수 있으니까 그걸로 해줬거든요.
원래 신호철에서 하는 거 신호철에서 하던 식으로 해줘요.
왜냐면 그 소식 너무 숫자 안 크게 만들려고 여기 진짜 무식하게 그냥 하고 0부터 그냥 자연수 더해줬는데 이렇게 잘 돌아요.
그래서 사실 아이 이게 그래서 이 교과서가 훌륭한데 정말 실체는 그냥 정말 연속적으로 분명히 증가하는 값을 준 거예요.
그냥 미치는 거다 그러면 된다는 거지 실제로 여러분 할 때는 코사인 사인 갖고 예쁘게 만들어 갖고 쓰면 되고 그게 다 그 수확이잖아요.
그쵸 그 정도로 하고 넘어갈게요. 여러분 알겠죠?
그래서 시퀀스 정보를 줄 수 있다.

참석자 1 15:45
굳이 루프로 안 돌리고도 루프로 돌리면 자꾸 까먹잖아요.
근데 계산하면 왜 이렇게 주면서 그렇죠 LST에 그래가지고 지금 인코더로 텍스트 분류하는 거 텍스트 분류하는 거 누가 긍정 분형 분류하는 거 340쪽에서 그렇게 해서 할 수 있다는 거고 그리고 이제 461쪽에 중요한 게 이게 뭐냐면 12.4.4과 제목이 bow 모델 대신 언드 시퀀스 모델을 사용하나요?
보면은 bow 모델하고 시퀀스 모델이랑 지금 두 가지를 다 센티미터널 시사 할 때는 지금까지 여러 가지 있었는데 내가 다시 강조 별로 안 했는데 백오브 월드 중에서 바이그램 있죠 걔가 제일 정확도가 높았어요.
제일 잘 분류했어요. 긍정 부정을 두 단어씩 묶어서 하는 거 이해돼요 여러분 다 그래 그래가지고 교과서에서 뭐라고 얘기했냐면은 이 사람 논문인데 이 저자 논문이에요.

참석자 1 16:43
보면은 462쪽에 이런 식으로 샘플의 개수가 많으면 많을수록 시퀀스 모델이 좋고 샘플 길이가 길면 길수록 바이그램이 좀 더 좋은 경향이 있어서 이게 지금 역수로 해가지고 지금 이거 특정 개수가 크면 시퀀스 모델로 1500 넘어 가고 이런 게 있거든요.
근데 이거 나는 모르겠어. 이거 진짜로 그런가 싶어서 그냥 이런 짓도 합니다.
사람들이 알고 있어요. 알겠죠 작년에 제가 이거 시험을 했거든요.
제일 중요한 것 같아서 근데 진짜 환상 그런 것도 아닌 것 같아서 이 사람 논문인데 아무것도 쌓이지도 않았는데 그래서 그냥 안 할래요 알겠죠?
일단 알고 있어요. 이런 연구들이 있다는 거예요.
알겠죠 그러니까 백업 원지가 여러분 되게 심플하거든요.
알겠죠 그러니까 항상 여러분 진짜 여러분 선배들 졸업 논문 쓸 때 추천하는 거 할 때 보통 tfid 진짜 많이 써요.

참석자 1 17:42
우리나라 국내 논문도 거의 TF IDF 많이 하고 워낙 빠르니까.
근데 내가 보기에는 트랜스포머 하는 게 더 나은 것 같아 알아서 하세요.
알겠죠 여러 가지 해보고 실제로 여러분 논문에서 트랜스포머보다 bow가 더 좋다 이런 거 논문 쓸 수도 있겠지 졸업 논문에서도 심지어 알겠죠.
근데 이거는 이거는 뭐 하냐면은 샘플 개수가 많으면 많을수록 어쨌든 시퀀스 모델이 더 잘 된다는 거지 샘플 데이터가 많은 거지.
그쵸 그리고 바이그램으로 봐도 여러분 샘플의 길이가 커 그래서 보통 잘라버리잖아.
그렇죠 다 쓰는 것도 아니고 단어를 그래서 잘 이렇게 애매해 정말 애매하다고 넘어갈게요.
여러분 그다음에 그럼 125가 드디어 시퀀스 학습을 하는 걸로 시퀀스가 들어가 이용로 들어가서 지금까지는 계속 지금 센티오턴을 쓰든 시퀀스가 들어가서 시퀀스가 아니라 그냥 positive negative 이거밖에 안 나왔죠 LSTM이나 트랜스포머 인코더도 그쵸 여기는 시퀀스가 들어가서 시퀀스까지 나오는 거예요.

참석자 1 18:42
CTPT 같은 거 여기는 근데 이 11.5절에서는 그렇게 안 하고 그냥 원래 번역하는 거 있죠 트랜스포머가 진정 원래 쓰였던 거 그걸로 돼 있어요.
제가 원래 강의했던 내용인데 이거 봅시다. 여러분 요거 요거 교과서 44 15 24쪽에 이거 기본적으로 여기도 여기서 처음 나오지만 사실은 이게 뒤에 나오는 생성형 AI 쪽 있죠 뭔가 생성을 하려고 그러면 다 이렇게 인코더 디코더로 이루어지는 경우도 있어요.
인코더로 해서 뭐 하느냐 뭔가 내부적으로 뭔가 축상화를 잘하려고 그러는 거야.
우리 법원에서 사실 이미 여러 번 강의했잖아요. 인코더에서 뭐 셀 포텐션 써가지고 칸테스트 인베딩 예쁘게 만들어내서 그쵸 뭔가 내부 표면 만들어내서 지금 아까 그 센티미터널시트에서는 그냥 거기서 바로 판단해 버렸죠.
데스트 붙여가지고 했을 거예요.

참석자 1 19:37
여기서 이제 여기 이 시퀀스에서는 번역할 때도 번역할 때 이렇게 먼저 원래 영어를 지금 여기 지금 스페인어로 번역하는데 스페인어 못 알아보겠잖아요.
우리 그렇죠 우리나라 말로 봅시다. 여러분 하우스도 웨럴 투데이 이랬죠 오늘 날씨가 어때 이거죠?
그쵸 파우지트 웨더 투데이를 먼저 이제 전부 인코딩해서 뭔가 이제 이거가 투데이랑 웨더 이거 전부 다 날씨에 대한 거네.
그렇죠 날씨 예보네 이런 거에 대한 인베딩을 하고 내부 표현을 만들어낸 거지.
인코더가 이 내부 표현을 줘가지고 디코더에서 뭐 하느냐 여기도 다시 또 셀 포텐션이랑 다시 또 소스 타겟 어텐션 쌓아가지고 소스 타겟 어텐션에서는 큐어리 밸류는 여전히 큐어리 밸류랑 키랑 다르다 그랬죠 큐리 큐어리랑 키 밸류가 다른 거죠.
그쵸 큐어리만 달라 어리 키밸류는 키 밸류는 똑같고 그쵸 포스타에서 그렇죠 그래가지고 여기서 뭐 하냐 인코더에서 정보 온 거 넘어온 거를 가지고 여기서 여기서 이제 단어가 하나씩 다 튀어나오는 거야.

참석자 1 20:48
그러니까 오늘 내일 오늘이네 이게 아니라 오늘 날씨가 어때요?
이렇게 나오는 거 그쵸 근데 여기 처음에 어떻게 스타트를 주면 오늘이 튀어나오고 네 여기 맨 처음에 스타트부터 시작해서 스타트 주면 오늘이 튀어나오게 하고 뭐 이런 식으로 횡단을 했었죠.
제가 좀 그렇죠 그다음에 오늘이 다시 들어가면 여기 오늘 날씨가 튀어나오게 하고 이런 식으로 디코드 하나씩 하나씩 이제까지 말한 걸 다시 넣어줘서 그런 식으로 하는 건데 훈련에서는 어쨌든 훈련에서는 원래 여기는 훈련이고 출련 추론 두 가지 있죠 여러분 그렇죠 교과서에 지금 훈련 추론 따로따로 써놨잖아요.
훈련에서는 어쨌든 원래 정답값 줘야 되겠죠 그쵸 정답값 여기 추론에서는 수원에서는 어쨌든 수론에서도 스타트 누에는 오늘 다 튀어나오게 그렇죠 날씨 오늘 날씨가 어때?
이렇게 보통 말하는 게 좋잖아요.

참석자 1 21:47
날씨가 오늘 어때도 상관없는데 훈련을 뭐 상관없지 여기 날씨가 튀어나올 수도 있고 오늘이 튀어나올 수도 있고 그렇죠 때에 따라서 어쨌든 여기 날씨가 튀어나왔으면은 오늘은 뒤에 나와야 될 거 아니에요 그쵸 그런 식으로 원래 이제 튀어나올 경우에 여기서 시퀀스가 나오는 거야.
시퀀스를 하나씩 하나씩 튀어나온다고 하나 절대로 한꺼번에 쫙 나오지 않고 여러분 CTP t어보면 알잖아 그렇지 하나씩 하나씩 튀어나오잖아 말이 쭉쭉 나오지 그쵸 쭉쭉쭉 나오잖아요 짝꿍 한꺼번에 착 안 나오지 이해돼요 여러분 그냥 그래서 여기 번역할 여기 코딩이 열심히 있는데 이거 시간 없으면 생략하고 11 5 115.2로 가면 11.5.2 여기 469쪽에 RNN으로도 이렇게 잉크로 디코드 해가지고 할 수도 있겠지.
당연히 그렇죠 RNN으로도 그렇죠 RNN으로도 원래 이렇게 트랜스포머 나오기 전에 다 이러고 살았어요.
사람들이 그리고 잘 안 되네 이러고 있었어요.

참석자 1 22:54
그래서 여기도 인코더 인코딩하고 디코더 해서 하우이스더 웨더 투데이 갖고 열심히 돌려 이제 여기서 rn 해서 여러 번 돌아서 다 시퀀스 처리해서 여기 그냥 전체 상태가 마지막 거라든지 아니면 중간에 나온 거 다 내부적인 걸 넘겨줘서 디코드에서 또 하나씩 하나씩 스타트부터 시작해가지고 오늘 날씨가 어때 이런 식으로 나오게 한다고요.
여기는 또 열러고 돌고 여기도 루프 돌고 여기도 루프 돌고 이런 식으로 해 알겠어요 여기 어쨌든 인코드에서 네오 편을 만들고 이런 식으로 했어요.
그렇게 했고 그리고 여기 지금 교과서에 나와 있는데 이거는 외웁시다.
470쪽에 470쪽에 번역 시스템은 blew 점수라는 거 쓴다고 적혀 있죠.
여기 보면 어디 있냐면은 470쪽에 맨 마지막에서 두 번째 단락에 세 번째 줄에 vaew 접수 보이죠.
다운 표 놓고 BLW 저스터 역주에 적혀 있죠. 역주에 뭐예요?
blew 바이링규얼 이밸류에이션 언더스터디라고 언더스터디라고 돼 있어 보이죠.

참석자 1 24:04
이게 번역할 때는 이 스코어를 많이 써요. 그냥 알고 있어요.
알겠죠 그리고 우리가 외대이기 때문에 외대 교수님들 뭔가 약간 발표하는 거 있으면 이정수 씨 맨날 나오는 면이 있어서 그냥 우리 외대니까 합시다.
알겠죠? 알겠죠? 외워 시험에 낼게요. badw 왜냐하면 모르면 좀 이상해 별로 안 좋다고 맨날 공격하는 애들 사실 이거 이게 정량 지표로 확실한데 이게 잘 안 맞아요.
BLW bleu 점수가 높은 게 10대 잘했다고 안 느껴 사람들이 그렇지만 정량적으로는 이게 제일 확실하니까 그러니까 이게 뭐냐면 원래 정답 값이랑 얼마나 맞는지 그냥 두 개를 비교해서 점수 매기는 거라서 이 글씨가 근데 날씨가 오늘 어때 오늘 날씨가 어때 상관없잖아 그쵸 근데 이거를 잘못 그냥 거의 1대 1 매핑해가지고 접수가 되기 때문에 잘 안 돼요.
알겠죠? 그래요. 있다는 거 알겠어요 여러분 번역 쪽에서 이거 쓴다는 거 알겠죠?

참석자 1 24:58
모르면은 외대에서 좀 좀 그러니까 알겠죠 우리 학교 특수성으로 여러분 우리 학교 사랑하는 방법 외워요.
알겠죠? 이거 별로 안 좋다. 이것도 알고 있고 실제로 그 사람한테 물어보는 게 제일 나아 알겠죠?
그래요. 번역 품질에서 저게 잘 안 맞아 어쨌든 그거에 대한 코드가 쭉 있고요.

참석자 1 25:17
1 그리고 이제 472쪽으로 넘어가면은 트랜스포머 써가지고 하는 게 있어 트랜스포머 써가지고 하는 게 있는데 이거는 875쪽으로 가면 전에 제가 875쪽 이거 15쪽에 이거 여러분 교과서 있죠 그렇죠 여러분 요거 이렇게 하는 게 왼쪽에 인코더 인코더 적혀 있죠 여러분 인코더 인코더 적혀 있죠 그쵸 오른쪽이 디코더 적혀 있죠 디코더 그쵸 인코더가 밑에 여기 있는 얼티 어데텐션이 얘 셀 퍼텐션 그쵸 그쵸 이쪽이 도 이것도 세일 포텐션 여기는 소스 타겟 어텐션이라고 그랬죠 그쵸 그쵸 그리고 여기 재밌는 거 여러분 여기 소스는 여기 여기 들어오는 거 뭔지 여러분 정확히 알지 소스로 들어오는 거 뭐다?
영어에서 만약에 한국어로 번역하는 거라면 여기가 영어예요.
알겠죠? 하우스 투데이 이런 거야 알겠어요? 타겟은 뭐겠어요?
여러분 타깃은 훈련할 때는 우리나라 말이야 알겠죠?

참석자 1 26:20
훈련할 때 영어 출어할 때는 우리나라 말인데 이게 소스가 타깃이 이게 뭐야 이게 스타트하고 그다음에 뭐야 오늘 날씨가 어때 이런 것들이에요 맨 처음에 스타트 그다음에 오늘 그다음에 날씨 이런 식으로 가 어때 이런 게 들어간다고요 타깃으로 알겠어요 그리고 이 타겟은 뭐냐 스타트가 만약 스타트 연초에 넣어준 다음에 오늘은 어디서 나온 거냐면 여기서 튀어나왔던 거 여기서 튀어나오는 거 있죠?
오늘이 튀어나오면 그다음 여기 시퀀스로 다시 오늘이 들어가서 스타트 오늘이 들어간 상태에서 여기 날씨가 날씨가 튀어나온다고 날씨 그러면은 다시 또 스타트 오늘 날씨가 들어가면 또다시 가가 튀어나온다고 여기서 이해돼요.
여러분 내가 뭐 하라는지 알겠죠? 그게 코드로 다 있어요.
교과서 알겠죠 그래서 그렇게 해서 주는 거야. 알겠어요 이해되지 그렇게 해서 하나씩 튀어나와요.
그리고 중요한 건 뭐냐 여기서는 셀 포텐션으로 전부 똑같은 게 영어가 들어가고 그쵸 여러분 이해하세요.

참석자 1 27:24
여러분 여기서는 여기 전부 한국어 들어가 셀프 텐션으로 그쵸 내가 만들어낸 문장 내가 어디까지 말했는지 그게 내가 말한 거 오늘 날씨까지 와 있으면 오늘 날씨까지가 들어가는 거잖아요.
전부 똑같이 그쵸. 알겠죠 여기 들어가는 거는 입력으로 여기 지금 펜 펜 펜 여기 있지 여기 이게 순서가 여기 들어가는 게 q k v가 들어가는 거 여러분 알고 있죠?
그쵸? 그렇게 입이 들어가죠. 그쵸

참석자 1 28:05
그래요. 그런데 중요한 게 뭐냐 여기 여기 이거 지금 잘 안 보이는데 교과서에서 여기 키랑 밸류랑 q가 있잖아요.
예. 큐리 소스 타이 어텐션이라고 그랬잖아요. 제가 그쵸 여기 큐어리가 여기서 지금 아까 셀 포텐션 해가지고 넘어온 거 있지 얘만 큐어리가 들어가요.
여기만 화살표 잘 보이죠. 튜어리 여기서 튜어리 한 거 있죠 여기서 셀 포텐셜 한 거 내가 오늘 날씨까지 얘기한 거 있잖아요.
셀 포텐션 한 거 있죠? 걔가 일로 들어가 QR 이해돼요.
여러분 그리고 t 밸류는 t 밸류는 뭐냐 여기서 나온 거 여기 여기 여기 영어라고 영어 시벨리는 영어야 영어를 인코딩하는 놈이에요.
알겠어요 걔는 계속 안 바뀌겠지 영어 인코딩 한 거 계속 보면서 영어 영어 오늘 날씨가 하우스 베르 투데이를 계속 셀프 포텐션에서 인코딩한 거를 계속 보면서 슈어리만 계속 셀프 포텐션으로 또 바뀌어 놓은 거 있죠 걔가 들어가는 거야 알겠어요 모르겠으면 질문하세요.

참석자 1 29:12
계속 여기 셀 포텐션 셀프 텐션 여기 이 두 개가 셀프 포텐션 내 옛날 강의 자료 다시 보여줄게요.
어

참석자 1 29:29
남의 거 뺏겨 온 거 여기 이거 여기 여기 있잖아요. 여기 요거 요거 요거 있죠 여기가 셀프 포텐션 여기가 소스 타겟 어제 이 그림 잘 돼 있다.
여기 보면은 이쪽에서 나오는 거 인코더에서 두 개 딱 들어가 있죠.
여기서 딱 들어가죠. 얘가 큐야 q 여 여기가 q야 이거 큐어리가 여기로 들어가고 여기 키 밸류는 이 그림 보이죠 여러분 키밸류 저쪽에서 온 거예요.
영어라고 영어 진짜 내가 키 밸류를 가지고 내가 얼마나 찾아야 돼 그렇죠 쇼 제일 맞는 거 영어 제일 맞는 거 찾는 거야.
그러니까 오늘 날씨 한 상태에서 그쵸 그다음에 저쪽이 키밸류 쪽에서 이제 영어 쪽이죠.
푸는 날씨에서 제일 뭐가 맞는지 찾고 그다음에 뭐가 나와야 될지 생각하는 거예요.
이제 내가 뭘 말해야 될지 이러면서 훈련도 그렇게 시켜놓고 알겠어요.
여러분 일단은 여러분 이거를 시험에 제가 이것이 되게 중요하기 때문에 낼 거야.
알겠죠?

참석자 1 30:29
엄청 중요해 이런 개념이 이거를 코드로 다 다 반영이 돼 있고요.
그게 374쪽에 376쪽에 다 나와 있어요. 376쪽에

참석자 1 30:53
36조 이거 뭐야? 어텐션 아웃풋 여기 있다. 여기 어텐션 어텐션이 지금 굉장히 아웃풋이 하나 첫 번째 인코더 쪽 둘 이거는 디코드 쪽에서 셀프트 하는 거

참석자 1 31:24
이거 이거 이거 뭔지 내가 헷갈린다. 표지 아웃풋 이건 디코더구나 미안해요.
디코더네 디코드니까 아웃풋 원은 저쪽에 인코더 쪽에서 있었고 이거는 이게 새 포텐션이구먼.
그쵸 아웃풋 1이 셀 퍼텐션이고 아웃풋 2가 소스 타겟 어텐션이고 교과서에 소스 타겟 어텐션이라는 말 없어요.
제가 헷갈려서 열심히 조사해 봤더니 역시 있더라고 그래서 아까 제 슬라이드에 있잖아요.
그쵸 포스타게 어텐션 알죠? 여러분 이거 잘 안 나와 그거 그런 개념 알겠어요?
여러분 이게 약간 필요한 것 같아 헷갈리잖아 안 그래요 교과서만 보고서는 진짜 알 수가 없어 잘 그래서 이게 여러분 여기 보면은 여기 밸류랑 q랑 똑같고 큐어리가 다른 거 보이죠 큐어리가 셀 포텐션 나온 거고 밸류랑 키가 저쪽 인코더 쪽이잖아.
알겠죠 보여요. 여러분 여기 지금 376쪽에 어텐션 아웃풋 2 여기 적혀 있는 거 그게 이거 어떻게 위에 잘려 있어?

참석자 1 32:17
이렇게 버튼 2가 퓨어리 밸류 키가 이렇게 돼야 된다는 거 알겠죠 여러분 알겠지 알겠어요 왜 달라 똑같아 476쪽에 여기 있잖아 여기 여기 다 제가 지금 어떻게 자르냐 이게 그러네.
신기하네 새로 나왔어요. 몇 판이야 이거 계정 2판이야 계정 이판 좋겠다 나도 똑같네.
뭐야 왜 어쨌든 뭐 알아서 해서 됐죠. 여기 이 정도면 뭐 설명은 다 했어요.
알겠죠? 예. 디코드 쪽 한 셀프 어텐션이 코드로 앞에 있잖아.
여기 세포 대지 코 앞에 여기 여기 여기 이플 리프 프 똑같은 거 이거 이거 다 우리나라 말 앞에 있잖아요.
그거 바로 이주에 어텐션 아웃풋 원 만들 때 우리 인코더 셀프 텐션 그거는 저쪽에 셀 포텐션에서 여기 이거 디코더 방향이야.
디코더 디코더 포델을 인코더 앞에서 짰으니까 그냥 이렇게 있어요.
인코더 앞에서 짰잖아. 아까 센티먼 터널스 할 때 그래서 그냥 다시 안 나왔어요.
알겠죠? 그 코드 그대로 쓰면 되거든 됐어요. 여러분 됐지 됐어요.

참석자 1 33:29
여러분 됐지 한번 저기 앞에 그림에서 보면 원래 입력 값을 아웃풋 값에다 연결시키려고 그러면 저것도 레디얼 내셔야 되거든.
요거 요거 요거요. 아래 노멀라이제이션 요거 요거 요거 요거 요거 레지디오 컬렉션이에요.
이거 네 맞아요. 맞아요. 정말 그 어쨌든 여러분 이게 또 약간 얘기하고 싶은 게 지금 교과서에서는 지금 인코르 디코더가 하나씩만 있죠.
실제로 코드는 그렇게 했어요. 잘 은근히 돌아가요.
그런데 실제로는 이거 인코더도 되게 여러 번 쌓고 디코르 되게 여러 번 쌓아요.
똑같은 거 또 반복을 여러 번 한다고 한 8개씩 해 알겠죠?
보통 그래 알겠죠 근데 실제로 여러분 세티 피티 할 때 더 많이 하겠지 알겠죠?
됐어요 알고 있어요. 알겠어요 상식적으로 알고 있어요.
상식적으로 여기 여기 보여줄게 내가 슬라이드

참석자 1 34:28
여기 여기 6이네 6이네 6개씩 했네 진짜 여기 진짜 여기는 알겠죠?
6개 6개 6개씩 있었어요. 알겠죠? 진짜 얼레 트랜스포 논문에서는 알겠죠?
그래 이걸 외우는 건 아니지만 뭐 그리고 여러분 제가 지금 시간이 20분 정도밖에 안 남았지만 20분도 안 남았지.
근데 제가 VA를 너무너무 사랑하는데 거기를 못해가지고 근데 약간 상관이 많거든요.
되게 이게 지금 뒤에 거 있잖아요. 생성형 AI 칭성 AI 12장 12장에 잠깐 볼까요?
여기 근데 어텐션도 너무 중요하니까 그냥 이름이 있는데 12장에

참석자 1 35:19
마지막까지 진도 나가는 거는 좀 웃기긴 한데 근데 이 문제는 이게 앞에 걸 모르면 제가 할 수가 없어서 그래요.
말을 할 수가 없잖아요. 많이 안 갖춰놓으면 이걸 얘기할 수가 없어요.
그러니까 이렇게 하는 거를 이해해 주시기 바랍니다.
여러분 여기 보면 교과서에서 이거는 알아야 되는 용어거든요.
뭐가 있냐면요. 여러분한테 얘기 기억이 하는 게 안 하는데 아까 사실 코드로 보면 나오는데 맨 마지막에 디코더 있죠 디코더 디코더에서 맨 마지막 계층이 댄스에 결국은 항상 맨 마지막에 뭐 하는 거예요?
오늘 날씨가 어때서 오늘을 얘기할지 날씨를 얘기할지 가가 얘기할지 온갖 독해블러이 있는 것 중에서 못 뽑아낼지를 확률적으로 하는 거였어요.
이해돼요. 여러분 그거를 얘기 안 해줬지 내가 별로 RNN도 그렇고 시퀀스도 그렇고 원래 맨 마지막에 있잖아요.
거기도 시그오이드로 그냥 시그모이다 소프트맥스로 원래 단어가 있잖아 케블러리 그쵸?
보여주는 게 낫나 보여 양식이 다 보여줄게요.

참석자 1 36:23
1 1이래 원래 RNN도 그랬고 RNN에서 코드가

참석자 1 36:33
채권 시켜서 보든지

참석자 1 36:45
실제로 이제 시컴 시크 디코더 디코드 할 때 여기 보면은 프로젝트 하잖아요.
프로젝트 트리트 이게 어디냐면 코드가 11시 31이에요.
똑같아요. 보통은 11시 31 한번 볼래요? 어디 몇 쪽이지?
11시 31 41 471쪽 패트 11 31 여기 보면은 디코드 세텐스 시퀀스라는 게 있는데 여기도 똑같이 문장에 이제 하나 여기도 이제 하나씩 하나씩 튀어나오면서 시퀀스 하는 거니까 여기도 시퀀스를 하게 도는 건데 여기도 아까 걔도 여러 번 도는 거고 얘도 여러 번 도는 거예요.
그쵸 근데 어쨌든 문장을 원래 만들어 만들어낸 거에다가 이렇게 초곡차곡차곡 넣었는데 여기 보면 프린트를 하고 나면 아까 디코로도 프린트를 하고 나는 거잖아요.
프린트하고 나면 마지막에 이제 프리딕션 된 거가 나오는데 거기서 MP 점 erg레스트 하고 있잖아요.
이게 뭐 하는 거 있는 거냐면 여러분 모든 단어들이 만약에 2만 단어를 쓰면 2만 단어에 확률이 쭉 나오는 거야.

참석자 1 37:56
진짜 무식하게 여러분 만약에 단어가 10만 개의 단어가 우리가 쓰고 있어.
그럼 10만 개의 단어 중에 각각의 확률이 나오고 그중에서 뭘 쓸 거냐를 결정하는 거라고 그래서 MPC AIs 베스트로 해서 실제로 그 인덱스 찾아가지고 단어를 그냥 연결하는 거라고 뭔 말인지 이해돼요.
여러분 옛날에 우리 옛날에 숫자 1 리스트 할 때 0부터 9까지 중에서 확률이 나온 거였잖아요.
0인지 1인지 AIs 맥스에서 이게 산이구나 하면서 튀어나오고 이랬잖아요.
그쵸 마찬가지로 여기는 100만 개의 단어가 있으면 100만 개 단어의 확률이 자꾸 나온다고 미 미쳤죠.
여러분 예 그렇게 하고 있어요. 이거는 다 알겠어요.
그러니까 채찍 피티 같은 경우 얼마나 힘들겠어요 그렇죠 그렇게 하는 거예요.
진짜 그걸 알겠죠. 그리고 그렇게 하는 거라서 확률 분포가 나오는 거야.
마지막에 결국은 소프트 메스 알겠죠. 그리고 연결해서 또 하고 또 하고 이러는 거예요.

참석자 1 38:47
알겠지만 마지막은 항상 그래서 지금 뭘 보여주고 싶냐면은 12장에서 12장에서 생성 모델이 이제 이제 핵심이 이제 여러분이 많이 알기 때문에 우리가 얘기할 수가 있는데 시퀀스를 생성을 어떻게 하느냐

참석자 1 39:11
지금 교과서 445쪽 봐봐요. 여기 보면 언어 모델 이 언어 모델 영어로 뭐예요?
여러분 LM이죠 LM 구글 LM도 있잖아요. 그쵸 구글 에 485쪽 그림이에요.
485쪽 그림 그림 12 1이에요. 언어 모델이 나오잖아 그쵸 더켓 온더 이렇게 하면 이제 나중에 이제 매트가 나오고 이러는 건데 이게 다음 문자의 확률 분포가 이렇게 나오는 게 이게 단어 모든 단어에 대해서도 확률 분포가 나오는 거예요.
다 더하면 6 대 1 알겠죠 소프트레스로 이해되죠.
그래서 제일 높은 거 뽑아내는 걸 수도 있잖아요. 그쵸 제일 높은 거 뽑아내는 걸로 했지 앞에서는 근데 그러면 너무 단어가 단조롭거든요.
그래서 매트 말고도 그러니까 이게 상류 부표가 이게 이렇게 생길 수 있잖아요.
이렇게 여기 단어들이 있는데 여기는 매트가 있지만 여기는 벤치도 있고 여기는 핸드도 있고 막 있다고 손해일 수도 있겠잖아요.

참석자 1 40:06
재미있게 하려고 그러면은 맨날 매트에만 앉아 있으면 지겹잖아.
똑같아 똑같은 말만 해. 약간 확률 공포로 다른 낮은 것들 있잖아요.
하면 말이 재밌잖아요. 근데 머리에 앉아 있다면 재미있잖아요.
어깨 숄드에 앉아 있다면 재밌잖아요. 그럴 수 있잖아.
그렇죠 그렇게 확률 포를 정형적인 거 말고 벗어나게 하기 위해서 샘플링을 하는 거예요.
샘플링을 무조건 제일 높은 걸로 하는 게 아니라 이해돼요.
여러분 말은 좀 약간 수면 약간 더 너무 낮은 거 말도 안 되게 나오겠죠.
더 킷 온더 런닝 런닝 이러면 말이 안 되잖아요. 그쵸?
동영 제일 낮은 거 하면 안 되고 적당히 낮은 걸로 하면 재밌는데 그래서 1 그다음 페이지 다음 페이지가 샘플링을 할 때 이게 샘플링 원래 이제 여기 보면 487쪽의 그림인데 원래 확률 목표가 이렇게 생긴 거 있죠?
이렇게 생긴 거 하나만 압도적으로 올까 이거 그럼 로보트 갖고 재미없고 유머가 전혀 유머러스하지 않아.

참석자 1 41:02
근데 확률 분포를 그래서 좀 이렇게 온도 여러분 여러분 업됐다는 말하지 사람들이 사람이 업됐다 그러면은 약간 좀 말을 좀 이상한 말도 하고 그러는 거잖아요.
그래서 약간 이렇게 좀 확률 분포를 좀 약간 좀 플랫하게 만드는 거예요.
너무 하나만 높지 않게 페퍼노트가 높다는 게 템포를 높이면 그렇게 만들 수가 있어요.
알겠죠? 아까 샘플링할 때 확률 표를 좀 무작위로 좀 약간 이렇게 좀 플랫하게 만드는 거예요.
그리고 좀 섞어가지고 그리고 이제 무조건 제일 높은 거 하는 게 아니라 낮은 것도 하고 그러면 이제 술 마셨을 때처럼 변해가지고 이제 갑자기 교수님한테 누나 이러고 형이 이러고 막 그러는데 그렇죠 상사 들어온 이 무슨 얘기인지 알겠죠?
여러분 그럼 유머가 있잖아. 그쵸 너무 심하면 안 되지만 그런 게 있다는 거고 그거를 이제 테퍼너츠라는 용어를 써요.
알겠죠? 템퍼 로치 알겠죠? 됐어요 그래요.

참석자 1 42:02
그런 얘기가 있고 그래서 여기 보면은 491쪽에 템포로츠라는 용어를 써요.
템포로츠 가벼운 온도 샘플링이라는 말도 하겠죠.
그러니까 온도가 변하는 거지 이해되죠? 여러분 온도를 좀 수시로 변하면서 유머 감각을 추구하고 이러는 거 있잖아요.
여러분 이게 곤도를 높인다는 거 생각하시면 돼요.
알겠죠? 민트스틸라에도 이모 모드 키고 그러면은 너무 허술해가지고 막 이모 모드 크고 막 이런 것들이 재밌었는데 똑같아 그거 다 이거 아는 사람이야 알겠죠?
원래 우리 쪽 전문 용어라니까 알겠죠. 알파고도 템플러츠가 있었어요.
훈련 시기 때 알파고도 퍼스트지 않게 하려고 그래서 12.2절은 재미없으니까 넘어가고 이들이 재미없고 아까 치피트린도 필요 없고 뭔가 지금 제가 보여주고 싶은 게 스타일 이것도 재미없고 옛날 얘기고 이것도 뭐 그냥 하면 되는 건데 스타일이 오지 지브리의 스타일이 이었지 지브 12.4절이 514쪽에 이거 보여드릴수 있을 거예요.

참석자 1 43:06
여러분 난 처음 시작할 때부터 제가 보여주고 막 그랬잖아요.
그쵸 그쵸? 이게 훈련 데이터가 있는데 여기도 보면은 여기 뭐 이렇게 적혀 있잖아요.
지금 잠재 공간이고 영어로 들어봐 왜요? 레이턴 이거는 외워 외워 레이턴트라는 스트레스라고 그러는데 레이턴트 잠재 공간의 벡터로 뭔가 매핑을 시켜서

참석자 1 43:38
생성을 하잖아요. 이게 이게 뭐겠어 여러분 이게 인코딩이겠지 이게 인코딩이겠지 이게 인코딩이겠지 이게 디코딩이겠지 생성하는 놈은 디코더 뭔가 내부적으로 뭔가 추상화시키는 게 인코더 이거는 이게 이게 다 달라 보이지만 여러분들 이제 깨달으라고 트랜스포머에서 인코로디 클로 썼었죠.
심지어 디코드 혼자 돌 때도 어떻게 됐어요? 세일 포텐션 해가지고 인코딩하고 디코드 했잖아요.
그쵸 챗gpt는 디코더만 있다고 그래서 제가 거기서 거기는 이제 키 밸류가 영어가 아니라 자기 자기 자신이지 뭐 또 키 밸류가 뭐겠어요?
자기가 갖고 있는 그냥 다 데이터들이 있지 여기가 또 있어 이해돼요 감이 오잖아요.
여러분 셀 포텐셜 한번 나고 티밸류 메티 세팅하려는 건 원래 지가 알고 있는 지식이겠지 챗gpt가 공구야 알아 지금 모르면 질문하세요.
시험에 낼 거야 진짜로 모를 수가 없어 안 그래요 그런 거 아는 게 중요하지 않나 여기 보면은 그래서 여기도 vle도 똑같다고 여기 보면 잠재 공간 만들어내잖아요.

참석자 1 44:45
이게 인코딩해서 만들어내고 그쵸 인코딩해서 그쵸 디코딩해서 생성하는 거예요.
그렇죠 여기가 훈련 데이터보다 잠재 공간은 항상 저차원일 수밖에 없죠.
그쵸 저차원이지 훨씬 원래 배웠잖아. 앞에서 5장에서도 저차원이라고 더 낮은 듯한 차원이죠.
그쵸 여러분 그치 이해되죠 그리고 이거를 여러분 확률 목표를 만들려고 노력하잖아요.
맨날 그렇죠 그래서 원래 옛날에는 변이 오토 인코더라는 게 변이형 안 쓰고 그냥 무조건 하려고 그러다가 나중에 보면은 517쪽에 3분 남았는데 이러고 있나 그래가지고 518쪽 가보면

참석자 1 45:32
여기도 인코더 디코더가 있죠. 인코딩 해가지고 레이턴트 레이턴트 스페이스로 그냥 매핑을 시켜보는 거예요.
환경 프로. 그래서 근데 이거를 어떻게 매핑을 했냐면은 확률 분포가 여러분 가운 장류 분포 같은 거 파라미터 뭐만 있으면 돼요 평균이랑 표준 편차만 있으면 되잖아 진짜 그 두 가지만 해요.
재밌죠 그래서 이게 얘는 사실 여러분 흑백 이미지는 x축 y축밖에 없잖아요.
그래서 평균 표준 편차를 2차원으로 해가지고 그냥 2개씩만 뽑아가지고 해서 나중에 생성시키면은 이렇게 교과서 525쪽에 있는 거 이런 걸 만들 수가 있어요.
아호가라 나중에 복습시킨 다음에 흥행 편적 표차를 통해 표적 표차 갖고 일일이 나하고 부산을 변액수를 찍으면은 구가 나오다가 구가 팔로 변하고 이런 거 보이죠.
여러분 그래갖고 여기 보면 재밌잖아요. 구랑 팔 중간에 있는 놈 9가 8로 변하는 거 보이죠.

참석자 1 46:30
여러분 이게 확률 포상으로 숫자가 있으면 여기 절대로 아무것도 안 써 사람들이 그렇죠 여기에서만 주로 쓰다가 여기 약간 넘어가기도 하고 막 그렇죠 이래 되죠.
그래서 진짜로 이게 그래서 vae가 진짜로 훌륭한 게 우리가 세상에 존재하는 온갖 실험 데이터를 생성할 때도 쓸 수 있을 것이고 세상에 존재하는 모든 자연에 존재하는 것들이 없는 것들에 대해서 예측할 수가 있어 화학이나 이런 쪽에서는 뭔가 물리나 이런 쪽에서도 쓰 우리 공학 쪽에서 뭔가 실험 데이터 이런 아니면 샘플 이런 거 할 때 쓸 수도 있고 그리고 마지막에 겐이 있는데 12.5절에 뭐가 똑같아 여기는 근데 재밌는 게 똑같지가 않지 겐을 할 시간이 없구먼 2분밖에 안 남았네.
사실 걔는 되게 옛날에 유명했었는데 지금은 좀 덜 유명한데 개인이 되게 사실적으로 만들 수가 있거든요.
있고 걔는 걔는 또 레이턴트 스페스가 있어요.

참석자 1 47:32
여러분 레이턴트 근데 이거는 아까 인코더가 없는 상태에서 그냥 레스턴트 스페이스를 미리 상정을 하고 훈련을 시키긴 시켜요.
얘도 근데 이게 디코더랑 여기 생성자랑 판별 잘하는 놈이 있어요.
근데 그래서 얘가 훈련을 어떻게 하냐면은 10제 이미지는 10제 이미지가 원래 데이스 샘플이 있을 거 아니에요.
그리고 이 가짜 이미지를 얘가 만들어내서 같이 줘요.
판별자한테 그리고 얘가 진짜인지 가짜인지 판별하게 하고 생성자한테 피드백을 주는데 그래서 얘도 얘를 훈련 어떻게 시키냐면은 얘가 만든 이미지 있죠.
생성자가 만든 이미지가 얘 판별자가 진짜라고 판별을 하면 원래 가짜인데 얘가 에큐슈가 높아지는 걸로 해놨어요.
그래서 둘을 같이 훈련시켜서 판별자는 어쨌든 가짜를 진짜 가짜라는 걸 가짜지 진짜 잘 구분하는 걸로 백프라포이션하고 얘는 판별자가 속는 방향으로 백프라포게이션 해가지고 이미지 생성하게 만들거든요.
그래서 둘 다 서로 경쟁하는 거죠. 나는 정말 진짜 잘 가짜를 잘 분별하겠어.

참석자 1 48:49
얘는 난 진짜를 생성하고 진짜 같은 걸 생성하고 싶어 이런 목적으로 둘이 싸우게 만드는 건데 굉장히 잘 안 돼 훈련이 근데 이게 훈련을 잘하는 방법이 여기 되게 인사이트가 있는데 이것만 약간 할게요.
그러면 이게 이거 훈련하는 거에서 제일 중요한 게 뭐냐면요.
판별자가 너무 정확해 처음부터 그러면 얘가 맨날 나보고 못했다.
그럴 거 아니야 그쵸 잘했다는 소리 들어야 잘할 거 아니에요 그래야 그레이디언트가 생기지 이거 교과서에 되게 잘 돼 있는데 다른 교과서에서는 보통 이거 먼저 훈련시킨 다음에 이거 한다고 돼 있거든요.
교과서에서는 근데 같이 훈련시켜 얘를 근데 그래도 그래도 이걸 먼저 하긴 해.
같이 루프에서 한 번 이거 먼저 훈련하고 이거 먼저 훈련하고 이러는데 당연히 한 글자를 먼저 훈련해야지.
생성자가 돌아가잖아요. 그래서 판별자 먼저 훈련시키는데 맨 처음에 이제 판별자한테는 진짜 완전 쓰레기 이미지 주고 그런데 판별자가 너무 잘하면 생성자가 발전을 못해요.

참석자 1 49:51
왜냐하면 너무 맨날 그레이디언트가 없어 그냥 무조건 다 너 틀렸어 이러니까 그럼 이거 니가 하는 게 좀 괜찮다고 얘기해 줘야지.
얘가 좀 브랜드도 생겨가지고 훈련이 되거든요. 이해돼요.
여러분 그래서 이거 이게 정말 인사이트예요. 왜냐면은 너무 선생님이 잘하면 안 돼.
너무 선생님이 진실만 얘기해줘도 안 돼. 여러분 잘 못하는 잘한다는 생각했잖아.
선생님이거든 얘가 사실은 선생님이 좀 못하는 것도 잘한다고 해주고 이래야지.
그쵸 그래야 내가 발전을 하지 그런 걸 알 수가 있을 거야.
재밌잖아. 이 코드로도 돼 있고 굉장히 재미있어요.
여러분 만약 그리고 만약에 이게 잘 풀리해서 안 되는 건 판별자로 너무 훈자 먼저 말이 잘 시켜서 그래 바보같이 얘가 좀 바보 같아야지 얘가 발전을 해요.
알겠죠 그래 그래야 그래 근데 계속 바보 같으면 또 안 되겠죠.
여러분 알겠죠 실제로 나중에 좋아져야 돼요. 그렇죠.

참석자 1 50:45
그래서 여기서 외워야 될 거는 여러분 판별자와 생성자가 있을 때 한 발자를 먼저 훈련시키는 건 맞는데 생성자는 판별자한테 피드백 받은 걸로만 훈련할 수 있어요.
그쵸 그리고 여러분 여기 중요한 게 절대로 생성자는 실제 이미지를 한 번도 보지 못해요.
철저하게 판별자가 얘기해 주는 정보만 갖고 막 생성을 하는 거야.
지금 그림 보이죠. 여러분 오픈북으로 이건 외워 외워 외웁시다.
여러분 내가 이거 이거 제가 한번 정리해 줄게 다음 시간 아니지 끝났지 이제 이거는 좀 모르면 좀 그렇긴 하잖아요.
그러니까 어쨌든 생성자는 절대 여기 교과서에 나와 있어요.
생성자는 절대로 절대로 실제 이미지를 보지 못해 훈련할 때 알겠죠.
오직 판별자의 피드백 같은 거 해요. 교과서에 다 나와 있어요.
알겠죠 그 정도만

참석자 1 51:45
그리고 다 제가 하고 싶은 얘기 다 했는데 다 한 건 아니고 사실은 요거 요거 요거 막판에 이거 하고 싶었는데 못했어 이거는 여러분 그냥 554쪽 요거 있죠 제가 이거는 일부러 강의 자료 올려줄게요.
이다가 한번 왜냐하면 이것도 좀 모르면 그런 것 같아.
3분인데 미안해요. 여러분 조금만 더 할게요. 한 이 부분만 이게 좀 오르면 좀 그런 것 같아요.
미안해요. 너무 처음에 내가 오늘 2시간짜리인 줄 알고 해가지고 그래도 좀 좋아 이거 모르냐 이게 내가 이렇게까지 한 적은 별로 없는데 어텐션 남미 자리 따로 만들어 가지고 미안해요.
진짜 거기 여기 보면 554쪽에 내가 지금 미안해 또 실수하고 있네.
13조를 열어야 되는데 여기 다 퍼라. 여기 다 약간 복습하다가 마지막에 이거 이거 이거 이거 14점 있죠 요거 요거 요거 요거 이게 중요해요.
점점 더 중요해서 혼합 정밀도 때문에 이게 이번에 좋은 논문 쓰는 친구들이 있었는데 이번에 이 팀 우리 송근락 교수님 학생해 준 게 있어.

참석자 1 53:02
여기 있구나 여기 저기 있다. 혼합 정밀도가 이게 되게 중요하거든요.
혼합 정밀도 그래서 프리시저 그러니까 이게 웨이트가 다 똑같은 웨이트를 쓰는 게 아니라 진짜 근데 요즘 다 그리고 온 디바이스 AI를 제가 열심히 저는 연구하기 때문에 요즘 인베리 시스템 쪽에서 온 디바이스 AI거든요.
여기서도 이제 결국은 메모리가 너무 많이 쓰거나 시간이 너무 많이 걸리면 안 되니까 결국 계산량 줄이는 데 최고가 저번에 얘기했듯이 정량화하는 거 경량화가 컨테이제이션이랑 뭐 있었어요?
여러분 쇼코인지 그쵸 컨텐츠 예전에 뭐가 있었지 쿠무닝 있었죠 퍼센테이션을 저번에는 그냥 무조건 다 일괄적으로 하는 것처럼 얘기했잖아요.
무조건 다 근데 요즘에는 혼합 정맥들을 많이 하거든요.

참석자 1 53:43
훈련 시키는 것조차도 심지어 프리스트는 되게 여러 가지를 한다고 그래서 이거 클로즈드 보 그거 알아야 되는 용어인데 사실 이거 용어 외워 16피트가 해프 3위가 싱글 64 트가 더블 이렇게 부르고요.
플루트 16 32 64 이거 외워라고 외워야 되는 거예요.
제가 이거 너무 중요하니까 제가 강의 자료 한 번 더 올려줄게요.
시험 범위 이번 기말고사 때 외우고 평생 따먹지 말고 알고 있으라는 거야.
알겠죠? 그리고 이거 외워야 될 거 여러분 이 필기라고 적어줄게 거기 하나 강에다 이거 1 2 3이고 이거 1 2의 마이너스 이게 뭐냐면 이게 정체가 뭐냐 제가 적어줄게요.
이거 왜요? 여기까지밖에 해상도가 안 돼 해상도 0.001이죠.
12 마이너스 310 그거보다 더 작은 건 안 되는 거야.
근데 그걸로 충분하다는 거지 여기 다 적혀 있어 교과서에 알겠죠 여러분 적어줄게요.
제가 이거 강의 자료에다가 한번 여기 다 있어 여기 적혀 있어요.

참석자 1 54:45
알겠죠 이거 외우라고 외우라고 그냥 이거 크로스 그게 너 팔 제가 적어줄게요.
한번 그리고 지금 수업 끝나고 알겠죠 외우라고 그리고 한번 혹시 오늘 안 오는 애들이 쪽지 한 번 알려줄게 이거 외우고 시험을 들 거라고 이거는 좀 약간 좀 이건 상식이라 상식적으로 요즘 좀 알아야 되는 것 같아요.
알겠죠 그리고 마지막에 적혀 있는 게 결국은 뭐냐면 가중치 이거 이게 실제로 여러분 우리가 이걸 많이 쓰거든요.
우리가 훈련시킬 때 보통 3시 밑으로 피셀 너무 크니까 그래서 이게 1 2 마이너스 7이잖아요.
그럼 0.0005 6개 한 다음에 7 1이잖아요.
그쵸? 개밖에 안 되니까 가중치 업데이트를 1 2에 마이너스 6으로 많이 할 수밖에 없어요.
이거 실은 너무 작으니까 이거보다 더 작은 건 안 되니까 숫자가 그 교과서 555쪽에 괄호 열고 일반적인 학습률은 1 2 마이너스 3이고 가중치 업데이트 1 2 마이너스 6이라고 이해돼요.
여러분 그거 적어줄게요.

참석자 1 55:38
제가 외우라고 시험에 내고 그때 기말고사 볼 때까지는 알고 있고 기억이 나겠지 그러면 알겠죠.
거기까지 시험 범위에서 외우는 거는 제가 이거 약간 양심상 저도 시험 보기 직전에 한 번 더 얘기해 주고 외우라고 왜냐하면 좀 좀 그렇잖아.
이거 물으면 좀 그렇더라고요. 그러니까 이게 여러분 남단값 이런 거 할 때 저기 단조 밀도 프론트 30이 있으면서 1 2 마이너스 8 이렇게 하면 미친 거야.
그쵸 미쳤지 아무것도 반영이 안 되잖아. 1 2 마이너스 7 7까지밖에 안 돼.
1 2 마이너스 6으로 보통 해줘야 되고 알겠죠 여러분 그 단위가 이게 러닝 레이트를 1 2 마이너스 3으로 해줘야지.
소수점 1 2 마이너스 3 곱하면 1 2 마이너 6이 나오잖아.
더 큰 숫자가 더 작은 숫자라면 사라지는 거 알겠어요 여러분 아니 1 2 마이너스 3을 곱하잖아.
알파를 러닝 레이트 원래 그레디언트랑 그 그 숫자가 1 2 마이너스 7 넘어가면 곤란하잖아요.

참석자 1 56:39
그레디언트가 보통 2 1 마이너스 3 번인이라고 생각하는 거지.
그래야 업데이트가 돼. 만약에 그레디언트가 1 2 마이너스 5여 봐.
둘이 곱하면 1 2 마이너스 8 나오는데 이거 말도 안 되지 그쵸 반영이 안 돼 아무것도 안 하는 거나 마찬가지야 그쵸 그렇다고 그리고 지금 해프 프레시전은 사실 제가 여기 야기 안 하지만 이번 졸업 논문 발표에서 성명욱 교수님 발표하는 거 들어신 분들은 알겠지만 아예 한 레이어에서도 숫자 줄일 수 있는 만큼 막 줄여요.
지금 지금 최신 논문들은 뭐냐면은 그거 그렇게 결국 그렇게 열심히 줄어놨는데 하드웨어에서는 그런 서포트가 안 됐기 때문에 막 불균형이 일어나가지고 지금 골치 아파 또 소프트웨어적으로는 그렇게 잘해놨으면 하드웨어에서 패럴하게 프로세싱 할 수가 없어가지고 지금 그것도 한참 연구하고 있어요.
지금 한창 연구해요.

참석자 1 57:26
지금 제일 연구하는 건 하드웨어 반도체 쪽에서 어떻게 그걸 또 서포트할 거냐 코라 종밀도로 나중에 이제 막 모델을 바꿔버리면 어떻게 해요?
이거 한 한 해만 일이 몰리고 어떤 애는 놀아봐 계속 어쨌든 중요한 거고 요즘에 이 정도는 그래서 여러분 알고 있습니다.
알겠죠 제가 그리고 이거는 올려줄게 다시 오늘 여기까지 하고 시험은 제가 지난번처럼 월요일 날 또 나타나 있을 텐데 여기 와서 공부하든지 안 하든지 상관없어요.
알겠죠? 출석 체크도 해줄 거고 지원 사업 하시고 그리고 메일 보냈네요.
여러분 그다음에 요.


clovanote.naver.com